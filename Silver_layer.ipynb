{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498c7a00-dc3a-4868-b460-6bd72641e47f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initializing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bebc545a-b02d-48ff-9ebd-1feef3d18c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"Silver_JobETL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "117a3b5b-5627-4444-b662-70c2d86ccf89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading Data from Bronze Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14720b6-81dc-4145-a63e-e990e86d9e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>apply_options</th><th>company_name</th><th>description</th><th>detected_extensions</th><th>extensions</th><th>job_id</th><th>location</th><th>search_role</th><th>share_link</th><th>thumbnail</th><th>title</th><th>via</th></tr></thead><tbody><tr><td>List(Map(title -> AstraZeneca Careers, link -> https://careers.astrazeneca.com/job/chennai/lead-consultant-technical-lead-fullstack-data-engineer/7684/83185188576?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://astrazeneca.wd3.myworkdayjobs.com/en-US/broadbean_external/job/Lead-Consultant---Technical-Lead---Fullstack-Data-Engineer_R-230153-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/jdp/8526236387968584391?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/lead-consultant-technical-lead-fullstack-data-engineer-at-astrazeneca-4258753290?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Iitjobs, link -> https://www.iitjobs.com/job/technical-lead-data-chennai-tamil-nadu-india-preludesys-1900?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/e2f20069a8e55a3ce4c21ba26ac31b52?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/lead-consultant-technical-lead-fullstack-data-engineer-astrazeneca-JV_IC2833209_KO0,54_KE55,66.htm?jl=1009795683395&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> The BWAM Job Board - Black Women In Asset Management, link -> https://jobs.bwam.network/companies/citi-2-db90f7f6-7bfb-4128-83f9-bb534f36fcb7/jobs/49956080-big-data-program-lead-svp-chennai?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>AstraZeneca</td><td>Job Title: Lead Consultant - Technical Lead - Data, Analytics & AI\n",
       "Career Level: E\n",
       "\n",
       "Introduction to role: Are you ready to redefine an industry and change lives? Join our Operations IT organization, where your work will have a direct impact on patients by redefining our ability to develop life-changing medicines. We empower the business to perform at its peak, combining innovative science with leading digital technology platforms and data. As part of the Data Analytics and AI (DA&AI) group, you'll deliver innovative solutions that drive the transformation of medicine development. As the Technical Lead, you'll oversee the technical delivery of products and projects aligned with business objectives, tapping into your expertise in Data and Cloud Engineering, Software Engineering, AI, and more. Collaborate with stakeholders to ensure seamless delivery and maximum business impact!\n",
       "\n",
       "Accountabilities:\n",
       "• Bridge business needs with technical solutions by leading IT application design and implementation.\n",
       "• Collaborate with stakeholders to define and deliver requirements, translating them into detailed specifications.\n",
       "• Own the technical vision of products and compile detailed technical designs, refining user Epics/stories.\n",
       "• Optimize performance, mitigate risks, and ensure alignment to timelines and resource allocations.\n",
       "• Advise on industry trends and standard methodologies to enhance performance and business outcomes.\n",
       "• Provide technical direction and guidance to IT teams and business units.\n",
       "• Contribute to Data & Software Engineering standards and best practices.\n",
       "• Research new technologies to boost system performance and scalability.\n",
       "• Lead and mentor technical teams (Data Engineering, Cloud Engineering, Software Engineering) and work with AI/GenAI leads to foster collaboration and innovation.\n",
       "• Ensure platform stability, scalability, and simplicity while adhering to regulatory requirements and data security standards.\n",
       "• Foster continuous improvement and innovation.\n",
       "• Supervise technical leadership throughout the software development lifecycle and provide hands-on support in development, technical delivery, review, and testing while collaborating with global teams.\n",
       "• Apply central Enterprise Data Platforms and guide DevOps, DataOps, and MLOps teams to ensure standard methodologies.\n",
       "• Ensure data solutions align with FAIR principles and support end-to-end data science and machine learning.\n",
       "\n",
       "Essential Skills/Experience:\n",
       "• Minimum 10 years of experience in the design, development, and delivery of software and data engineering solutions.\n",
       "• Extensive technical expertise in Data Engineering, Software Engineering, and Cloud Engineering.\n",
       "• Strong foundational knowledge of AI Engineering principles and practices.\n",
       "• Deep understanding of DevOps, MLOps, and DataOps methodologies with practical implementation experience.\n",
       "• Demonstrated success in product development and/or product management, delivering complex solutions end-to-end.\n",
       "• Demonstrable ability to provide technical thought leadership across Data, Analytics, and AI domains.\n",
       "• Exceptional communication, customer management, and multi-functional collaboration skills.\n",
       "• Robust analytical and problem-solving abilities with a collaborative, team-oriented approach.\n",
       "• Hands-on experience driving innovation throughout the full product development lifecycle.\n",
       "• Solid understanding of Data Mesh and Data Product concepts and architectures.\n",
       "• Proficiency in Agile methodologies and facilitating iterative, multi-functional team delivery.\n",
       "• Hands-on experience designing, implementing, and optimizing data pipelines using leading ETL tools.\n",
       "• Skilled in architecting, deploying, and managing scalable, secure AWS cloud environments.\n",
       "• Proficient in workflow orchestration tools such as Apache Airflow.\n",
       "• Practical experience implementing DataOps practices with tools like DataOps.Live.\n",
       "• Strong expertise in data storage and analytics platforms such as Snowflake.\n",
       "• Ability to deliver actionable insights through business intelligence tools, including Power BI.\n",
       "• Extensive full-stack development experience, including backend proficiency with Node.js and Python and frontend expertise with ReactJS or NextJS.\n",
       "• Experience designing and deploying Generative AI solutions.\n",
       "• Hands-on implementation of AI/ML models using platforms such as Amazon SageMaker.\n",
       "• Advanced programming skills, especially in Python.\n",
       "• Solid knowledge of both SQL and NoSQL database technologies.\n",
       "• Familiarity with agile ways of working and iterative development environments.\n",
       "• Experience working in large, multinational organizations or pharmaceutical environments is highly desirable.\n",
       "• Demonstrated leadership and mentoring skills, with a demonstrable ability to develop high-performing technical teams.\n",
       "\n",
       "Desirable Skills/Experience:\n",
       "• Bachelor's or master's degree in health sciences, Life Sciences, Data Management, IT, or a related field.\n",
       "• Experience in the pharmaceutical industry or a similar multinational environment.\n",
       "• AWS Cloud or relevant data/software engineering certifications.\n",
       "• Awareness of use case specific GenAI tools available in the market and their application in day-to-day work scenarios.\n",
       "• Possess working knowledge of basic prompting techniques and continuously improve these skills.\n",
       "• Stay up to date with developments in AI and GenAI, applying new insights to work-related situations.\n",
       "\n",
       "When we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That's why we work, on average, a minimum of three days per week from the office. But that doesn't mean we're not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\n",
       "\n",
       "At AstraZeneca, we are at a crucial stage of our journey to become a digital and data-led enterprise. Our commitment to innovation empowers us to make the impossible possible by building partnerships and ecosystems that drive scale and speed for exponential growth. With investment backing us all the way, we are focused on disrupting the industry while making a meaningful impact through our work. Here you'll find countless opportunities to learn, grow, and contribute to developing life-changing medicines.\n",
       "\n",
       "Ready to make a difference? Apply now to join our team!\n",
       "\n",
       "Date Posted\n",
       "30-Jun-2025\n",
       "\n",
       "Closing Date\n",
       "\n",
       "AstraZeneca embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills. We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements.</td><td>Map(posted_at -> 3 days ago, schedule_type -> Full-time)</td><td>List(3 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIENvbnN1bHRhbnQgLSBUZWNobmljYWwgTGVhZCAtIEZ1bGxzdGFjayBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiQXN0cmFaZW5lY2EiLCJhZGRyZXNzX2NpdHkiOiJDaGVubmFpLCBUYW1pbCBOYWR1LCBJbmRpYSIsImh0aWRvY2lkIjoiRGVXa2pFZDgxNXcwSUpJY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Chennai, Tamil Nadu, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=DeWkjEd815w0IJIcAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3LuwrCQBBAUWz9BKtplTxEsNFKfKGIIKSyCZPNkF1dZ0JmAv6bP2e0uc3hjj-j8e1CWMNWWPtoyAYpFOQ8B4cR_pbCoY9RDd0TdmgIe24CE3WDnKUCJeycB2E4ijSRJmtv1uoqz1Vj1gyjBZc5eeXCVMk7f0ilv5TqsaM2olG5WM7fWcvNbLpR6_BOTA4hMGw9MWNIoMBXiHDFuk_gxHXALxpOsRa-AAAA&shmds=v1_AdeF8KgBYGRmUf76k78qAEmFyfl_BrGpLC_L6zaXvWx_bHB8DQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=DeWkjEd815w0IJIcAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f200a1978d9d4338da/images/56fc5e069c9f2d48c59a9b83fcb6628a5563c2c8e003cb81494faae7f495eb69.png</td><td>Lead Consultant - Technical Lead - Fullstack Data Engineer</td><td>AstraZeneca Careers</td></tr><tr><td>List(Map(title -> Cognizant Careers, link -> https://careers.cognizant.com/us-en/jobs/00063792912/aws-data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> NTT DATA Careers, link -> https://careers.services.global.ntt/global/en/job/15cb4c0c4158ad0/AWS-Redshift-administrator-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> NTT DATA Careers, link -> https://careers-inc.nttdata.com/job/Bangalore-AWS-Redshift-administrator-Engineer-KA/1290126800/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Foundit, link -> https://www.foundit.in/job/aws-data-engineer-mount-talent-consulting-bengaluru-bangalore-34974804?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/aws-data-engineer/capgemini-technology-services-india-limited/17091071?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SmartRecruiters Job Search, link -> https://jobs.smartrecruiters.com/Datazymes/744000051989725-aws-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/aws-data-engineer-usefulbi-JV_IC2940587_KO0,17_KE18,26.htm?jl=1009205219719&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Cutshort, link -> https://cutshort.io/job/Data-Engineer-AWS-Bengaluru-Bangalore-Kloud9-Technologies-MxGnCgGy?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Cognizant</td><td>Job Summary:\n",
       "\n",
       "Experience : 4 - 8 years\n",
       "\n",
       "Location : Bangalore\n",
       "\n",
       "The Data Engineer will contribute to building state-of-the-art data Lakehouse platforms in AWS, leveraging Python and Spark. You will be part of a dynamic team, building innovative and scalable data solutions in a supportive and hybrid work environment. You will design, implement, and optimize workflows using Python and Spark, contributing to our robust data Lakehouse architecture on AWS. Success in this role requires previous experience of building data products using AWS services, familiarity with Python and Spark, problem-solving skills, and the ability to collaborate effectively within an agile team.\n",
       "\n",
       "Must Have Tech Skills:\n",
       "\n",
       "· Demonstrable previous experience as a data engineer.\n",
       "• Technical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\n",
       "\n",
       "· Proficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n",
       "\n",
       "Nice To Have Tech Skills:\n",
       "\n",
       "· Familiar with data services in a Lakehouse architecture.\n",
       "\n",
       "· Familiar with technical design practices, allowing for the creation of scalable, reliable data products that meet both technical and business requirements\n",
       "\n",
       "· A master’s degree or relevant certifications (e.g., AWS Certified Solutions Architect, Certified Data Analytics) is advantageous\n",
       "\n",
       "Key Accountabilities:\n",
       "• Writes high quality code, ensuring solutions meet business requirements and technical standards.\n",
       "• Works with architects, Product Owners, and Development leads to decompose solutions into Epics, assisting the design and planning of these components.\n",
       "• Creates clear, comprehensive technical documentation that supports knowledge sharing and compliance. Experience in decomposing solutions into components (Epics, stories) to streamline development.\n",
       "• Actively contributes to technical discussions, supporting a culture of continuous learning and innovation.\n",
       "\n",
       "Key Skills:\n",
       "• Proficient in Python and familiar with a variety of development technologies.\n",
       "• Previous experience of implementing data pipelines, including use of ETL tools to streamline data ingestion, transformation, and loading.\n",
       "• Solid understanding of AWS services and cloud solutions, particularly as they pertain to data engineering practices. Familiar with AWS solutions including IAM, Step Functions, Glue, Lambda, RDS, SQS, API Gateway, Athena.\n",
       "• Proficient in quality assurance practices, including code reviews, automated testing, and best practices for data validation.\n",
       "• Experienced in Agile development, including sprint planning, reviews, and retrospectives\n",
       "\n",
       "Educational Background:\n",
       "• Bachelor’s degree in computer science, Software Engineering, or related essential.\n",
       "\n",
       "Bonus Skills:\n",
       "• Financial Services expertise preferred, working with Equity and Fixed Income asset classes and a working knowledge of Indices.\n",
       "• Familiar with implementing and optimizing CI/CD pipelines. Understands the processes that enable rapid, reliable releases, minimizing manual effort and supporting agile development cycles.</td><td>Map(posted_at -> 6 days ago, schedule_type -> Full-time)</td><td>List(6 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJBV1MgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkNvZ25pemFudCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiWjYyVVNZZkhtUHk0dHdwNkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Hyderabad, Telangana, India (+1 other)</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Z62USYfHmPy4twp6AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCQAwAYFz7CE4Zi9SeCC46FRV_VgXHkmvD9eRMyiVD9S18Y_EbvuI7K8rmcYMDGsKRQ2SiDEu4igclzN0AwnASCYnmu8Fs1K1zqqkOamixqzt5OWHyMrmneP3X6oCZxoRG7XqzmuqRw6LcS-D4QTaIDOd3Txk99hXcKSEHZKzgwn3EH5RzejuUAAAA&shmds=v1_AdeF8KgvQshfbWQnT9KEkSQ9vQxlf8gOj45PNHhqexzNlz8RDw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Z62USYfHmPy4twp6AAAAAA%3D%3D</td><td>null</td><td>AWS Data Engineer</td><td>Cognizant Careers</td></tr><tr><td>List(Map(title -> Verizon Careers, link -> https://mycareer.verizon.com/jobs/r-1074436/engineer-iii-consultant-data-engineering/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/engineer-iii-specialist-ai-ml-engineering-verizon-JV_IC2940587_KO0,41_KE42,49.htm?jl=1009592483182&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/jdp/6894517318855681925?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/engineer-iii-specialist-ai-ml-engineering-at-verizon-4235356641?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Flexa Careers, link -> https://flexa.careers/jobs/verizon-engineer-iii-consultant-data-engineering-682c1bc407733afef4fc1f03?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/OU3ePsc3DL68Xm2pEJ66r4zCD12Qw89bAnDUvRVQAnOPV9aRh_ZBIw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Verizon</td><td>When you join Verizon\n",
       "\n",
       "You want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n",
       "\n",
       "What you’ll be doing…\n",
       "\n",
       "We are looking for data engineers who can work with world class team members to help drive telecom business to its full potential. We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n",
       "\n",
       "As a data engineer you will be collaborating with business product owners, coaches, industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n",
       "\n",
       "As a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n",
       "• Understanding the business requirements and converting them to technical design.\n",
       "• Working on Data Ingestion, Preparation and Transformation.\n",
       "• Developing data streaming applications.\n",
       "• Debugging the production failures and identifying the solution.\n",
       "• Working on ETL/ELT development.\n",
       "• Understanding devops process and contributing for devops pipelines\n",
       "\n",
       "What we’re looking for...\n",
       "\n",
       "You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n",
       "\n",
       "You’ll need to have…\n",
       "• Bachelor’s degree or four or more years of work experience.\n",
       "• Four or more years of relevant work experience.\n",
       "• Experience with Data Warehouse concepts and Data Management life cycle.\n",
       "• Experience in Big Data technologies - GCP/Hadoop/Spark/Composer/DataFlow/Bigquery.\n",
       "• Experience in complex SQL.\n",
       "• Experience working on Streaming ETL pipelines\n",
       "• Expertise in Java\n",
       "• Experience with MemoryStore / Redis / Spanner\n",
       "• Experience in troubleshooting the data issues.\n",
       "• Experience with data pipeline and workflow management & Governance tools.\n",
       "• Knowledge of Information Systems and their applications to data management processes.\n",
       "\n",
       "Even better if you have one or more of the following…\n",
       "• Three or more years of relevant experience.\n",
       "• Any relevant Certification on ETL/ELT developer.\n",
       "• Certification in GCP-Data Engineer.\n",
       "• Accuracy and attention to detail.\n",
       "• Good problem solving, analytical, and research capabilities.\n",
       "• Good verbal and written communication.\n",
       "• Experience presenting to and influence stakeholders.\n",
       "• Experience in driving a small team of 2 or more members for technical delivery\n",
       "\n",
       "#AI&D\n",
       "\n",
       "Where you’ll be working\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "Scheduled Weekly Hours\n",
       "40\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Verizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJFbmdpbmVlciBJSUkgQ29uc3VsdGFudC1EYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiVmVyaXpvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEhtc2E1cThWc3RlSGhKaUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Hyderabad, Telangana, India (+2 others)</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=xHmsa5q8VsteHhJiAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zXMsQrCMBCAYVz7CE43OYhtRHDRwUFF4yyu5dIeaSTeleSE6gP5nNbB5V9--IrPpNgd2QcmSmCthb1wfkZF1vKAivCfgT2UcBEHmTA1HQjDScRHmm471T5vjMk5Vj4ramiqRh5GmJwM5i4u_1LnDhP1EZXq1Xo5VD37-ew22u8RCwznV0sJHbYLuFJE9si4AMttwC8rSRd-qQAAAA&shmds=v1_AdeF8KiZ7TXW6uRH9Apq6vbBfICpbvSKG8yQ27xEOPJ9xgguaA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=xHmsa5q8VsteHhJiAAAAAA%3D%3D</td><td>null</td><td>Engineer III Consultant-Data Engineering</td><td>Verizon Careers</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/data-engineer-python-pyspark-and-azure-databricks-4-6-years-at-emids-4259163468?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Instahyre, link -> https://www.instahyre.com/job-367026-azure-data-enginner-f2f-drive-at-hcl-technologies-bangalore/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Dr. Job Pro, link -> https://www.drjobpro.com/job/view/MC5MAW4C083H9Q4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Sercanto, link -> https://www.sercanto.in/detail/a/data-engineering-azure-data-engineering-databricks-powershell_bengaluru_35748259?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/7954895824423485440?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Emids</td><td>Hi All,\n",
       "\n",
       "Greetings for the day!!\n",
       "\n",
       "We are currently hiring for Data Engineer (Python, Pyspark, and Azure Databricks) for Emids(MNC) at Bangalore location.\n",
       "\n",
       "Role: Data Engineer\n",
       "\n",
       "Exp: 5 to 8 Years\n",
       "\n",
       "Location: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\n",
       "\n",
       "NP: Immediate to 15 Days (Try to find only immediate joiners)\n",
       "\n",
       "Note: Candidate Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.\n",
       "\n",
       "Role Overview:\n",
       "\n",
       "We are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Lead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\n",
       "• Architect scalable data streaming and processing solutions to support healthcare data workflows.\n",
       "• Develop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\n",
       "• Ensure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\n",
       "• Collaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\n",
       "• Troubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\n",
       "• Mentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\n",
       "• Stay updated with the latest cloud technologies, big data frameworks, and industry trends.\n",
       "\n",
       "Required Skills & Qualifications:\n",
       "• 4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\n",
       "• Expertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\n",
       "• Experience with Azure Databricks (or willingness to learn and adopt it quickly).\n",
       "• Hands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\n",
       "• Proficiency in SQL, NoSQL databases, and data modeling for big data processing.\n",
       "• Knowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\n",
       "• Experience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\n",
       "• Strong analytical skills, problem-solving mindset, and ability to lead complex data projects.\n",
       "• Excellent communication and stakeholder management skills.\n",
       "\n",
       "Note: This is not a contract position, this will be a permanent position with Emids.\n",
       "\n",
       "Interested candidates Can Share Your Updated Profile with details for below Email.\n",
       "\n",
       "NAME:\n",
       "\n",
       "CCTC:\n",
       "\n",
       "ECTC:\n",
       "\n",
       "Notice Period:\n",
       "\n",
       "Offers in Hand :\n",
       "\n",
       "Email ID: Ravi.chekka@emids.com</td><td>Map(posted_at -> 8 hours ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(8 hours ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChQeXRob24sIFB5c3BhcmssIGFuZCBBenVyZSBEYXRhYnJpY2tzKSB8fCAoNCAtIDYgWWVhcnMpIiwiY29tcGFueV9uYW1lIjoiRW1pZHMiLCJhZGRyZXNzX2NpdHkiOiJCZW5nYWx1cnUsIEthcm5hdGFrYSwgSW5kaWEiLCJodGlkb2NpZCI6Im9TMEEyREhLeU9wd0lVdnVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Bengaluru, Karnataka, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=oS0A2DHKyOpwIUvuAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOu27CQBBFRcsnUN0iBYn8iKKQAiqioIikoU0RoVl7tF5sz1g7awkQX5cvi2mudI5Oced_s_nvByXCTnwQ5ojl4ZIalQyHiw0U2wwkNbbXMTLupYuhau0RtxuWr8jxhh-mOIkcX-pgE1QNVPCp6jtebJqUBluXpVlXeEuUQlVU2pcq7PRcntTZfY7WUOSho8THl9XzuRjEPz3s-lAbguCdxVM3xjHDN0WZjrSUYS91oH_1b8lDxQAAAA&shmds=v1_AdeF8KhLqsOGlWBbjd2BCVC5tFwl-_UblBRFb7FpYzcHeQH24g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=oS0A2DHKyOpwIUvuAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f200a1978d9d4338da/images/56fc5e069c9f2d48e9cafb9aee8ab0386edd45db1d4eb8a4eb227fed4079aa51.jpeg</td><td>Data Engineer (Python, Pyspark, and Azure Databricks) || (4 - 6 Years)</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Mastercard Careers, link -> https://careers.mastercard.com/us/en/job/R-245980/Senior-Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Avalara Careers, link -> https://careers.avalara.com/careers-home/jobs/15360?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Citi Careers, link -> https://jobs.citi.com/job/pune/senior-data-engineer-avp/287/81288712640?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Barclays Careers, link -> https://search.jobs.barclays/job/pune/senior-data-engineer/13015/82501694448?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Iitjobs, link -> https://www.iitjobs.com/job/senior-data-engineer-at-pune-pune-maharashtra-india-iitjobs-inc-92196?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Lever, link -> https://jobs.lever.co/5xdata/d6d32a93-a965-4c91-811f-4610446c683f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Foundit, link -> https://www.foundit.in/job/senior-data-engineer-foundit-bengaluru-bangalore-35050149?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/senior-data-engineer-mastercard-JV_IC2856202_KO0,20_KE21,31.htm?jl=1009711720715&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Mastercard</td><td>Job Title:\n",
       "\n",
       "Senior Data Engineer\n",
       "\n",
       "Overview:\n",
       "\n",
       "Position Overview:\n",
       "\n",
       "The Senior Data Engineer, MyMPA will be part of GBSC’s Automation & Engineering Team, responsible for implementing enhancements and periodic refreshes of an enterprise-wide data platform.\n",
       "\n",
       "This role will also work closely with the VP of Analytics & Metrics and Director of FP&A and gather requirements for changes and enhancements to the application and contribute to the technology platform's evolution as it grows to support the rapidly expanding Mastercard business.\n",
       "\n",
       "The ideal candidate will have hands-on development skills combined with an ability to analyze and understand end user requirements that are critical success factors within this role. This role requires the skills and desire to work as an individual contributor as well as, collaborate cross functionally with various business constituents.\n",
       "\n",
       "1. Have you ever worked on an enterprise-wide reporting solution that relied heavily on your own knowledge and abilities to build and maintain the solution?\n",
       "2. Are you constantly hungry to learn? Do you have the “growth mindset” as opposed to the “fixed mindset”?\n",
       "3. Do you love working with people, helping them, and turning their requirements into something that can make a difference?\n",
       "\n",
       "Role:\n",
       "\n",
       "• Skilled at writing performant and reliable SQL queries to source data from Centralized data repositories such as a Data Warehouse.\n",
       "• Proficient at designing and developing Relational and Multi-dimensional Databases to host the sourced data and work with Tableau and Power BI developers within and outside the team to support reporting and dashboarding needs of the organization.\n",
       "• Develop ETL workflows and macros in Alteryx to source data for projects and build high-level and detailed data validations to ensure accuracy of the sourced data\n",
       "• Contribute to initiatives aimed at automating the data extraction, application of data quality checks to ensure datasets are released on time and with 100% accuracy.\n",
       "• Develop technical components to ensure department’s compliance with audit requirements such as SOX and other statutory/audit requirements as applicable from time to time.\n",
       "• Organize and lead discussions with customers to brainstorm on data quality issues and contribute to discussions to devise business rules to address data quality issues\n",
       "• Use MS-Excel and MS-PPT to capture findings and present to customers in an easy-to-understand and impactful manner\n",
       "• Liaison with the internal groups in MasterCard Operations and Technology to ensure our solutions remain in compliance with MasterCard technical standards. Navigate O&T requirements around change management and new development.\n",
       "\n",
       "All About You:\n",
       "\n",
       "• Strong understanding of Windows and Linux server.\n",
       "• Good understanding of SQL Server or Oracle DB.\n",
       "• Solid understanding of Essbase technology – understand how this technology works, for both BSO\n",
       "and ASO cubes.\n",
       "• Develop BSO and ASO cubes with a strong eye for performance.\n",
       "• Strong commitment to quality and error testing code you develop. Strong ability to step in and analyze the code of others on the team.\n",
       "• Be able to work within an Agile environment that is highly responsive to the business. Our team is part of the Finance organization – you must be comfortable with working as part of the business with a strong “roll up your sleeves” mentality.</td><td>Map(schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6Ik1hc3RlcmNhcmQiLCJhZGRyZXNzX2NpdHkiOiJQdW5lLCBNYWhhcmFzaHRyYSwgSW5kaWEiLCJodGlkb2NpZCI6IlV1SE5zREk1eXZ2ckFfdVBBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Pune, Maharashtra, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=UuHNsDI5yvvrA_uPAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCQAwAUFz7CU6ZHLT2RHDRVRGFguAHlPQa7k5qUpIIfodfrC5vetVnVq3uxEUUjugIJ06FiRTWcJUejFBjBmE4i6SR5ofsPtk-BLOxSeboJTZRnkGYenmHh_T2p7OMStOITt12t3k3E6flokVz0og6QGG4vZhqaPE30bIr1nDhoeAXlqvX1JUAAAA&shmds=v1_AdeF8Kjilf5bxnv3Bc7iM6HGmbYmIZhgjXWK3XVsMCSyUs2b_A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=UuHNsDI5yvvrA_uPAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f200a1978d9d4338da/images/56fc5e069c9f2d48097f40d761b5eca5cc8e962f017d44ef1d011b40189d9eb4.png</td><td>Senior Data Engineer</td><td>Mastercard Careers</td></tr><tr><td>List(Map(title -> PepsiCo Careers, link -> https://www.pepsicojobs.com/main/jobs/365762?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> GE Vernova Careers, link -> https://careers.gevernova.com/global/en/job/GVXGVWGLOBALR5003613EXTERNALENGLOBAL/Sr-Staff-Software-Engineer-Architect?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/sr-staff-software-engineer-architect-ge-vernova-JV_IC2865319_KO0,36_KE37,47.htm?jl=1009653004807&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/architect-data-engineer/pepsico/17195234?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> The Muse, link -> https://www.themuse.com/jobs/gevernova/srstaff-software-engineer-architect?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Hirist, link -> https://www.hirist.tech/j/amgen-information-system-engineer-0-7-yrs-1463997?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://gevernova.wd5.myworkdayjobs.com/it-IT/Vernova_ExternalSite/job/Hyderabad/SrStaff-Software-Engineer---Product-Architect_R5003613/apply/applyManually?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/sr-staff-software-engineer-architect-at-ge-vernova-4169907901?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>PepsiCo</td><td>Overview\n",
       "\n",
       "Provide the job title you would like to be displayed on the job posting:\n",
       "\n",
       "Data Platform Engineer – Transformation & Modernization\n",
       "\n",
       "Job Overview: A Data Platform Engineer to be a key player in our transformation and modernization programs, leading the migration of applications from legacy systems to Azure-based architectures. This role involves designing, implementing, and optimizing scalable, cloud-native data solutions using Databricks, Azure DevOps (ADO), and Agile development methodologies.\n",
       "\n",
       "As an active contributor to code development, you will help drive automation, operational excellence, and data quality across our platforms. You will collaborate with data science and product teams to create solutions that enhance our data-driven decision-making capabilities.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Responsibilities: • Lead the migration and modernization of data platforms, moving applications and pipelines to Azure-based solutions.\n",
       "• Actively contribute to code development in projects and services.\n",
       "• Manage and scale data pipelines from internal and external data sources to support new product launches and ensure high data quality.\n",
       "• Develop automation and monitoring frameworks to capture key metrics and operational KPIs for pipeline performance.\n",
       "• Implement best practices around systems integration, security, performance, and data management.\n",
       "• Collaborate with internal teams, including data science and product teams, to drive solutioning and proof-of-concept (PoC) discussions.\n",
       "• Develop and optimize procedures to transition data into production.\n",
       "• Define and manage SLAs for data products and operational processes.\n",
       "• Prototype and build scalable solutions for data engineering and analytics.\n",
       "• Research and apply state-of-the-art methodologies in data and Platform engineering.\n",
       "• Create and maintain technical documentation for knowledge sharing.\n",
       "• Develop reusable packages and libraries to enhance development efficiency.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Qualifications: • Bachelor’s degree in Computer Science, MIS, Business Management, or related field\n",
       "• 10 + years’ experience in Information Technology\n",
       "• 4 + years of Azure, AWS and Cloud technologies\n",
       "• Experience in data platform engineering, with a focus on cloud transformation and modernization.\n",
       "• Strong knowledge of Azure services, including Databricks, Azure Data Factory, Synapse Analytics, and Azure DevOps (ADO).\n",
       "• Proficiency in SQL, Python, and Spark for data engineering tasks.\n",
       "• Hands-on experience building and scaling data pipelines in cloud environments.\n",
       "• Experience with CI/CD pipeline management in Azure DevOps (ADO).\n",
       "• Understanding of data governance, security, and compliance best practices.\n",
       "• Experience working in an Agile development environment.\n",
       "• Prior experience in migrating applications from legacy platforms to the cloud.\n",
       "• Knowledge of Terraform or Infrastructure-as-Code (IaC) for cloud resource management.\n",
       "• Familiarity with Kafka, Event Hubs, or other real-time data streaming solutions.\n",
       "• Experience with lagacy RDBMS (Oracl, DB2, Teradata)\n",
       "• Background in supporting data science models in production.\n",
       "\n",
       "Does the person hired for this job need to be based in a PepsiCo office, or can they be remote?: Employee must be based in a Pepsico office\n",
       "\n",
       "Primary Work Location: Hyderabad HUB-IND\n",
       "\n",
       "Is this role approved for relocation?: No\n",
       "\n",
       "Would you like to initially post this job internally-only or both internally and externally?: Post both internally and externally</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJBcmNoaXRlY3QgLSBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiUGVwc2lDbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNzVXaWFLWW1Xa1V2Vk5VY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Hyderabad, Telangana, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=75WiaKYmWkUvVNUcAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKPQrCQBBAYWxzBKupLCRmg2Cjlaj4U1nYh9nNsFlZZ8LOFPEkXldtHnzwqs-savclDMkoGKzgiIZw4piYqPx8Ew9K-DtAGM4iMdN8N5iNunVONTdRDS2FJsjLCZOXyT3F6z-dDlhozGjUrTft1Iwcl4s7jZoOAonh8u6poMe-hgdl5IiMNVy5T_gFzhsgGJoAAAA&shmds=v1_AdeF8KgYr74ZFWV4KJEaWZHMkZ0LC87cwjr0f-JrKRTAYyHS8g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=75WiaKYmWkUvVNUcAAAAAA%3D%3D</td><td>null</td><td>Architect - Data Engineer</td><td>PepsiCo Careers</td></tr><tr><td>List(Map(title -> Okta, link -> https://www.okta.com/company/careers/business-technology/senior-analytics-data-engineer-6716722/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Wellfound, link -> https://wellfound.com/jobs/3267194-senior-data-analytics-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/senior-data-analytics-engineer/4679310?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/senior-data-analytics-engineer-ethos-life-JV_IC2940587_KO0,30_KE31,41.htm?jl=1009705371850&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://prudential.wd3.myworkdayjobs.com/ko-KR/prudential/job/Senior-Data-Analytics-Engineer_23120122?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/vIJWmyry5Lf7NASB70kqBc-8xTYRfcbx8ePcNVXpKj9vYXmwX_LxVQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Antal Tech Jobs, link -> https://www.antaltechjobs.in/job/senior-data-analytics-engineer-8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/senior-analytics-data-engineer-at-okta-4231155979?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Okta, Inc.</td><td>Get to know OktaOkta is The World’s Identity Company. We free everyone to safely use any technology, anywhere, on any device or app. Our flexible and neutral products, Okta Platform and Auth0 Platform, provide secure access, authentication, and automation, placing identity at the core of business security and growth.At Okta, we celebrate a variety of perspectives and experiences. We are not looking for someone who checks every single box - we’re looking for lifelong learners and people who can make us better with their unique experiences. Join our team! We’re building a world where Identity belongs to you.Senior Analytics Engineer\n",
       "We are looking for an experienced Analytics Engineer to join Okta’s enterprise data team. This analyst will have strong background in SaaS subscription and product analytics, a passion for providing customer usage insights to internal stakeholders, and experience organizing complex data into consumable data assets. In this.</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHl0aWNzIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJPa3RhLCBJbmMuIiwiYWRkcmVzc19jaXR5IjoiQmVuZ2FsdXJ1LCBLYXJuYXRha2EsIEluZGlhIiwiaHRpZG9jaWQiOiJhZHlINU5lRGFEQWwyUW1aQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Bengaluru, Karnataka, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=adyH5NeDaDAl2QmZAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2NMQrCQBAAsc0TrLZV4p0IabRSFFELCx8gm3O5nDl3w-0F4mv8qtFmmmGY4jMpqhtxkARbxvjOwSnsMSMc2AcmSrCAs9SghMk1IAxHER9pumly7nRtrWo0XjOOqXHyssJUy2CfUusPd20wURcx031VLQfTsZ_Prm3GEk7sDASGHbHH2Ke-hAsmHvft3z4CfgG5SMGUogAAAA&shmds=v1_AdeF8KhyLWW_m3EB7N8ESd9cJTImIS9eRNMNaNNrFoYMaBtpYA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=adyH5NeDaDAl2QmZAAAAAA%3D%3D</td><td>null</td><td>Senior Analytics Data Engineer</td><td>Okta</td></tr><tr><td>List(Map(title -> Cencora Careers, link -> https://careers.cencora.com/ca/fr/job/R2424901/Lead-Data-Engineer-Data-Engineering?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://myhrabc.wd5.myworkdayjobs.com/ro-RO/Global/job/Lead-Data-Engineer---Data-Engineering_R2424901?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/jdp/-4848617790076881756?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Apna, link -> https://apna.co/job/bengaluru/lead-software-engineer-java-dev-and-data-engineeri-1128030568?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Bayt.com, link -> https://www.bayt.com/en/india/jobs/lead-software-engineer-bi-data-platform-lead-72980898/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Expertini, link -> https://in.expertini.com/jobs/job/cgi-lead-software-engineersenior-software-engineer-data-engineering-india-cgi-315-46444987/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/148315132/n273-cgi-lead-software-engineer-senior-software-engineer-data-engineering-hyderabad/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Cencora</td><td>Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\n",
       "\n",
       "Job Details\n",
       "\n",
       "PRIMARY DUTIES AND RESPONSIBILITIES:\n",
       "• Leads design, development, and automation of scalable Analytic data engineering products leveraging Machine Learning, cloud infrastructure\n",
       "• Leads the design, build and operationalization of monitoring and tracking of data quality and data flow dynamics\n",
       "• Optimizes existing data processes and implements best-in-class data transformation capabilities\n",
       "• Leverages advanced statistical and computational methodologies to deliver insights and identify strategic opportunities\n",
       "• Implements Data Ingestion Framework and other re-usable components leveraged for delivering data pipeline and supporting ad-hoc data ingestions\n",
       "• Assists with development and storage of analytics-ready data for development of analytic deliverables\n",
       "• Recommends data products to solve business problems meeting multiple stakeholder requirements\n",
       "• Drives project planning processes, delegates non-complex tasks to junior team members\n",
       "• Mentors other team members and assists them with priority setting and issue resolution\n",
       "• Maintains data and information environment that provides consistent and accurate reporting of the client’s data\n",
       "• Leverages Machine Learning to enhance the developed solution\n",
       "• Collaborates with Business and Client representatives to target strategic analytics opportunities and define deliverable scope\n",
       "• Facilitates the resolution of issues regarding projects being worked on to include proper documentation of recommendations for those issues\n",
       "• Manages multiple initiatives in parallel partnering with other Analytics team members and conducts review meetings with stakeholders to drive prioritization effort\n",
       "• Analyzes model errors and design strategies to overcome them\n",
       "• Implements best practices for data engineering to ensure quality delivery of enterprise solutions\n",
       "• Brings expert knowledge of data visualization tools and techniques to drive business analytics and semantic data access requirements.\n",
       "• Work closely with business users, vendors, and delivery teams to understand the business requirements that drive the analysis and design of business analytics and reporting solutions.\n",
       "• Conceptualize, design, and develop data visualization solutions that synthesize data concepts into clear communications for key business stakeholders.\n",
       "• Drives business stakeholder adoption of insights-driven decision making and/or business process innovation.\n",
       "• Leads knowledge transfer around using data visualizations to business stakeholders.\n",
       "• Assist in developing best practices for data presentation and sharing across the organization.\n",
       "• Ensures data visualization standards are maintained and implemented.\n",
       "• Demonstrate an ability to reduce data to the bare minimum of what is needed to optimally communicate a message.\n",
       "• Provides technical leadership, coaching and mentoring to team members and business users.\n",
       "• Participates in POC projects and provides business analytics solutions recommendations.\n",
       "• Evaluates new visualization tools and performs research on best practices.\n",
       "• Contributes to strategic planning meetings and provides guidance and expertise on system options, risk, cost vs. benefits, and impacts on business processes and goals.\n",
       "• Has expertise in multiple technical environments and possesses business knowledge that spans multiple business areas.\n",
       "• Responsible for BI Tool administration & security functions as designated\n",
       "\n",
       ".\n",
       "\n",
       "EDUCATIONAL QUALIFICATIONS:\n",
       "\n",
       "Bachelor’s Degree in Statistics, Computer Science, Information Technology or any other related discipline or equivalent related experience.\n",
       "\n",
       "Preferred Certifications:\n",
       "• Advanced Data Analytics Certifications\n",
       "• AI and ML Certifications\n",
       "• SAS Statistical Business Analyst Professional Certification\n",
       "\n",
       "WORK EXPERIENCE:\n",
       "6+ years of directly-related or relevant experience, preferably in healthcare data analytics or data engineering.\n",
       "\n",
       "Working Hours:\n",
       "\n",
       "7PM IST to 2AM IST; Hybrid Working Model\n",
       "\n",
       "SKILLS & KNOWLEDGE:\n",
       "\n",
       "Behavioral Skills:\n",
       "• Conflict Resolution\n",
       "• Creativity & Innovation\n",
       "• Decision Making\n",
       "• Planning\n",
       "• Presentation Skills\n",
       "• Risk-taking\n",
       "\n",
       "Technical Skills:\n",
       "• Advanced Data Visualization Techniques\n",
       "• Advanced Statistical Analysis\n",
       "• Big Data Analysis Tools and Techniques\n",
       "• Data Governance\n",
       "• Data Management\n",
       "• Data Modelling\n",
       "• Data Quality Assurance\n",
       "• Machine Learning and AI Fundamentals\n",
       "• Programming languages like SQL, R, Python\n",
       "\n",
       "Tools Knowledge:\n",
       "• Business Intelligence Software like Tableau, Power BI, Alteryx, QlikSense\n",
       "• Data Visualization Tools\n",
       "• Microsoft Office Suite\n",
       "• Statistical Analytics tools (SAS, SPSS3)\n",
       "\n",
       "What Cencora offers\n",
       "\n",
       "​Benefit offerings outside the US may vary by country and will be aligned to local market practice. The eligibility and effective date may differ for some benefits and for team members covered under collective bargaining agreements.\n",
       "\n",
       "Full time\n",
       "\n",
       "Affiliated Companies\n",
       "Affiliated Companies: CENCORA INDIA TECHNOLOGY SERVICES PRIVATE LIMITED\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Cencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\n",
       "\n",
       "The company’s continued success depends on the full and effective utilization of qualified individuals. Therefore, harassment is prohibited and all matters related to recruiting, training, compensation, benefits, promotions and transfers comply with equal opportunity principles and are non-discriminatory.\n",
       "\n",
       "Cencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com. We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned</td><td>Map(posted_at -> 3 days ago, schedule_type -> Full-time)</td><td>List(3 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgLSBEYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiQ2VuY29yYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJHdzRzcW9FbWlPdEVnTDAwQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Gw4sqoEmiOtEgL00AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_1XIsQoCMQyAYVxvcnbKLNiK4KJuKqL4DkfaC70eNSlNhnsJ31kdXX74_u696E5PwgEuaAhXTpmJGmz-nTl910MCKGGLIwjDTSQVWh1Hs6oH71WLS2poObooLy9MQWY_SdBfeh2xUS1o1O_229lVTuvlmThKQ8gMdx4yfgDb05UQkAAAAA&shmds=v1_AdeF8Kgf1ac1-PLaIiWlr3bT-tlzdySgj_wz-q2WLGlnsUEgdg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Gw4sqoEmiOtEgL00AAAAAA%3D%3D</td><td>null</td><td>Lead Data Engineer - Data Engineering</td><td>Cencora Careers</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/azure-data-engineer-%E2%80%93-azure-databricks-at-aiprus-software-private-limited-4260810830?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Antal Tech Jobs, link -> https://www.antaltechjobs.in/job/microsoft-azure-data-engineer-apac?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobaaj, link -> https://www.jobaaj.com/job/accenture-microsoft-azure-modern-data-platform-data-platform-engineer-bengaluru-karnataka-india-9-to-68-years-289678?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Expoint, link -> https://expoint.co/job/433094843?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-2927-4b20954cde7d8d75f85d90daefcf05f4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/8651830065782325248?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Aiprus Software Private Limited</td><td>Job Title: Azure Data Engineer – Azure Databricks\n",
       "\n",
       "Location: Bangalore, India\n",
       "\n",
       "Experience: 5 to 10 Years\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "As a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Design and develop scalable data pipelines using Azure Databricks and PySpark.\n",
       "• Transform raw data into actionable insights through advanced data engineering techniques.\n",
       "• Build, deploy, and maintain machine learning models using MLlib, TensorFlow, and MLflow.\n",
       "• Optimize data integration workflows from Azure Blob Storage, Data Lake, and SQL/NoSQL sources.\n",
       "• Execute large-scale data processing using Spark Pools, fine-tuning configurations for performance and cost-efficiency.\n",
       "• Collaborate with data scientists, analysts, and business stakeholders to deliver robust data solutions.\n",
       "• Maintain and enhance Databricks notebooks and Delta Lake architectures.\n",
       "\n",
       "Required Skills & Qualifications:\n",
       "• Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field.\n",
       "• 7+ years of experience in data engineering, with at least 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n",
       "• Strong proficiency in:\n",
       "• Python, PySpark, Pandas, NumPy, SciPy\n",
       "• Spark SQL, DataFrames, RDDs\n",
       "• Delta Lake, Databricks Notebooks, MLflow\n",
       "• Hands-on experience with:\n",
       "• Azure Data Lake, Blob Storage, Synapse Analytics\n",
       "• Excellent problem-solving and communication skills.\n",
       "• Ability to work independently and in a collaborative team environment.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with CI/CD pipelines for data workflows.\n",
       "• Familiarity with data governance and security best practices in Azure.\n",
       "• Knowledge of real-time data processing and streaming technologies.</td><td>Map(posted_at -> 6 hours ago, schedule_type -> Full-time)</td><td>List(6 hours ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJBenVyZSBEYXRhIEVuZ2luZWVyIOKAkyBBenVyZSBEYXRhYnJpY2tzIiwiY29tcGFueV9uYW1lIjoiQWlwcnVzIFNvZnR3YXJlIFByaXZhdGUgTGltaXRlZCIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiek5tRUxOMG5Iam5zazl6TkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Bengaluru, Karnataka, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=zNmELN0nHjnsk9zNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WOO27CUBBFlZYlpJoaERsh0SRSEAiE-BRILACN7eF5wMxYM8-AqNgD-2ExWQmmSnOLc05xO8-Pzmh8a4xgihFhJoGFyODv_oB_nhnnR4cvWGoGTmh5CSowVw0Vff6UMdb-nabuVRI8YuQ8yfWUqlCm1_Sgmb9n5yUa1RVG2g2G_WtSS-j-jrm2xmGr-3hpNWyMz20Baz5xpAJYYEISsGqs6cEKTdpDR-zBQgrGF9MrPYTBAAAA&shmds=v1_AdeF8KgIVBow-Ns54zCqcTfwOEP5aEU5Cn9QBz40t-r_Q8lxhg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=zNmELN0nHjnsk9zNAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f200a1978d9d4338da/images/56fc5e069c9f2d485f3524bedd4cd584c799aed00abeba663015fe2649128562.jpeg</td><td>Azure Data Engineer – Azure Databricks</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Splunk, link -> https://www.splunk.com/en_us/careers/jobs/principle-software-engineer-for-31866.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/software-engineer-ii-backend-data-platform/6337823?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Antal Tech Jobs, link -> https://www.antaltechjobs.in/job/software-engineer---reference-data-platform?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Instahyre, link -> https://www.instahyre.com/job-297629-software-engineer-data-platform-at-walmart-global-tech-india-bangalore/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/software-engineer-ii-backend-data-platform-at-abnormal-ai-4240981999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/rjdp/-4348083940499712143?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/88ZuUX6jSORTKI7TUZv6WxDbh8f1BmoPtIK0E8CBjU8d3oBbYFfUxw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> The Muse, link -> https://www.themuse.com/jobs/abnormalsecurity/software-engineer-ii-backend-data-platform-c9ec20?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Splunk</td><td>Splunk, a Cisco company, is building a safer and more resilient digital world with an end-to-end full stack platform made for a hybrid, multi-cloud world. Leading enterprises use our unified security and observability platform to keep their digital systems secure and reliable. Our customers love our technology, but it's our caring employees that make Splunk stand out as an amazing career destination. No matter where in the world or what level of the organization, we approach our work with kindness. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Come help organizations be their best, while you reach new heights with a team that has your back.Role SummarySplunk AppDynamics, a leader in observability solutions for both cloud and on-premises environments, empowers customers with end-to-end visibility across their entire application ecosystems, cloud, on-premises, or hybrid. Our team is dedicated to developing, maintaining, and supporting SaaS and on-premises offerings within the observability product suite.We're looking for a Senior Software Engineer to build upon our application and data platform as we continue to innovate on application observability. We move fast and iterate quickly. We are passionate about solving customers’ problems. We have ambitious goals to build best-of-its-kind products. You will help the team win in a fast-growing market. If you are passionate about innovation and embrace the challenge of working on highly scalable systems that handle large volumes of data, this position is for you.Meet the Team (Optional)Data platform is the heart of the AppDynamics architecture. We process more than 100 billion data packets per day, more than 5 peta byte of data flows through our platform. We are in 8 regions having 40 clusters of 7000 nodes. We ingest, process and store all the data that power the dashboards that our customers rely on for observing their infrastructure and their applications.What you'll get to do\n",
       "• Design and build highly scalable solutions\n",
       "• Work with a team of exceptionally capable and dedicated peers, all the way from engineering to product management and customer support\n",
       "• Work in an open environment, work together to get things done and adapt to the team's changing needs\n",
       "• Leverage technologies including Kafka, ElasticSearch, Docker, and Kubernetes across different cloud environments like AWS and Azure\n",
       "• lead critical initiatives for the organisation\n",
       "Must-have Qualifications\n",
       "• 15+ years of full-stack developer experience in designing and developing highly scalable, distributed applications, products, and services.\n",
       "• Expertise in Java programming language.\n",
       "• Strong proficiency in data structures, algorithms, threads, concurrent programming\n",
       "• Extensive knowledge of SQL and at least one relational database engine: MySQL. Hands on experience in RDS or NoSQL (Dynamo, MongoDB) is a big plus\n",
       "• Experience of building applications using microservices architecture with expertise in Dockers and Kubernetes.\n",
       "• Strong communication skills, both verbal and written. Ability to multi-task and adapt quickly to changing requirements, scope, and priorities.\n",
       "• Optimize data pipelines, storage systems, and query engines for performance and efficiency.\n",
       "• Work closely with product teams, data engineers, software developers, and product managers to deliver on business goals.\n",
       "• Mentor team members in architecture principles, coding best practices, and system design.\n",
       "• Ensure robust monitoring, logging, and alerting systems for proactive issue detection and resolution.\n",
       "• Support CI/CD processes and automate testing for data systems\n",
       "• Stay abreast of emerging trends in big data, machine learning, and distributed systems to recommend innovative solutions.\n",
       "Nice-to-have QualificationsWe’ve taken special care to separate the must-have qualifications from the nice-to-haves. “Nice-to-have” means just that: Nice. To. Have. So, don’t worry if you can’t check off every box. We’re not hiring a list of bullet points–we’re interested in the whole you.\n",
       "• Added advantage of having an experience in working on Cloud Observability Space.\n",
       "• experience of other languages like python, etc\n",
       "• experience of front-end technologies\n",
       "Splunk is an Equal Opportunity EmployerSplunk, a Cisco company, is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.\n",
       "\n",
       "Note:</td><td>Map(posted_at -> 9 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(9 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJQcmluY2lwbGUgU29mdHdhcmUgRW5naW5lZXIgZm9yIERhdGEgUGxhdGZvcm0gLSAzMTg2NiIsImNvbXBhbnlfbmFtZSI6IlNwbHVuayIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiY2k4emNxSnNNanNSemNYUEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Bengaluru, Karnataka, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=ci8zcqJsMjsRzcXPAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOOw7CMBBERZsjUG1DgyDhIyIEHQIhPkUkDoA2YeOYOLuW7YicixNimhm9Zt4k31FyL5zmSltD8JA6fNARnFhpJnJQi4MjBoTCYIjQwRzWy22ex75KCZ7QVQ0Iw1lEGRrvmxCs32WZ9yZVPmDQVVpJlwlTKUP2ltL_4-mbKLJxlZ6rzWJILavp5GFNzy1ohgOxQtO7fgY3dBwvtDiDC780_gAY7Jx1tQAAAA&shmds=v1_AdeF8Kg4BT_m5sIxVWF_oYIBoa0Kgrztog4xYjhPZHAegoh_Tg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=ci8zcqJsMjsRzcXPAAAAAA%3D%3D</td><td>null</td><td>Principle Software Engineer for Data Platform - 31866</td><td>Splunk</td></tr><tr><td>List(Map(title -> Groupe BNP Paribas, link -> https://group.bnpparibas/en/careers/job-offer/software-developer-python?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=6709c3557b007841&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/sr-software-developer-asp-net-midas-it-services-JV_KO0,29_KE30,47.htm?jl=1006209888609&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/software-developer-sase-at-check-point-software-4253111300?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Foundit, link -> https://www.foundit.in/job/software-developer-banking-sector-ambitiwiz-india-34842371?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Apna, link -> https://apna.co/job/hyderabad/software-developer-android-1699992987?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> TechGig, link -> https://www.techgig.com/jobs/Software-Developer-PHP-technologies/70786114?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobaaj, link -> https://www.jobaaj.com/job/larsen-toubro-software-developer-web-applications-greater-chennai-area-5-to-10-years-686697?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>BNP Paribas India Solutions</td><td>About BNP Paribas India Solutions:\n",
       "\n",
       "Established in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n",
       "\n",
       "About BNP Paribas Group:\n",
       "\n",
       "BNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n",
       "\n",
       "Commitment to Diversity and Inclusion\n",
       "\n",
       "At BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n",
       "\n",
       "About Business line/Function:\n",
       "\n",
       "The Intermediate Holding Company (“IHC”) program structured at the U.S. level across poles of activities of BNP Paribas provides guidance, supports the analysis, impact assessment and drives adjustments of the U.S. platform’s operating model due to the drastic changes introduced by the Enhanced Prudential Standards (“EPS”) for Foreign Banking Organizations (“FBOs”) finalized by the Federal Reserve in February 2014, implementing Section 165 of U.S. Dodd-Frank Act.\n",
       "\n",
       "The IT Transversal Team is part of the Information Technology Group which works simultaneously on a wide range of projects arising from business, strategic initiatives, and regulatory changes and reengineering of existing applications to improve functionality and efficiency.\n",
       "\n",
       "Job Title:\n",
       "\n",
       "Python Developer\n",
       "\n",
       "Date:\n",
       "\n",
       "June-25\n",
       "\n",
       "Department:\n",
       "\n",
       "ITG- Fresh\n",
       "\n",
       "Location:\n",
       "\n",
       "Chennai, Mumbai\n",
       "\n",
       "Business Line / Function:\n",
       "\n",
       "Finance Dedicated Solutions\n",
       "\n",
       "Reports to:\n",
       "\n",
       "(Direct)\n",
       "\n",
       "Grade:\n",
       "\n",
       "(if applicable)\n",
       "\n",
       "(Functional)\n",
       "\n",
       "Number of Direct Reports:\n",
       "\n",
       "NA\n",
       "\n",
       "Directorship / Registration:\n",
       "\n",
       "NA\n",
       "Position Purpose\n",
       "\n",
       "The Python Developer will play a critical role in building and maintaining financial applications and tools that support data processing, analysis, and reporting within a fast-paced financial services environment. This position involves developing scalable and secure systems. The developer will collaborate with business analysts, finance users/or finance BA to translate complex business requirements into efficient, high-quality software solutions.\n",
       "\n",
       "A strong understanding of financial concepts, data integrity, and regulatory compliance is essential. The detailed responsibilities are mentioned below.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Direct Responsibilities\n",
       "\n",
       "- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n",
       "\n",
       "- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n",
       "\n",
       "- Expertise in PySpark for large-scale data processing and loading into databases.\n",
       "\n",
       "- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n",
       "\n",
       "- Strong communication skills to effectively collaborate with team members and stakeholders.\n",
       "\n",
       "- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n",
       "Technical & Behavioral Competencies\n",
       "\n",
       "- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n",
       "\n",
       "- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n",
       "\n",
       "- Expertise in PySpark for large-scale data processing and loading into databases.\n",
       "\n",
       "- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n",
       "\n",
       "- Strong communication skills to effectively collaborate with team members and stakeholders.\n",
       "\n",
       "- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n",
       "\n",
       "- Good analytical, problem solving, & communication skills\n",
       "\n",
       "- Engage in technical discussions and to help in improving the system, process etc\n",
       "\n",
       "Nice to Have\n",
       "\n",
       "- Familiarity with Plotly and Matplotlib for data visualization of large datasets.\n",
       "\n",
       "- Skilled in API programming, handling JSON, CSV, and other unstructured data from various systems.\n",
       "\n",
       "- Familiarity with JavaScript, CSS, and HTML.\n",
       "\n",
       "- Experience with cloud architecture applications such as Dataiku or Databricks; competency with ETL tools.\n",
       "\n",
       "- Knowledge of regulatory frameworks, RISK, CCAR, and GDPR.\n",
       "Specific Qualifications (if required)\n",
       "\n",
       "Skills Referential\n",
       "\n",
       "Behavioural Skills: (Please select up to 4 skills)\n",
       "\n",
       "Ability to collaborate / Teamwork\n",
       "\n",
       "Critical thinking\n",
       "\n",
       "Ability to deliver / Results driven\n",
       "\n",
       "Communication skills - oral & written\n",
       "\n",
       "Transversal Skills: (Please select up to 5 skills)\n",
       "\n",
       "Analytical Ability\n",
       "\n",
       "Ability to develop and adapt a process\n",
       "\n",
       "Ability to understand, explain and support change\n",
       "\n",
       "Ability to develop others & improve their skills\n",
       "\n",
       "Choose an item.\n",
       "\n",
       "Education Level:\n",
       "\n",
       "Bachelor Degree or equivalent\n",
       "\n",
       "Experience Level\n",
       "\n",
       "At least 5 years\n",
       "\n",
       "Other/Specific Qualifications (if required)</td><td>Map(posted_at -> 7 days ago, schedule_type -> Full-time)</td><td>List(7 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBEZXZlbG9wZXItIFB5dGhvbiIsImNvbXBhbnlfbmFtZSI6IkJOUCBQYXJpYmFzIEluZGlhIFNvbHV0aW9ucyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ6VTdCX1R2emRzczk0M0ZoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=zU7B_Tvzdss943FhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWMMQrCQBAAsc0TrLawEpJTwUY7EUQLCeQBYS-uuZPz9rhdNX7E9xqxGZgppvhMimXDV31hJtjTkwInyiXUb3UcoYQTWxDC3DkY_cDcB5punWqSjTEioepFUX1XdXw3HMnyYG5s5YdW3PhNAZXa1XoxVCn289nuXEON2VsUOMaLR2g4PNRzFPDxn74MpNGLmQAAAA&shmds=v1_AdeF8KhddRLnKmMGyyu5XSrgUY0KL77JpalABEFKQDRF04WXHg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=zU7B_Tvzdss943FhAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f3e6590e63b71be3be/images/7ca52d7311167a4b2e85f52b4c2b84195050eda524e990896d525b1805249c20.jpeg</td><td>Software Developer- Python</td><td>Groupe BNP Paribas</td></tr><tr><td>List(Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=f610807b1c0a08e1&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/freelance-python-developer-teqlawn-JV_KO0,26_KE27,34.htm?jl=1009797935274&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Teqlawn</td><td>We’re looking for an experienced Python Developer (4–7 years) to build scalable web and application solutions, with opportunities to integrate AI capabilities. Experience in AI integration is a must.\n",
       "\n",
       "Responsibilities:\n",
       "• Develop scalable web and application solutions using Python, with integration of AI/ML components\n",
       "• Collaborate with clients to understand project goals and technical requirements\n",
       "• Write clean, maintainable, and well-documented code\n",
       "• Troubleshoot, debug, and optimize applications for performance and reliability\n",
       "• Ensure timely and efficient delivery of milestones and final deliverables\n",
       "• Participate in code reviews and contribute to maintaining coding standards and best practices\n",
       "• Work with relevant frameworks and libraries such as Django, Flask, FastAPI, NumPy, pandas, and scikit-learn\n",
       "\n",
       "Note: Please share the link to your portfolio along with your application.\n",
       "\n",
       "Job Types: Full-time, Contractual / Temporary, Freelance\n",
       "Contract length: 2 months\n",
       "\n",
       "Pay: ₹50,000.00 - ₹80,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Work from home\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "• Monday to Friday\n",
       "\n",
       "Experience:\n",
       "• Python Development: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>Map(salary -> ₹50K–₹80K a month, qualifications -> No degree mentioned, schedule_type -> Full-time, work_from_home -> true, posted_at -> 1 day ago)</td><td>List(1 day ago, ₹50K–₹80K a month, Work from home, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlRlcWxhd24iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiY20xcUU4aHhjbFQ3dnVyT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Anywhere</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJMQ9BMRAA4FjfZDbdLNEisbAKYTLYX651aftSd9VreFa_HMu3fN1n0q0OlSgje4LLu0Vh2NOTshSqsICzOFDC6iP85igSMs12sbWiW2tVswnasCVvvNytMDkZ7SBO__QasVLJ2Khfb5ajKRzm0ys9Mr4YEsOJbwm_PB-WVoUAAAA&shmds=v1_AdeF8KgBybPFrWw15GHBBGaXtzfcSVMmfp01tpCXCitzSPYcjg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D</td><td>null</td><td>Freelance Python Developer</td><td>Indeed</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/python-developer-%E2%80%94-full-time-1-2-years-exp-in-office-bangalore-at-serp-hawk-4257156470?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Matt Wallaert Job Board, link -> https://network.mattwallaert.com/companies/capgemini-2-af6e920e-79b5-48e9-88db-41d4adb84606/jobs/41903824-sb-author-4-6-years-bangalore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/156848102/python-developer-fast-years-bangalore-wfo-kfy791-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>SERP Hawk</td><td>\uD83D\uDE80 We’re Hiring: Python Developer\n",
       "\n",
       "SERP Hawk is looking for a talented Python Developer to join our dynamic team in Bangalore!\n",
       "\n",
       "\uD83C\uDF1F About Us\n",
       "\n",
       "SERP Hawk is a leading digital marketing and technology solutions company based in Bangalore. We specialize in building robust backend systems, scalable applications, and AI-powered solutions for clients across various industries.\n",
       "\n",
       "\uD83D\uDCCD Location:B-1, Bannerghatta Slip Road, KEB Colony, New Gurappana Palya, 1st Stage, BTM Layout 1, Bengaluru, Karnataka 560029\n",
       "\n",
       "\uD83C\uDF10 Website: www.serphawk.com\n",
       "\n",
       "\uD83D\uDCBC What You’ll Do\n",
       "• Design and develop scalable backend architectures.\n",
       "• Write clean, efficient Python code.\n",
       "• Integrate APIs and databases.\n",
       "• Implement CI/CD pipelines and automated tests.\n",
       "• Ensure high performance, security, and reliability.\n",
       "\n",
       "✅ What We’re Looking For\n",
       "\n",
       "✔️ 1–2 years of experience in Python development.\n",
       "\n",
       "✔️ Proficiency in frameworks like Django, Flask, or FastAPI.\n",
       "\n",
       "✔️ Strong understanding of APIs and databases.\n",
       "\n",
       "✔️ Experience with CI/CD tools and best practices.\n",
       "\n",
       "✔️ Excellent problem-solving skills and a collaborative mindset.\n",
       "\n",
       "\uD83D\uDCA1 Nice to Have\n",
       "\n",
       "⭐ Experience with AI/chatbots.\n",
       "\n",
       "⭐ Knowledge of cloud services and containerization.\n",
       "\n",
       "\uD83D\uDCB0 Salary\n",
       "• ₹20,000 – ₹25,000 per month (based on skills and experience).\n",
       "\n",
       "\uD83D\uDCCC Additional Details\n",
       "\n",
       "\uD83D\uDCBC This is a full-time, in-office role in Bangalore.\n",
       "\n",
       "\uD83C\uDFE2 Candidates must report to the office daily.\n",
       "\n",
       "\uD83C\uDF10 Should be flexible to work and attend meetings or calls as per client time zones.\n",
       "\n",
       "✨ Apply now and grow with us!</td><td>Map(posted_at -> 5 hours ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(5 hours ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAlCBGdWxsLVRpbWUgfCAxLTIgeWVhcnMgRXhwIHwgSW4tT2ZmaWNlIHwgQmFuZ2Fsb3JlIiwiY29tcGFueV9uYW1lIjoiU0VSUCBIYXdrIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjNBMWtiUmowZFhsRVZsWnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=3A1kbRj0dXlEVlZyAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNvQrCMBSFce3o6HRnIa0WXHQT699iUVcpabxto2luSKK24OBD-CA-k09iXQ4cvsN3gk8vOKWtr0jDAu-oyKCF7-sNy5tS7ChrhCeMWQwtcusgaUzXN5rtikKKP5tzXXJFFoHBlnJw3U5U0PlWRKXCwazy3rhpFDmnwtJ57qUIBdURacypiS6Uu39kruIWjeIes3gyakKjy2H_kOxTWPPHFaTufs-S_wDzm2IWswAAAA&shmds=v1_AdeF8KjN42J0FJmp3krgNjBjj45F10uMxQlIRBdqv0myDo7lHw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=3A1kbRj0dXlEVlZyAAAAAA%3D%3D</td><td>null</td><td>Python Developer — Full-Time | 1-2 years Exp | In-Office | Bangalore</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/sql-%2B-python-at-wissen-technology-4259166199?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Foundit, link -> https://www.foundit.in/job/sdet-etl-testing-python-sql-anoma-india-34924961?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/python-no-sql/revolite-infotech-pvt-ltd/17358425?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/173300637608443904?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Wissen Technology</td><td>Wissen Technology is Hiring for SQL With Python\n",
       "\n",
       "About Wissen Technology:\n",
       "\n",
       "Wissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges.\n",
       "\n",
       "Role Overview:\n",
       "\n",
       "We are looking for a skilled and detail-oriented candidate with a strong foundation in SQL, Python, and data processing techniques. The ideal candidate is passionate about transforming raw data into meaningful insights and has hands-on experience across the data pipeline—from data wrangling to visualization.\n",
       "\n",
       "Experience: 3-7 Years\n",
       "\n",
       "Location: Bengaluru\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with SQL (e.g., joins, subqueries, CTEs, window functions).\n",
       "• Proficiency in Python for data manipulation (e.g., pandas, NumPy).\n",
       "• Experience working with relational databases like MySQL, PostgreSQL, SQL Server, or Oracle.\n",
       "• Hands-on experience in data wrangling, cleaning, and feature engineering.\n",
       "• Understanding of ETL processes and tools.\n",
       "• Familiarity with version control systems like Git.\n",
       "• Knowledge of data visualization techniques and tools.\n",
       "• Strong problem-solving and analytical skills.\n",
       "\n",
       "The Wissen Group was founded in the year 2000. Wissen Technology, a part of Wissen Group, was established in the year 2015. Wissen Technology is a specialized technology company that delivers high-end consulting for organizations in the Banking & Finance, Telecom, and Healthcare domains. We help clients build world class products.\n",
       "\n",
       "We offer an array of services including Core Business Application Development, Artificial Intelligence & Machine Learning, Big Data & Analytics, Visualization & Business Intelligence, Robotic Process Automation, Cloud Adoption, Mobility, Digital Adoption, Agile & DevOps, Quality Assurance & Test Automation.\n",
       "\n",
       "Over the years, Wissen Group has successfully delivered $1 billion worth of projects for more than 20 of the Fortune 500 companies. Wissen Technology provides exceptional value in mission critical projects for its clients, through thought leadership, ownership, and assured on-time deliveries that are always ‘first time right’.\n",
       "\n",
       "The technology and thought leadership that the company commands in the industry is the direct result of the kind of people Wissen has been able to attract. Wissen is committed to providing them with the best possible opportunities and careers, which extends to providing the best possible experience and value to our clients.\n",
       "\n",
       "We have been certified as a Great Place to Work® company for two consecutive years (2020-2022) and voted as the Top 20 AI/ML vendor by CIO Insider. Great Place to Work® Certification is recognized world over by employees and employers alike and is considered the ‘Gold Standard’. Wissen Technology has created a Great Place to Work by excelling in all dimensions - High-Trust, High-Performance Culture, Credibility, Respect, Fairness, Pride and Camaraderie.\n",
       "\n",
       "Website: www.wissen.com\n",
       "\n",
       "LinkedIn: https://www.linkedin.com/company/wissen-technology\n",
       "\n",
       "Wissen Leadership: https://www.wissen.com/company/leadership-team/\n",
       "\n",
       "Wissen Live: https://www.linkedin.com/company/wissen-technology/posts/feedView=All\n",
       "\n",
       "Wissen Thought Leadership: https://www.wissen.com/articles/\n",
       "\n",
       "Employee Speak:\n",
       "\n",
       "https://www.ambitionbox.com/overview/wissen-technology-overview\n",
       "\n",
       "https://www.glassdoor.com/Reviews/Wissen-Infotech-Reviews-E287365.htm\n",
       "\n",
       "Great Place to Work:\n",
       "\n",
       "https://www.wissen.com/blog/wissen-is-a-great-place-to-work-says-the-great-place-to-work-institute-india/\n",
       "\n",
       "https://www.linkedin.com/posts/wissen-infotech_wissen-leadership-wissenites-activity-6935459546131763200-xF2k\n",
       "\n",
       "About Wissen Interview Process:\n",
       "\n",
       "https://www.wissen.com/blog/we-work-on-highly-complex-technology-projects-here-is-how-it-changes-whom-we-hire/\n",
       "\n",
       "Latest in Wissen in CIO Insider:\n",
       "\n",
       "https://www.cioinsiderindia.com/vendor/wissen-technology-setting-new-benchmarks-in-technology-consulting-cid-1064.html</td><td>Map(posted_at -> 7 hours ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(7 hours ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJTUUwgKyBQeXRob24iLCJjb21wYW55X25hbWUiOiJXaXNzZW4gVGVjaG5vbG9neSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJITW5vZUwxWlF1QWVma0hlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=HMnoeL1ZQuAefkHeAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHsQrCMBAA0L2f0Omgm2Iigot-gCgOFQXHksQjSYl3oXdDu_jt6vLgNZ-mu9-usIZ-0cQEG7iwB0E3hQS_n5hjwfaYVKscrBUpJoo6zcEEflsm9Dzbkb38GSS5CWtxisNuv51Npbhqn1kECR4YEnHhuEAmONMruy_V8I7vgAAAAA&shmds=v1_AdeF8Kg816FNs6a2t4PUxzzK9j5QqwX8eJyEBns2tE281vSbJA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=HMnoeL1ZQuAefkHeAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f3e6590e63b71be3be/images/7ca52d7311167a4bb7fce52e6e3afcd63870bf70216e5e0689db804b95796ad4.jpeg</td><td>SQL + Python</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=506261436cb2c2f5&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/backend-python-developer-freelance-only/increscent-software/17280744?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/rjdp/-6935223859228097491?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/30083578733789184?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/158839731/dv559-python-developer-part-time-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> PrepIntro, link -> https://www.prepintro.com/job/python-developer-part-time-quick-compare/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Variance Technologies Private Limited</td><td>Job Opportunity: Python Developer at Variance Technologies Private Limited!\n",
       "\n",
       "Role: Python Developer\n",
       "\n",
       "Duration: 1 Months\n",
       "\n",
       "Location: Hybrid / Remote\n",
       "\n",
       "Responsibilities:\n",
       "\n",
       "Collaborate with our development team to develop and maintain Python-based applications tailored for finance-related tasks\n",
       "\n",
       "Implement object-oriented programming principles to ensure the scalability and maintainability of codebase\n",
       "\n",
       "Gain hands-on experience with Pandas for data manipulation and analysis within finance contexts\n",
       "\n",
       "Support integration of REST and WebSocket APIs for seamless communication with financial data providers and platforms\n",
       "\n",
       "Assist in automating financial modeling and analysis workflows using Python, including Excel automation for enhanced efficiency\n",
       "\n",
       "Requirements:\n",
       "\n",
       "Currently pursuing or recently completed a degree in Computer Science, Finance, or a related field\n",
       "\n",
       "Basic proficiency in Python programming language, with a strong willingness to learn and grow\n",
       "\n",
       "Exceptional attention to detail and proactive attitude towards problem-solving\n",
       "\n",
       "Genuine interest in the intersection of finance and technology\n",
       "\n",
       "Bonus Skills:\n",
       "\n",
       "Familiarity with fundamental financial concepts and markets\n",
       "\n",
       "Exposure to Python libraries such as Pandas, NumPy, or SciPy\n",
       "\n",
       "Demonstrated interest in financial data analysis and visualization techniques\n",
       "\n",
       "Basic understanding of REST and WebSocket APIs\n",
       "\n",
       "Perks:\n",
       "\n",
       "Hands-on experience working on real-world projects at the forefront of finance and technology\n",
       "\n",
       "Mentorship and guidance from seasoned professionals in the field\n",
       "\n",
       "Networking opportunities with industry experts to expand your professional connections\n",
       "\n",
       "Flexible scheduling to accommodate academic commitments\n",
       "\n",
       "Potential for transition to a full-time position based on exceptional performance and availability\n",
       "\n",
       "Ready to kickstart your career with a prestigious 1-month internship at Variance Technologies Private Limited? Apply now by sending your resume and a tailored cover letter expressing your interest and qualifications to careers@variancefintech.com. Join our team and embark on a journey towards shaping the future of finance through technology!\n",
       "\n",
       "Variance Technologies Private Limited is committed to fostering a diverse and inclusive workplace where all individuals are empowered to thrive. We welcome applicants from all backgrounds and identities to apply.\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: From ₹35,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Work from home\n",
       "\n",
       "Schedule:\n",
       "• Monday to Friday\n",
       "\n",
       "Education:\n",
       "• Bachelor's (Preferred)\n",
       "\n",
       "Experience:\n",
       "• Python: 1 year (Preferred)\n",
       "• total work: 1 year (Preferred)\n",
       "\n",
       "Work Location: Remote</td><td>Map(posted_at -> 5 days ago, work_from_home -> true, schedule_type -> Full-time)</td><td>List(5 days ago, Work from home, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIEZ1bGwgdGltZSIsImNvbXBhbnlfbmFtZSI6IlZhcmlhbmNlIFRlY2hub2xvZ2llcyBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiaXZCN3JUQTN5YnkxRzV2MUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Anywhere</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=ivB7rTA3yby1G5v1AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFz7CU43CzYquOgqiuLQQVxLmh7JSZoLubPUv_FTrcubXvVdVNvmo4ETnHDEyBkLnN8xgtKAsIYbdyBoiwswnwuzj7g8BtUsB2NEYu1FrZKrHQ-GE3Y8mRd38qeVYAvmaBXb3X4z1Tn5Vf20hWxyCA90IXFkTyjQFBrnB3caSLEHSnBNPdkfzRn8NKMAAAA&shmds=v1_AdeF8Ki_eVNJcL3hCOFzq-36japCXlxDofyZJcWoB2ZGzGRkKA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=ivB7rTA3yby1G5v1AAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f3e6590e63b71be3be/images/7ca52d7311167a4bbcf9496c8d47b104687ec43b62a624497a9763b0ee63de1f.png</td><td>Python Developer Full time</td><td>Indeed</td></tr><tr><td>List(Map(title -> Glassdoor, link -> https://www.glassdoor.com/job-listing/python-developer-%E2%80%93-ai-and-llm-integrations-discover-webtech-JV_KO0,42_KE43,59.htm?jl=1009797088926&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-3396-b3df6a9d9d368e7d40f9d409ed265e6b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Discover WebTech Private Limited</td><td>We are hiring a skilled Python Developer with a minimum of 3 years of experience, who has hands-on expertise in developing AI modules, building intelligent agents using LLMs, and working with cutting-edge frameworks such as LangChain, LangGraph, and LangSmith.\n",
       "\n",
       "The ideal candidate should be capable of designing, developing, and deploying backend services as well as intelligent AI-driven tools and systems.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design and implement intelligent agents using LLM-based frameworks like LangChain, LangGraph, and LangSmith.\n",
       "• Build backend systems using Python (Django, FastAPI, or Flask).\n",
       "• Develop and integrate APIs, third-party tools, and cloud services.\n",
       "• Create AI modules that interact with knowledge bases, APIs, and perform multi-step reasoning.\n",
       "• Implement prompt engineering, memory chains, and agent behavior logic.\n",
       "• Collaborate with cross-functional teams to deliver robust AI features.\n",
       "• Optimize code for scalability, performance, and reliability.\n",
       "\n",
       "Required Skills and Qualifications\n",
       "• 3+ years of hands-on experience with Python.\n",
       "• Proficiency in LangChain, LangGraph, or LangSmith.\n",
       "• Strong experience in working with LLMs like OpenAI, Cohere, or Anthropic.\n",
       "• Deep understanding of prompt engineering and agent orchestration.\n",
       "• Experience with APIs, JSON, and external integrations.\n",
       "• Knowledge of data storage systems (PostgreSQL, MongoDB).\n",
       "• Familiarity with Docker, Git, and CI/CD tools.\n",
       "• Excellent problem-solving and debugging skills.\n",
       "\n",
       "Preferred Qualifications\n",
       "• Knowledge of vector databases (e.g., Pinecone, Weaviate, FAISS).\n",
       "• Experience with microservices, asynchronous programming, and message queues (Celery, RabbitMQ).\n",
       "• Familiarity with frontend technologies (React.js or Vue.js) is a plus.\n",
       "• Exposure to cloud platforms such as AWS, GCP, or Azure.\n",
       "\n",
       "Job Types: Full-time, Permanent\n",
       "\n",
       "Pay: ₹30,000.00 - ₹70,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Health insurance\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "\n",
       "Work Location: In person</td><td>Map(posted_at -> 3 days ago, schedule_type -> Full-time)</td><td>List(3 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBBSSBcdTAwMjYgTExNIEludGVncmF0aW9ucyIsImNvbXBhbnlfbmFtZSI6IkRpc2NvdmVyIFdlYlRlY2ggUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Im1JUlJqRWJSV0pCSkdQR0VBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=mIRRjEbRWJBJGPGEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOMQrCQBBFsc0RrKYSFExEsNEqEJBIhBSCZdjdDJuVZCfsDCF23sFjeCtP4tr86v33f_JZJHn9lI48FDhhTyMG-L7ekJewgqq6QukFbVDiyDNs4UIaGFUwHcTOmcj2uDx1IiMfs4y5Ty1LpE1qaMjIo6Y5e5DmfzTcqYBjrwSb_WE3p6O3m3Xh2NAUd--obxjFdXBTRKBygxNswfn4onXqB38mJRuuAAAA&shmds=v1_AdeF8KjaSOxt1sFNPUIlTSZP3rzQxsP1gUV27BAuF8V580Z6aw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=mIRRjEbRWJBJGPGEAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f3e6590e63b71be3be/images/7ca52d7311167a4b2428c1372d3bee3b8c018c5c58d9bae8bd00bd66393bc89c.jpeg</td><td>Python Developer – AI & LLM Integrations</td><td>Glassdoor</td></tr><tr><td>List(Map(title -> Hitachi Careers, link -> https://careers.hitachi.com/jobs/16246064-full-stack-developer-python-slash-react-js?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/full-stack-developer-python-react-js/hitachi-careers/17318841?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/full-stack-developer-appian-java/6385724?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Startup Jobs, link -> https://startup.jobs/full-stack-developer-python-react-js-hitachi-vantara-corporation-6843457?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> JobLeads, link -> https://www.jobleads.com/in/job/full-stack-developer-python-react-js--india--e5f404c1e83327d5061e6410ccf06c525?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Hitachi Careers</td><td>Our Company\n",
       "\n",
       "We're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n",
       "\n",
       "Our group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n",
       "\n",
       "Imagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n",
       "\n",
       "The team\n",
       "\n",
       "We are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "\n",
       "The role: Full Stack Developer/Specialist\n",
       "\n",
       "Responsibilities:\n",
       "• Design, develop, and maintain applications.\n",
       "• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n",
       "• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n",
       "• Optimise applications for performance, scalability, and user experience.\n",
       "• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n",
       "• Provide training and support to end-users and IT staff on functionalities and best practices.\n",
       "• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n",
       "• Participate in project planning, execution, and post-implementation support.\n",
       "• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n",
       "• What you'll bring\n",
       "Qualifications:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n",
       "• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n",
       "• Expertise in development and customisation.\n",
       "• Proficiency in Python and React JS\n",
       "• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n",
       "• Experience with RESTful APIs and web services.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Strong communication and collaboration skills.\n",
       "• Ability to work independently and as part of a team in a fast-paced environment.\n",
       "• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n",
       "• Prior experience in building AI applications is a plus.\n",
       "• Prior experience with automation tools like UIPath is a plus.\n",
       "• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\n",
       "Preferred Skills\n",
       "• Certification in\n",
       "• Experience with other low-code/no-code platforms.\n",
       "• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n",
       "• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n",
       "• Experience with Agile development methodologies.\n",
       "About us\n",
       "\n",
       "We're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n",
       "\n",
       "#LI-MS3\n",
       "\n",
       "Championing diversity, equity, and inclusion\n",
       "\n",
       "Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n",
       "\n",
       "How we look after you\n",
       "\n",
       "We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n",
       "\n",
       "We're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.</td><td>Map(posted_at -> 14 days ago, schedule_type -> Full-time)</td><td>List(14 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=gHUO0z59qIifBLiVAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNPQ7CMAxAYbH2BIjJIyDRICQWGBhA_HRCcIDKDVYTCHEUG1RuwnEpy1u-4RXfQbHZv0KAq6J9wI7eFDhRhvH5o44jGLgQWoXqOoEZVNyAEGbroLcDcxtotHaqSVbGiISyFUX1trT8NByp4c7cuZF_anGYKQVUqhfLeVem2E6HR9-fnYdtb5QFfIRTvHn8AQLWDNibAAAA&shmds=v1_AdeF8KiBbHp5HoJs2LSbE900z5gBevA-S4BK5odgvTSsP5in2w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=gHUO0z59qIifBLiVAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f3e6590e63b71be3be/images/7ca52d7311167a4b2adaf1e3020b5d6aed08e2ec45299500cc6be0568fcaacfb.png</td><td>Full Stack Developer (Python / React JS)</td><td>Hitachi Careers</td></tr><tr><td>List(Map(title -> Goodyear Jobs, link -> https://jobs.goodyear.com/job/Gachibowli-Hyderabad-Python-Back-End-Developer-TG/1295198900/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=50601a8fed987a6b&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/python-back-end-developer-impactqa-JV_KO0,25_KE26,34.htm?jl=1009707951248&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/python-back-end-developer-at-the-goodyear-tire-rubber-company-4240997333?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/d459ad4d3fd04946ece63568728abb4d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.com/job-listing/python-back-end-developer-impactqa-JV_KO0,25_KE26,34.htm?jl=1009707951248&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/147722131/c345-python-back-end-developer-hyderabad/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Goodyear</td><td>Location: IN - Hyderabad Telangana\n",
       "\n",
       "Goodyear Talent Acquisition Representative: M Bhavya Sree\n",
       "\n",
       "Sponsorship Available: No\n",
       "\n",
       "Relocation Assistance Available: No\n",
       "\n",
       "Duties and Responsibilities:\n",
       "\n",
       "• Develop and support Data Driven applications\n",
       "\n",
       "• Help design and develop back-end services and APIs for data-driven applications and simulations.\n",
       "\n",
       "• Work with our technical partners to collaborate on system requirements and data integration needs for our new applications.\n",
       "\n",
       "• Support the deployment and scaling of new back-end technologies and cloud-native architectures within the organization.\n",
       "\n",
       "• Work closely with our data scientists to support model deployment into production environments.\n",
       "\n",
       "• Develop and maintain server-side components for digital tools and products using Python or other modern back-end technologies and frameworks. Build scalable, secure, and efficient services that support a seamless experience across multiple platforms.\n",
       "\n",
       "• Design, implement, and maintain robust database systems (SQL and NoSQL), ensuring high availability and performance for critical applications.\n",
       "\n",
       "• Contribute to DevOps practices including CI/CD pipelines, infrastructure as code, containerization (Docker), and orchestration (Kubernetes).\n",
       "\n",
       "• Learn about the tire industry and tire manufacturing processes from subject matter experts.\n",
       "\n",
       "• Be a part of cross-functional teams working together to deliver impactful results.\n",
       "\n",
       "Skills Required:\n",
       "\n",
       "• Significant experience in server-side development using Python\n",
       "\n",
       "• Strong understanding of RESTful API design, microservices architecture, and service-oriented design\n",
       "\n",
       "• Experience with relational and non-relational databases such as PostgreSQL, MySQL, MongoDB, or DynamoDB\n",
       "\n",
       "• Application of software design skills and methodologies (algorithms, data structures, design patterns, software architecture and testing)\n",
       "\n",
       "• Hands-on experience working with cloud platforms such as AWS, Microsoft Azure, or Google Cloud Platform\n",
       "\n",
       "• Good teamwork skills - ability to work in a team environment and deliver results on time.\n",
       "\n",
       "• Strong communication skills - capable of conveying information concisely to diverse audiences.\n",
       "\n",
       "• Exposure to DevOps practices including CI/CD pipelines (e.g., GitHub Actions, Jenkins), containerization (e.g., Docker), and orchestration tools (e.g., Kubernetes)\n",
       "\n",
       "• Familiarity with front-end technologies like React, HTML, CSS, and JavaScript for API integration purposes\n",
       "\n",
       "Goodyear is an Equal Employment Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to that individual's race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender identity, age, physical or mental disability, ethnicity, citizenship, or any other characteristic protected by law.\n",
       "\n",
       "Goodyear is one of the world’s largest tire companies. It employs about 68,000 people and manufactures its products in 53 facilities in 20 countries around the world. Its two Innovation Centers in Akron, Ohio and Colmar-Berg, Luxembourg strive to develop state-of-the-art products and services that set the technology and performance standard for the industry. For more information about Goodyear and its products, go to www.goodyear.com/corporate\n",
       "\n",
       "#Li-Hybrid</td><td>Map(schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gQmFjay1FbmQgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiR29vZHllYXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoid0VrdmdmMVREWEhlTFhuQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=wEkvgf1TDXHeLXnAAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBCAYVy7uTrdLDQpgotuoohOvkG5JEdSjbmQC9KuPrlx-aefr_uuuuGx1MAJTmhf_SU5ONOHImcq0MOdDQhhsQHacmX2kTbHUGuWg9YiUXmpWCerLL81JzI86ycb-WeUgIVyxErjbj_MKie_XTfELY2EKcEtuQl_JVU7c4UAAAA&shmds=v1_AdeF8KgHgMxjKE_DbEzp4XIf1w6Su6mkNvoSraz2PSMk2_EXfA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=wEkvgf1TDXHeLXnAAAAAAA%3D%3D</td><td>null</td><td>Python Back-End Developer</td><td>Goodyear Jobs</td></tr><tr><td>List(Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=0c13aa90c829c820&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.com/job-listing/generative-ai-and-backend-developer-python-intellypod-JV_KO0,42_KE43,53.htm?jl=1009799100097&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Intellypod</td><td>Job Description (JD) For Gen Ai with Python:\n",
       "\n",
       "We're Hiring: GenAI & Backend Developer (Python)\n",
       "\n",
       "Work Location: Remote (Work From Home)\n",
       "\n",
       "Experience: 2+ Years\n",
       "\n",
       "Immediate Joiners Preferred\n",
       "\n",
       "Company: IntellyPod\n",
       "\n",
       "Apply at: hrd@intellypod.com | hr@intellypod.com\n",
       "\n",
       "About the Role:\n",
       "\n",
       "IntellyPod is looking for a passionate Backend Developer with hands-on experience in GenAI, LLMs, and Python. If you’re excited about building scalable APIs, integrating AI models, and working with the latest in backend and GenAI technologies — we’d love to connect!\n",
       "\n",
       "Key Responsibilities:\n",
       "\n",
       "· Develop and maintain Python-based backend services.\n",
       "\n",
       "· Design and implement RESTful APIs.\n",
       "\n",
       "· Integrate GenAI/LLM solutions into applications.\n",
       "\n",
       "· Manage and optimize SQL/NoSQL databases.\n",
       "\n",
       "· Collaborate with cross-functional tech teams.\n",
       "\n",
       "Must-Have Skills:\n",
       "\n",
       "· 2+ years of experience in backend development (Python).\n",
       "\n",
       "· Experience with GenAI and Large Language Models (e.g., GPT, LLaMA).\n",
       "\n",
       "· Strong knowledge of REST APIs and database design.\n",
       "\n",
       "· Familiarity with Git and backend architecture best practices.\n",
       "\n",
       "Need to Have:\n",
       "\n",
       "· Experience with AWS/GCP/Azure.\n",
       "\n",
       "· Docker, Kubernetes, or CI/CD exposure.\n",
       "\n",
       "· Familiarity with vector databases (e.g., Pinecone, FAISS).\n",
       "\n",
       "· Prompt engineering or LLM fine-tuning knowledge.\n",
       "\n",
       "Why Join Us?\n",
       "\n",
       "· 100% Remote – Flexible work setup\n",
       "\n",
       "· Work on next-gen AI products\n",
       "\n",
       "· Fast-growing, collaborative tech team\n",
       "\n",
       "· Opportunity to innovate with emerging AI tools\n",
       "\n",
       "Ready to build the future with us? Send your resume to: hrd@intellypod.com | hr@intellypod.com #GenAI #BackendDeveloper #PythonJobs #LLM #RemoteJob #HiringNow #IntellyPod\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: Up to ₹70,000.00 per month\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Application Question(s):\n",
       "• Are an immediate joiner -\n",
       "\n",
       "Are on notice period if yes [Then how many days]\n",
       "• Write YES or NO\n",
       "\n",
       "1) Need to ask have you worked on LLM based project -\n",
       "\n",
       "2) Have you worked on chatbot types apps -\n",
       "\n",
       "3) Have you strong knowleged of OOps and Python basic -\n",
       "\n",
       "4) Have you knowledge of Rest APi development -\n",
       "\n",
       "Experience:\n",
       "• 5G: 3 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>Map(posted_at -> 5 hours ago, work_from_home -> true, schedule_type -> Full-time)</td><td>List(5 hours ago, Work from home, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJHZW5lcmF0aXZlIEFJIFx1MDAyNiBCYWNrZW5kIERldmVsb3BlcihweXRob24pIiwiY29tcGFueV9uYW1lIjoiSW50ZWxseXBvZCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiI4ekp1Sk9ZUFlTbWtOcFRuQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Anywhere</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=8zJuJOYPYSmkNpTnAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFy7ujndJCrYiOBSFxWh1J8oaXo00XgXckdp_8MPVpc3vuKzKC41EmarYUS4NrCGm3UvpB7uOGLkhHmTZvVMW9jDgzsQtNl5YIKaeYi4OnvVJJUxIrEcRH-XKx2_DRN2PJknd_KnFW8zpmgV2-PpMJWJht2yIcUY58Q9BIKG-mC_XT6-kZcAAAA&shmds=v1_AdeF8KhFSQIHvN7Byhuea57DUPzza7X2-vxxKhhpa_88YSEXhg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=8zJuJOYPYSmkNpTnAAAAAA%3D%3D</td><td>null</td><td>Generative AI & Backend Developer(python)</td><td>Indeed</td></tr><tr><td>List(Map(title -> Jobs, link -> https://jobs.ashbyhq.com/dehazelabs/3fc045c8-c43d-4523-b258-80e35b2930ed/application?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/junior-python-developer-dehazelabs-JV_KO0,23_KE24,34.htm?jl=1009433325415&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Wellfound, link -> https://wellfound.com/jobs/3280338-junior-python-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In Pune, link -> https://builtinpune.in/job/junior-python-developer/6461539?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/junior-python-developer-delhi-at-gradxpert-4256193297?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Dive Into Python, link -> https://diveintopython.org/jobs/junior-python-developer-17232?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Salesforce, link -> https://phlex.my.salesforce-sites.com/recruit/fRecruit__ApplyJob?vacancyNo=VN838&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/junior-python-developer/6539360?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Dehazelabs</td><td>Location: Onsite, Kokapet, Hyderabad, Telangana.\n",
       "\n",
       "Job Type: Full-Time\n",
       "\n",
       "About Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n",
       "\n",
       "Responsibilities:\n",
       "• Develop and maintain Python applications and services\n",
       "• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n",
       "• Write clean, efficient, and well-documented code\n",
       "• Troubleshoot, debug, and optimize applications for performance and scalability\n",
       "• Participate in code reviews and maintain coding standards\n",
       "• Stay up-to-date with the latest industry trends and technologies\n",
       "\n",
       "Requirements:\n",
       "• Ability to code in Python and SQL\n",
       "• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n",
       "• Knowledge of version control systems (e.g., Git)\n",
       "• Excellent problem-solving skills and attention to detail\n",
       "• Strong communication and teamwork abilities.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n",
       "• Knowledge of RESTful APIs and microservices architecture.</td><td>Map(salary -> ₹216K–₹420K a year, work_from_home -> true, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(₹216K–₹420K a year, Work from home, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Anywhere</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=LYbcWfU4skOE2uhgAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQoCMQwAUFxvdXPKLNiK4KLrgXiTf3CkNVwrNSlNlNPRL1eXt7zus-jc8OAsDS4vS8LQ05OKVGqwgUECKGGLCX5zEpkKrY7JrOrBe9XiJjW0HF2UuxemILO_SdA_oyZsVAsajbv9dnaVp_Wyp4RvKhgUMsOZrxm_tV-eZYUAAAA&shmds=v1_AdeF8KiLjAS30ie4ZRkW1kEru4d1GwgKEP1yzRfISnv7NJRViQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=LYbcWfU4skOE2uhgAAAAAA%3D%3D</td><td>null</td><td>Junior Python Developer</td><td>Jobs</td></tr><tr><td>List(Map(title -> Recruit.net, link -> https://www.recruit.net/job/etl-developer-jobs/1EFDB05A4E7926F2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Vivid Resourcing</td><td>Job Title:\n",
       "Data Engineer / ETL Developer\n",
       "\n",
       "Location:\n",
       "US, remote from India\n",
       "\n",
       "Department:\n",
       "IT / Data & Analytics\n",
       "\n",
       "Reports To:\n",
       "Head of Data & Analytics\n",
       "\n",
       "Employment Type:\n",
       "Contract, 12 months\n",
       "\n",
       "Pay:\n",
       "$28-35 per hour payrolled or self-employed\n",
       "\n",
       "About the Role\n",
       "\n",
       "We are seeking a Data Engineer / ETL Developer to help design, build, and maintain data pipelines that support our internal reporting and analytics needs. This role will focus on integrating data from systems such as ERP, MES , and production databases into a centralized data platform to enable reliable and timely insights across the business. You will work alongside BI developers, IT staff, and business users to ensure the smooth flow of data and contribute to the foundation of a modern business intelligence environment.\n",
       "\n",
       "Key Responsibilities\n",
       "\n",
       "ETL & Data Pipeline Development\n",
       "• Develop and maintain ETL/ELT pipelines to move data from source systems (ERP, MES, SQL databases, flat files) into our centralized platform.\n",
       "• Use tools such as Azure Data Factory , SSIS , or similar to orchestrate and automate data workflows.\n",
       "\n",
       "Data Modeling & Integration\n",
       "• Assist in designing and building data models and data marts optimized for Power BI dashboards.\n",
       "• Support the creation and maintenance of dataflows and datasets in Power BI Service.\n",
       "\n",
       "Data Quality & Documentation\n",
       "• Implement data validation, transformation, and cleansing logic to ensure high-quality, reliable data.\n",
       "• Document data processes, business rules, and data mappings to support knowledge sharing and governance.\n",
       "\n",
       "Cross-Functional Collaboration\n",
       "• Work with internal teams (production, quality, operations, finance) to understand data requirements and reporting needs.\n",
       "• Collaborate with BI developers to ensure the data pipeline supports efficient and user-friendly reporting.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\n",
       "• 2+ years of experience in data engineering or ETL development roles.\n",
       "• Solid experience with SQL and ETL tools (e.G., Azure Data Factory , SSIS , Informatica , etc.).\n",
       "• Familiarity with Power BI , including working with datasets, dataflows, or basic DAX.\n",
       "• Understanding of data integration, transformation, and warehousing concepts.\n",
       "• Experience working with or integrating manufacturing systems (ERP, MES) is a strong plus.\n",
       "\n",
       "Preferred Skills\n",
       "• Experience with cloud data platforms (especially Microsoft Azure ).\n",
       "• Exposure to Python or scripting for automation.\n",
       "• Familiarity with data governance and documentation practices.\n",
       "• Experience with manufacturing environments or industrial data is beneficial.\n",
       "\n",
       "Soft Skills\n",
       "• Strong attention to detail and a logical, structured approach to problem-solving.\n",
       "• Willingness to learn and grow in a fast-paced environment.\n",
       "• Good communication and collaboration skills across technical and non-technical teams.\n",
       "• Proactive and solutions-oriented mindset.</td><td>Map(posted_at -> 14 hours ago, schedule_type -> Contractor)</td><td>List(14 hours ago, Contractor)</td><td>eyJqb2JfdGl0bGUiOiJFdGwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiVml2aWQgUmVzb3VyY2luZyIsImFkZHJlc3NfY2l0eSI6IkJpbGFzcHVyLCBDaGhhdHRpc2dhcmgsIEluZGlhIiwiaHRpZG9jaWQiOiJueDl3cW1oMXpfcnBSX2dFQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Bilaspur, Chhattisgarh, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=nx9wqmh1z_rpR_gEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOw7CMAwAULH2CEwemBA0gMQCGx8hGBlYK7e1EqMQR7Fb9SScF_GGV31n1eJqES40UpRMBdbwkBaUsHQBJMFNxEeaH4NZ1oNzqrH2amjc1Z18nCRqZXJvafVfowEL5YhGzW6_meqc_HL74pF7eJLKUDpOHjjBiSNqHsoKziGgGavHElZwTz3jD2cHAICZAAAA&shmds=v1_AdeF8KioJG0pd9n2Q01rAjESLIj0HIja7A9i_QyV6EIckPzASg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=nx9wqmh1z_rpR_gEAAAAAA%3D%3D</td><td>null</td><td>Etl Developer</td><td>Recruit.net</td></tr><tr><td>List(Map(title -> S&P Global, link -> https://careers.spglobal.com/jobs/316835?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/senior-etl-and-backend-developer-salesforce-sp-global-JV_IC2935226_KO0,43_KE44,53.htm?jl=1009760759089&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/EDeu4j5BnY0WMfXpapbv1XDfoizam8v2C0MIvUs-eAQ_RxwZzld-Sw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs, link -> https://spglobalcareers.dejobs.org/ahmedabad-ind/senior-etl-and-backend-developer/A24C5AF22D7C4078A2550A706263A7F5/job/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/senior-etl-and-backend-developer-salesforce-%C2%A0-at-s-p-global-4238481789?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Expertini, link -> https://in.expertini.com/jobs/job/senior-etl-and-backend-developer-ahmedabad-sp-global-spglobal-316835ahmedabadindia/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-3150-0be8b3a1a781f9d4e53e1881178412e0?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Talentify, link -> https://www.talentify.io/job/senior-software-developer-ahmedabad-gujarat-in-sp-global-316835?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>S&P Global</td><td>About the Role:\n",
       "\n",
       "Grade Level (for internal use):\n",
       "10\n",
       "\n",
       "Title: Senior ETL and Backend Developer (Salesforce)\n",
       "\n",
       "Job Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n",
       "\n",
       "The Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n",
       "\n",
       "The team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n",
       "\n",
       "The Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n",
       "\n",
       "Responsibilities:\n",
       "• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n",
       "• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n",
       "• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n",
       "• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n",
       "• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n",
       "• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\n",
       "Monitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n",
       "• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\n",
       "Qualifications:\n",
       "\n",
       "Basic Qualifications:\n",
       "• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n",
       "• A minimum of 8+ years of experience in software engineering & Architecture.\n",
       "• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n",
       "• A minimum of 3+ years of Salesforce development, administration/Integration.\n",
       "• Proficiency in Informatica PowerCenter and other ETL tools.\n",
       "• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n",
       "• Experience with Salesforce integration and administration.\n",
       "• Proficiency in backend development languages (e.g., Java, Python, C#).\n",
       "• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Ability to work independently and as part of a team.\n",
       "• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n",
       "• Experience with other ETL tools and data integration platforms.\n",
       "• Informatica Certified Professional\n",
       "Salesforce Certified Administrator or Developer\n",
       "• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n",
       "• Excellent problem solving, analytical and technical troubleshooting skills.\n",
       "• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n",
       "• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n",
       "\n",
       "About S&P Global Commodity Insights\n",
       "At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n",
       "\n",
       "We’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n",
       "\n",
       "S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n",
       "\n",
       "For more information, visit http://www.spglobal.com/commodity-insights.\n",
       "\n",
       "What’s In It For You?\n",
       "\n",
       "Our Purpose:\n",
       "\n",
       "Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n",
       "\n",
       "Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n",
       "\n",
       "Our People:\n",
       "\n",
       "We're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n",
       "\n",
       "From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n",
       "\n",
       "Our Values:\n",
       "\n",
       "Integrity, Discovery, Partnership\n",
       "\n",
       "At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n",
       "\n",
       "Benefits:\n",
       "\n",
       "We take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n",
       "\n",
       "Our benefits include:\n",
       "• Health & Wellness: Health care coverage designed for the mind and body.\n",
       "• Flexible Downtime: Generous time off helps keep you energized for your time on.\n",
       "• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n",
       "• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n",
       "• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n",
       "• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n",
       "\n",
       "For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n",
       "\n",
       "Global Hiring and Opportunity at S&P Global:\n",
       "\n",
       "At S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "Equal Opportunity Employer\n",
       "\n",
       "S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n",
       "\n",
       "If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n",
       "\n",
       "US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n",
       "\n",
       "Job ID: 316835\n",
       "Posted On: 2025-06-03\n",
       "Location: Hyderabad, Telangana, India</td><td>Map(posted_at -> 30 days ago, schedule_type -> Full-time)</td><td>List(30 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjJlUWpKemNOclM4MXR6QzhBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India (+1 other)</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=2eQjJzcNrS81tzC8AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXFsQrCMBAAUFz7CU43iZXaiuCim1Sq4iC0e7kkZ1uNdyEJUn_Kb1SX95LPJClr4kE8HJoLIBvYo37Q75JeZMWRh3mNlsJNvKYUlnAWBYHQ6x6EoRLpLE13fYwubIsiBJt3IWIcdK7lWQiTkrG4iwp_2tCjJ2cxUrverMbccbdI69kVKisKLQwMx7chjwpNBg1Z5A4ZMzixGfALm-qj5LEAAAA&shmds=v1_AdeF8KhDGHxAXJEjotU-fQxkvoBBqCk4nXlFRQEfCz-nQy1hiw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=2eQjJzcNrS81tzC8AAAAAA%3D%3D</td><td>null</td><td>Senior ETL and Backend Developer (Salesforce)</td><td>S&P Global</td></tr><tr><td>List(Map(title -> Recruit.net, link -> https://www.recruit.net/job/senior-etl-developer-jobs/644A9040253E845D?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Vivid Resourcing</td><td>Job Title:\n",
       "Senior Data Engineer / ETL Developer\n",
       "\n",
       "Location:\n",
       "US, from India\n",
       "\n",
       "Department:\n",
       "IT / Data & Analytics\n",
       "\n",
       "Reports To:\n",
       "Director of Data & Analytics\n",
       "\n",
       "Employment Type:\n",
       "Contract, 12 months\n",
       "\n",
       "Pay:\n",
       "$30-38 per hour payrolled or self-employed\n",
       "\n",
       "About the Role\n",
       "\n",
       "We are seeking a skilled and motivated Senior Data Engineer / ETL Developer to lead the design and implementation of end-to-end data integration solutions powering this new platform. You will work closely with IT, business stakeholders, and BI developers to build scalable data pipelines, robust data models, and a high-performance reporting environment centered around Power BI.\n",
       "\n",
       "This role is a key contributor in shaping our data infrastructure and delivering timely, accurate insights across the organization—from the shop floor to the boardroom.\n",
       "\n",
       "Key Responsibilities\n",
       "\n",
       "Data Engineering & Integration\n",
       "• Design, build, and maintain scalable ETL/ELT pipelines to extract, transform, and load data from diverse manufacturing systems (ERP, MES, IoT sensors, SQL databases, flat files, APIs).\n",
       "• Develop and manage data workflows using tools such as Azure Data Factory, SQL Server Integration Services (SSIS), or other modern data orchestration platforms.\n",
       "\n",
       "Platform & Architecture Support\n",
       "• Contribute to the design and architecture of the new internal analytics platform, ensuring flexibility, scalability, and cost-efficiency.\n",
       "• Design and optimize data lake and data warehouse solutions using cloud-native tools (preferably Microsoft Azure).\n",
       "\n",
       "Power BI Enablement\n",
       "• Build and optimize semantic data models, dataflows, and datasets for use in Power BI.\n",
       "• Collaborate with Power BI developers and business analysts to ensure data models meet reporting requirements, including DAX performance tuning and efficient data structure design.\n",
       "\n",
       "Data Governance & Quality\n",
       "• Implement data validation, cleansing, and monitoring processes to ensure data quality, reliability, and accuracy.\n",
       "• Contribute to data governance policies, including documentation, lineage tracking, and security controls.\n",
       "\n",
       "Collaboration & Mentorship\n",
       "• Work closely with cross-functional teams (IT, production, quality, finance, supply chain) to gather data requirements and deliver meaningful solutions.\n",
       "• Provide technical leadership and mentor junior developers or analysts in best practices for data engineering and analytics.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor’s or Master’s degree in Computer Science, Information Systems, Engineering, or a related field.\n",
       "• 5+ years of experience in data engineering, ETL development, or database architecture roles.\n",
       "• Proven experience with ETL tools such as Azure Data Factory, SSIS, or Informatica.\n",
       "• Advanced SQL skills, including performance tuning, stored procedures, and query optimization.\n",
       "• Strong experience with Power BI, including DAX, data modeling, and dataset optimization.\n",
       "• Experience with data warehousing (e.G., Azure Synapse, SQL Server, Snowflake) and cloud platforms (preferably Azure).\n",
       "• Understanding of manufacturing systems and data (e.G., ERP, MES, shop floor data, SCADA, historians).\n",
       "\n",
       "Preferred Skills\n",
       "• Experience with Python or other scripting languages for automation and data manipulation.\n",
       "• Familiarity with time-series data and integration from IoT or edge devices.\n",
       "• Knowledge of DevOps practices, CI/CD for data pipelines, and version control using Git.\n",
       "• Exposure to data governance frameworks and tools like Purview, Alation, or Collibra.\n",
       "• Power BI Service administration experience and integration with Power Platform (e.G., Power Automate, Power Apps)\n",
       "\n",
       "Key Competencies\n",
       "• Strong analytical and problem-solving skills with attention to detail.\n",
       "• Excellent communication skills, with the ability to bridge technical and business domains.\n",
       "• Self-starter with the ability to lead initiatives and work independently or collaboratively.\n",
       "• A passion for continuous improvement and innovation in a manufacturing setting.</td><td>Map(posted_at -> 14 hours ago, schedule_type -> Contractor)</td><td>List(14 hours ago, Contractor)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRXRsIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlZpdmlkIFJlc291cmNpbmciLCJhZGRyZXNzX2NpdHkiOiJCaWxhc3B1ciwgQ2hoYXR0aXNnYXJoLCBJbmRpYSIsImh0aWRvY2lkIjoiMnQxYXo2bnUzRU1abUlLLUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Bilaspur, Chhattisgarh, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=2t1az6nu3EMZmIK-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFz7CE43a21UcNHNH0RHBdeSpkdyEnMhdy19Gd9V_Iav-s6q5RMTcYGLRjjjiJEzFljBnTsQtMUF4ARXZh9xfgiqWfbGiMTGi1ol1zj-GE7Y8WTe3Mm_VoItmKNVbLe79dTk5BebF43UwwOFh-IoeaAER4pW8lBqOIVgVUm8LaGGW-rJ_gAo5AAooAAAAA&shmds=v1_AdeF8KgmUPxQQ3or2dZeuo-n_D7oSa5c3VpT-ZiKwta1oTJcNA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=2t1az6nu3EMZmIK-AAAAAA%3D%3D</td><td>null</td><td>Senior Etl Developer</td><td>Recruit.net</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/etl-developer-%E2%80%93-ibm-datastage-at-tata-consultancy-services-4256166032?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/4a2af5f029af9a8d94f2bdca1fd2d9e0?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-169-44aa45285a4aa3fccf6f03ea6fc9cf38?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobg8, link -> https://jobs.jobg8.com/jobs/etl-developer-ibm-datastage-in-hyderabad-andhra-pradesh-india/5673-2972096984D8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> WhatJobs, link -> https://en-in.whatjobs.com/jobs/software-developer?id=156858651&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/1046235732449951744?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Recruit.net, link -> https://www.recruit.net/job/etl-developer-ibm-datastage-jobs/C9D3D0220324FB4B?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Learn4Good, link -> https://www.learn4good.com/jobs/hyderabad/india/info_technology/4340112163/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Tata Consultancy Services</td><td>Job Title: ETL Developer – IBM DataStage\n",
       "\n",
       "Experience: 5 to 10 years\n",
       "\n",
       "Location: Hyderabad, Chennai, Mumbai, Bangalore, Ahmedabad, Indore\n",
       "\n",
       "Employment Type: Full-time\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "We are seeking a skilled ETL Developer with hands-on experience in IBM DataStage to join our dynamic data engineering team. The ideal candidate will have a strong background in ETL development, data warehousing concepts, and performance optimization. This role involves designing and implementing robust ETL solutions, collaborating with cross-functional teams, and ensuring high data quality and integrity.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Design, develop, and implement ETL processes using IBM DataStage.\n",
       "• Work across the full project lifecycle including low-level design, development, testing, and deployment.\n",
       "• Develop parallel jobs using various DataStage stages such as Copy, Join, Merge, Lookup, Funnel, Filter, Sort, Remove Duplicates, Aggregator, Change Capture, and Transformer.\n",
       "• Integrate data from heterogeneous sources including Oracle, SQL Server, and flat files.\n",
       "• Optimize and troubleshoot existing ETL processes for performance improvements.\n",
       "• Collaborate with data analysts and architects to understand business requirements and translate them into technical solutions.\n",
       "• Ensure data quality and integrity across multiple data sources.\n",
       "• Create and maintain technical documentation for ETL processes.\n",
       "• Participate in code reviews and adhere to ETL best practices.\n",
       "• Work in Agile environments and use tools like JIRA for tracking tasks and issues.\n",
       "• Demonstrate strong problem-solving skills and the ability to troubleshoot technical issues effectively.\n",
       "• Understand and support operational requirements as part of business delivery.\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with IBM DataStage for ETL development and migration.\n",
       "• Solid understanding of database and data warehousing concepts.\n",
       "• Proficiency in SQL and UNIX.\n",
       "• Experience working with large datasets and complex data transformations.\n",
       "• Familiarity with Agile methodologies and tools like JIRA.\n",
       "• Excellent communication and collaboration skills.</td><td>Map(posted_at -> 7 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(7 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIOKAkyBJQk0gRGF0YVN0YWdlIiwiY29tcGFueV9uYW1lIjoiVGF0YSBDb25zdWx0YW5jeSBTZXJ2aWNlcyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoibkdQWmp0dFNpdlVKUWVEYkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Hyderabad, Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=nGPZjttSivUJQeDbAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMOw6CQBAA0NhyBKupDYIx0Rjt_EQxWklPhmWyYNYZsrMa6LyDB_FOnkRtXvmi9yCa7_ITbOlBTlry8Hm-IFufYYsBLwEtwRiOUoISelODMOxFrKPhqg6h1WWaqrrEasDQmMTILRWmUrr0KqX-KbRGT63DQMV0NumSlu1okf962Ajr3QVk08OF_KMxpNAwHPqKPJZYxZCTQ7bIGEPGVYNfx2UgOrIAAAA&shmds=v1_AdeF8KhYUGEHFHlN-1EsA9uwB1Zzbk12mKgso4NzzISq5LIJHw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=nGPZjttSivUJQeDbAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f6b9fc5f72634052b6/images/b0a1e06c4ca31a9abdb7ee0d6152dbca2ffa1facb3dd8dd0abf7d1cc766aba59.jpeg</td><td>ETL Developer – IBM DataStage</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Insight Global, link -> https://insightglobal.com/jobs/find_a_job/job-411040?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> FirstDCS, link -> https://firstdcs.in/jobs/etl-developer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SmartRecruiters Job Search, link -> https://jobs.smartrecruiters.com/SQUIRCLEITCONSULTINGSERVICESPVTLTD/87728117-etl-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Hirist, link -> https://www.hirist.tech/j/etl-developer-1472601?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/etl-developer-cgi-JV_IC2865319_KO0,13_KE14,17.htm?jl=1009777977370&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/jTrE3sYd8ESsw_aMfgsEoiKUf2JLjAdYHf4v4N1Qrjh_vf6-R8mNrw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Iitjobs, link -> https://www.iitjobs.com/job/etl-developer-hyderabad-telangana-india-talent-join-77101?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://unisys.wd5.myworkdayjobs.com/zh-CN/External/job/Hyderabad-Waverock/ETL-Developer_REQ565515?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Insight Global</td><td>We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\n",
       "Responsibilities:\n",
       "Lead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\n",
       "Collaborate with developers to ensure a smooth transition and integration of the upgraded system.\n",
       "Enhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\n",
       "Provide support and guidance to the ETL development lead.\n",
       "Engage with end-users to ensure their needs are met during and after the upgrade process.\n",
       "Utilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\n",
       "Requirements:\n",
       "Minimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\n",
       "Proven experience with SQL Server upgrades, particularly from 2012 to 2022.\n",
       "Strong SQL Server skills, including building tables, stored procedures, views, and functions.\n",
       "Experience with .NET development is highly desirable to be leveraged for other projects\n",
       "Excellent problem-solving skills and attention to detail.\n",
       "Ability to work effectively in a team environment and communicate with end-users.\n",
       "Preferred Qualifications:\n",
       "Experience with SQL Server 2016.\n",
       "Experience with Oracle\n",
       "Familiarity with the latest features and enhancements in SQL Server 2022.\n",
       "Strong understanding of database architecture and migration strategies.\n",
       "\n",
       "We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>Map(schedule_type -> Contractor, qualifications -> No degree mentioned)</td><td>List(Contractor, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ikw2UEZlVW5YOU90VExsNnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=L6PFeUnX9OtTLl6yAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU43OEnbiOCiq1Irjt3LpT2SlHgXckHqb_jF4hte9d1Uu9vwhCu9KUqiDA08xIIS5smDMHQiLtL24ktJejZGNbZOC5YwtZO8jDBZWc0iVv-N6jFTilhoPJ4Oa5vY7ZueNThfoItiMUJguH9mymhxrmGgiOyQsYae54A__sKURpUAAAA&shmds=v1_AdeF8KiTpOWlgVKEBN36SqjGjguWk7ZGmxLJNUkotMGGo1lIug&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=L6PFeUnX9OtTLl6yAAAAAA%3D%3D</td><td>null</td><td>ETL Developer</td><td>Insight Global</td></tr><tr><td>List(Map(title -> Glassdoor, link -> https://www.glassdoor.com/job-listing/etl-developer-sonata-software-JV_IC2865319_KO0,13_KE14,29.htm?jl=1009683443468&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/VZteELnCIFig_PpewT1H4Xi550ZVJ7LxPCKfGowAKkMlN6qBv9WgZQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>sonataOne</td><td>Real-time data Ingestion, Streaming data, Kafka, AWS Cloud streaming tools, ETL, Semi-structured data formats like JSON, XML\n",
       "\n",
       "Tools: Talend, Kafka, AWS Event Bridge, Lamda and and Strong SQL & Python</td><td>Map(schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoic29uYXRhT25lIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=cEmBeIcvfhfX5HK2AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFz7CE43OIjURAQXXRV_EFy6l0t7JJF4F3JB6kP4zuI3fM131ixO3R2O9KYkmQqs4SYOlLAMAYThLOITzQ-h1qx7a1WT8VqxxsEM8rLC5GSyT3H6r9eAhXLCSv12t5lMZr9aqjBWfDBBZLh8RirocGyho4TskbGFK48Rf42DLLCQAAAA&shmds=v1_AdeF8KjYvob7TkJsNO7njAvFnKCDIIe6qNw9Ivsd5EiD_jwNFg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=cEmBeIcvfhfX5HK2AAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f6b9fc5f72634052b6/images/b0a1e06c4ca31a9ab5436031f86f8495670d9227c5f49d66c2ea8c68acaec26e.png</td><td>ETL Developer</td><td>Glassdoor</td></tr><tr><td>List(Map(title -> Hirist, link -> https://www.hirist.tech/j/epam-etl-developer-ssis-ssrs-1495428?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/jdp/777603120491457089?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/df2976441b528bff274847b68dd6d964?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Expertini, link -> https://in.expertini.com/jobs/job/epam-etl-developer-ssisssrs-hyderabad-epam-systems-india-private-limited-315-46314721/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobted, link -> https://www.jobted.in/job/49ed391f8cb580e15d52ff7e58ad3743?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/5315199303575142400?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>EPAM Systems India Private Limited</td><td>Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n",
       "\n",
       "Experience : 5+ Years\n",
       "\n",
       "Location : Hyderabad\n",
       "\n",
       "Job Description :\n",
       "\n",
       "We are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n",
       "\n",
       "You will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n",
       "\n",
       "Key Responsibilities :\n",
       "\n",
       "- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n",
       "\n",
       "- Develop, deploy, and maintain SSIS packages for ETL processes.\n",
       "\n",
       "- Work on data warehouse design, development, and maintenance.\n",
       "\n",
       "- Ensure high performance and reliability of data integration workflows.\n",
       "\n",
       "- Troubleshoot SQL queries and ETL issues; optimize database performance.\n",
       "\n",
       "- Collaborate with cross-functional teams to define data solutions.\n",
       "\n",
       "- Work with complex data structures including JSON and XML.\n",
       "\n",
       "- Understand and implement different data models (star, snowflake, etc.).</td><td>Map(schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IkVQQU0gU3lzdGVtcyBJbmRpYSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InZ6RDdVNWpsc2hzYS03eW5BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=vzD7U5jlshsa-7ynAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNQQrCMBBFcdsjuJq11EYEEXQlKFpREOO-TNshjaSZkAmiR_N2Vt183oMHP3uPsuXusjnDFHa3E2zpQY4DxcG1LrXS-qoHPnINQhibDtjDntk4Gq-7lIKslBJxhZGEyTZFw71iTzU_1Z1r-U4lHUYKDhNV88XsWQRvJpvfq35Jol6g9K1FuET7GCI42d4masF6OLxailhjm8ONHHqDHvN__gHRy8qtvAAAAA&shmds=v1_AdeF8Khi9AUFim2i7dQRHCiMZmDc-jtbDVoLgUBJzi2wIeCrhA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=vzD7U5jlshsa-7ynAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f6b9fc5f72634052b6/images/b0a1e06c4ca31a9abbdaf665a0013fc4549112c9395a972e123709ddc360f0b1.png</td><td>EPAM - ETL Developer - SSIS/SSRS</td><td>Hirist</td></tr><tr><td>List(Map(title -> Built In, link -> https://builtin.com/job/etl-developer-hands-microsoft-sql-ssis-etl-and-t-sql-pune-location/2942610?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Fiserv</td><td>Calling all innovators – find your future at Fiserv.\n",
       "\n",
       "We’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n",
       "\n",
       "Job Title\n",
       "\n",
       "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n",
       "\n",
       "Work Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n",
       "• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n",
       "• Participates in client kickoff activities including requirements and data gathering sessions.\n",
       "• Ability to analyze and migrate incoming external data into Fiserv solutions.\n",
       "• Strong working knowledge of ETL processes and best practices.\n",
       "• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n",
       "• Documents business processes and identifies opportunities for process redesign.\n",
       "• Reports system defects and identifies opportunities for system enhancements.\n",
       "• Performs as a technical consulting resource for new clients during the implementation process.\n",
       "• Supports several project managers during the pre and post go-live activities\n",
       "• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n",
       "• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n",
       "• Provides technical and analytical guidance to the project team.\n",
       "• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n",
       "• The position requires working in shifts, 2 PM to 11 PM IST\n",
       "\n",
       "What you will need to have:\n",
       "• Minimum of 5 – 10 years of experience in software development.\n",
       "• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n",
       "• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n",
       "• Self-starter who can work effectively, both independently and in a team environment.\n",
       "• Strong analytical, organizational, and problem-solving skills.\n",
       "• Excellent verbal and written communication skills.\n",
       "• Ability to maintain a professional attitude and demeanor in high pressure situations.\n",
       "• Ability to multi-task and manage multiple projects simultaneously.\n",
       "• Weekend/evening availability and support (10% - 25%).\n",
       "\n",
       "What would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n",
       "\n",
       "Thank you for considering employment with Fiserv. Please:\n",
       "• Apply using your legal name\n",
       "• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n",
       "\n",
       "What you should know about us:\n",
       "\n",
       "Fiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n",
       "\n",
       "Our commitment to Diversity and Inclusion:\n",
       "\n",
       "We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n",
       "\n",
       "Warning about fake job posts:\n",
       "\n",
       "Please be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n",
       "\n",
       "Any communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n",
       "\n",
       "If you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=j_liFo3VQHqrD7_BAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMuwrCQBQEUGzT2FtNqZKHCDba-opEUGIvm801WYl7w941-GF-oGszxXBmou8oqne3AlsaqOOeHKZHZWsBW5yNdiz88CivRYyyzMsYAccIArcktJghweVtCQVr5U1YJThxBSHldPt_OTA3HU02rfe9rLNMpEsb8QHrVPMrY0sVf7InV_KPu7TKUd8pT_flavFJe9vMx3sj5AYYi9zWRv0AHMymQbcAAAA&shmds=v1_AdeF8KhckDUoX9CilVCOaJqU36weofSX83Pgps7flcqZRoh66g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=j_liFo3VQHqrD7_BAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f6b9fc5f72634052b6/images/b0a1e06c4ca31a9a99b750505837edd957d2221342d6b3cee8ec7a41d0cbd4c8.png</td><td>ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location</td><td>Built In</td></tr><tr><td>List(Map(title -> Talent500, link -> https://talent500.com/jobs/hsbc/etl-developersenior-consultant-specialist-india-T500_HB_267031/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/148979890/fvy-143-etl-workflow-specialist-mumbai/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>HSBC</td><td>Some careers shine brighter than others.If you're looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.HSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.We are currently seeking an experienced professional to join our team in the role of Senior Consultant Specialist.In this role, you will:- Communicating effectively with senior stakeholders.- Work with a team of technologists, ensuring prioritization of tasks and supporting removal of blockers.- Working on burning the product backlog and oversee product demo's- Understanding complex business requirements and new technologies and understand how to influence teams to adopt new practice and embrace automation.- Managing of risks and issues\n",
       "\n",
       "Requirements\n",
       "• name : HSBC\n",
       "• location : India, IN</td><td>Map(schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyL1NlbmlvciBDb25zdWx0YW50IFNwZWNpYWxpc3QiLCJjb21wYW55X25hbWUiOiJIU0JDIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRIYmhtVUVOWmF3OGh3Q0lBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=THbhmUENZaw8hwCIAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFwLvoDTzYKJCC46aRV_cKt7ucajjcS7kDulb-Eri9_wVd9JtTveb3CgDyXJVHxDHKVALazvZMgGTaYQMUU1WMBVOlDCEgYQhpNIn2i2HcyybrxXTa5XQ4vBBXl5Yepk9E_p9F-rAxbKCY3a1Xo5usz9fHpu9jVEhgs_Iv4A6wTUP5IAAAA&shmds=v1_AdeF8KgnDfRiHj39w5Z-ucwcYFT3-FJ0Z_lyEKOroyTU0w9PgA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=THbhmUENZaw8hwCIAAAAAA%3D%3D</td><td>null</td><td>ETL Developer/Senior Consultant Specialist</td><td>Talent500</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/senior-informatica-developer-at-everestdx-inc-4258304922?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>EverestDX Inc</td><td>About the Company:\n",
       "\n",
       "Everest DX – We are a Digital Platform Services company, headquartered in Stamford. Our Platform/Solution includes Orchestration, Intelligent operations with BOTs’, AI-powered analytics for Enterprise IT. Our vision is to enable Digital Transformation for enterprises to deliver seamless customer experience, business efficiency and actionable insights through an integrated set of futuristic digital technologies.\n",
       "\n",
       "Digital Transformation Services - Specialized in Design, Build, Develop, Integrate, and Manage cloud solutions and modernize Data centers, build a Cloud-native application and migrate existing applications into secure, multi-cloud environments to support digital transformation. Our Digital\n",
       "\n",
       "Platform Services enable organizations to reduce IT resource requirements and improve productivity, in addition to lowering costs and speeding digital transformation.\n",
       "\n",
       "Digital Platform - Cloud Intelligent Management (CiM) - An Autonomous Hybrid Cloud Management Platform that works across multi-cloud environments. helps enterprise Digital Transformation get most out of the cloud strategy while reducing Cost, Risk and Speed.\n",
       "\n",
       "To know more please visit: http://www.everestdx.com\n",
       "\n",
       "Responsibilities:\n",
       "• Candidate should hands-on experience on ETL and SQL.\n",
       "• Design, develop, and optimize ETL workflows using Informatica PowerCenter.\n",
       "• Implement cloud-based ETL solutions using Informatica IDMC and IICS.\n",
       "• Should have expertise on all transformations in Power Center and IDMC/IICS.\n",
       "• Should have experience or knowledge on the PC to IICS migration using CDI PC tool or some other tool.\n",
       "• Lead data migration projects, transitioning data from on-premise to cloud environments.\n",
       "• Write complex SQL queries and perform data validation and transformation.\n",
       "• Conduct detailed data analysis to ensure accuracy and integrity of migrated data.\n",
       "• Troubleshoot and optimize ETL processes for performance and error handling.\n",
       "• Collaborate with cross-functional teams to gather requirements and design solutions.\n",
       "• Create and maintain documentation for ETL processes and system configurations.\n",
       "• Implement industry best practices for data integration and performance tuning.\n",
       "\n",
       "Required Skills:\n",
       "• Hands-on experience with Informatica Power Center, IDMC and IICS.\n",
       "• Strong expertise in writing complex SQL queries and database management.\n",
       "• Experience in data migration projects (on-premise to cloud).\n",
       "• Strong data analysis skills for large datasets and ensuring accuracy.\n",
       "• Solid understanding of ETL design & development concepts.\n",
       "• Familiarity with cloud platforms (AWS, Azure).\n",
       "• Experience with version control tools (e.g., Git) and deployment processes.\n",
       "\n",
       "Preferred Skills:\n",
       "• Experience with data lakes, data warehousing, or big data platforms.\n",
       "• Familiarity with Agile methodologies.\n",
       "• Knowledge of other ETL tools</td><td>Map(posted_at -> 2 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(2 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgSW5mb3JtYXRpY2EgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiRXZlcmVzdERYIEluYyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNDdnQ200aEh5aHpSWlhDOEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Hyderabad, Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=47gCm4hHyhzRZXC8AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNuw7CMAwAxdpPYPKMSoNALLAW8VhhYKuc1qRBqR3FUVX-hk-lLLecTld8F8XuTuwlwZVfkgbMvkWoaaQgkRKs4SYWlDC1PQjDWcQFWh77nKMejFENldP8z6pWBiNMVibzFqt_NNpjohgwU7Pdb6YqsluVp5ESaa6f87QFz3D5dJTQYlfCgwKyQ8Zylp3HH2BwtZmjAAAA&shmds=v1_AdeF8Kh2tiS2TqlfE8K6mB5AqUFgHR-MKCtcsj-IXlL7YHB-6g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=47gCm4hHyhzRZXC8AAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f6b9fc5f72634052b6/images/b0a1e06c4ca31a9aca07e30835060ab20487250180fd7e64e147a75b89f26c9d.jpeg</td><td>Senior Informatica Developer</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Iitjobs, link -> https://www.iitjobs.com/job/spark-engineer-usa-staffingine-llc-78519?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Staffingine LLC</td><td>- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(4 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3NsQrCMBAA0L1fIE63uAgmIrjoKCJKt35AucRrkhrvQi5DJ79ddHnr6z7dZihYX3DlkJiowg4e4kAJq48gDDeRkGl9jq0VPVmrmk3Qhi154-VthcnJYmdx-mPUiJVKxkbj4bhfTOGwXQ0Npyn9C-j7CySGOz8TfgHTIbRdgAAAAA&shmds=v1_AdeF8KjUl41s9rD9xNd7RywARJoJQ3Nel1qruGSZXxbjxJAYWA&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f765f7018a3bebbd7b/images/51c80805102136f7e4a0d0dc158f67fd721978ef95aab8a775788a042a09089d.jpeg</td><td>Spark Engineer</td><td>Iitjobs</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/data-engineer-spark-python-at-etelligens-technologies-4230192247?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/data-engineer-spark-python/etelligens-technologies/17263944?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/149564571/vq345-data-engineer-spark-python-pune/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Findjob24h.com, link -> https://in.findjob24h.com/it/big-data-engineer-sparkpython-job26347?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Etelligens Technologies</td><td>Job Description\n",
       "\n",
       "We are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n",
       "• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n",
       "• Perform data cleansing and migration from diverse sources to target systems.\n",
       "• Collaborate with stakeholders to understand requirements and convert them into technical\n",
       "\n",
       "solutions.\n",
       "• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n",
       "• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n",
       "• Support, troubleshoot, and maintain data pipelines and workflows.\n",
       "• Participate in all phases of software development lifecycle including unit testing, integration\n",
       "\n",
       "testing, and performance testing.\n",
       "• Contribute to the modernization of data platforms and analytics tools on Azure.\n",
       "• Ensure data quality and integrity across all pipelines and systems.\n",
       "\n",
       "Required Skills & Qualifications\n",
       "• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n",
       "• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n",
       "• 1+ year of experience in Python and Spark scripting.\n",
       "• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n",
       "• Good understanding of Data Warehousing (DWH) concepts.\n",
       "• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n",
       "• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n",
       "• Hands-on experience in data cleansing, transformation, and migration projects.\n",
       "• Ability to work independently and within a team environment.\n",
       "• Microsoft Certified : Azure Data Engineer Associate\n",
       "• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n",
       "\n",
       "Location : Artha SEZ, Greater Noida West\n",
       "\n",
       "(ref:hirist.tech)</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=Dc6DSNLMiNYCEl95AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHsQrCMBAAUFz7CU43CzaiuOhqEZ0E3cs1HklqvAu5G-p_-MHW5cFrvotmd0JD6DgkJqqwhnvB-nK3j0XhuVcZQAmrjzD_LBIyLY_RrOjBOdXcBjW05FsvbydMg0xulEH_9BqxUslo1G_3m6ktHFbQGeWcArHCg3xkyRISKSSGCz8T_gDku5DQlwAAAA&shmds=v1_AdeF8KhUS3kDcBE04kKw4L-SXFj_kSbNYqcLjnZpQzO2TV5B5A&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=Dc6DSNLMiNYCEl95AAAAAA%3D%3D</td><td>null</td><td>Data Engineer - Spark/Python</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> PyJobs, link -> https://www.pyjobs.com/job/staff-data-engineer-spark-python-hadoop-oMy9olMy?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Visa</td><td>Company Description\n",
       "\n",
       "Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n",
       "\n",
       "When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n",
       "\n",
       "Join Visa: A Network Working for Everyone.\n",
       "Job Description\n",
       "\n",
       "The Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n",
       "\n",
       "Are you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n",
       "\n",
       "As a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n",
       "\n",
       "Essential Functions\n",
       "• Work with manager and clients to fully understand business requirements and desired business outcomes\n",
       "• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n",
       "• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n",
       "• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n",
       "• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n",
       "• Execute data engineering projects ranging from small to large either individually or as part of a project team\n",
       "• Ensure project delivery within timelines and budget requirements\n",
       "• Provide coaching and mentoring to junior team members\n",
       "\n",
       "This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\n",
       "Qualifications\n",
       "\n",
       "• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n",
       "• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n",
       "• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n",
       "• Minimum of 4 years of hands-on expertise with Java or Scala\n",
       "• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n",
       "• Experience working in an Agile and Test Driven Development environment.\n",
       "• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n",
       "• Experience with SAS as a statistical package is preferred\n",
       "• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFwLvoDTjSq1EcFFN1H8mYSCa7m2aRKNdyF3Q30LH1n8hq_4TopDrTgMcERFOJELZG2GeZ0wv0q4f9QzlXDBnjktYAU3bkEs5s4DE5yZXbSzvVdNsjNGJFZOFDV0Vcdvw2RbHs2TW_nXiMdsU0S1zWa7HqtEbjl9BEEIBFfqA_4AG-QyopMAAAA&shmds=v1_AdeF8KjOBn1yJUpUCbIoZfiL9XOirLPsVpNYluZLK1C-IlMUqQ&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f765f7018a3bebbd7b/images/51c80805102136f775d6990ffc61dcc6da45082c8a36a2aa06bfab96684f1183.jpeg</td><td>Staff Data Engineer (Spark, Python, Hadoop)</td><td>PyJobs</td></tr><tr><td>List(Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=b95950f98d5b6678&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/databricks-engineer-spark-pyspark-enkefalos-technologies-llp-JV_KO0,33_KE34,60.htm?jl=1009780925122&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/6215885900103024640?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/153480629/c-941-data-engineer-etl-sql-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Enkefalos Technologies LLP</td><td>Databricks Engineer – Spark / PySpark\n",
       "\n",
       "Location : Remote / Mysore\n",
       "\n",
       "Joining : Immediate\n",
       "\n",
       "Experience : 5+ years\n",
       "\n",
       "Responsibilities :\n",
       "\n",
       "Will implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n",
       "\n",
       "Requirements:\n",
       "• PySpark (RDDs, DataFrames, performance tuning)\n",
       "• Building gold‐layer data models for financial reporting\n",
       "• Experience with complex joins, aggregations, GL hierarchies\n",
       "• Version handling (Actuals vs Budget), currency conversions\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500,395.35 - ₹1,840,348.25 per year\n",
       "\n",
       "Benefits:\n",
       "• Flexible schedule\n",
       "• Paid sick time\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Application Question(s):\n",
       "• Have you worked on ADF/ADLS ?\n",
       "• Do you have hands-on experience of Spark / PySpark\n",
       "\n",
       "Experience:\n",
       "• Databricks Engineering: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>Map(salary -> ₹500,395.34–₹1,840,348.20 a year, qualifications -> No degree mentioned, schedule_type -> Full-time, work_from_home -> true, posted_at -> 15 days ago)</td><td>List(15 days ago, ₹500,395.34–₹1,840,348.20 a year, Work from home, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Anywhere</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=k4Rb92L6YIBechULAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXGsQrCMBAAUFz7CU4HbkIbEVzU0SJKh4Luco1nEhvvQi5D_Rs_VcTl8arPrNofsOCQgx0VWnaBiTLUcEmYRzDQv_-r4SwDKGG2HoThKOIizXe-lKRbY1Rj47RgCbax8jLCNMhknjLoj5t6zJQiFrqtN6upSeyWi5ZHemAUhStZzxLFBVLouh4Cw4nvAb-C4XW0owAAAA&shmds=v1_AdeF8KizOswJH_SyEGgjpgJ7bj5Cwqu4B0GS78yPFj9XWFNoVQ&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=k4Rb92L6YIBechULAAAAAA%3D%3D</td><td>null</td><td>Databricks Engineer - Spark / PySpark</td><td>Indeed</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/senior-pyspark-data-engineer-big-data-cloud-data-solutions-python-at-synechron-4258672529?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Towards AI Jobs, link -> https://jobs.towardsai.net/job/nagarro-senior-staff-engineer-big-data-xrl0?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Synechron</td><td>Job Summary\n",
       "\n",
       "Synechron is seeking a skilled PySpark Data Engineer to design, develop, and optimize data processing solutions leveraging modern big data technologies. In this role, you will lead efforts to build scalable data pipelines, support data integration initiatives, and work closely with cross-functional teams to enable data-driven decision-making. Your expertise will contribute to enhancing business insights and operational efficiency, positioning Synechron as a pioneer in adopting emerging data technologies.\n",
       "\n",
       "Software Requirements\n",
       "\n",
       "Required Software Skills:\n",
       "• PySpark (Apache Spark with Python) – experience in developing data pipelines\n",
       "• Apache Spark ecosystem knowledge\n",
       "• Python programming (versions 3.7 or higher)\n",
       "• SQL and relational database management systems (e.g., PostgreSQL, MySQL)\n",
       "• Cloud platforms (preferably AWS or Azure)\n",
       "• Version control: GIT\n",
       "• Data workflow orchestration tools like Apache Airflow\n",
       "• Data management tools: SQL Developer or equivalent\n",
       "\n",
       "Preferred Software Skills:\n",
       "• Experience with Hadoop ecosystem components\n",
       "• Knowledge of containerization (Docker, Kubernetes)\n",
       "• Familiarity with data lake and data warehouse solutions (e.g., AWS S3, Redshift, Snowflake)\n",
       "• Monitoring and logging tools (e.g., Prometheus, Grafana)\n",
       "\n",
       "Overall Responsibilities\n",
       "• Lead the design and implementation of large-scale data processing solutions using PySpark and related technologies\n",
       "• Collaborate with data scientists, analysts, and business teams to understand data requirements and deliver scalable pipelines\n",
       "• Mentor junior team members on best practices in data engineering and emerging technologies\n",
       "• Evaluate new tools and methodologies to optimize data workflows and improve data quality\n",
       "• Ensure data solutions are robust, scalable, and aligned with organizational data governance policies\n",
       "• Stay informed on industry trends and technological advancements in big data and analytics\n",
       "• Support production environment stability and performance tuning of data pipelines\n",
       "• Drive innovative approaches to extract value from large and complex datasets\n",
       "\n",
       "Technical Skills (By Category)\n",
       "\n",
       "Programming Languages:\n",
       "• Required: Python (PySpark experience minimum 2 years)\n",
       "• Preferred: Scala (for Spark), SQL, Bash scripting\n",
       "\n",
       "Databases/Data Management:\n",
       "• Relational databases (PostgreSQL, MySQL)\n",
       "• Distributed storage solutions (HDFS, cloud object storage like S3 or Azure Blob Storage)\n",
       "• Data warehousing platforms (Snowflake, Redshift – preferred)\n",
       "\n",
       "Cloud Technologies:\n",
       "• Required: Experience deploying and managing data solutions on AWS or Azure\n",
       "• Preferred: Knowledge of cloud-native services like EMR, Data Factory, or Azure Data Lake\n",
       "\n",
       "Frameworks and Libraries:\n",
       "• Apache Spark (PySpark)\n",
       "• Airflow or similar orchestration tools\n",
       "• Data processing frameworks (Kafka, Spark Streaming – preferred)\n",
       "\n",
       "Development Tools and Methodologies:\n",
       "• Version control with GIT\n",
       "• Agile management tools: Jira, Confluence\n",
       "• Continuous integration/deployment pipelines (Jenkins, GitLab CI)\n",
       "\n",
       "Security Protocols:\n",
       "• Understanding of data security, access controls, and GDPR compliance in cloud environments\n",
       "\n",
       "Experience Requirements\n",
       "• Minimum of 5+ years in data engineering, with hands-on PySpark experience\n",
       "• Proven track record of developing, deploying, and maintaining scalable data pipelines\n",
       "• Experience working with data lakes, data warehouses, and cloud data services\n",
       "• Demonstrated leadership in projects involving big data technologies\n",
       "• Experience mentoring junior team members and collaborating across teams\n",
       "• Prior experience in financial, healthcare, or retail sectors is beneficial but not mandatory\n",
       "\n",
       "Day-to-Day Activities\n",
       "• Develop, optimize, and deploy big data pipelines using PySpark and related tools\n",
       "• Collaborate with data analysts, data scientists, and business teams to define data requirements\n",
       "• Conduct code reviews, troubleshoot pipeline issues, and optimize performance\n",
       "• Mentor junior team members on best practices and emerging technologies\n",
       "• Design solutions for data ingestion, transformation, and storage\n",
       "• Evaluate new tools and frameworks for continuous improvement\n",
       "• Maintain documentation, monitor system health, and ensure security compliance\n",
       "• Participate in sprint planning, daily stand-ups, and project retrospectives to align priorities\n",
       "\n",
       "Qualifications\n",
       "• Bachelor’s or Master’s degree in Computer Science, Information Technology, or related discipline\n",
       "• Relevant industry certifications (e.g., AWS Data Analytics, GCP Professional Data Engineer) preferred\n",
       "• Proven experience working with PySpark and big data ecosystems\n",
       "• Strong understanding of software development lifecycle and data governance standards\n",
       "• Commitment to continuous learning and professional development in data engineering technologies\n",
       "\n",
       "Professional Competencies\n",
       "• Analytical mindset and problem-solving acumen for complex data challenges\n",
       "• Effective leadership and team management skills\n",
       "• Excellent communication skills tailored to technical and non-technical audiences\n",
       "• Adaptability in fast-evolving technological landscapes\n",
       "• Strong organizational skills to prioritize tasks and manage multiple projects\n",
       "• Innovation-driven with a passion for leveraging emerging data technologies\n",
       "\n",
       "S YNECHRON’S DIVERSITY & INCLUSION STATEMENT\n",
       "\n",
       "Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.\n",
       "\n",
       "All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.\n",
       "\n",
       "Candidate Application Notice</td><td>Map(posted_at -> 1 day ago, schedule_type -> Full-time)</td><td>List(1 day ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgUHlTcGFyayBEYXRhIEVuZ2luZWVyIChCaWcgRGF0YSwgQ2xvdWQgRGF0YSBTb2x1dGlvbnMsIFx1MDAyNiBQeXRob24pIiwiY29tcGFueV9uYW1lIjoiU3luZWNocm9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjJYOEpqZUhEQmxiRndHQklBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=2X8JjeHDBlbFwGBIAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWMvQrCMBRGce3o6HQnUamtCC66-YPoJGS3pOklicZ7Q5JC-1i-oVWXbziH82XvUXYXSJYD3HrhZXjCUSYJJ9KWEAPM9lb_UA4Hx23z14JdmyxTzGE6hMkwzWEJV64hogzKABOcmbXDyc6k5OO2LGN0hY5JJqsKxa-SCWvuygfX8TtVNDKgdzJhtd6susKTXoxFT6hMGO4swYUaKz-GmguwtAAAAA&shmds=v1_AdeF8Kh_6lQF_zZmfGWNPm5MUF0xae5qgHPDxabaDnydWNTtyA&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=2X8JjeHDBlbFwGBIAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f765f7018a3bebbd7b/images/51c80805102136f792392bb11dadac654bb21ac87a136a842fd1b92d1d91915d.jpeg</td><td>Senior PySpark Data Engineer (Big Data, Cloud Data Solutions, & Python)</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Workday, link -> https://goto.wd5.myworkdayjobs.com/GoToCareers/job/Bangalore-KA-IN/Senior-Software-Engineer---Big-Data_R25-1293?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/senior-data-engineer-delta-lake-spark-unity-catalog-at-goto-4255173621?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/150484214/senior-sdet-engineer-analytics-kk159-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>306 - GoTo Technologies India Private Limited</td><td>Job Description Where you’ll work: India (Remote) Engineering at GoTo We’re the trailblazers of remote work technology. We build powerful, flexible work software that empowers everyone to live their best life, at work and beyond. And blaze even more trails along the way. There’s ample room for growth – so you can blaze your own trail here too. When you join a GoTo product team, you’ll take on a key role in this process and see your work be used by millions of users worldwide. Your Day to Day As a Senior Data Engineer, you would be: Design, develop, and maintain robust, scalable, and efficient ETL/ELT data pipelines to process structured and unstructured data from diverse sources. Architect and implement cloud-native data solutions using AWS services including S3, EMR, Lambda, and EKS. Build and optimize large-scale data processing workflows using Apache Spark and Databricks, with hands-on experience implementing and managing Delta Lake tables for high-volume batch and streaming use cases. Manage and govern data using Unity Catalog, maintaining strong data lineage, access controls, and metadata management. Develop and maintain data models (relational and dimensional) to support analytics, reporting, and machine learning use cases. Schedule, monitor, and orchestrate workflows using Apache Airflow or similar orchestration tools. Implement data quality checks, logging, monitoring, and alerting to ensure reliability and visibility of data pipelines. Collaborate with analysts, data scientists, and business stakeholders to deliver high-quality, trusted data for downstream applications and enable self-service BI tooling. Ensure adherence to best practices in data governance, security, and compliance. Mentor junior engineers and contribute to engineering standards, including CI/CD, automated testing, and documentation What We’re Looking For As an Senior Data Engineer, your background will look like: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. 5+ years of relevant experience in data engineering or software development, with a proven track record designing and maintaining production-grade data pipelines. Proficiency in Python programming and SQL for data transformation and analytics. Strong hands-on experience with Apache Spark and Databricks, including direct exposure to Delta Lake for data lake management, ACID transactions, schema enforcement and evolution, and time travel functionalities. Experience managing and organizing data access with Unity Catalog. In-depth experience with AWS services—specifically S3, EMR, Lambda, and EKS—and an excellent understanding of cloud-based data architecture and security best practices. Strong data modeling skills (dimensional, normalized) and understanding of data warehousing and lakehouse paradigms. Hands-on experience orchestrating workflows using Apache Airflow or similar orchestration frameworks. Familiarity with BI and data visualization tools (e.g., Tableau, Power BI). Experience establishing data quality processes, monitoring, and observability for pipelines. Exceptional communication, collaboration, and problem-solving skills, with the ability to thrive in an agile, multicultural team environment. At GoTo, authenticity and inclusive culture are key to our thriving workplace, where diverse perspectives drive innovation and growth. Our team of GoGetters is passionate about learning, exploring, and working together to achieve success while staying committed to delivering exceptional experiences for our customers. We take pride in supporting our employees with comprehensive benefits, wellness programs, and global opportunities for professional and personal development. By maintaining an inclusive environment, we empower our teams to do their best work, make a meaningful impact, and grow their career. Learn more. At GoTo, we’re bold enough to imagine a world of work without limits—where curiosity and AI-driven innovation fuel our constant growth. As the leader in cloud communications and IT, we solve real-world challenges through practical, cutting-edge solutions and an unwavering customer-first mindset. Our culture is rooted in inclusion, ownership, and transparency, fueling an environment where every voice contributes to both personal and collective achievement. Here, collaboration sparks bold ideas, and authenticity is celebrated—empowering you to adapt, evolve, and make a real impact. Join GoTo, and help shape the future of work while accelerating your own growth alongside exceptional people who are redefining what’s possible.</td><td>Map(posted_at -> 10 days ago, schedule_type -> Full-time)</td><td>List(10 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAoRGVsdGEgTGFrZSwgU3BhcmsgXHUwMDI2IFVuaXR5IENhdGFsb2cpIiwiY29tcGFueV9uYW1lIjoiMzA2IC0gR29UbyBUZWNobm9sb2dpZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlJxSFNWX2s0aVZkRWhBNGRBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=RqHSV_k4iVdEhA4dAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNwWoCQRBEydVPyKlOoqK7EtGDHlXEICioZ-ldm9nWsXuZGYL5wXyXI7kUPOpR1fn76OyPrGIBK0qEtTpR5oDein3mHd15iGNL4Y4uzirpF8ssenN9jPBtFSJTqBuYYmPmPH8umpTaOC_LGH3hYqIkdVHbozTlyp7lzar4jktsKHDrKfHlazp-Fq26wWwynuXhjZ0MJ64btXwlHLHVqxAOQX6yj508JPEVov_FC83n_Q_JAAAA&shmds=v1_AdeF8Kg5aF4RzwNE8-lh6QnTWkwzZMZ_FT7aljYehDjX-BkuNQ&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=RqHSV_k4iVdEhA4dAAAAAA%3D%3D</td><td>null</td><td>Senior Data Engineer (Delta Lake, Spark & Unity Catalog)</td><td>Workday</td></tr><tr><td>List(Map(title -> Jobgether, link -> https://jobgether.com/offer/66f5f6104841e0ee22e41ea5-cloud-data-engineer--spark-databricks?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Brighttier</td><td>This a Full Remote job, the offer is available from: India\n",
       "\n",
       "Job Title: Cloud Engineer – Spark/Databricks Specialist\n",
       "Location: Remote\n",
       "Job Type: Contract\n",
       "Industry: IT/Cloud Engineering\n",
       "Job Summary:\n",
       "We are looking for a highly skilled Cloud Engineer with a specialization in Apache Spark and Databricks to join our dynamic team. The ideal candidate will have extensive experience working with cloud platforms such as AWS, Azure, and GCP, and a deep understanding of data engineering, ETL processes, and cloud-native tools. Your primary responsibility will be to design, develop, and maintain scalable data pipelines using Spark and Databricks, while optimizing performance and ensuring data integrity across diverse environments.\n",
       "Key Responsibilities:\n",
       "Design and Development:\n",
       "• Architect, develop, and maintain scalable ETL pipelines using Databricks, Apache Spark (Scala, Python), and other cloud-native tools such as AWS Glue, Azure Data Factory, and GCP Dataflow.\n",
       "• Design and build data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n",
       "• Implement efficient data ingestion, transformation, and processing workflows with Spark and Databricks.\n",
       "• Optimize the performance of ETL processes for faster data processing and lower costs.\n",
       "• Develop and manage data pipelines using other ETL tools such as Informatica, SAP Data Intelligence, and others as needed.\n",
       "Data Integration and Management:\n",
       "• Integrate structured and unstructured data sources (relational databases, APIs, ERP systems) into the cloud data infrastructure.\n",
       "• Ensure data quality, validation, and integrity through rigorous testing.\n",
       "• Perform data extraction and integration from SAP or ERP systems, ensuring seamless data flow.\n",
       "Performance Optimization:\n",
       "• Monitor, troubleshoot, and enhance the performance of Spark/Databricks pipelines.\n",
       "• Implement best practices for data governance, security, and compliance across data workflows.\n",
       "Collaboration and Communication:\n",
       "• Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders, to define data requirements and deliver scalable solutions.\n",
       "• Provide technical guidance and recommendations on cloud data engineering processes and tools.\n",
       "Documentation and Maintenance:\n",
       "• Document data engineering solutions, ETL pipelines, and workflows.\n",
       "• Maintain and support existing data pipelines, ensuring they operate effectively and align with business goals.\n",
       "Qualifications:\n",
       "Education:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n",
       "Experience:\n",
       "• 7+ years of experience in cloud data engineering or similar roles.\n",
       "• Expertise in Apache Spark and Databricks for data processing.\n",
       "• Proven experience with cloud platforms like AWS, Azure, and GCP.\n",
       "• Experience with cloud-native ETL tools such as AWS Glue, Azure Data Factory, Kafka, GCP Dataflow, etc.\n",
       "• Hands-on experience with data platforms like Redshift, Snowflake, Azure Synapse, and BigQuery.\n",
       "• Experience in extracting data from SAP or ERP systems is preferred.\n",
       "• Strong programming skills in Python, Scala, or Java.\n",
       "• Proficient in SQL and query optimization techniques.\n",
       "Skills:\n",
       "• In-depth knowledge of Spark/Scala for high-performance data processing.\n",
       "• Strong understanding of data modeling, ETL/ELT processes, and data warehousing concepts.\n",
       "• Familiarity with data governance, security, and compliance best practices.\n",
       "• Excellent problem-solving, communication, and collaboration skills.\n",
       "Preferred Qualifications:\n",
       "• Certifications in cloud platforms (e.g., AWS Certified Data Analytics, Google Professional Data Engineer, Azure Data Engineer Associate).\n",
       "• Experience with CI/CD pipelines and DevOps practices for data engineering.\n",
       "• Exposure to Apache Hadoop, Kafka, or other data frameworks is a plus.\n",
       "\n",
       "This offer from \"Brighttier\" has been enriched by Jobgether.com and got a 74% flex score.</td><td>Map(work_from_home -> true, schedule_type -> Contractor)</td><td>List(Work from home, Contractor)</td><td>eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyLSBTcGFyayBcdTAwMjYgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJyaWdodHRpZXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTzdzcjNWUGpVa2l1R0VpZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Anywhere</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=O7sr3VPjUkiuGEigAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3MPQvCMBCAYVy7ujnd5CC0EcFFoYMfiK7-gHJJjzQ25sJdhP4Of7Efyzs8w1u9Z1V7jPzq4YQF4Zx8SERSwz2jjLD8s5XgRoUabmxBCcUNwAkuzD7SYj-UknVnjGpsvBYswTWOn4YTWZ7Mg63-0umAQjlioW6zXU9NTn41P0jw30EggZDgmvqAH8LOuxqVAAAA&shmds=v1_AdeF8KiYkYThhxCHaf21kIvz7iAYkbibpbTZilPvLl1NaSnZKQ&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=O7sr3VPjUkiuGEigAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f765f7018a3bebbd7b/images/51c80805102136f7288e0093cef41baca5f816735e5c318463cf69b421b2cd83.png</td><td>Cloud Data Engineer- Spark & Databricks</td><td>Jobgether</td></tr><tr><td>List(Map(title -> Workday, link -> https://citi.wd5.myworkdayjobs.com/en-US/2/job/Data-Engineer---AVP_24785645?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=481bb9445cf687c8&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Wellfound, link -> https://wellfound.com/jobs/2924371-data-engineer-intern-rolling-2027-28-grad?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Hirist, link -> https://www.hirist.tech/j/celonis-data-engineer-etlsql-10-12-yrs-1479538?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobgether, link -> https://jobgether.com/offer/67969bf4c58f91dcb9676b4e-ria---data-engineer-with-q-a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Foundit, link -> https://www.foundit.in/job/debut-infotech-data-engineer-python-scala-debut-infotech-pvt-ltd-india-34609162?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/fe79db0a9583dea16798df933003c829?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/data-engineer-2-5-yrs-exp-guardian-management-services-JV_KO0,25_KE26,54.htm?jl=1009784233452&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>12542 Citicorp Services India Private Limited</td><td>The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=lBQYlRnJjd-RRHF9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWLTQ_BQBBA4-onOM0RqS4NDpzqI0IcJE04yrQd7eja2exu8Nf8OxWXd3nvdT-d7mqDAWFrKjZEDvqZkddNY0MRZBZdE0F6yQYwgvR8anmQHDyhK2oQAzuRSlNvWYdg_UIp73Vc-YCBi7iQhxJDubzVXXL_w9XX6MhqDHRNZuN3bE01nE-S2TSBNbeTOAsZuScX5GFvSkY4OX62PRz5wYFKYPMXX3VpMJW8AAAA&shmds=v1_AdeF8Kis8dqR1eNIZfspzvlHRHi-cHVVT9hIYJVFIboxveOlTA&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=lBQYlRnJjd-RRHF9AAAAAA%3D%3D</td><td>null</td><td>Data Engineer (Snowflake, Spark, AWS) - AVP</td><td>Workday</td></tr><tr><td>List(Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=fd4bd619675010e2&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Adzuna, link -> https://www.adzuna.in/details/5082710850?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Siemens Healthineers</td><td>jobid\n",
       "• 460574\n",
       "\n",
       "jobfamily\n",
       "• Research & Development\n",
       "\n",
       "company\n",
       "• Siemens Healthcare Private Limited\n",
       "\n",
       "organization\n",
       "• Siemens Healthineers\n",
       "\n",
       "jobType\n",
       "• Full-time\n",
       "\n",
       "experienceLevel\n",
       "• Experienced Professional\n",
       "\n",
       "contractType\n",
       "• Permanent\n",
       "\n",
       "As a Data Engineer , you are required to:\n",
       "\n",
       "Design, build, and maintain data pipelines that efficiently process and transport data from various sources to storage systems or processing environments while ensuring data integrity, consistency, and accuracy across the entire data pipeline.\n",
       "\n",
       "Integrate data from different systems, often involving data cleaning, transformation (ETL), and validation. Design the structure of databases and data storage systems, including the design of schemas, tables, and relationships between datasets to enable efficient querying. Work closely with data scientists, analysts, and other stakeholders to understand their data needs and ensure that the data is structured in a way that makes it accessible and usable.\n",
       "\n",
       "Stay up-to-date with the latest trends and technologies in the data engineering space, such as new data storage solutions, processing frameworks, and cloud technologies. Evaluate and implement new tools to improve data engineering processes.\n",
       "\n",
       "Qualification : Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Science, Engineering is desirable.\n",
       "\n",
       "Experience level : At least 3 - 5 years hands-on experience in Data Engineering\n",
       "\n",
       "Desired Knowledge & Experience :\n",
       "• Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\n",
       "• Knowing Spark internals: Catalyst/Tungsten/Photon\n",
       "• Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\n",
       "• IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\n",
       "• Test: pytest, Great Expectations\n",
       "• CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\n",
       "• Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\n",
       "• Languages: Python/Functional Programming (FP)\n",
       "• SQL : TSQL/Spark SQL/HiveQL\n",
       "• Storage : Data Lake and Big Data Storage Design\n",
       "\n",
       "additionally it is helpful to know basics of:\n",
       "• Data Pipelines : ADF/Synapse Pipelines/Oozie/Airflow\n",
       "• Languages: Scala, Java\n",
       "• NoSQL : Cosmos, Mongo, Cassandra\n",
       "• Cubes : SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\n",
       "• SQL Server : TSQL, Stored Procedures\n",
       "• Hadoop : HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\n",
       "• Data Catalog : Azure Purview, Apache Atlas, Informatica\n",
       "\n",
       "Required Soft skills & Other Capabilities :\n",
       "\n",
       "Great attention to detail and good analytical abilities.\n",
       "\n",
       "Good planning and organizational skills\n",
       "\n",
       "Collaborative approach to sharing ideas and finding solutions\n",
       "\n",
       "Ability to work independently and also in a global team environment.</td><td>Map(posted_at -> 8 days ago, schedule_type -> Full-time)</td><td>List(8 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVB5dGhvbixQeVNwYXJrLFNRTCAsU3BhcmsgQXJjaGl0ZWN0dXJlLEF6dXJlIERhdGFicmlja3MiLCJjb21wYW55X25hbWUiOiJTaWVtZW5zIEhlYWx0aGluZWVycyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJXVkxrZFF1QU4tSGFXQjIzQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=WVLkdQuAN-HaWB23AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2OuwrCQBBFsfUTBGFqiYkINloJig8sImmFMFmH7GqcCTsjGD_Nr1PTHG5xLpzhZzC8bNAQtlwHJorTvDMvnORd0WK8J8X5BEk_YR2dD0bOnpGS9ftH-F-rGNxdYQpHqUAJfxYIw06kbmi08matLrNMtUlrNbTgUiePTJgqeWU3qfSPUj1Gahs0KueL2SttuZ6Mi0APYoU9YWO-D1QIDAe-BvwCc1oX1b0AAAA&shmds=v1_AdeF8KgUpMOygYFmUQXSA0j_a1MHh8_TRHWepqUL3g9i4LSfLw&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=WVLkdQuAN-HaWB23AAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540f765f7018a3bebbd7b/images/51c80805102136f7f2857876d96d4a19482a79d8d87bbea86705b552fa82bf54.jpeg</td><td>Data Engineer-Python,PySpark,SQL ,Spark Architecture,Azure Databricks</td><td>Indeed</td></tr><tr><td>List(Map(title -> Antal Tech Jobs, link -> https://www.antaltechjobs.in/job/cloud-data-engineer-spark-databricks-1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-3150-67d1a486ddf45f6afb09b190cee713d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/8580629334899818496?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Antal Job Board</td><td>Vacancy No\n",
       "VN1228\n",
       "\n",
       "Business Unit\n",
       "EMEA\n",
       "\n",
       "Job Location\n",
       "India\n",
       "\n",
       "Employment Type\n",
       "Full Time\n",
       "\n",
       "Job Details and Responsibilities\n",
       "We are seeking an experienced Cloud Data Engineer with a strong background in AWS, Azure, and GCP. The ideal candidate will have extensive experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, and other ETL tools like Informatica, SAP Data Intelligence, etc. You will be responsible for designing, implementing, and maintaining robust data pipelines and building scalable data lakes. Experience with various data platforms like Redshift, Snowflake, Databricks, Synapse, Snowflake and others is essential. Familiarity with data extraction from SAP or ERP systems is a plus.\n",
       "\n",
       "Key Responsibilities:\n",
       "\n",
       "Design and Development:\n",
       "• Design, develop, and maintain scalable ETL pipelines using cloud-native tools (AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.).\n",
       "• Architect and implement data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n",
       "• Develop and optimize data ingestion, transformation, and loading processes using Databricks, Snowflake, Redshift, BigQuery and Azure Synapse.\n",
       "• Implement ETL processes using tools like Informatica, SAP Data Intelligence, and others.\n",
       "• Develop and optimize data processing jobs using Spark Scala.\n",
       "Data Integration and Management:\n",
       "• Integrate various data sources, including relational databases, APIs, unstructured data, and ERP systems into the data lake.\n",
       "• Ensure data quality and integrity through rigorous testing and validation.\n",
       "• Perform data extraction from SAP or ERP systems when necessary.\n",
       "Performance Optimization:\n",
       "• Monitor and optimize the performance of data pipelines and ETL processes.\n",
       "• Implement best practices for data management, including data governance, security, and compliance.\n",
       "Collaboration and Communication:\n",
       "• Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\n",
       "• Collaborate with cross-functional teams to design and implement data solutions that meet business needs.\n",
       "Documentation and Maintenance:\n",
       "• Document technical solutions, processes, and workflows.\n",
       "• Maintain and troubleshoot existing ETL pipelines and data integrations.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Education:\n",
       "\n",
       "Bachelor’s degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n",
       "\n",
       "Experience:\n",
       "• 7+ years of experience as a Data Engineer or in a similar role.\n",
       "• Proven experience with cloud platforms: AWS, Azure, and GCP.\n",
       "• Hands-on experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.\n",
       "• Experience with other ETL tools like Informatica, SAP Data Intelligence, etc.\n",
       "• Experience in building and managing data lakes and data warehouses.\n",
       "• Proficiency with data platforms like Redshift, Snowflake, BigQuery, Databricks, and Azure Synapse.\n",
       "• Experience with data extraction from SAP or ERP systems is a plus.\n",
       "• Strong experience with Spark and Scala for data processing.\n",
       "\n",
       "Skills:\n",
       "• Strong programming skills in Python, Java, or Scala.\n",
       "• Proficient in SQL and query optimization techniques.\n",
       "• Familiarity with data modeling, ETL/ELT processes, and data warehousing concepts.\n",
       "• Knowledge of data governance, security, and compliance best practices.\n",
       "• Excellent problem-solving and analytical skills.\n",
       "• Strong communication and collaboration skills.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with other data tools and technologies such as Apache Spark, or Hadoop.\n",
       "• Certifications in cloud platforms (AWS Certified Data Analytics – Specialty, Google Professional Data Engineer, Microsoft Certified: Azure Data Engineer Associate).\n",
       "• Experience with CI/CD pipelines and DevOps practices for data engineering\n",
       "• Selected applicant will be subject to a background investigation, which will be conducted and the results of which will be used in compliance with applicable law.\n",
       "\n",
       "What we offer in return:\n",
       "• Remote Working: Lemongrass always has been and always will offer 100% remote work\n",
       "• Flexibility: Work where and when you like most of the time\n",
       "• Training: A subscription to A Cloud Guru and generous budget for taking certifications and other resources you’ll find helpful\n",
       "• State of the art tech: An opportunity to learn and run the latest industry standard tools\n",
       "• Team: Colleagues who will challenge you giving the chance to learn from them and them from you\n",
       "\n",
       "Lemongrass Consulting is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate on the basis of race, religion, color, national origin, religious creed, gender, sexual orientation, gender identity, gender expression, age, genetic information, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics\n",
       "\n",
       "About Lemongrass\n",
       "Lemongrass (lemongrasscloud.com) is a global leader in SAP consulting, focused on helping organizations transform their business processes through innovative solutions and technologies. With a strong commitment to customer success, Lemongrass partners with companies to drive their digital transformation journeys, enabling them to unlock the full potential of their SAP investments.\n",
       "\n",
       "We do this with our continuous innovation, automation, migration and operation, delivered on the world's most comprehensive cloud platforms – AWS, Azure and GCP and SAP Cloud ERP. We have been working with AWS and SAP since 2010 and we are a Premier Amazon Partner Network (APN) Consulting Partner. We are also a Microsoft Gold Partner, a Google Cloud Partner and an SAP Certified Silver Partner.\n",
       "\n",
       "Our team is what makes Lemongrass exceptional and why we have the excellent reputation in the market that we enjoy today. At Lemongrass, you will work with the smartest and most motivated people in the business. We take pride in our culture of innovation and collaboration that drives us to deliver exceptional benefits to our clients every day.</td><td>Map(posted_at -> 28 days ago, schedule_type -> Full-time)</td><td>List(28 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyIChTcGFyay9EYXRhYnJpY2tzKSIsImNvbXBhbnlfbmFtZSI6IkFudGFsIEpvYiBCb2FyZCIsImFkZHJlc3NfY2l0eSI6Ik5hZ3B1ciwgTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJCQXpseTdfU0lsQjByTmF2QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Nagpur, Maharashtra, India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=BAzly7_SIlB0rNavAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2MSwrCMBQAcdsjuHpLlbYRwY3iwh-ioBsPUF7SkMTGvJCXQu_kJbVuZjEwU3wmxe7oqW_hhBnhHIwLWieYPSOmToxSJqc6nkMFN5LAGpOyQAEuRMbr6dbmHHkjBLOvDWfMTtWK3oKCljSIF0ke0bDFpKPHrJvVejnUMZhFtQ8Z_X98IEwtuAAPNLFPJdzxFyDbnLCEa2gdfgGeXekBrgAAAA&shmds=v1_AdeF8KjAGOLnsK3rAfqRDABw5KppbKxIXlgUH4D1KfdIwL1zUw&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=BAzly7_SIlB0rNavAAAAAA%3D%3D</td><td>null</td><td>Cloud Data Engineer (Spark/Databricks)</td><td>Antal Tech Jobs</td></tr><tr><td>List(Map(title -> BMS Careers - Bristol Myers Squibb, link -> https://jobs.bms.com/careers/job/137468822836-data-analyst-iii-hyderabad-ts-in?domain=bms.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://bristolmyerssquibb.wd5.myworkdayjobs.com/en-US/BMS/job/Hyderabad---TS---IN/Data-Analyst-III_R1593008?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/data-analyst-iii-at-bristol-myers-squibb-4259495625?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/data-analyst-iii-bristol-myers-squibb-JV_IC2865319_KO0,16_KE17,37.htm?jl=1009795686545&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/data-analyst-iii/6537119?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobaaj, link -> https://www.jobaaj.com/job/bristol-myers-squibb-data-analyst-iii-hyderabad-telangana-india-5-to-7-years-356224?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/J-Cd6cs1eJL1KIfnTPhq8N20XLmjDINnr97u-VtK5tbrR3KwMhLnJA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.com/job-listing/data-analyst-iii-bristol-myers-squibb-JV_IC2865319_KO0,16_KE17,37.htm?jl=1009795686545&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Bristol Myers Squibb</td><td>Working with Us\n",
       "Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n",
       "\n",
       "Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n",
       "\n",
       "The US Commercial Analytics, Market Access Data Analyst will play a crucial role in supporting data-driven decision-making processes, generating insights, and providing strategic guidance to optimize our Market Access & Pricing activities in the US. This position requires a deep expertise in secondary analytics and statistical analytic techniques, along with a firm grasp of the US healthcare landscape, especially market access and pricing dynamics. The Business Analyst III will typically be responsible for leading support for multiple therapeutic areas in Market Access and will collaborate with cross-functional teams to enhance understanding of business performance and drive growth.\n",
       "\n",
       "Roles and Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Perform statistical analyses, data mining and predictive modelling to uncover trends and drivers of performance\n",
       "• Collaborate with stakeholders (US based Market Access Business Insights & Analytics team and US Market Access team members) to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects including payer, PBM and patient level subnational & claims data analyses, formulary and policy level analysis, and ad-hoc analytics based on secondary data to support US Market Access strategy\n",
       "• Prepare reports, dashboards and presentations to communicate findings, insights and recommendations to stakeholders\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up-to-date with industry trends (US access and health care landscape), best practices, and emerging technologies\n",
       "• Partner with US based Market Access BIA/Commercial team members (for a given project scope)\n",
       "• Provide overall project oversight and training/mentorship to junior analysts and team members as required\n",
       "• Supports culture of fact-based decision making through application of best-in-class, innovative, and appropriate methodologies to address key business questions\n",
       "\n",
       "Skills & Competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions, around US pricing and reimbursement strategies\n",
       "• Proficiency in statistical analysis techniques and applications of those to US commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and strategic thinking, with the ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome Candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• 5-8 years prior Pharmaceutical industry experience with 1-2 years within US Market Access & Pricing Analytics\n",
       "• Experience working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims, medical policy and formulary data), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with reimbursement and regulatory requirements and compliance in the US biopharma industry\n",
       "• Certification or training in relevant analytics or business intelligence tools is a plus\n",
       "\n",
       "If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n",
       "\n",
       "Uniquely Interesting Work, Life-changing Careers\n",
       "With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n",
       "\n",
       "On-site Protocol\n",
       "\n",
       "BMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n",
       "\n",
       "Site-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n",
       "\n",
       "BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n",
       "\n",
       "BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n",
       "\n",
       "BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n",
       "\n",
       "If you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n",
       "\n",
       "Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.</td><td>Map(posted_at -> 3 days ago, schedule_type -> Full-time)</td><td>List(3 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUlJIiwiY29tcGFueV9uYW1lIjoiQnJpc3RvbCBNeWVycyBTcXVpYmIiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZHU3JGRGo4OTcybHdnQWpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=VGSrFDj8972lwgAjAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCQAwAUFz7CU7ZBNGeKC46KYJWcNK95NpwPTmTeonQfos_K77hFd9JMTuhIRwY06gGVVXBEq7iQQlz04EwnEVCoum-M-t155xqKoMaWmzKRl5OmLwM7ile_9XaYaY-oVG93q6Gsucw3xxzVJMEt5Gywv39id5DZLiMLWX02C7gQQk5IOMCKm4j_gDVdrxangAAAA&shmds=v1_AdeF8KimfTfXd_kjZHggUmeLzGSdHyGfC3ozzFChVrXaynaLjA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=VGSrFDj8972lwgAjAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540fc1f758ae0067cfba0/images/fada94f4e909550f217f2561801770bc1323d74230147da3ed24178c6aa37305.png</td><td>Data Analyst III</td><td>BMS Careers - Bristol Myers Squibb</td></tr><tr><td>List(Map(title -> Novartis, link -> https://www.novartis.com/careers/career-search/job/details/req-10045492-data-science-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobvite, link -> https://jobs.jobvite.com/ispschools/job/ovzqwfwK?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Iitjobs, link -> https://www.iitjobs.com/job/data-science-analyst-usa-tech-inspiron-90887?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/4b491748d6b9a8ee939c83831871113d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Apna, link -> https://apna.co/job/bengaluru/data-science-analyst-1504543814?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Unstop, link -> https://unstop.com/jobs/data-science-analyst-infogain-1507456?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/data-science-analyst/3295763?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Adzuna, link -> https://www.adzuna.in/details/5260944043?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>IN10 (FCRS = IN010) Novartis Healthcare Private Limited</td><td>About the Role:Within Insights and Commercial Solutions, below are the key service groups:  Marketing Effectiveness (Business Analytics, Forecasting, Competitive Intelligence)  Field Force Operations  Commercial Consulting  Brand Insights  Patient access services (Pricing and market access, etc.)  Primary Market Research Provide analytics support to Novartis internal customers (CPOs & Regional marketing and sales teams) on various projects • Support and facilitate data enabled decision making for Novartis internal customers using data analysis and data science techniques/methods on internal and external (3rd party) data to solve business problems • Support I&CS business in building capabilities by involving in various initiatives like knowledge sharing, on-boarding and training support, support in all business-related tasks/activities, building process documentation and knowledge repositories.Analyst, Data ScienceLocation – Hyderabad #LI Hybrid Key Responsibilities:Delivering projects and managing internal customer expectations across multi-channel marketing analytics, Portfolio Analytics, Targeting and Segmentation, Predictive Analytics, Resource Allocation and Optimization, Other ad-hoc requests.Create and deliver customer requirements as agreed SLAs (timeliness, accuracy, quality, etc.) and drive excellent customer satisfaction. Deliver advanced analytical and statistical solutions for various projects related to promotion evaluation, multi-channel marketing (MCM) campaign design, return on investment (ROI) analysis, resource allocation, segmentation, targeting, and other ad-hoc business questionsSupport exploratory research to identify new areas of application of advanced analytics/data science in providing enhanced decision-making support. Deliver services through a structured project management approach with appropriate documentation and communication throughout the delivery of servicesAutomation of project codes and development of front-end delivery solutions. Support in creation and maintenance of standard operating procedures (SOPs), quality checklists that will enable excellent quality outputs within the function.Support in developing and maintaining knowledge repositories that captures qualitative and quantitative reports of brands, disease areas, macroeconomic trends of Novartis operating markets, etc.Comply with all internal functional operating procedures like time tracking, KPI tracking and reporting, and other internal systems and processes. Comply to all Novartis operating procedures as per legal/IT/HR requirementsEssential Requirements:Technical Skills: Statistical Modeling Experience (Covering but not limited to, Regression, Classification, Supervised and Unsupervised Learning Methods, Optimization techniques, Predictive Models, Deep learning Models, e.g. Neural Networks, Natural Language Processing, Text Mining etc.)Programming languages/tools – R, SAS, Python, SQLVisualization tools – Qlikview, Qliksense, TableauWorking knowledge of MS-Office (MS Excel, VBA, PowerPoint, Access)Pharmaceutical industry domain/datasets knowledge – desirableEducation:Graduation/Post Graduation in Business Administration, Mathematics, Economics, Statistics, Engineering or Quantitative streamsExperience: Experience (Minimum 2years) in analytics in marketing analytics, experience in pharma industry is preferableShould have strong quantitative and systems backgroundStrong analytical thinking with problem solving approachShould have good ability to understand new data sources in short timeframe and embed them into standard analysesA good understanding of pharmaceutical domain and data would be ideal. Should have worked in an international company with exposure to working in a cross-cultural environmentStrong and proactive business results focus, and proven ability to provide insights that increase productivityWhy Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook. https://www.novartis.com/careers/benefits-rewards Commitment to Diversity and Inclusion: Novartis is committed to building an outstanding, inclusive work environment and diverse teams' representative of the patients and communities we serve. Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network.</td><td>Map(posted_at -> 7 days ago, schedule_type -> Full-time)</td><td>List(7 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVuY2UgQW5hbHlzdCIsImNvbXBhbnlfbmFtZSI6IklOMTAgKEZDUlMgPSBJTjAxMCkgTm92YXJ0aXMgSGVhbHRoY2FyZSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiejA4Y3NEMUZUdmdvZExfWkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=z08csD1FTvgodL_ZAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLzQqCQBAAYLr6CJ3m2A-pBV2KoCgqIyTyAWRcB91Yd8QZxN6ph6wu3-0LPqNgfkJFyIwlbwgOHt1bFBZw4wKEsDM1sIcLc-VovK1VW9lEkYgLK1FUa0LDTcSeCh6iFxfyJ5caO2odKuWrdTyEra9m-yRdxjA5H58Z7CBJ42U8hZR77NQKXAmd1ubX4NHZ_jfhbhurVIL1kPjS4hdRPmF7rwAAAA&shmds=v1_AdeF8KitnZKbCILU3LI82mL07zTe7Dv_v83dUFqGICYpHkWx8w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=z08csD1FTvgodL_ZAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540fc1f758ae0067cfba0/images/fada94f4e909550f21c750561ff1b21c265ac6278579d4b78e5ef5cb44208d2c.png</td><td>Data Science Analyst</td><td>Novartis</td></tr><tr><td>List(Map(title -> Glassdoor, link -> https://www.glassdoor.com/job-listing/data-sr-modeler-data-analyst-immediate-joiner-the-talent-quest-JV_IC2865319_KO0,45_KE46,62.htm?jl=1009798098764&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/s9r4eMpYNfeYtngj8mAjRCkDSPZL87Zzwc5Y_lpLOO4rT9VOfcRAZw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>The Talent Quest</td><td>Vacancy with a company focused on digital transformation, specializing in intelligent automation, digitalization, data science & analytics, and mobile enablement. They help businesses improve cost efficiency, productivity, and agility by reducing turnaround time and errors. The company provides services and solutions including operations digital transformation consulting, next-gen shared services setup consulting, cognitive RPA deployment, and AI-enabled CX enhancement. Founded in 2020 ;with HQ in Gurugram, India; the Company is now operating from Noida, Mumbai, Hyderabad, and Bengaluru as well.\n",
       "\n",
       "Job Role:We are seeking a highly skilled and detail-oriented Data Modeler to join our Data\n",
       "\n",
       "Management team. The ideal candidate will be responsible for designing and\n",
       "\n",
       "implementing logical and physical data models to support enterprise data\n",
       "\n",
       "initiatives. This role requires close collaboration with business stakeholders, data\n",
       "\n",
       "architects, and engineers to ensure data is structured and accessible for analytics,\n",
       "\n",
       "reporting, and operational needs.\n",
       "\n",
       "The successful candidate will:\n",
       "\n",
       "Provides technical expertise in needs identification, data modelling, data\n",
       "\n",
       "movement and transformation mapping (source to target), automation and testing\n",
       "\n",
       "strategies, translating business needs into technical solutions with adherence to\n",
       "\n",
       "established data guidelines and approaches from a business unit or project\n",
       "\n",
       "perspective.\n",
       "\n",
       "7-10 Years industry implementation experience with one or more data\n",
       "\n",
       "modelling tools such as Erwin, ERStudio, PowerDesigner etc.\n",
       "\n",
       " Minimum of 8 years of data architecture, data modelling or similar\n",
       "\n",
       "experience\n",
       "\n",
       " 5-7 years of management experience required\n",
       "\n",
       " 5-7 years consulting experience preferred\n",
       "\n",
       " Experience working with dimensionally modelled data\n",
       "\n",
       " Bachelor’s degree or equivalent experience, Master’s Degree Preferred\n",
       "\n",
       " Understanding of cloud (Azure, AWS, GCP, Snowflake preferred) and on\n",
       "\n",
       "premises architectures\n",
       "\n",
       "Job Types: Full-time, Permanent\n",
       "\n",
       "Pay: Up to ₹3,000,000.00 per year\n",
       "\n",
       "Benefits:\n",
       "• Cell phone reimbursement\n",
       "• Internet reimbursement\n",
       "• Life insurance\n",
       "• Paid sick time\n",
       "• Paid time off\n",
       "• Work from home\n",
       "\n",
       "Work Location: In person</td><td>Map(posted_at -> 2 days ago, schedule_type -> Full-time)</td><td>List(2 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIFNyLk1vZGVsZXIvRGF0YSBBbmFseXN0KCBJbW1lZGlhdGUgSm9pbmVyKSIsImNvbXBhbnlfbmFtZSI6IlRoZSBUYWxlbnQgUXVlc3QiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ino0YkdrY3RHOUdZdmFTRVVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=z4bGkctG9GYvaSEUAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2OwQqCUBBFaesntJplhfkiaFOrICqDFpF7GXVQ4zkjbybQH-v7sjaXczaHG31m0fmEhvAMyV0q8hTc34-MflRbQNp1VLVoBDdpmcIS1hMVoIShbEAYLiK1p_mhMet175yqT2o1tLZMSumcMBUyuJcU-ptcGwzU-ymZb3ebIem5XrmsIcjQExs83qQGLcN1rChggVUMGXnkGhljSHm68wWOhHMouQAAAA&shmds=v1_AdeF8KixMp25pqJo0NTjPByEzl4f3AmNgLfXodeVjxPNvUpjNg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=z4bGkctG9GYvaSEUAAAAAA%3D%3D</td><td>null</td><td>Data Sr.Modeler/Data Analyst( Immediate Joiner)</td><td>Glassdoor</td></tr><tr><td>List(Map(title -> Insight Global, link -> https://insightglobal.com/jobs/find_a_job/job-422244?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Insight Global</td><td>Project Background:\n",
       "Mosaic is our financial planning and reporting transformation program, bringing an aligned business process and toolset to all key business units enabling us to get consistent and standardized financial metrics across geographies. Started in 2019 the program is already rolled out to Europe and most of North America and continues across the world.\n",
       "There are three key components to the program:\n",
       "1. A standardized planning tool IBM Cognos TM1\n",
       "2. A global data warehouse based on Azure Cloud know as SPOT (single point of truth). The SPOT team is made up of functional data analysts & data engineers\n",
       "3. An extensive suite of dashboards known as Cockpit using Tableau. The Cockpit team are generally finance analysts with Tableau skills\n",
       "Role Background:\n",
       "We are looking for an additional functional/data analyst to join the existing team of functional/data analysts working on the SPOT data warehouse.\n",
       "The role of the functional/data analyst is to be the bridge between the Cockpit team, Data Engineering team, and Cognos TM1 team. The analyst understands the business data requirements and translates these into data definition and data transformation documents that are then build by the data engineers. The analyst can also be involved in testing, data validation and any sustain activity related to the flow and accuracy of the data in SPOT & Cockpit.\n",
       "Typically, the functional data analyst receives requirements for new and/or amended data pipelines from the Cockpit team or works with the Cognos TM1 team on new data sets that need to flow to SPOT & Cockpit.\n",
       "The current functional data analysts come from a range of backgrounds from data engineers to business analysts. What unites the team is a passion for data, attention to detail, ability to navigate and understand complex data flows, and a customer-centric mindset. This is a high profile and at times high pressure transformation program and everyone on the team must be responsive to our stakeholders needs.\n",
       "Key Accountabilities:\n",
       "This role will primarily work with the North America Cockpit team primarily based in Mexico City, but as part of the team they will be expected to build a good knowledge of all data pipelines and work on any area as needed. The key elements of the role are:\n",
       "Develop and maintain SPOT solution design & data architecture:\n",
       "o Ensure SPOT solution design & data model is up to date with latest business requirements\n",
       "o Contribute to the delivery of current technical roadmap that can support the predicted growth in data volumes\n",
       "Translate and communicate business requirements across all IT delivery teams and/or partners:\n",
       "o Collaborate closely with SPOT Cockpit teams to ensure business requirements received from Global & Regional Planning teams are well-understood and translated into the SPOT solution design (functional & data model)\n",
       "Act as an owner in the diagnosis and resolution of any data issues raised with the team, partnering with other teams as needed.\n",
       "\n",
       "We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>Map(posted_at -> 6 days ago, schedule_type -> Contractor, qualifications -> No degree mentioned)</td><td>List(6 days ago, Contractor, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBJTlRMIC0gTWV4aWNvIG9yIEluZGlhIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Il96ZU5oSUE0Ujg3ak1KZHNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=_zeNhIA4R87jMJdsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWOsQrCMBBAce0nON0sbSOCi7oIQq2oU_dySUMSiXcll6H9Jb_Sist78KZXfFbF6YIZ4UwYZ8lQQfvs7osedgqGgRO0NARcyo01iMVkPDBBw-yiXR99zqMclBKJtZOMOZja8FsxWc2TerGWH3rxmOwYMdt-t99O9UhuU7UkwfkMTWSNEQLBdR5sQo1DCZ2NSA4Jy__CF9IICi6tAAAA&shmds=v1_AdeF8KhFqSmgnQVrm9cAq5NS80RUOZdWCXZSAECxyvvZZQK5RQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=_zeNhIA4R87jMJdsAAAAAA%3D%3D</td><td>null</td><td>Data Analyst - INTL - Mexico or India</td><td>Insight Global</td></tr><tr><td>List(Map(title -> Aijobs.net, link -> https://aijobs.net/job/1401617-in-specialist-3-data-analyst-tras-assurance-bangalore/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/150285117/ctc889-advanced-system-dynamics-analyst-marine-vessel-analysis-specialist-kota/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>PwC</td><td>Line of ServiceAssuranceIndustry/SectorNot ApplicableSpecialismAssuranceManagement LevelSpecialistJob Description & SummaryAt PwC, our people in audit and assurance focus on providing independent and objective assessments of financial statements, internal controls, and other assurable information enhancing the credibility and reliability of this information with a variety of stakeholders. They evaluate compliance with regulations including assessing governance and risk management processes and related controls.In digital assurance at PwC, you will focus on providing assurance services over clients' digital environment, including processes and controls, cyber security measures, data and AI systems, and their associated governance, to help organisations and their stakeholders build trust in their technology while complying with relevant regulations.*Why PWCAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm’s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations. \"Job Description & SummaryABOUT TECHNOLOGY RISK ASSURANCE SERVICES:The Technology Risk Assurance Services practice in the firm, provides both audit and non-audit services related to controls around the financial reporting process, including financial business process and IT management controls.SPA provides:· Financial and operation applications/business process controls reviews· Database security controls reviews· IT general controls reviews· Infrastructure security reviews· Third party assurance and opinion services· Sarbanes-Oxley readiness, process improvement and sustainability services· Due diligence on systems and controls· Pre- and post-implementation systems reviews· Project assurance services· Data services (e.g., CAATs, data quality reviews)· Computer security reviewsResponsibilities (essential functions and responsibilities of the job):The tasks, roles and responsibilities concerned with the job will include the following:Executing various audit engagements as a support to field in-charge andApplying The Firm’s methodology while executing various job rolesApply current knowledge of IT trends and systems to identify security and risk management issues, and other opportunities for improvementAssist in the planning and execution of business process control reviews, information systems audits, and other Assurance offerings across a variety of industriesA good understanding of clients Business Risks, Audit Risks, IT Risks and the mitigating controls in addressing these risksGain strong comprehension of client operations, processes, and business objectives and utilize that knowledge on engagements.Evaluate and test business processes and controls and identify areas of risks.Perform general computer and application controls reviews including ERPs like SAP, Oracle Application, JD Edwards, MFG Pro, etc.Comply with requirements of ICAI Auditing Standards like AAS 29Data Analysis using CAATs tool like Excel, ACL, SQL etc*Mandatory skill sets - Data Analyst*Preferred Skill Sets – Business Analyst*Years of experience required – 0-1 Years*Education Qualification – Post Graduation in Data ScienceEducation (if blank, degree and/or field of study not specified)Degrees/Field of Study required: Postgraduate (Diploma)Degrees/Field of Study preferred:Certifications (if blank, certifications not specified)Required SkillsData AnalyticsOptional SkillsAccepting Feedback, Accepting Feedback, Active Listening, Artificial Intelligence (AI) Platform, Auditing Methodologies, Cloud Engineering, Communication, Compliance and Governance, Compliance and Standards, Compliance Auditing, Corporate Governance, Cybersecurity Governance, Cybersecurity Risk Management, Data Quality, Data Quality Assessment, Data Quality Assurance Testing, Data Validation, Emotional Regulation, Empathy, Governance Framework, Inclusion, Information Assurance, Information Security Governance, Intellectual Curiosity, Internal Controls {+ 11 more}Desired Languages (If blank, desired languages not specified)Travel RequirementsNot SpecifiedAvailable for Work Visa Sponsorship?NoGovernment Clearance Required?NoJob Posting End Date</td><td>Map(posted_at -> 1 day ago, salary -> 29,679–55,120 a year, schedule_type -> Full-time)</td><td>List(1 day ago, 29,679–55,120 a year, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJJTiBTcGVjaWFsaXN0IDMtIERhdGEgQW5hbHlzdCAtIFRSQVMgLUFzc3VyYW5jZeKAkyBCYW5nYWxvcmUiLCJjb21wYW55X25hbWUiOiJQd0MiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiZTlNdVBjTnNJWHlmb3ZrSkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=e9MuPcNsIXyfovkJAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJMQrCMBSAYVyLJ3B6s9BGFBedqoLUQcS6l9f4SCMxL-RFrJt38AKezZOoyw8ff_YeZHW1hzqQtuisJJjlsMGEUHp0j59zOB3LGvJS5BbRa_o8X7BCb9BxpN_ecQtCGHUH7GHLbByNll1KQRZKibjCSMJkdaH5qthTy726cCv_NNJhpOAwUTOdT_oieDMeHu5rsB4qf7b4BdFQI-WjAAAA&shmds=v1_AdeF8KgHugeD7CzRIEP7YN_reUh11kgJaYmeQyaRo77I08Aw9w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=e9MuPcNsIXyfovkJAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540fc1f758ae0067cfba0/images/fada94f4e909550fd832cbd45dde8873d2028e59e4230f6145f1024b5313cd32.png</td><td>IN Specialist 3- Data Analyst - TRAS -Assurance– Bangalore</td><td>Aijobs.net</td></tr><tr><td>List(Map(title -> BMS Careers - Bristol Myers Squibb, link -> https://jobs.bms.com/careers/job/137468822835-data-analyst-1-hyderabad-ts-in?domain=bms.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://bristolmyerssquibb.wd5.myworkdayjobs.com/en-US/BMS/job/Hyderabad---TS---IN/Data-Analyst-1_R1593138?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobaaj, link -> https://www.jobaaj.com/job/data-analyst-1-hyderabad-telangana-india-3-to-5-years-461411?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Aijobs.net, link -> https://aijobs.net/job/1400726-data-analyst-1/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-1940-174b71c3e8dc4cd53918c839b8b9b2ec?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Notify, link -> https://notify.careers/postings/290a6e16-bf16-4c53-8329-0aaade1c0e57?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/153589814e5a2d7440988ab7dfe948d7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Talent.com, link -> https://in.talent.com/view?id=da5efbc05dde&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Bristol Myers Squibb</td><td>The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n",
       "\n",
       "Roles & Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up to date with industry trends, best practices, and emerging technologies\n",
       "\n",
       "Skills and competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n",
       "• commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• Proven experience (1-3 years) in a similar business analyst role\n",
       "• Prior Pharmaceutical industry and/or healthcare consulting experience required\n",
       "• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with regulatory requirements and compliance in the US biopharma industry\n",
       "\n",
       "The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n",
       "\n",
       "Roles & Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up to date with industry trends, best practices, and emerging technologies\n",
       "\n",
       "Skills and competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n",
       "• commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• Proven experience (1-3 years) in a similar business analyst role\n",
       "• Prior Pharmaceutical industry and/or healthcare consulting experience required\n",
       "• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with regulatory requirements and compliance in the US biopharma industry</td><td>Map(posted_at -> 16 days ago, schedule_type -> Full-time)</td><td>List(16 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ1ZUlVZURnaVJaU1ludkdlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=ueIUeDgiRZSYnvGeAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOwrCQBAAUGxzBKtpbESzfrDRShH8gJX2YTYZNivrTNwZITmKtxVf8YrvqJgc0RD2jGlQgyXM4SoelDDXLQjDSSQkGu9as063zqmmMqihxbqs5eWEyUvvnuL1X6UtZuoSGlWrzaIvOw7T9SFHNUlwGygr3N-f6D1EhvPQUEaPzQwelJADMs7gwk3EH-5fs9ScAAAA&shmds=v1_AdeF8KiIPde6t361nwB3hQNEzTdlaOVGqbQjNngpI_huwKekOA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=ueIUeDgiRZSYnvGeAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540fc1f758ae0067cfba0/images/fada94f4e909550f7835ac31df150746db7b335654baaa715d0cf8b6c3643f43.png</td><td>Data Analyst 1</td><td>BMS Careers - Bristol Myers Squibb</td></tr><tr><td>List(Map(title -> DuPont Careers, link -> https://careers.dupont.com/us/en/job/246328W/Data-Analyst-Finance-VBA-Power-Query-Power-BI-Python-4-years-of-experience?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/data-analyst-finance-vba-power-query-power-bi-python-4%2B-years-of-experience-at-dupont-4255861626?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/data-analyst-finance-vba-power-query-power-bi-python-4-years-experience/6531858?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobgether, link -> https://jobgether.com/offer/685f49cfde1ebd49c8f53cab-data-analyst-finance-vba-power-query-power-bi-python-4-years-of-experience?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Talentify, link -> https://www.talentify.io/job/data-analyst-finance-vba-power-query-power-bi-python-4-years-of-experience-hyderabad-telangana-in-dupont-246328w?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> WhatJobs, link -> https://en-in.whatjobs.com/jobs/business-intelligence?id=157977416&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/6356895448866750464?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Recruit.net, link -> https://www.recruit.net/job/data-analyst-finance-vba-power-jobs/9BA3CAEB78743536?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Dupont</td><td>At DuPont, our purpose is to empower the world with essential innovations to thrive. We work on things that matter. Whether it’s providing clean water to more than a billion people on the planet, producing materials that are essential in everyday technology devices from smartphones to electric vehicles, or protecting workers around the world. Discover the many reasons the world’s most talented people are choosing to work at DuPont. Why Join Us | DuPont Careers\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "The Data Analyst/Developer will play a crucial role in developing automated data solutions, creating comprehensive dashboards, and utilizing advanced Excel functionalities. This position will require collaboration with various teams to translate data needs into actionable intelligence.\n",
       "\n",
       "Key Areas of Expertise and Responsibilities:\n",
       "\n",
       "1. Visual Basic for Applications (VBA)\n",
       "• Responsibilities:\n",
       "• Develop and maintain complex VBA applications to automate repetitive tasks.\n",
       "• Incorporate SAP Scripting within VBA to optimize business processes.\n",
       "• Troubleshoot and debug existing VBA code to ensure smooth functionality.\n",
       "• Criteria:\n",
       "• Advanced proficiency in VBA programming.\n",
       "• Demonstrated experience with SAP interfaces and scripting.\n",
       "• Ability to write modular, efficient, and maintainable code.\n",
       "• Knowledge of Excel object model and its functionalities.\n",
       "\n",
       "2. Power Query\n",
       "• Responsibilities:\n",
       "• Utilize Power Query for data extraction, transformation, and loading (ETL) processes.\n",
       "• Develop and maintain data models in Excel to streamline data preparation.\n",
       "• Create and optimize Power Query scripts for efficient data processing.\n",
       "• Criteria:\n",
       "• Intermediate experience with Power Query including M language for data transformation.\n",
       "• Familiarity with connecting Power Query to various data sources (e.g., databases, APIs).\n",
       "• Ability to perform data cleansing and manipulation through Power Query.\n",
       "\n",
       "3. Power BI\n",
       "• Responsibilities:\n",
       "• Create interactive, user-friendly dashboards and reports using Power BI.\n",
       "• Collaborate with stakeholders to gather reporting requirements and deliver insights.\n",
       "• Optimize Power BI reports for performance and usability.\n",
       "• Criteria:\n",
       "• Intermediate knowledge of Power BI Desktop and Power BI Service.\n",
       "• Ability to create DAX measures and calculated columns for enhanced analytics.\n",
       "• Familiarity with data visualization best practices and techniques.\n",
       "\n",
       "4. Python\n",
       "• Responsibilities:\n",
       "• Develop Python scripts to automate data manipulation and Excel-related tasks.\n",
       "• Utilize libraries such as Pandas and NumPy for data analysis and automation.\n",
       "• Collaborate with the data team to integrate Python solutions with existing tools.\n",
       "• Criteria:\n",
       "• Intermediate proficiency in Python, especially in data manipulation and automation.\n",
       "• Experience with libraries like Pandas, NumPy, and openpyxl for Excel automation.\n",
       "• Understanding of APIs and ability to retrieve data programmatically.\n",
       "\n",
       "Qualifications:\n",
       "• Bachelor’s degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n",
       "• A minimum of 4 of experience in a data-related role, focusing on the technologies mentioned.\n",
       "• Strong analytical and problem-solving skills with attention to detail.\n",
       "• Excellent communication skills and the ability to work collaboratively with diverse teams.\n",
       "\n",
       "Preferred Skills:\n",
       "• Experience with SQL and relational databases for data querying and data management.\n",
       "• Familiarity with other business intelligence tools (like Tableau or QlikView) is a plus.\n",
       "• Knowledge of machine learning principles is an advantage.\n",
       "• Understanding of data warehousing concepts and methodologies.\n",
       "\n",
       "Join our Talent Community to stay connected with us!\n",
       "\n",
       "On May 22, 2024, we announced a plan to separate our Electronics and Water businesses in a tax-free manner to its shareholders. On January 15, 2025, we announced that we are targeting November 1, 2025, for the completion of the intended separation of the Electronics business (the “Intended Electronics Separation”)*. We also announced that we would retain the Water business. We are committed to ensuring a smooth and successful separation process for the Future Electronics business. We look forward to welcoming new talent interested in contributing to the continued success and growth of our evolving organization.\n",
       "\n",
       "(1)The separation transactions are subject to satisfaction of customary conditions, including final approval by DuPont's Board of Directors, receipt of tax opinion from counsel, the filing and effectiveness of Form 10 registration statements with the U.S. Securities and Exchange Commission, applicable regulatory approvals, and satisfactory completion of financing.  For further discussion of risks, uncertainties and assumptions that could impact the achievement, expected timing and intended benefits of the separation transactions, see DuPont’s announcement.\n",
       "\n",
       "DuPont is an equal opportunity employer. Qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability or any other protected class. If you need a reasonable accommodation to search or apply for a position, please visit our Accessibility Page for Contact Information.\n",
       "\n",
       "DuPont offers a comprehensive pay and benefits package. To learn more visit the Compensation and Benefits page.</td><td>Map(posted_at -> 2 days ago, schedule_type -> Full-time)</td><td>List(2 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgRmluYW5jZSAoVkJBLCBQb3dlciBRdWVyeSwgUG93ZXIgQkksIFB5dGhvbikgNCsgeWVhcnMgb2YgZXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJEdXBvbnQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImNGeEdTMkhJQXpJRW5kb2ZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=cFxGS2HIAzIEndofAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zWLMU7DQBBFRZsjUP0GiQRjI5Q0oUoUAUkFEqKNxvbEu2iZWe1OhPeSOROmoPl6T09_drmauR0ZYSMUSjY8eyHpGLef202FN_3hhPczp_Iv2_1ExZzKHMs7FKaUoSfwGDl5nr5z3OOgLfKUOgcVvKgOga-fnFnM66bJOdRDNjLf1Z1-Nyrc6th8aZv_5pgdJY6BjI-Pq4exjjIsbnbnqGLwgtfSc6KW-gofHEgGEqqwl97TL1wZQbrRAAAA&shmds=v1_AdeF8KjE_yEZOuigr5UIe33mQKphFiyPOSsPa5mAJd61oGdJsA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=cFxGS2HIAzIEndofAAAAAA%3D%3D</td><td>https://serpapi.com/searches/686540fc1f758ae0067cfba0/images/fada94f4e909550f6f73161419591ea7d7c57bacc38d027cbd58da61246d96d7.png</td><td>Data Analyst Finance (VBA, Power Query, Power BI, Python) 4+ years of experience)</td><td>DuPont Careers</td></tr><tr><td>List(Map(title -> Search Carrier Jobs - Carrier, link -> https://jobs.carrier.com/en/job/hyderabad/sustainability-data-analyst/29289/82926538448?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Terra.do, link -> https://www.terra.do/climate-jobs/job-board/Sustainability-Data-Analyst-Nextracker-8332022/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/sustainability-data-analyst-carrier-JV_IC2865319_KO0,27_KE28,35.htm?jl=1009789533619&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Saur Energy, link -> https://www.saurenergy.com/energy-jobs/sustainability-data-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/3SsLNieYjqema8Vy7j0dXwbxh9GFa-yltDVlR_r4oBzmPNd0PH7BCQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/sustainability-data-analyst-at-carrier-4255900397?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobaaj, link -> https://www.jobaaj.com/job/nextracker-inc-sustainability-data-analyst-hyderabad-telangana-india-0-to-15-years-354454?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.com/job-listing/sustainability-data-analyst-carrier-JV_IC2865319_KO0,27_KE28,35.htm?jl=1009789533619&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Carrier</td><td>Role: Sustainability Data Analyst\n",
       "\n",
       "Location: Hyderabad, India\n",
       "\n",
       "Full/ Part-time: Full time\n",
       "\n",
       "Build a career with confidence\n",
       "\n",
       "Carrier Global Corporation, global leader in intelligent climate and energy solutions is committed to creating solutions that matter for people and our planet for generations to come. From the beginning, we've led in inventing new technologies and entirely new industries. Today, we continue to lead because we have a world-class, diverse workforce that puts the customer at the center of everything we do.\n",
       "\n",
       "About the role\n",
       "\n",
       "We are seeking a results-driven Sustainability Data Analyst to join Carrier’s product sustainability analytics team. The ideal candidate will have a strong background in sustainability practices and data analytics, with a focus on analyzing environmental impact and identifying opportunities for improvement. This role involves conducting comprehensive sustainability/energy audits, analyzing data, and developing strategies to enhance sustainability performance.\n",
       "\n",
       "Key responsibilities:\n",
       "• Perform detailed sustainability audits and analyze data to identify trends and areas for improvement.\n",
       "• Develop and implement strategies to enhance sustainability and reduce environmental impact.\n",
       "• Monitor and evaluate the performance of sustainability initiatives using data analytics.\n",
       "• Collaborate with cross-functional teams to optimize sustainability practices.\n",
       "• Prepare reports and presentations on sustainability metrics and audit findings.\n",
       "• Stay updated on industry trends and best practices in sustainability and data analytics.\n",
       "\n",
       "Minimum Requirements:\n",
       "\n",
       "Education: Bachelor’s or Master’s degree in Mechanical, Energy Engineering, Environmental Science, Sustainability, Data Analytics, or a related field.\n",
       "\n",
       "Experience: atleast 6 years in sustainability, data analytics, or BI development roles to be able to drive himself towards business goals independently.\n",
       "\n",
       "Key Skills:\n",
       "• Strong analytical skills, attention to detail and ability to think from first principles.\n",
       "• Excellent communication and teamwork abilities.\n",
       "• Proficiency in data analytics software and tools. Knowledge of python, SQL, Power-BI, would be added advantage.\n",
       "• Knowledge of relevant regulations and standards in sustainability.\n",
       "• Familiarity with data visualization tools and techniques.\n",
       "• Willingness to be flexible, learn new tools, techniques and deliver.\n",
       "\n",
       "Benefits\n",
       "\n",
       "We are committed to offering competitive benefits programs for all of our employees, and enhancing our programs when necessary.\n",
       "• Enjoy your best years with our retirement savings plan\n",
       "• Have peace of mind and body with our health insurance\n",
       "• Make yourself a priority with flexible schedules, parental leave and our holiday purchase scheme\n",
       "• Drive forward your career through professional development opportunities\n",
       "• Achieve your personal goals with our Employee Assistance Programme.\n",
       "\n",
       "Our commitment to you\n",
       "\n",
       "Our greatest assets are the expertise, creativity and passion of our employees. We strive to provide a great place to work that attracts, develops and retains the best talent, promotes employee engagement, fosters teamwork and ultimately drives innovation for the benefit of our customers. We strive to create an environment where you feel that you belong, with diversity and inclusion as the engine to growth and innovation. We develop and deploy best-in-class programs and practices, providing enriching career opportunities, listening to employee feedback and always challenging ourselves to do better. This is The Carrier Way.\n",
       "\n",
       "Join us and make a difference.\n",
       "\n",
       "Apply Now!\n",
       "\n",
       "Carrier is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.\n",
       "\n",
       "Job Applicant's Privacy Notice:\n",
       "\n",
       "Click on this link to read the Job Applicant's Privacy Notice</td><td>Map(posted_at -> 9 days ago, schedule_type -> Full-time)</td><td>List(9 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTdXN0YWluYWJpbGl0eSBEYXRhIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJDYXJyaWVyIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJPMkpEMHBVcG0xc3dxYnc1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=O2JD0pUpm1swqbw5AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLvQrCQAwAYFz7CE6ZHKT2pOCiU1HwZ9W95NpwPTmTconQPopvqy7f9hWfRVHf32oYGX1M0WY4oSE0jGlWgw3cxIMS5m4AYTiLhETLw2A26t451VSFX7fYVZ28nDB5mdxTvP5pdcBMY0Kjtt5tp2rksF4dMedIGSLDZe4po8e-hAcl5ICMJVy5j_gFiNgRqpwAAAA&shmds=v1_AdeF8KgSn7fUrNOz8pCz2N5vkRmMXPHGqqIwTzCKcQ0k093PfQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=O2JD0pUpm1swqbw5AAAAAA%3D%3D</td><td>null</td><td>Sustainability Data Analyst</td><td>Search Carrier Jobs - Carrier</td></tr><tr><td>List(Map(title -> ICIMS Careers, link -> https://careers.icims.com/jobs/6072?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>iCIMS Talent Acquisition</td><td>Job Overview\n",
       "\n",
       "The Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.\n",
       "\n",
       "About Us\n",
       "\n",
       "When you join iCIMS, you join the team helping global companies transform business and the world through the power of talent. Our customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile. As the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent. We’re passionate about helping companies build a diverse, winning workforce and about building our home team. We're dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\n",
       "\n",
       "Responsibilities\n",
       "• Perform various data analysis functions to analyze data from a variety of sources including external labor market data and research and internal data sets from our platforms \n",
       "• Incorporate information from a variety of systems to produce comprehensive and compelling narratives for thought-leadership initiatives and customer engagements\n",
       "• Demonstrate critical thinking - identify the story in context using multiple data sets, and present results. A strong proficiency in data storytelling will be critical to success in this role.\n",
       "• Understand principles of quality data visualization and apply them in Tableau to create and maintain custom dashboards for consumption by other employees\n",
       "• Find and investigate data quality issues, root causes and recommend remedies to be implemented by the data scientists and engineers  \n",
       "• Liaise with teams around our business to understand their problems, determine how our team can help, then use our database to produce the content they need\n",
       "• Identify data mapping and enrichment requirements. Familiarity with SQL, especially the logic behind different types of data joins and writing efficient queries, will be necessary\n",
       "• Consistently ensure that business is always conducted with integrity and that behavior aligns with iCIMS policies, procedures, and core competencies\n",
       "\n",
       "Additional Job Responsibilities: \n",
       "• Produce and adapt data visualizations in response to business requests for internal and external use\n",
       "• Shows good judgement in prioritizing their own commitments and those of the larger team, while demonstrating initiative and appropriate urgency when needed\n",
       "• Mentor junior team members in best practices for analytics, data visualization, and data storytelling. Exemplify these standards and guide teammates in following them.\n",
       "• Think creatively to produce unique, actionable insights from complex datasets, which can deliver value to our business and to our customers.\n",
       "\n",
       "Qualifications\n",
       "• 5-10 years professional experience working in an analytics capacity\n",
       "• Excellent communication skills, especially with regards to data storytelling – finding insights from complex datasets and sharing those findings with key stakeholders\n",
       "• Strong data analytics and visualization skills\n",
       "• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n",
       "• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n",
       "• Advanced knowledge of Excel (Pivot tables, VLOOKUPs, IF statements)\n",
       "• Familiarity with data guardrails to ensure compliance with applicable data governance regulations and privacy laws (i.e., GDPR)\n",
       "\n",
       "Preferred\n",
       "• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n",
       "• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n",
       "\n",
       "EEO Statement\n",
       "\n",
       "iCIMS is a place where everyone belongs. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at iCIMS.\n",
       "\n",
       "We are proud to be an equal opportunity and affirmative action employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at careers@icims.com.\n",
       "\n",
       "Compensation and Benefits\n",
       "\n",
       "Competitive health and wellness benefits include medical insurance (employee and dependent family members), personal accident and group term life insurance, bonding and parental leave, lifestyle spending account reimbursements, wellness services offerings, sick and casual/emergency days, paid holidays, tuition reimbursement, retirals (PF - employer contribution) and gratuity. Benefits and eligibility may vary by location, role, and tenure. Learn more here: https://careers.icims.com/benefits</td><td>Map(posted_at -> 21 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(21 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJTci4gRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiaUNJTVMgVGFsZW50IEFjcXVpc2l0aW9uIiwiYWRkcmVzc19jaXR5IjoiUmFpIER1cmcsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImZMUlRBQjlESVFWdFdOeGpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Rai Durg, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=fLRTAB9DIQVtWNxjAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFz7CU63CVITEXTQqViQCi62e7nWkJ7ES82dUH_Gb1WXN77sM8sWdTJQoiIUjOEtCis4xw7EYeoHiAynGH1w88OgOsreWpFgvCgq9aaPDxvZdXGy99jJn1YGTG4MqK7dbNeTGdkvd3SsLjU0GBwrFP3zRUJKv50YrkhQvpLPoXEB2SNjDhXfCL--pAUHoQAAAA&shmds=v1_AdeF8KitaSipmTLybCgiXZCrA1aS7QWwHWuaNBynowT2b-63_w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=fLRTAB9DIQVtWNxjAAAAAA%3D%3D</td><td>null</td><td>Sr. Data Analyst</td><td>ICIMS Careers</td></tr><tr><td>List(Map(title -> Workday, link -> https://alfalaval.wd3.myworkdayjobs.com/en-US/Alfa_Laval_jobs/job/Master-Data-Analyst_JR0038405?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Iitjobs, link -> https://www.iitjobs.com/job/master-data-analyst-ohio-usa-nexpro-technologies-inc-102212?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> JobRinger, link -> https://www.jobringer.com/job/master-data-analyst/d813db76?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Aijobs.net, link -> https://aijobs.net/job/1390525-master-data-analyst/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/2298526562489729024?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Talents Jobs, link -> https://talentsjobs.in/job-detail?job-id=161057&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>C32511 Alfa Laval India Private Limited</td><td>Purpose of the Job: To provide direct support to the Sales and Order Handling teams by managing customer master data and administering the Order Management System. This role ensures data integrity, system readiness, and compliance with internal controls during order booking and execution processes. Key Responsibilities: Customer Master Data Management Manage the full lifecycle of customer codes, including creation, amendment, and deactivation. Conduct background checks using the Global World Check system and coordinate with the Export Control Super User for compliance validation. Ensure customer data is configured correctly in the system for seamless order booking and execution. Maintain data accuracy and cleanliness in line with Alfa Laval’s global data standards. System Administration Act as the system administrator for the Customer Order Management system. Maintain system master data and troubleshoot operational issues. Ensure the system is up-to-date and aligned with business requirements. Order Booking Compliance Perform basic compliance checks during order booking. Verify completeness and correctness of mandatory information. Ensure all required approvals are in place as per the authorization matrix. Process Activation Initiate internal processes required for order execution. Collaborate with cross-functional teams to ensure timely and accurate order processing. Key Competencies: Strong attention to detail and data accuracy Knowledge of compliance and export control processes Proficiency in ERP and order management systems Analytical and problem-solving skills Effective communication and collaboration across teams Qualifications & Experience: Bachelor’s degree in Business Administration, Information Systems, or related field 3+ years of experience in data management or order processing roles Familiarity with data governance and compliance frameworks Experience with customer master data and ERP systems (e.g., SAP) \"We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\" Every day, we get opportunities to make a positive impact – on our colleagues, partners, customers and society. Together, we’re pioneering the solutions of the future and unlocking the full potential of precious resources. Trusted to act on initiative, we challenge conventional thinking to develop world-leading technologies that inspire progress in vital areas, including energy, food, water and shipping. As we push forward, the innovative, open spirit that fuels our 140-year-old start-up culture and rapid growth also drives our personal growth. So, as we shape a more resourceful, less wasteful world, we build our careers too. Meet our dedicated people who always go the extra mile Alfa Laval Career Site Or sign up for Job Alerts to stay in touch Sign up for Job Alerts</td><td>Map(posted_at -> 6 days ago, schedule_type -> Full-time)</td><td>List(6 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJNYXN0ZXIgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiQzMyNTExIEFsZmEgTGF2YWwgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImhJMnc5MHYxS1E3QXVQTkNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=hI2w90v1KQ7AuPNCAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXKwQqCQBCAYbr6CJ3mLOSq4aVOUhCFQW8go066su7IziD2LL1sRZf_8v3RexPFdxSlAGdUhNKje4nCDm7cgBCGdgD2cGHuHW2Pg-osB2NEXNKLoto2aXky7Knh1YzcyC-1DBhodqhU50W6JrPv4_S0z4ssg9I9ESpc0MHVdxbhEezyPaGyk1XqwPo_fADlWYTFngAAAA&shmds=v1_AdeF8Kj2JTzacKV1mRNPu-q1EBnGdhXo_LYLYriehURv_C9NnQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=hI2w90v1KQ7AuPNCAAAAAA%3D%3D</td><td>null</td><td>Master Data Analyst</td><td>Workday</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/engr-ii-data-engineering-at-verizon-4254112027?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Adzuna, link -> https://www.adzuna.in/details/5204263702?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/147941185/f-097-engr-ii-data-engineering-tamil-nadu/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobindian.in, link -> https://jobindian.in/de/engr-ii-data-engineering-job126726?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Verizon</td><td>When you join Verizon\n",
       "\n",
       "You want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n",
       "\n",
       "What You’ll Be Doing...\n",
       "\n",
       "As a Data Engineer with ETL/ELT expertise for our growing data platform and analytics teams, you will understand and enable the required data sets from different sources. This includes both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n",
       "• Understanding the business requirements.\n",
       "• Transforming technical design.\n",
       "• Working on data ingestion, preparation and transformation.\n",
       "• Developing the scripts for data sourcing and parsing.\n",
       "• Developing data streaming applications.\n",
       "• Debugging the production failures and identifying the solution.\n",
       "• Working on ETL/ELT development.\n",
       "\n",
       "What We’re Looking For...\n",
       "\n",
       "You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems.\n",
       "\n",
       "You'll Need To Have\n",
       "• Bachelor’s degree or one or more years of relevant experience required, demonstrated through work experience and/or military experience.\n",
       "• Experience with Data Warehouse concepts and Data Management life cycle.\n",
       "\n",
       "Even better if you have one or more of the following:\n",
       "• Any related Certification on ETL/ELT developer.\n",
       "• Accuracy and attention to detail.\n",
       "• Good problem solving, analytical, and research capabilities.\n",
       "• Good verbal and written communication.\n",
       "• Experience presenting to and influencing partners.\n",
       "\n",
       "Why Verizon?\n",
       "\n",
       "Verizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.\n",
       "• We are a ‘pay for performance’ company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.\n",
       "• Your benefits are market competitive and delivered by some of the best providers.\n",
       "• You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.\n",
       "• We offer generous paid time off benefits to help you manage your work life balance and opportunities for flexible working arrangements*.\n",
       "• Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.\n",
       "• You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.\n",
       "\n",
       "Your benefits package will vary depending on the country in which you work.\n",
       "• subject to business approval\n",
       "\n",
       "If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.\n",
       "\n",
       "Where you’ll be working\n",
       "\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "Scheduled Weekly Hours\n",
       "\n",
       "40\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Verizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.</td><td>Map(posted_at -> 1 day ago, schedule_type -> Full-time)</td><td>List(1 day ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkpaY3lnNzJpTUJGcHpfUTJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=JZcyg72iMBFpz_Q2AAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXMsQoCMQyAYVxvcnbKLFwrgouuitQHcD3SGtpKLylNh8PJR_dcfviXb_huBnvj2MC58YodYZ3MRC1zhBEe4kEJW0ggDHeRWGh3Sb1XPVurWkzUjj0HE2S2wuRlsW_x-s-kCRvVgp2m4-mwmMpxv32u9mfFMoPjV8YfwwKm9YMAAAA&shmds=v1_AdeF8KgbJllH7cXh9TgLLP3wueXhHQRV_Z4prkZgeJwYfCo8Jw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=JZcyg72iMBFpz_Q2AAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c1f32f11d4cca51f4fc/images/56fc5e069c9f2d48c59a9b83fcb6628a5563c2c8e003cb81494faae7f495eb69.jpeg</td><td>Engr II-Data Engineering</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> PepsiCo Careers, link -> https://www.pepsicojobs.com/main/jobs/385591?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Hirist, link -> https://www.hirist.tech/j/amgen-associate-data-engineer-4-7-yrs-1467180?ref=kp_prm&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/data-engineer-associate/massmutual-india/17177733?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/SbR_fqoOTfdMjrCgOyzgqnQbq1eYlbR3kezB3CmrxSWJR44WXnbOXw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/associate-analyst-data-engineer-at-pepsico-4233231137?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Aijobs.net, link -> https://aijobs.net/job/1267375-associate-manager-data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Job Board - Vertex Ventures HC, link -> https://jobs.vertexventureshc.com/companies/eyebio/jobs/51211685-associate-manager-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> JobzMall, link -> https://www.jobzmall.com/pepsico/job/associate-manager-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>PepsiCo</td><td>Overview\n",
       "\n",
       "PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT.  The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics, and new product development.  PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\n",
       "\n",
       "What PepsiCo Data Management and Operations does:\n",
       "\n",
       "Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\n",
       "\n",
       "Responsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\n",
       "\n",
       "Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\n",
       "\n",
       "Increase awareness about available data and democratize access to it across the company.\n",
       "\n",
       " \n",
       "\n",
       "               As a data engineer, you will be the key technical expert building PepsiCo's data products to drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help developing very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n",
       "\n",
       "Responsibilities\n",
       "• Act as a subject matter expert across different digital projects.\n",
       "• Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\n",
       "• Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\n",
       "• Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\n",
       "• Responsible for implementing best practices around systems integration, security, performance, and data management.\n",
       "• Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\n",
       "• Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\n",
       "• Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\n",
       "• Develop and optimize procedures to “productionalize” data science models.\n",
       "• Define and manage SLA’s for data products and processes running in production.\n",
       "• Support large-scale experimentation done by data scientists.\n",
       "• Prototype new approaches and build solutions at scale.\n",
       "• Research in state-of-the-art methodologies.\n",
       "• Create documentation for learnings and knowledge transfer.\n",
       "• Create and audit reusable packages or libraries.\n",
       "\n",
       "Qualifications\n",
       "• 4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n",
       "• 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n",
       "• 3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n",
       "• 2+ years in cloud data engineering experience in Azure.\n",
       "• Fluent with Azure cloud services. Azure Certification is a plus.\n",
       "• Experience in Azure Log Analytics\n",
       "• Experience with integration of multi cloud services with on-premises technologies.\n",
       "• Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\n",
       "• Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\n",
       "• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n",
       "• Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\n",
       "• Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\n",
       "• Experience with Statistical/ML techniques is a plus.\n",
       "• Experience with building solutions in the retail or in the supply chain space is a plus.\n",
       "• Experience with version control systems like Github and deployment & CI tools.\n",
       "• Working knowledge of agile development, including DevOps and DataOps concepts.\n",
       "• B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.\n",
       "\n",
       " Skills, Abilities, Knowledge:\n",
       "• Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\n",
       "• Strong change manager. Comfortable with change, especially that which arises through company growth.\n",
       "• Ability to understand and translate business requirements into data and technical requirements.\n",
       "• High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\n",
       "• Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\n",
       "• Strong organizational and interpersonal skills; comfortable managing trade-offs.</td><td>Map(posted_at -> 1 month ago, schedule_type -> Full-time)</td><td>List(1 month ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUgQW5hbHlzdCAtIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJQZXBzaUNvIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJkdUI4NkhSU1JwRWNPdlBCQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=duB86HRSRpEcOvPBAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXNvwrCMBCAcVz7CE43OUhtRBBEp6Lin8nBvVzSI43Eu5DLYJ_GV7UuH_ymr_rOql2rKi5gIWgZ46gFVnDCgnBmH5goT76LBSXMbgBhuIj4SPPDUErSvTGqsfFasATXOHkbYbLyMS-x-k-nA2ZKcVp0m-360yT2y8WDkoajQGC4jj1ltNjX8KSI7JGxhhv3AX-3W2KaogAAAA&shmds=v1_AdeF8KiLN_MtFDkdScKO8VdsUpBnPWjFPW93bXtnciDsvcO4jw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=duB86HRSRpEcOvPBAAAAAA%3D%3D</td><td>null</td><td>Associate Analyst - Data Engineer</td><td>PepsiCo Careers</td></tr><tr><td>List(Map(title -> Qualcomm Careers, link -> https://careers.qualcomm.com/careers/job/446704941923-senior-big-data-engineer-hyderabad-telangana-india?domain=qualcomm.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/senior-big-data-engineer-qualcomm-JV_IC2865319_KO0,24_KE25,33.htm?jl=1009710483297&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/p5JHOrXYLuAzJ6DHcAierjARZhkHwHZ_rEUlfxDTE4l9fkr6fj354A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/senior-big-data-engineer-at-grid-dynamics-3971156692?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Atlantic Bridge Job Board, link -> https://jobs.abven.com/companies/nuvia/jobs/49028670-senior-big-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Taro, link -> https://www.jointaro.com/jobs/qualcomm/senior-big-data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/c771d19594f49f9a197d760525fd19f4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobaaj, link -> https://www.jobaaj.com/job/epam-systems-senior-engineer-big-data-hyderabad-telangana-india-5-to-8-years-289425?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Qualcomm</td><td>Company:\n",
       "Qualcomm India Private Limited\n",
       "\n",
       "Job Area:\n",
       "Engineering Group, Engineering Group > Software Engineering\n",
       "\n",
       "General Summary:\n",
       "\n",
       "As a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n",
       "\n",
       "Minimum Qualifications:\n",
       "• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n",
       "OR\n",
       "Master's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n",
       "OR\n",
       "PhD in Engineering, Information Systems, Computer Science, or related field.\n",
       "\n",
       "• 2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n",
       "\n",
       "General Summary:\n",
       "\n",
       "Preferred Qualifications\n",
       "• 3+ years of experience as a Data Engineer or in a similar role\n",
       "• Experience with data modeling, data warehousing, and building ETL pipelines\n",
       "• Solid working experience with Python, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n",
       "• Experience with Big Data tools, platforms and architecture with solid working experience with SQL\n",
       "• Experience working in a very large data warehousing environment, Distributed System.\n",
       "• Solid understanding on various data exchange formats and complexities\n",
       "• Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n",
       "• Strong data visualization skills\n",
       "• Basic understanding of Machine Learning; Prior experience in ML Engineering a plus\n",
       "• Ability to manage on-premises data and make it inter-operate with AWS based pipelines\n",
       "• Ability to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n",
       "\n",
       "Education\n",
       "• Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n",
       "• Preferred Qualifications: Masters in CS/ECE with a Data Science / ML Specialization\n",
       "\n",
       "Minimum Qualifications:\n",
       "• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n",
       "\n",
       "OR\n",
       "\n",
       "Master's degree in Engineering, Information Systems, Computer Science, or related field\n",
       "\n",
       "OR\n",
       "\n",
       "PhD in Engineering, Information Systems, Computer Science, or related field.\n",
       "• 3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n",
       "\n",
       "Develops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n",
       "\n",
       "Principal Duties and Responsibilities:\n",
       "• Completes assigned coding tasks to specifications on time without significant errors or bugs.\n",
       "• Adapts to changes and setbacks in order to manage pressure and meet deadlines.\n",
       "• Collaborates with others inside project team to accomplish project objectives.\n",
       "• Communicates with project lead to provide status and information about impending obstacles.\n",
       "• Quickly resolves complex software issues and bugs.\n",
       "• Gathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n",
       "• Seeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n",
       "• Participates in technical conversations with tech leads/managers.\n",
       "• Anticipates and communicates issues with project team to maintain open communication.\n",
       "• Makes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n",
       "• Prioritizes project deadlines and deliverables with minimal supervision.\n",
       "• Resolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n",
       "• Writes readable code for large features or significant bug fixes to support collaboration with other engineers.\n",
       "• Determines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n",
       "• Unit tests own code to verify the stability and functionality of a feature.\n",
       "\n",
       "Applicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n",
       "\n",
       "Qualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n",
       "\n",
       "To all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n",
       "\n",
       "If you would like more information about this role, please contact Qualcomm Careers.</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time)</td><td>List(4 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQmlnIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJRdWFsY29tbSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieVRVTHdrQjB2Ujk5bmJQUUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Hyderabad, Telangana, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=yTULwkB0vR99nbPQAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXKvQrCMBAAYFz7CE63CVIbEVx0K4o_m-heLumRRtK7kItQn8TXVZdv-qrPrDJ34iAZ2uDhgAXhyD4wUYYVXMWCEmY3gDCcRHyk-X4oJenOGNXYeC1YgmucjEaYrEzmKVb_dDpgphSxULfZrqcmsV8ubi-MvzxCYDi_e8posa_hQRHZI2MNF-4DfgEmuzRrmgAAAA&shmds=v1_AdeF8KjxAA1LvW477axq0hGA-OFF2hfFqSX5wKA2PopSDIT20g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=yTULwkB0vR99nbPQAAAAAA%3D%3D</td><td>null</td><td>Senior Big Data Engineer</td><td>Qualcomm Careers</td></tr><tr><td>List(Map(title -> SmartRecruiters Job Search, link -> https://jobs.smartrecruiters.com/Blend360/744000066099595-manager-data-engineer-aws-databricks?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/manager-data-engineer-aws-databricks/6471278?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/manager-data-engineer-aws-databricks-at-blend-4251765113?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/fca80f9788fd2535e238c0cca3e73eea?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> JobNet, link -> https://www.jobnet.com.au/in/en/find-jobs-in-India/DATA-ENGINEER-DATABRICKS-168C894F40A0C5BE28/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Aijobs.net, link -> https://aijobs.net/job/1364959-manager-data-engineer-aws-databricks/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/2851598743965270016?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Recruit.net, link -> https://www.recruit.net/job/data-engineer-aws-databricks-platform-jobs/50A2CBFD15AB79EE?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Blend360</td><td>Company Description\n",
       "\n",
       "Blend is a premier AI services provider, committed to co-creating meaningful impact for its clients through the power of data science, AI, technology, and people. With a mission to fuel bold visions, Blend tackles significant challenges by seamlessly aligning human expertise with artificial intelligence. The company is dedicated to unlocking value and fostering innovation for its clients by harnessing world-class people and data-driven strategy. We believe that the power of people and AI can have a meaningful impact on your world, creating more fulfilling work and projects for our people and clients. For more information, visit www.blend360.com\n",
       "\n",
       "Job Description\n",
       "\n",
       "We are seeking a seasoned Data Engineering Manager with 8+ years of experience to lead and grow our data engineering capabilities. This role demands strong hands-on expertise in Python, SQL, Spark, and advanced proficiency in AWS and Databricks. As a technical leader, you will be responsible for architecting and optimizing scalable data solutions that enable analytics, data science, and business intelligence across the organization.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Lead the design, development, and optimization of scalable and secure data pipelines using AWS services such as Glue, S3, Lambda, EMR, and Databricks Notebooks, Jobs, and Workflows.\n",
       "• Oversee the development and maintenance of data lakes on AWS Databricks, ensuring performance and scalability.\n",
       "• Build and manage robust ETL/ELT workflows using Python and SQL, handling both structured and semi-structured data.\n",
       "• Implement distributed data processing solutions using Apache Spark/PySpark for large-scale data transformation.\n",
       "• Collaborate with cross-functional teams including data scientists, analysts, and product managers to ensure data is accurate, accessible, and well-structured.\n",
       "• Enforce best practices for data quality, governance, security, and compliance across the entire data ecosystem.\n",
       "• Monitor system performance, troubleshoot issues, and drive continuous improvements in data infrastructure.\n",
       "• Conduct code reviews, define coding standards, and promote engineering excellence across the team.\n",
       "• Mentor and guide junior data engineers, fostering a culture of technical growth and innovation.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Requirements\n",
       "• 8+ years of experience in data engineering with proven leadership in managing data projects and teams.\n",
       "• Expertise in Python, SQL, Spark (PySpark), and experience with AWS and Databricks in production environments.\n",
       "• Strong understanding of modern data architecture, distributed systems, and cloud-native solutions.\n",
       "• Excellent problem-solving, communication, and collaboration skills.\n",
       "• Prior experience mentoring team members and contributing to strategic technical decisions is highly desirable.</td><td>Map(posted_at -> 3 days ago, schedule_type -> Full-time)</td><td>List(3 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJNYW5hZ2VyIERhdGEgRW5naW5lZXIgLSBBV1MgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJsZW5kMzYwIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJxelhlVGQtWWc2ekE5U3R4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=qzXeTd-Yg6zA9StxAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_x2OPQvCMBBAce1PcLpNkNoURQfFQVH8ACcFx3JpjjQa70qSof4g_6fV5cF708s-g2x9QUZLAXaYEPZsHVNvE9jcr_-mg6ufsQ9n0RAJQ92AMBxErKfhqkmpjUulYvSFjQmTq4taXkqYtHTqITr-UMUGA7UeE1XTedkVLdvxaOuJzWxRgmM4vg0F1GhyuJFHtv1XDic2Dr816jqRqAAAAA&shmds=v1_AdeF8KjB09fTQmZGocZSKjJ3x436LxkG8VsacVRcD1YoflacNg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=qzXeTd-Yg6zA9StxAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c1f32f11d4cca51f4fc/images/56fc5e069c9f2d48e9cafb9aee8ab0386edd45db1d4eb8a4eb227fed4079aa51.png</td><td>Manager Data Engineer - AWS Databricks</td><td>SmartRecruiters Job Search</td></tr><tr><td>List(Map(title -> Insight Global, link -> https://insightglobal.com/jobs/find_a_job/illinois/chicago/data-engineer-intl-india-eor-6fb570f8/job-420771/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/corp-tech-im-data-engineer-advanced/6460507?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Hirist, link -> https://www.hirist.tech/j/novastrid-data-engineer-apache-sparkkafkajava-1486693?ref=red_old&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/proclink-data-engineer-looker-snowflake-db-at-proclink-4232324074?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/jdp/6463234110532246560?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/c37234d3474727c2ac29e31d23869997?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Expertini, link -> https://in.expertini.com/jobs/job/data-engineer-pune-hyderabad-bangalore-gurgaon-hyderabad-oncorre-inc-4187-11583867/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Getmereferred.com, link -> https://getmereferred.com/job-listing/sw-realtime-data-engineer-iii-data-streaming-ncr-voyix-hyderabad-2-to-5-years-experience-964f4074-1dd3-4725-944b-56208e38609a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Insight Global</td><td>- In this role, you will be building data pipeline solutions by designing, adopting, and applying big data strategies and architectures. You must have knowledge of scalable system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration, data transformation, data modeling, and data cleansing.\n",
       "- The Sr. Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for business stakeholders.\n",
       "- Design and develop a logical data model for a large multi-tenant application with intent to facilitate consistency and efficiency.\n",
       "- Collaborate with analysts and internal clients to understand the goals of the overall system data architecture, and further develop on business needs/issues.\n",
       "- Test/troubleshoot problems and conduct root cause analysis.\n",
       "- Own communications with technical contacts at software vendors to escalate software issues, recommend enhancements, and find sustainable workarounds to issues.\n",
       "- Work in tandem with analysts and other stakeholders to develop and execute necessary processes and controls around the flow of data to meet data governance standards.\n",
       "- Verify accuracy of table changes and data transformation processes\n",
       "- Deliver fully tested code prior to prod-deployment when appropriate.\n",
       "- Recommend and implement enhancements that address system performance requirements, streamline processes and improve data integrity.\n",
       "- Create sound technical documentation and train peer developers on this documentation as development completes.\n",
       "- Additional duties as assigned to ensure company success.\n",
       "The compensation for this role varies depending on depth and length of experience, and can range from 27-36 LAKH.\n",
       "\n",
       "We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time and Contractor)</td><td>List(4 days ago, Full-time and Contractor)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElOVEwgSW5kaWEgLSBFT1IgNmZiNTcwZjgiLCJjb21wYW55X25hbWUiOiJJbnNpZ2h0IEdsb2JhbCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiZUtoaVo5cWozWC1wLXlBVEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Hyderabad, Telangana, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=eKhiZ9qj3X-p-yATAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_yXOwQqCQBCAYbr6CJ3mHKYSWFHQKTEjCsK7zOq4bmwzsrsHe6cesqLLD9_tj96z6HDEgFCwNkzkoLrWF6i4MwhLKG53WPcq32T99suzKPCErh1AGEoRbWm-H0IY_S5NvbeJ9gGDaZNWnqkwKZnShyj_S-MHdDRaDNSs8mxKRtaLZcXe6CFAaUWhBcNwenXkUGEXQ00WWSNj_D_6AJgewVCvAAAA&shmds=v1_AdeF8KjtnRae7jcc6Wnu7qoLy9twbkVc7hyFD6P1jKcU7Ff-aA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=eKhiZ9qj3X-p-yATAAAAAA%3D%3D</td><td>null</td><td>Data Engineer INTL India - EOR 6fb570f8</td><td>Insight Global</td></tr><tr><td>List(Map(title -> Sanofi, link -> https://jobs.sanofi.com/en/job/hyderabad/r-and-d-data-engineer/2649/24948624128?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Insight Global, link -> https://insightglobal.com/jobs/find_a_job/job-416162?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://zoetis.wd5.myworkdayjobs.com/pt-BR/zoetis_intl/job/Hyderabad/Data-Engineer---Testing_JR00018487/apply/autofillWithResume?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Cutshort, link -> https://cutshort.io/job/Big-Data-Engineer-Hyderabad-Jobdost-lJsfLSlo?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/big-data-engineer-methodhub-software-pvt-ltd-JV_IC2865319_KO0,17_KE18,44.htm?jl=1009753191511&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Wellfound, link -> https://wellfound.com/jobs/3298010-product-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/data-engineer-hyd-noida/arrise-solutions-india-pvt-ltd/17050236?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/4hTLBb8k83gtypVXlPanXweOFnq7S_xenP6fxizzQ6cQjRbm4VtNnQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Sanofi</td><td>Position Title: R&D Data Engineer\n",
       "\n",
       "About the Job\n",
       "\n",
       "At Sanofi, we’re committed to providing the next-gen healthcare that patients and customers need. It’s about harnessing data insights and leveraging AI responsibly to search deeper and solve sooner than ever before. Join our R&D Data & AI Products and Platforms Team as an R&D Data Engineer\n",
       "\n",
       "and you can help make it happen.\n",
       "\n",
       "What you will be doing:\n",
       "\n",
       "Sanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives.\n",
       "\n",
       "The R&D Data & AI Products and Platforms Team is a key team within R&D Digital, focused on developing and delivering Data and AI products for R&D use cases. This team plays a critical role in pursuing broader democratization of data across R&D and providing the foundation to scale AI/ML, advanced analytics, and operational analytics capabilities.\n",
       "\n",
       "As an R&D Data Engineer, you will join this dynamic team committed to driving strategic and operational digital priorities and initiatives in R&D. You will work as a part of a Data & AI Product Delivery Pod, lead by a Product Owner, in an agile environment to deliver Data & AI Products. As a part of this team, you will be responsible for the design and development of data pipelines and workflows to ingest, curate, process, and store large volumes of complex structured and unstructured data. You will have the ability to work on multiple data products serving multiple areas of the business.\n",
       "\n",
       "Our vision for digital, data analytics and AI\n",
       "\n",
       "Join us on our journey in enabling Sanofi’s Digital Transformation through becoming an AI first organization. This means:\n",
       "• AI Factory - Versatile Teams Operating in Cross Functional Pods: Utilizing digital and data resources to develop AI products, bringing data management, AI and product development skills to products, programs and projects to create an agile, fulfilling and meaningful work environment.\n",
       "• Leading Edge Tech Stack: Experience build products that will be deployed globally on a leading-edge tech stack.\n",
       "• World Class Mentorship and Training: Working with renown leaders and academics in machine learning to further develop your skillsets.\n",
       "\n",
       "We are an innovative global healthcare company with one purpose: to chase the miracles of science to improve people’s lives. We’re also a company where you can flourish and grow your career, with countless opportunities to explore, make connections with people, and stretch the limits of what you thought was possible. Ready to get started?\n",
       "\n",
       "Main Responsibilities:\n",
       "\n",
       "Data Product Engineering:\n",
       "• Provide input into the engineering feasibility of developing specific R&D Data/AI Products\n",
       "• Provide input to Data/AI Product Owner and Scrum Master to support with planning, capacity, and resource estimates\n",
       "• Design, build, and maintain scalable and reusable ETL / ELT pipelines to ingest, transform, clean, and load data from sources into central platforms / repositories\n",
       "• Structure and provision data to support modeling and data discovery, including filtering, tagging, joining, parsing and normalizing data\n",
       "• Collaborate with Data/AI Product Owner and Scrum Master to share Progress on engineering activities and inform of any delays, issues, bugs, or risks with proposed remediation plans\n",
       "• Design, develop, and deploy APIs, data feeds, or specific features required by product design and user stories\n",
       "• Optimize data workflows to drive high performance and reliability of implemented data products\n",
       "• Oversee and support junior engineer with Data/AI Product testing requirements and execution\n",
       "\n",
       "Innovation & Team Collaboration:\n",
       "• Stay current on industry trends, emerging technologies, and best practices in data product engineering\n",
       "• Contribute to a team culture of of innovation, collaboration, and continuous learning within the product team\n",
       "\n",
       "About You:\n",
       "\n",
       "Key Functional Requirements & Qualifications:\n",
       "• Bachelor’s degree in software engineering or related field, or equivalent work experience\n",
       "• 3-5 years of experience in data product engineering, software engineering, or other related field\n",
       "• Understanding of R&D business and data environment preferred\n",
       "• Excellent communication and collaboration skills\n",
       "• Working knowledge and comfort working with Agile methodologies\n",
       "\n",
       "Key Technical Requirements & Qualifications:\n",
       "• Proficiency with data analytics and statistical software (incl. SQL, Python, Java, Excel, AWS, Snowflake, Informatica)\n",
       "• Deep understanding and proven track record of developing data pipelines and workflows\n",
       "\n",
       "Why Choose Us?\n",
       "• Bring the miracles of science to life alongside a supportive, future-focused team\n",
       "• Discover endless opportunities to grow your talent and drive your career, whether it’s through a promotion or lateral move, at home or internationally\n",
       "• Enjoy a thoughtful, well-crafted rewards package that recognizes your contribution and amplifies your impact\n",
       "• Take good care of yourself and your family, with a wide range of health and wellbeing benefits including high-quality healthcare, prevention and wellness programs\n",
       "\n",
       "Pursue Progress. Discover Extraordinary.\n",
       "\n",
       "Progress doesn’t happen without people – people from different backgrounds, in different locations, doing different roles, all united by one thing: a desire to make miracles happen. You can be one of those people. Chasing change, embracing new ideas and exploring all the opportunities we have to offer. Let’s pursue progress. And let’s discover Extraordinary together.\n",
       "\n",
       "At Sanofi, we provide equal opportunities to all regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or gender identity.\n",
       "\n",
       "Watch our ALL IN video and check out our Diversity Equity and Inclusion actions at sanofi.com!</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJSXHUwMDI2RCBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiU2Fub2ZpIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJWUzRuREMyZ2hRajk3SVhDQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=VS4nDC2ghQj97IXCAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXEOw7CMAwAULH2CExeQAiVBCGxwFrEZwT2ykndNCjYVZyhXIIzI97wqu-sWt2XDTRYEE4cIhNl2MBNHChh9gMIw1kkJJofh1JGPVirmkzQgiV64-VthcnJZF_i9F-rA2YaExZqd_vtZEYO68UDWfoIkeHy6Sijw66GJyXkgIw1XLmL-AOLzZUukQAAAA&shmds=v1_AdeF8KiUiGHMq1ccmErcIZrh8u6TiBmwaiEY0ku0I3erwamrWQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=VS4nDC2ghQj97IXCAAAAAA%3D%3D</td><td>null</td><td>R&D Data Engineer</td><td>Sanofi</td></tr><tr><td>List(Map(title -> FedEx Careers, link -> https://careers.fedex.com/data-engineer-senior-ii/job/P25-195813-1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/149927346/software-engineer-senior-pi604-hyderabad/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Federal Express Corporation AMEA</td><td>Responsible for supporting the development and maintenance of business intelligence and analytics systems, implementing data warehouse architecture, enabling self-service data exploration, designing data models, implementing data security strategies, optimizing production processes, providing user training and support, and overseeing project management activities as a Data Engineer to facilitate data-driven decision-making and generate insights for business stakeholders.\n",
       "\n",
       "1. Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making.\n",
       "2. Implement of business intelligence and analytics systems, ensuring alignment with business requirements.\n",
       "3. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets.\n",
       "4. Enable self-service data exploration capabilities for users to analyze and visualize data independently.\n",
       "5. Develop reporting and analysis applications to generate insights from data for business stakeholders.\n",
       "6. Design and implement data models to organize and structure data for analytical purposes.\n",
       "7. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information.\n",
       "8. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability.\n",
       "9. Assist in training and support to users on business intelligence tools and applications.\n",
       "10. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions.\n",
       "\n",
       "Education: Bachelors’ degree or equivalent in Computer Science, MIS, or similar discipline.\n",
       "Accreditation: Specific business accreditation for Business Intelligence.\n",
       "\n",
       "Experience: Relevant work experience in data engineering based on the following number of years:\n",
       "Associate: Prior experience not required\n",
       "Standard I: Two (2) years\n",
       "Standard II: Three (3) years\n",
       "Senior I: Four (4) years\n",
       "Senior II: Five (5) years\n",
       "\n",
       "Knowledge, Skills and Abilities\n",
       "• Fluency in English\n",
       "• Analytical Skills\n",
       "• Accuracy & Attention to Detail\n",
       "• Numerical Skills\n",
       "• Planning & Organizing Skills\n",
       "• Presentation Skills\n",
       "\n",
       "Preferred Qualifications:\n",
       "\n",
       "Pay Transparency:\n",
       "\n",
       "Pay:\n",
       "\n",
       "Additional Details:\n",
       "\n",
       "FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n",
       "\n",
       "All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\n",
       "Our Company\n",
       "\n",
       "FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World’s Most Admired Companies by \"Fortune\" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\n",
       "Our Philosophy\n",
       "\n",
       "The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\n",
       "Our Culture\n",
       "\n",
       "Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970’s. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today’s global marketplace.</td><td>Map(posted_at -> 1 day ago, schedule_type -> Full-time)</td><td>List(1 day ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVNlbmlvciBJSSIsImNvbXBhbnlfbmFtZSI6IkZlZGVyYWwgRXhwcmVzcyBDb3Jwb3JhdGlvbiBBTUVBIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJhZDU5RmU0Vl94QzdTVGFqQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=ad59Fe4V_xC7STajAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xWNMQrCQBAAsc0TrLaWeBHBRgsJGjWClfZhkyyXk3P3uL0ifso3mjRTDTPZb5GZMyaEiq1jorh-EjuJUNewhru0oISxG0AYriLW0_IwpBR0XxSq3lhNmFxnOvkUwtTKWLyl1RmNDhgpeEzUbHeb0QS2q-OFeorooRpDJFU4SQwSp8Y0KB9VCY7h9p2dFvscXuSRLTLmUHPv8A_4kYajsQAAAA&shmds=v1_AdeF8KjaG6aMORYGf4051X3suZ-qy95n3Jiwr0ohUmRB9vMyoQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=ad59Fe4V_xC7STajAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c1f32f11d4cca51f4fc/images/56fc5e069c9f2d486126bca3dea35dd097fe2d1a2d8eb4211d8e7c9b3a358a8c.png</td><td>Data Engineer-Senior II</td><td>FedEx Careers</td></tr><tr><td>List(Map(title -> Thomson Reuters Careers, link -> https://careers.thomsonreuters.com/us/en/job/JREQ188466/Lead-Data-Engineer-Snowflake-PowerBi?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> S&P Global Apply, link -> https://careers.spglobal.com/jobs/312785?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Tide, link -> https://www.tide.co/careers/lead-data-engineersnowflake-dbt-6039463003/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/lead-data-engineer-snowflake-dbt-careers-at-tide-JV_IC2865319_KO0,32_KE33,48.htm?jl=1009341668378&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> The Muse, link -> https://www.themuse.com/jobs/thomsonreuters/lead-data-engineersnowflakepowerbi?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/JYBWkXhp61Q0w6JFgWr0APlCESuwOqQe2Xf_ccRFbUHZhbBYmzmXGQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Antal Tech Jobs, link -> https://www.antaltechjobs.in/job/lead-data-scientist-r-247933?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/lead-data-engineer-snowflake-powerbi-at-thomson-reuters-4203795186?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Thomson Reuters</td><td>Want to be part of the Data & Analytics organization, whose strategic goal is to create a world-class Data & Analytics company by building, embedding, and maturing a data-driven culture across Thomson Reuters.\n",
       "\n",
       "About The Role\n",
       "We are looking for a highly motivated individual with strong organizational and technical skills for the position of Lead Data Engineer/ Data Engineering Manager (Snowflake). You will play a critical role working on cutting edge of Data Engineering and analytics, leveraging predictive models, machine learning and generative AI to drive business insights and facilitating informed decision-making and help Thomson Reuters rapidly scale data-driven initiatives.\n",
       "\n",
       "Effectively communicate across various levels, including Executives, and functions within the global organization.\n",
       "Demonstrate strong leadership skills with ability to drive projects/tasks to delivering value\n",
       "Engage with stakeholders, business analysts and project team to understand the data requirements.\n",
       "Design analytical frameworks to provide insights into a business problem.\n",
       "Explore and visualize multiple data sets to understand data available and prepare data for problem solving.\n",
       "Design database models (if a data mart or operational data store is required to aggregate data for modeling).\n",
       "\n",
       "About You\n",
       "You're a fit for the Lead Data Engineer/ Data Engineering Manager (Snowflake), if your background includes:\n",
       "Qualifications: B-Tech/M-Tech/MCA or equivalent\n",
       "Experience: 7-9 years of corporate experience\n",
       "Location: Bangalore, India\n",
       "Hands-on experience in developing data models for large scale data warehouse/data Lake – Snowflake, BW\n",
       "Map the data journey from operational system sources through any transformations in transit to its delivery into enterprise repositories (Warehouse, Data Lake, Master Data, etc.)\n",
       "Enabling on the overall master and reference data strategy, including the procedures to ensure the consistency and quality of Finance reference data.\n",
       "Experience across ETL, SQL and other emerging data technologies with experience in integrations of a cloud-based analytics environment\n",
       "Build and refine end-to-end data workflows to offer actionable insights\n",
       "Fair understanding of Data Strategy, Data Governance Process\n",
       "Knowledge in BI analytics and visualization tools: Power BI, Tableau\n",
       "\n",
       "#LI-NR1\n",
       "\n",
       "What’s in it For You?\n",
       "• Hybrid Work Model: We’ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\n",
       "• Flexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\n",
       "• Career Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow’s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\n",
       "• Industry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\n",
       "• Culture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\n",
       "• Social Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\n",
       "• Making a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\n",
       "\n",
       "About Us\n",
       "\n",
       "Thomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\n",
       "\n",
       "We are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\n",
       "\n",
       "As a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\n",
       "\n",
       "We also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here.\n",
       "\n",
       "Learn more on how to protect yourself from fraudulent job postings here.\n",
       "\n",
       "More information about Thomson Reuters can be found on thomsonreuters.com.</td><td>Map(posted_at -> 13 days ago, schedule_type -> Full-time)</td><td>List(13 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIoU25vd2ZsYWtlLFBvd2VyQmkpIiwiY29tcGFueV9uYW1lIjoiVGhvbXNvbiBSZXV0ZXJzIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJzSFl5TXBoOXlWbDl4R0pBQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India (+1 other)</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=sHYyMph9yVl9xGJAAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXMOw6CQBAA0NhyBKsp1SAYExu1Mho_sTBKTwYYl9VlhuysAc_kJdXmlS_6DKL1mbCCLQaEHRvLRH50Y-nuDp8UX6Qjv7FjmMJJClBCX9YgDHsR42i4qkNodZmmqi4xGjDYMimlSYWpkD59SKF_cq3RU-swUD5fzPqkZTNJsloa_WVXegXyCpbh8K7IY4FVDBk5ZIOMMRy5svgFQsAuQq4AAAA&shmds=v1_AdeF8Kh8-j-XzHUufgizyl-nmbADY2mWLIPoo7WgtBtGnVAKsw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=sHYyMph9yVl9xGJAAAAAAA%3D%3D</td><td>null</td><td>Lead Data Engineer(Snowflake,PowerBi)</td><td>Thomson Reuters Careers</td></tr><tr><td>List(Map(title -> Verizon Careers, link -> https://mycareer.verizon.com/jobs/r-1056569/engr-ii-data-engineering/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/engr-ii-data-engineering-verizon-JV_IC2833209_KO0,24_KE25,32.htm?jl=1009742802532&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/7ab9InpGyrsCrGtIuQOt5AEX2zo92xBczrSyhf0x_v6SUgfGwEzs9A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/engr-ii-data-engineering-at-verizon-4234705104?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Flexa Careers, link -> https://flexa.careers/jobs/verizon-engr-ii-data-engineering-682498987cb2829cff4713e1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Analytics Hiring, link -> https://analyticshiring.com/job-detail/engr-ii-data-science-verizon-1-3-years-1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Verizon</td><td>When you join Verizon\n",
       "\n",
       "You want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n",
       "\n",
       "What you’ll be doing…\n",
       "\n",
       "We are looking for data engineers who can work with world class team members to help drive telecom business to its full potential . We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n",
       "\n",
       "As a data engineer you will be collaborating with business product owners , coaches , industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n",
       "\n",
       "As a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n",
       "\n",
       "Understanding the business requirements and the technical design.\n",
       "\n",
       "Working on Data Ingestion, Preparation and Transformation.\n",
       "\n",
       "Developing data streaming applications.\n",
       "\n",
       "Debugging the production failures and identifying the solution.\n",
       "\n",
       "Working on ETL/ELT development.\n",
       "\n",
       "Where you'll be working:\n",
       "\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "What we’re looking for...\n",
       "\n",
       "You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n",
       "\n",
       "You’ll need to have:\n",
       "\n",
       "Bachelor’s degree or one or more years of work experience.\n",
       "\n",
       "Experience with Data Warehouse concepts and Data Management life cycle.\n",
       "\n",
       "Experience in any DBMS\n",
       "\n",
       "Experience in Shell scripting, Spark, Scala.\n",
       "\n",
       "Knowledge in GCP/BigQuery.\n",
       "\n",
       "Even better if you have:\n",
       "\n",
       "Two or more years of relevant experience.\n",
       "\n",
       "Any relevant Certification on ETL/ELT developer.\n",
       "\n",
       "Certification in GCP-Data Engineer.\n",
       "\n",
       "Accuracy and attention to detail.\n",
       "\n",
       "Good problem solving, analytical, and research capabilities.\n",
       "\n",
       "Good verbal and written communication.\n",
       "\n",
       "Experience presenting to and influence stakeholders.\n",
       "\n",
       "#AI&D\n",
       "\n",
       "Where you’ll be working\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "Scheduled Weekly Hours\n",
       "40\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Verizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJJZ05hWTZ2NWxkNUFrcWRyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India (+1 other)</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=IgNaY6v5ld5AkqdrAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBCAYVz7CE43OUjbiOCiq6J1FtdyaY40Jd6VXIbqg_i8xuWHf_mq76oyF_YJuq45Y0YoE5goBfbQwF0sKGEaRhCGq4iPtD6NOc96NEY1tl4z5jC0g7yMMFlZzCRW_-l1xERzxEz9_rBb2pn9dvMs9qdggeH2dpTQoqvhQRHZI2MNHbuAP7q54GCZAAAA&shmds=v1_AdeF8Kiuwef0H887FylI9GPjfrbykKYgIDyHUa7VFIs0XgrQfA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=IgNaY6v5ld5AkqdrAAAAAA%3D%3D</td><td>null</td><td>Engr II-Data Engineering</td><td>Verizon Careers</td></tr><tr><td>List(Map(title -> EPAM, link -> https://www.epam.com/careers/job-listings/job.epamgdo_blte2153bda103f7b14_en-us_Hyderabad_India.senior-data-engineer-data-integration_hyderabad_india?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> EPAM, link -> https://careers.epam-poland.pl/careers/job-listings/job.epamgdo_bltb8357bde43ea0988_en-us_Hyderabad_India.senior-data-engineer_hyderabad_india?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> The Muse, link -> https://www.themuse.com/jobs/epamsystems/senior-data-engineer-data-integration-1fd4c1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Careers.epam.cn, link -> https://careers.epam.cn/job-listings/job.epamgdo_blte2153bda103f7b14_en-us_Hyderabad_India.senior-data-engineer-data-integration_hyderabad_india?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/40a4d1721b584eb315ac658dd0f0d8de?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> FinTech Australia Job Board, link -> https://jobs.fintechaustralia.org.au/companies/epam-systems/jobs/46250365-senior-data-engineer-data-integration?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>EPAM Systems</td><td>EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.\n",
       "\n",
       "Our company is looking for an experienced Senior Data Engineer to join our team.\n",
       "\n",
       "As a Senior Data Engineer, you will be working on a project that focuses on data integration and ETL for cloud-based platforms. You will be responsible for designing and implementing complex data solutions, ensuring that the data is accurate, reliable, and easily accessible.\n",
       "\n",
       "RESPONSIBILITIES\n",
       "• Design and implement complex data solutions for cloud-based platforms\n",
       "• Develop ETL processes using SQL, Python, and other relevant technologies\n",
       "• Ensure that data is accurate, reliable, and easily accessible for all stakeholders\n",
       "• Collaborate with cross-functional teams to understand data integration needs and requirements\n",
       "• Develop and maintain documentation, including technical specifications, data flow diagrams, and data mappings\n",
       "• Monitor and optimize data integration processes for performance and efficiency, ensuring data accuracy and integrity\n",
       "\n",
       "REQUIREMENTS\n",
       "• Bachelor's degree in Computer Science, Electrical Engineering, or a related field\n",
       "• 5-8 years of experience in data engineering\n",
       "• Experience with cloud-native or Spark-based ETL tools such as AWS Glue, Azure Data Factory, or GCP Dataflow\n",
       "• Strong knowledge of SQL for data querying and manipulation\n",
       "• Experience with Snowflake for data warehousing\n",
       "• Experience with cloud platforms such as AWS, GCP, or Azure for data storage and processing\n",
       "• Excellent problem-solving skills and attention to detail\n",
       "• Good verbal and written communication skills in English at a B2 level\n",
       "\n",
       "NICE TO HAVE\n",
       "• Experience with ETL using Python\n",
       "\n",
       "WE OFFER\n",
       "• Opportunity to work on technical challenges that may impact across geographies\n",
       "• Vast opportunities for self-development: online university, knowledge sharing opportunities globally, learning opportunities through external certifications\n",
       "• Opportunity to share your ideas on international platforms\n",
       "• Sponsored Tech Talks & Hackathons\n",
       "• Unlimited access to LinkedIn learning solutions\n",
       "• Possibility to relocate to any EPAM office for short and long-term projects\n",
       "• Focused individual development\n",
       "• Benefit package\n",
       "• Health benefits\n",
       "• Retirement benefits\n",
       "• Paid time off\n",
       "• Flexible benefits\n",
       "• Forums to explore beyond work passion (CSR, photography, painting, sports, etc.)</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAtIERhdGEgSW50ZWdyYXRpb24iLCJjb21wYW55X25hbWUiOiJFUEFNIFN5c3RlbXMiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZnZkJvSVM5OFhDSUw3NWNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India</td><td>Data Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=VgfBoIS98XCIL75cAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_yWOuwrCQBBFsc0nWE2tMSuCjYIgGHyAIMQ-TJJhs7KZCTtbmF_yK12xOXAOt7jZZ5YdKmInAU4YEUq2jokCrP5-5Ug2YHTCKd2kASUMbQ_JzyLW03zfxzjqzhhVX1iNadwWrQxGmBp5m5c0-kOtPQYaPUaqN9v1uxjZLpbl43iHatJIg4JjuEwdBWywy-FJHtkiY55udA6_8FxceK0AAAA&shmds=v1_AdeF8KipGi_N-DHgUjXB42f5_RXP26KCycnrS-oh3qg8MfFCoQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=VgfBoIS98XCIL75cAAAAAA%3D%3D</td><td>null</td><td>Senior Data Engineer - Data Integration</td><td>EPAM</td></tr><tr><td>List(Map(title -> Glassdoor, link -> https://www.glassdoor.com/job-listing/python-developer-%E2%80%93-telegram-bot-integration-and-excel-automation-sanga-and-associates-equidote-JV_KO0,64_KE65,94.htm?jl=1009786902391&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>SANGA & ASSOCIATES - EQUIDOTE</td><td>Job Title:\n",
       "\n",
       "Python Developer – Telegram Bot Integration & Excel Automation\n",
       "\n",
       "Job Description:\n",
       "\n",
       "We are seeking a skilled and detail-oriented Python Developer to help automate the process of sending structured trade updates from an Excel file to a Telegram channel using a Telegram bot.\n",
       "\n",
       "This is a freelance / part-time project with the potential for ongoing work based on performance.\n",
       "\n",
       "Responsibilities:\n",
       "• Read data from an Excel file that is regularly updated using Python.\n",
       "• Format and send messages to a Telegram channel or group using the Telegram Bot API.\n",
       "• Implement conditions to filter or trigger messages based on specific columns (e.g., P&L thresholds).\n",
       "• Ensure the messages are well-formatted and synchronized.\n",
       "• (Optional) Schedule the script to run at regular intervals (e.g., every 5 mins or on update).\n",
       "• Ensure error handling, avoid duplicate messages, and maintain clean logs.\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with Python scripting\n",
       "• Proficiency in using pandas for Excel/CSV handling\n",
       "• Working knowledge of the Telegram Bot API\n",
       "• Experience with HTTP requests (requests library)\n",
       "• Ability to format dynamic messages (Markdown/HTML for Telegram)\n",
       "• (Optional but a plus) Familiarity with scheduling tools like schedule, cron jobs, or apscheduler\n",
       "\n",
       "Nice to Have:\n",
       "• Understanding of stock market data or options trading (for better context)\n",
       "• Experience integrating with trading APIs or using TradingView alerts\n",
       "• Basic knowledge of Excel automation or VBA\n",
       "\n",
       "Project Details:\n",
       "• Project Type: One-time setup, with possible ongoing maintenance\n",
       "• Location: Remote (India preferred)\n",
       "• Start Date: Immediate\n",
       "\n",
       "How to Apply:\n",
       "\n",
       "Please apply with:\n",
       "• A short summary of your experience with Python + Telegram Bots\n",
       "• A link to any relevant projects or GitHub repos\n",
       "• Your expected rate and estimated time to complete the task\n",
       "\n",
       "Job Type: Freelance\n",
       "\n",
       "Benefits:\n",
       "• Health insurance\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "\n",
       "Supplemental Pay:\n",
       "• Performance bonus\n",
       "• Yearly bonus\n",
       "\n",
       "Work Location: Remote</td><td>Map(posted_at -> 1 day ago, work_from_home -> true, schedule_type -> Contractor, qualifications -> No degree mentioned)</td><td>List(1 day ago, Work from home, Contractor, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBUZWxlZ3JhbSBCb3QgSW50ZWdyYXRpb24gXHUwMDI2IEV4Y2VsIEF1dG9tYXRpb24iLCJjb21wYW55X25hbWUiOiJTQU5HQSBcdTAwMjYgQVNTT0NJQVRFUyAtIEVRVUlET1RFIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNBNXdGSEplRUV3c2lQVWJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Anywhere</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=cA5wFHJeEEwsiPUbAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWOwQqCQBRFaesntHorF0EaQZtaTSliiyw0WspoDzXGeeK8wnb9Q__Tx_QljbS5cC4H7nU-E-dyfHJNGgJ8oKIOe_i-3pChwqqXLWyJIdY8AjdWcyEcSlQg7kztv5rDngowKPuyBssRUaVwuqmZO7P2fWOUVxm2cumV1PqksaDBv1FhxshNLXvslGTMl6vF4HW6mrmpOETCrok0TXaxyMLU7oSncxwkWQiNtqeujfwBo0pJkMEAAAA&shmds=v1_AdeF8KgGojAhM22tqdWqg5a2PXRmpF45HPM2Fl94icndi1v0Vg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=cA5wFHJeEEwsiPUbAAAAAA%3D%3D</td><td>null</td><td>Python Developer – Telegram Bot Integration & Excel Automation</td><td>Glassdoor</td></tr><tr><td>List(Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=7922169ef6f54d0f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/rjdp/3736138824119121405?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/python-developer-remote/algorithmx/17085610?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobaaj, link -> https://www.jobaaj.com/job/vinnovate-solutions-python-developer-trainee-01-remote-greater-kolkata-area-0-to-15-years-486676?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/8babfc440260f4e75574129a6af82ad7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/remote-python-developer-17852-at-turing-4253263895?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Journal Du Coin Jobs, link -> https://jobs.journalducoin.com/job/remote-python-developer-17852-StPRG6NYIb-at-turing-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Expertini, link -> https://in.expertini.com/jobs/job/remote-python-developer-17852-narela-turing-3121-11626181/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Xpress Health</td><td>Job Title: Python Developer\n",
       "Location: Remote\n",
       "Salary: Up to ₹12 LPA (based on experience and skillset)\n",
       "Experience: 3–6 years (preferred)\n",
       "Employment Type: Full-time\n",
       "\n",
       "About Xpress Health\n",
       "\n",
       "Xpress Health is a healthtech platform transforming clinical workforce management by connecting healthcare professionals to facilities in real time. With operations across Ireland and rapid expansion underway, we’re solving critical staffing challenges in the healthcare sector using advanced technology and automation.\n",
       "\n",
       "Role Overview\n",
       "\n",
       "We are looking for a Python Developer with solid experience in backend development and building scalable, secure systems. You will work closely with cross-functional teams to build backend services, automate operations, and power data-driven features in our real-time healthcare staffing platform. If you're passionate about solving real-world problems through code, this role is for you.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and maintain backend services and APIs using Python (preferably with frameworks like Django or Flask).\n",
       "• Build scalable systems for real-time scheduling, user management, and analytics.\n",
       "• Integrate third-party APIs and internal services securely and efficiently.\n",
       "• Work with databases (SQL and NoSQL) to design efficient schemas and queries.\n",
       "• Optimize performance and ensure system reliability under scale.\n",
       "• Collaborate with frontend, product, and QA teams to deliver complete features.\n",
       "• Write clean, maintainable, and well-documented code.\n",
       "• Participate in code reviews, system design discussions, and architecture planning.\n",
       "\n",
       "Requirements\n",
       "• 3–6 years of professional experience with Python backend development.\n",
       "• Strong knowledge of Django, Flask, or other web frameworks.\n",
       "• Proficient in working with relational databases (MySQL/PostgreSQL) and REST APIs.\n",
       "• Experience with Git, CI/CD pipelines, and working in an Agile environment.\n",
       "• Strong debugging, testing, and problem-solving skills.\n",
       "• Good communication and ability to collaborate with remote teams.\n",
       "\n",
       "Preferred Qualifications\n",
       "• Experience in healthcare, staffing, or enterprise SaaS platforms.\n",
       "• Familiarity with containerization tools like Docker and orchestration platforms (Kubernetes).\n",
       "• Exposure to cloud platforms like AWS, GCP, or Azure.\n",
       "• Knowledge of async programming and task queues (e.g., Celery, Redis).\n",
       "• Experience working with frontend teams using React/Vue (a plus).\n",
       "\n",
       "What We Offer\n",
       "• Competitive salary up to ₹12 LPA, depending on experience.\n",
       "• A mission-driven environment working on meaningful, real-world problems.\n",
       "• Opportunity to shape a rapidly scaling healthtech product.\n",
       "• Flexible work culture with remote options and learning opportunities.\n",
       "• Collaborative, cross-functional team with international exposure.\n",
       "\n",
       "Be part of a product-first team building real-time tech that supports critical healthcare systems. Join Xpress Health as a Python Developer !!!\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: Up to ₹1,200,000.00 per year\n",
       "\n",
       "Benefits:\n",
       "• Paid time off\n",
       "• Work from home\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Evening shift\n",
       "• Fixed shift\n",
       "• Monday to Friday\n",
       "• UK shift\n",
       "\n",
       "Application Question(s):\n",
       "• What is your current and expected CTC?\n",
       "• Are you currently working? If yes, what is your notice period?\n",
       "\n",
       "Experience:\n",
       "• Python : 5 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>Map(posted_at -> 2 days ago, work_from_home -> true, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(2 days ago, Work from home, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIC0gUmVtb3RlIiwiY29tcGFueV9uYW1lIjoiWHByZXNzIEhlYWx0aCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJGWjc0LW4wM2NjU2xGWVVOQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Anywhere</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=FZ74-n03ccSlFYUNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKPQoCMRBAYWz3ABZWUwsmi2CjreBPJVZ2yyQOSSSbCZlB1hN4bdfm8RWv-y66_vbRyAWO9KbMlRps4E4jK824sgMhbD7CvJyYQ6bVIapW2Vsrkk0QRU3eeB4tF3I82Rc7-WeQiI1qRqVhu-snU0tYLx-1kQicCbNGSAUu5ZnwB4FBw8-KAAAA&shmds=v1_AdeF8KjVUwvIIq3UN363ahkdRLQt5573bng1Exift4YiGZG0IQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=FZ74-n03ccSlFYUNAAAAAA%3D%3D</td><td>null</td><td>Python Developer - Remote</td><td>Indeed</td></tr><tr><td>List(Map(title -> Hitachi Careers, link -> https://careers.hitachi.com/jobs/16246064-full-stack-developer-python-slash-react-js?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/full-stack-developer-python-react-js/hitachi-careers/17318841?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/full-stack-developer-appian-java/6385724?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Startup Jobs, link -> https://startup.jobs/full-stack-developer-python-react-js-hitachi-vantara-corporation-6843457?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Hitachi Careers</td><td>Our Company\n",
       "\n",
       "We're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n",
       "\n",
       "Our group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n",
       "\n",
       "Imagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n",
       "\n",
       "The team\n",
       "\n",
       "We are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "\n",
       "The role: Full Stack Developer/Specialist\n",
       "\n",
       "Responsibilities:\n",
       "• Design, develop, and maintain applications.\n",
       "• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n",
       "• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n",
       "• Optimise applications for performance, scalability, and user experience.\n",
       "• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n",
       "• Provide training and support to end-users and IT staff on functionalities and best practices.\n",
       "• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n",
       "• Participate in project planning, execution, and post-implementation support.\n",
       "• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n",
       "• What you'll bring\n",
       "Qualifications:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n",
       "• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n",
       "• Expertise in development and customisation.\n",
       "• Proficiency in Python and React JS\n",
       "• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n",
       "• Experience with RESTful APIs and web services.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Strong communication and collaboration skills.\n",
       "• Ability to work independently and as part of a team in a fast-paced environment.\n",
       "• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n",
       "• Prior experience in building AI applications is a plus.\n",
       "• Prior experience with automation tools like UIPath is a plus.\n",
       "• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\n",
       "Preferred Skills\n",
       "• Certification in\n",
       "• Experience with other low-code/no-code platforms.\n",
       "• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n",
       "• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n",
       "• Experience with Agile development methodologies.\n",
       "About us\n",
       "\n",
       "We're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n",
       "\n",
       "#LI-MS3\n",
       "\n",
       "Championing diversity, equity, and inclusion\n",
       "\n",
       "Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n",
       "\n",
       "How we look after you\n",
       "\n",
       "We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n",
       "\n",
       "We're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time)</td><td>List(4 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=gHUO0z59qIifBLiVAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNPQ7CMAxAYbH2BIjJIyDRICQWGBhA_HRCcIDKDVYTCHEUG1RuwnEpy1u-4RXfQbHZv0KAq6J9wI7eFDhRhvH5o44jGLgQWoXqOoEZVNyAEGbroLcDcxtotHaqSVbGiISyFUX1trT8NByp4c7cuZF_anGYKQVUqhfLeVem2E6HR9-fnYdtb5QFfIRTvHn8AQLWDNibAAAA&shmds=v1_AdeF8KiBbHp5HoJs2LSbE900z5gBevA-S4BK5odgvTSsP5in2w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=gHUO0z59qIifBLiVAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c217223f45d80e1a169/images/7ca52d7311167a4b971ff2d0af582580ff6d1717ced792605a7b527753be5d04.png</td><td>Full Stack Developer (Python / React JS)</td><td>Hitachi Careers</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/python-developer-role-at-pitangent-analytics-and-technology-solutions-pvt-ltd-4253593146?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/python-developer-role/pitangent-analytics-and-technology-solutions-pvt-ltd/17329373?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-2927-738d23fc657433716975fb6ed90bb670?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> PrepIntro, link -> https://www.prepintro.com/job/python-developer-role-pitangent-analytics-and-technology-solutions-pvt-ltd/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Pitangent Analytics and Technology Solutions Pvt. Ltd.</td><td>Overview\n",
       "\n",
       "Pi tangent Group of Companies is an ISO:9001: 2015 Certified, CMMIL-3, Award winning Software Development Company in Eastern India. It caters to areas like AI/ML to Web development to SAAS engineering. The Group is actively seeking out talented, enthusiastic Python Developers as a value addition to the growing team at Kolkata.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design and develop robust backend applications using Python.\n",
       "• Collaborate with front-end developers to integrate user-facing elements with server-side logic.\n",
       "• Implement RESTful APIs for seamless communication between server and client.\n",
       "• Write reusable, testable, and efficient code following best practices.\n",
       "• Manage and optimize multiple databases and data storage solutions.\n",
       "• Perform unit and integration testing to ensure software reliability.\n",
       "• Participate in code reviews and maintain version control in Git.\n",
       "• Gather and analyze user requirements to provide optimal solutions\n",
       "• Contribute to project documentation and specifications.\n",
       "• Collaborate with QA engineers to troubleshoot and resolve issues.\n",
       "• Maintain quality assurance processes to ensure best practices are enforced.\n",
       "• Engage in agile development practices, participating in sprints and meetings.\n",
       "• Mentor junior developers and provide guidance as needed.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor's degree in computer science or related field.\n",
       "• 1-2 yrs of experience in Python development.\n",
       "• Strong understanding of Django or Flask web frameworks.\n",
       "• Proficient with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB).\n",
       "• Experience with version control systems, preferably Git.\n",
       "• Solid understanding of RESTful API design principles.\n",
       "• Familiarity with front-end technologies (HTML, CSS, JavaScript).\n",
       "• Experience with containerization tools such as Docker.\n",
       "• Strong communication and teamwork abilities.\n",
       "• Familiarity with cloud services (AWS, Azure) is a plus.\n",
       "• Understanding of security principles and best practices.\n",
       "• Experience with Agile/Scrum methodologies.\n",
       "• Proven ability to manage multiple tasks and meet deadlines.\n",
       "\n",
       "Skills: agile,communication,nosql,scrum,aws lambda,docker,sql,azure,html,javascript,mongodb,css,django,aws,git,postgresql,restful apis,python,flask</td><td>Map(posted_at -> 2 days ago, schedule_type -> Full-time)</td><td>List(2 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIFJvbGUiLCJjb21wYW55X25hbWUiOiJQaXRhbmdlbnQgQW5hbHl0aWNzIGFuZCBUZWNobm9sb2d5IFNvbHV0aW9ucyBQdnQuIEx0ZC4iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiLUNHQ2RNdUVNQ2V4X3F2MkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=-CGCdMuEMCex_qv2AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLsQrCMBCAYVz7CE43iyYiuOgggiCKQ1H3krZHEol3oXeW9p18SOvyLx9_8Z0Vy3LUwAQn7DFxxg7unBBWcOUaBF3XBJj4zOwTzvdBNcvOWpFkvKjT2JiG35YJax7si2v5p5LgOszJKVab7XowmfziUEZ15JEUjuTSOL0Cjlp4YhOIE_sRHpw-GpkEyl4N3LQ1EAku1Eb3A0HKY1ivAAAA&shmds=v1_AdeF8KiDxLDc0gfRIaXbzvY14S2B0nHEtBpAt1Jh8t-Bcylozw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=-CGCdMuEMCex_qv2AAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c217223f45d80e1a169/images/7ca52d7311167a4bb7fce52e6e3afcd63870bf70216e5e0689db804b95796ad4.jpeg</td><td>Python Developer Role</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/python-and-groovy-framework-developer-at-aptita-4252137179?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Adzuna, link -> https://www.adzuna.in/details/5260946971?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/98bb991ce97889cb619dc6d5e2e2f6cb?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/5148408131285417984?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Indianjob24h.in, link -> https://indianjob24h.in/ko/snmp-and-framework-developer-job100578?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobindian.in, link -> https://jobindian.in/it/snmp-and-framework-developer-job100578?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Aptita</td><td>Urgent Hiring!!!\n",
       "\n",
       "Role : Python and Groovy Framework Developer\n",
       "\n",
       "Mandatory Skills: Python, Appium, Groovy, Git\n",
       "\n",
       "Experience: 3 to 8 Years\n",
       "\n",
       "Location: Bengaluru\n",
       "\n",
       "Contract - 1Year\n",
       "\n",
       "Job Description:\n",
       "\n",
       "Qualifications\n",
       "\n",
       " Bachelor’s or master’s degree in Computer Science, Software Engineering, or a\n",
       "\n",
       "related field\n",
       "\n",
       " 6 to 8 years of relevant experience in quality assurance, with a proven track record in\n",
       "\n",
       "WebKit or browser engine testing, including team leadership responsibilities.\n",
       "\n",
       " Proficiency in CI/CD and test automation frameworks such as Appium, and scripting\n",
       "\n",
       "languages like Python, or Shell.\n",
       "\n",
       "Job Overview\n",
       "\n",
       "We are seeking a dynamic and experienced Lead Python & Groovy Framework developer\n",
       "\n",
       "to join our team You will be part of a fast-paced, Agile development team and work on a\n",
       "\n",
       "variety of projects, from building new tools and solutions to improving existing ones.\n",
       "\n",
       "In this role, you will have the chance to grow your skills and take your career to the next\n",
       "\n",
       "level. We offer a supportive, challenging, and exciting work environment, with\n",
       "\n",
       "opportunities for professional development, training, and advancement.\n",
       "\n",
       "If you are a Python & Groovy Framework Developer Engineer with a passion for\n",
       "\n",
       "technology and a drive to continuously improve processes, we want to hear from you!\n",
       "\n",
       "If you are passionate about browser engine technologies, performance optimization, and\n",
       "\n",
       "leadership, we encourage you to apply!\n",
       "\n",
       "Primary Skills:\n",
       "\n",
       " Strong experience in Python Framework development, with the ability to automate\n",
       "\n",
       "and optimize processes using Jenkins Pipeline script\n",
       "\n",
       " Good knowledge in Groovy scripting\n",
       "\n",
       " Expertise in CI/CD tools such as Jenkins, CircleCI, or GitLab\n",
       "\n",
       " Good understanding of Appium.\n",
       "\n",
       "Strong Problem solving and debugging skills.\n",
       "\n",
       " Excellent communication and collaboration skills, both with technical and non-\n",
       "\n",
       "technical stakeholders\n",
       "\n",
       " Version Control: Familiarity with version control systems such as Git for reviewing\n",
       "\n",
       "changes and ensuring test coverage.\n",
       "\n",
       " Communication: Strong communication and collaboration skills for working with\n",
       "\n",
       "cross-functional teams.\n",
       "\n",
       " Agile Methodologies: Experience with Agile Scrum methodologies\n",
       "\n",
       "Notice Period: Immediate- 30 Days\n",
       "\n",
       "Email to : sharmila.m@aptita.com\n",
       "\n",
       "·</td><td>Map(posted_at -> 2 days ago, schedule_type -> Contractor)</td><td>List(2 days ago, Contractor)</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gYW5kIEdyb292eSBGcmFtZXdvcmsgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQXB0aXRhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Imx1ckt5NFVRSi0wb3JkMi1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=lurKy4UQJ-0ord2-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFy7uDvdLJiI4KIuglh08g_KpT2aaJoLuaO2_-BHq8ubXvVZVKfHrJ4TYOqgLszjDNeCA725vOBCI0XOVGADd3YghKX18Os1cx9pdfSqWQ7WikTTi6KG1rQ8WE7keLJPdvKnEY-FckSlZrffTianfr08Zw2KEBLcUhfwC2SmAnCPAAAA&shmds=v1_AdeF8Kg1C23WGoyyYiHbcJJj5H-E6x8Uv3rxsbNlyZq6zp-KYw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=lurKy4UQJ-0ord2-AAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c217223f45d80e1a169/images/7ca52d7311167a4bbcf9496c8d47b104687ec43b62a624497a9763b0ee63de1f.jpeg</td><td>Python and Groovy Framework Developer</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Allianz, link -> https://careers.allianz.com/global/en/job/74590/AI-Python-Developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Foundit.in, link -> https://www.foundit.in/job/python-ai-developer-antal-international-network-india-34929408?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobgether, link -> https://jobgether.com/offer/663670de270b7304e3768e52-python-ai-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/5931ed0d09d410418c22498c411007aa?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Unstop, link -> https://unstop.com/jobs/ai-python-developer-allianz-technology-india-1502774?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobaaj, link -> https://www.jobaaj.com/job/mogi-i-o-ott-podcast-short-video-apps-for-you-python-ai-developer-india-4-to-8-years-528567?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> WIZBII Jobs, link -> https://en.wizbii.com/company/allianz/job/ai-python-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-2927-f587076fe15195988499f1600c4dde9c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Allianz Insurance</td><td>We are seeking a skilled AI Developer proficient in Python to support various AI use cases within our dynamic team. The ideal candidate will possess a strong understanding of Python programming and a basic familiarity with PyTest. This role will also provide opportunities to work with cloud computing technologies, particularly Azure, and develop APIs.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Utilize advanced Python skills to support and enhance AI use cases, ensuring the efficient and effective implementation of AI solutions.\n",
       "• Develop and execute comprehensive test scripts using PyTest to ensure the reliability and accuracy of AI models and applications.\n",
       "• Collaborate with cross-functional teams, including data scientists, engineers, and product managers, to integrate AI capabilities into existing systems and workflows.\n",
       "• Analyze and optimize AI algorithms for performance and scalability, ensuring they meet business requirements.\n",
       "• Stay updated with the latest advancements in AI and machine learning technologies and apply them to improve existing solutions.\n",
       "\n",
       "Requirements:\n",
       "\n",
       "Must-Have\n",
       "• Python: Advanced proficiency with extensive experience in writing efficient, maintainable, and scalable code. Demonstrated ability to solve complex problems using Python.\n",
       "• PyTest: Basic experience in writing and executing unit tests, with a fundamental understanding of test-driven development practices.\n",
       "\n",
       "Good-to-Have\n",
       "• Cloud Computing: Familiarity with cloud platforms and services, with hands-on experience in deploying and managing AI applications in a cloud environment.\n",
       "• Azure: Experience with Microsoft Azure, particularly in using its AI and machine learning services.\n",
       "• APIs: Knowledge of designing, developing, and consuming APIs, particularly RESTful APIs, for integrating AI solutions with other systems and applications\n",
       "\n",
       "About Allianz Technology\n",
       "\n",
       "Allianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 13,000 employees located in 22 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry. We oversee the full digitalization spectrum – from one of the industry’s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n",
       "\n",
       "D&I statement\n",
       "\n",
       "Allianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, race or ethnicity, age, nationality, religion, disability, or philosophy of life. Join us. Let´s care for tomorrow.</td><td>Map(posted_at -> 24 days ago, schedule_type -> Full-time)</td><td>List(24 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJBSSBQeXRob24gRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQWxsaWFueiBJbnN1cmFuY2UiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQkg5Z0tqUFM2TjBGeExOb0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=BH9gKjPS6N0FxLNoAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCMBCAYVz7CJ1uLtiI4KJTQSh18g3KJR5JJN6FXJTq7ntblx8-_ua7abphguu7BmE404uSZCqwhYtYUMLiAqxnFPGJ2lOoNevRGNXUe61Yo-udPIwwWVnMXaz-M2vAQjlhpXl_2C19Zt-1Q0oR-QMT67MgO4LIK24Rf5NpGzqIAAAA&shmds=v1_AdeF8KiayGId8JpU67KYeQvNrYxf2FRgPNJcuJ1hpC22xryV5g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=BH9gKjPS6N0FxLNoAAAAAA%3D%3D</td><td>null</td><td>AI Python Developer</td><td>Allianz</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/developer-angular-python-azure-at-the-value-maximizer-4253121431?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Startup Jobs, link -> https://startup.jobs/developer-angular-python-azure-qode-6889841?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/155673037/n135-developer-angular-python-azure-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> PrepIntro, link -> https://www.prepintro.com/job/developer-angular-python-azure-the-value-maximizer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>The Value Maximizer</td><td>About the Role :\n",
       "\n",
       "As a Senior Developer, you will play a critical role in designing, implementing, and optimizing IoT applications, Salesforce development, and web solutions. This role is ideal for experienced developers eager to lead projects, mentor junior team members, and shape technical strategies across diverse platforms and technologies. You will be an important part of our international project teams, will work closely with team members from the headquarters and if qualified can lead international software development projects.\n",
       "\n",
       "Key Responsibilities :\n",
       "• Central role and expert for development of IoT frontend applications with Angular, Capacitor, and PrimeNG\n",
       "• Design and implement IoT backend systems using C#, .Net 8.0, and the Azure platform\n",
       "• Management and optimization of CI/CD pipelines using Azure DevOps and Bicep\n",
       "• Drive advanced Salesforce development efforts, including customizations, integrations, and automation\n",
       "• Manage and enhance the SharePoint Online intranet platform\n",
       "• Architect and implement Power Platform solutions tailored to business needs\n",
       "• Develop and maintain complex web applications using Django (Python) and PHP\n",
       "• Supervise website development and optimization with Typo3, PHP, and SolR, collaborating closely with the marketing department\n",
       "• Monitor and ensure the performance, compliance, and SEO optimization of global websites and portals\n",
       "\n",
       "Key Requirements :\n",
       "• 6-8 years of experience\n",
       "• Strong expertise in Angular, Python, and C#\n",
       "• Advanced experience with CI/CD pipelines and version control systems (e.g., Azure DevOps or Git)\n",
       "• Proficiency in web technologies such as PHP, MySQL, and modern frameworks\n",
       "• In-depth knowledge of Salesforce, Azure cloud solutions, and SharePoint Online\n",
       "• Proven ability to lead technical projects, mentor team members, and collaborate across departments\n",
       "• Excellent problem-solving skills, attention to detail, and a proactive approach to innovation\n",
       "• Proficiency in German is an advantage but not a requirement. Candidates must, however, have a minimum of B2-level English proficiency to communicate effectively within the international teams and our headquarters</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time)</td><td>List(4 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEZXZlbG9wZXItIEFuZ3VsYXIsIFB5dGhvbiBcdTAwMjYgQXp1cmUiLCJjb21wYW55X25hbWUiOiJUaGUgVmFsdWUgTWF4aW1pemVyIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRteXhMclAxTG5oaWFJZEVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=TmyxLrP1LnhiaIdEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXIsQrCMBAAUFz7CeJwk4PYRAQHdSoIoiA4iGu51iOJpLmQS6X2Q_xedXnDKz6TYnugF3mOlEqoguk9piVc39lygDlUY58ISjhzA0KYWgu_PzIbT9O9zTnKTmsRr4xkzK5VLXeaAzU86Cc38qcWi4mix0z1erMaVAxmMbtZgjv6nuCCg-vcSAlcgFN4OPwCbU-Do5kAAAA&shmds=v1_AdeF8KjYHF0YxKON-lXUVNHhoroFrj1uQG5NbM40EoMOFsKyGA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=TmyxLrP1LnhiaIdEAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c217223f45d80e1a169/images/7ca52d7311167a4b2adaf1e3020b5d6aed08e2ec45299500cc6be0568fcaacfb.png</td><td>Developer- Angular, Python & Azure</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=24b103e5e62e2d5e&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/freelance-python-developer-gbim-technologies-JV_KO0,26_KE27,44.htm?jl=1009784121958&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>GBIM Technologies Pvt.Ltd.</td><td>We’re Hiring – Freelance Python Developer (Experienced)\n",
       "We are currently looking for a freelance Python developer who brings hands-on experience and can help us build and debug efficient backend systems, automate processes, and solve critical issues with speed and precision.\n",
       "Key Expertise Required:\n",
       "\n",
       "Python (Backend Development)\n",
       "\n",
       "Web Scraping & Data Extraction\n",
       "\n",
       "Web Automation\n",
       "\n",
       "Flask | Pandas | ETL\n",
       "\n",
       "AWS (Basic to Intermediate)\n",
       "\n",
       "Google / Meta / LinkedIn / Third-Party API Integration\n",
       "\n",
       "Problem-solving mindset – quick in identifying & fixing bugs/errors\n",
       "\n",
       "If you are a solution-driven individual with a solid track record in handling Python-based backend projects and automation tasks, we’d love to connect with you!\n",
       "Please DM or share your portfolio/work samples at [hr@gbim.com / sarabjeet.mann@gbim.in ].\n",
       "#PythonDeveloper #FreelanceOpportunity #WebScraping #Automation #BackendDevelopment #Flask #AWS #APIIntegration #DataExtraction #HiringNow #LinkedInJobs\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500.00 - ₹10,000.00 per hour\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "• Monday to Friday\n",
       "\n",
       "Work Location: Remote\n",
       "\n",
       "Speak with the employer\n",
       "+91-XXXXXXXXXX</td><td>Map(posted_at -> 3 days ago, work_from_home -> true, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(3 days ago, Work from home, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkdCSU0gVGVjaG5vbG9naWVzIFB2dC5MdGQuIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNtMXFFOGh4Y2xUN3Z1ck9BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Anywhere</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCMBAAUFz7CU4HboKJCi66FbFUFDq4lzQ9kkjMhdxR6of4v-rylld9FtXuUhCjSRahe4unBGecMFLGAhu40gCMplgPv2mIXMTlyYtkPmrNHJVjMRKssvTSlHCgWT9p4D89e1MwRyPY7w_bWeXk1qumbu_wQOsTRXIBGbpJ1E1GBSFBm8ZgvjqIsEWYAAAA&shmds=v1_AdeF8KjwVw1gvcgbXG9h7aNO0NWe7x2POcgCQEOiGyU2sC4Mww&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D</td><td>null</td><td>Freelance Python Developer</td><td>Indeed</td></tr><tr><td>List(Map(title -> Jobs, link -> https://jobs.ashbyhq.com/dehazelabs/3fc045c8-c43d-4523-b258-80e35b2930ed/application?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=584b970ed8dd3bcc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/junior-python-developer-altheory-technologies-pvt-ltd-JV_KO0,23_KE24,53.htm?jl=1009767622150&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Wellfound, link -> https://wellfound.com/jobs/3280338-junior-python-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Growflex, link -> https://growflex.in/apply/2133/junior-python-developer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs.weekday.works, link -> https://jobs.weekday.works/capricorn-identity-services-private-limited-junior-python-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Talents Jobs, link -> https://talentsjobs.in/job-detail?job-id=20667&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/148905208/junior-python-developer-oa591-kozhikode/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Dehazelabs</td><td>Location: Onsite, Kokapet, Hyderabad, Telangana.\n",
       "\n",
       "Job Type: Full-Time\n",
       "\n",
       "About Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n",
       "\n",
       "Responsibilities:\n",
       "• Develop and maintain Python applications and services\n",
       "• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n",
       "• Write clean, efficient, and well-documented code\n",
       "• Troubleshoot, debug, and optimize applications for performance and scalability\n",
       "• Participate in code reviews and maintain coding standards\n",
       "• Stay up-to-date with the latest industry trends and technologies\n",
       "\n",
       "Requirements:\n",
       "• Ability to code in Python and SQL\n",
       "• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n",
       "• Knowledge of version control systems (e.g., Git)\n",
       "• Excellent problem-solving skills and attention to detail\n",
       "• Strong communication and teamwork abilities.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n",
       "• Knowledge of RESTful APIs and microservices architecture.</td><td>Map(salary -> ₹216K–₹420K a year, work_from_home -> true, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(₹216K–₹420K a year, Work from home, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Anywhere</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=LYbcWfU4skOE2uhgAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQoCMQwAUFxvdXPKLNiK4KLrgXiTf3CkNVwrNSlNlNPRL1eXt7zus-jc8OAsDS4vS8LQ05OKVGqwgUECKGGLCX5zEpkKrY7JrOrBe9XiJjW0HF2UuxemILO_SdA_oyZsVAsajbv9dnaVp_Wyp4RvKhgUMsOZrxm_tV-eZYUAAAA&shmds=v1_AdeF8KiLjAS30ie4ZRkW1kEru4d1GwgKEP1yzRfISnv7NJRViQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=LYbcWfU4skOE2uhgAAAAAA%3D%3D</td><td>null</td><td>Junior Python Developer</td><td>Jobs</td></tr><tr><td>List(Map(title -> Aijobs.net, link -> https://aijobs.net/job/1369160-det-senior-gig-python-developer-gdsnf02/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>EY</td><td>At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. Senior Python Developer – Client – Merck, H and M, TIAA, MINT Job Description Bachelor’s or master’s degree with 3+ years of strong Python development experience Design, develop, and maintain high-performance web applications using Python and related frameworks. Strong understanding of Python OOPs, Data type, Data Structure and algorithm , Exception handling, Decarotor , Generator, Iterator, Automation. Strong understanding of Python Libraries (Pandas, TensorFlow, Numpy, SciPy) Experience in Cloud Azure / AWS Develop, optimize, and manage complex APIs (RESTful or GraphQL). Collaborate with cross-functional teams to define, design, and ship new features. Troubleshoot and resolve advanced technical issues in development and production environments. Conduct technical evaluations of new tools and frameworks, recommending their adoption when appropriate. Stay ahead of emerging trends in Python development, ensuring the team remains at the forefront of innovation. Advanced proficiency in Python and frameworks like Django,Flask, or FastAPI. Good understanding of Database Postgres / MySQL & ORM Library i.e. SQL Alchemy/ any ORM libraries Understanding of Code Repository tools i.e. GIT , SVN Strong understanding of DevOps principles(Docker, Kubernetes and microservices) EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.</td><td>Map(posted_at -> 18 hours ago, salary -> 51,038–94,785 a year, schedule_type -> Full-time)</td><td>List(18 hours ago, 51,038–94,785 a year, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJERVQtU2VuaW9yIEdJRyBQeXRob24gRGV2ZWxvcGVyLUdEU05GMDIiLCJjb21wYW55X25hbWUiOiJFWSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ4Mk5pUFZHVVJ3cjczaVE1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>Python Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=x2NiPVGURwr73iQ5AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFzrHzjdLLQpBRcFp9ZQBxHq4lSSeiQpMRdyQeov-NXq8qZXfFbFse1u5YDBUQLZS7i-s6UALb7QU8RUyna4nOoGSjiTBkaVJgu_IYmMx83B5hx5LwSzrwxnld1UTfQUFFDTImbS_GdkqxJGrzKOza5eqhjMdt3dwQXow8OpL0RpNWeNAAAA&shmds=v1_AdeF8KgY457QONkoomgpIdSrLr3hsEX7tOwgdKEcCHxjKfOGyQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=x2NiPVGURwr73iQ5AAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c217223f45d80e1a169/images/7ca52d7311167a4b433854c3181b5ba3b78dcb1e1d229c20c1d60ca9f1f91f65.png</td><td>DET-Senior GIG Python Developer-GDSNF02</td><td>Aijobs.net</td></tr><tr><td>List(Map(title -> Aijobs.net, link -> https://aijobs.net/job/1354875-informatica-etl-developer-agile-dev-team-member-iv/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Startup Jobs, link -> https://startup.jobs/informatica-etl-developer-agile-dev-team-member-iv-capgemini-6880043?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Expertini, link -> https://in.expertini.com/jobs/job/informatica-etl-developer-agile-dev-team-member-iv-hyderabad-capgemini-1831-15741907/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/86ca67461e96d6d76960fcd37185e33f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Recruit.net, link -> https://www.recruit.net/job/informatica-etl-developer-agile-dev-jobs/19527C8BDF086A2E?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-3396-8964efb4f35b96b661630e25828096b5?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/119202638297300992?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs, link -> https://superjobss.com/jobs/b138ae46-4be7-4444-a591-9ed1308f8e03?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Capgemini</td><td>The ideal candidate will be responsible for designing, developing, and deploying scalable ETL processes using Informatica PowerCenter to support our data warehousing and analytics initiatives. You will collaborate with business and technical stakeholders to ensure high data quality, availability, and performance. Key Responsibilities:Design, develop, and maintain ETL workflows and mappings using Informatica PowerCenter or Informatica Intelligent Cloud Services (IICS).Extract, transform, and load data from various source systems (e.g., SQL Server, Oracle, flat files, cloud APIs) into data warehouses or operational data stores.Optimize ETL performance, conduct tuning, and ensure error handling and logging.Collaborate with data architects and analysts to understand data requirements and deliver high-quality data solutions.Work with QA teams to support data validation and testing efforts.Support data integration, migration, and transformation initiatives.Document ETL processes, data flows, and job schedules.Monitor daily ETL jobs and resolve production issues in a timely manner.RequirementsBachelor’s degree in Computer Science, Information Systems, or a related field (or equivalent work experience).3+ years of experience with Informatica PowerCenter or Informatica IICS.Strong SQL skills and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).Solid understanding of data warehousing concepts and dimensional modeling.Experience in performance tuning and troubleshooting ETL processes.Hands-on experience with job scheduling tools (e.g., Autosys, Control-M, Tidal).Familiarity with version control systems and DevOps practices.Preferred Qualifications:Experience with cloud data platforms (e.g., Snowflake, AWS Redshift, Azure Synapse).Exposure to data governance and data quality tools.Knowledge of scripting languages (e.g., Shell, Python).Experience working in Agile/Scrum environments.Familiarity with BI tools (e.g., Tableau, Power BI) is a plus. BenefitsThis position comes with competitive compensation and benefits package: Competitive salary and performance-based bonuses Comprehensive benefits package Home Office model Career development and training opportunities Flexible work arrangements (remote and/or office-based) Dynamic and inclusive work culture within a globally known group Private Health Insurance Pension Plan Paid Time Off Training & Development *Note: Benefits differ based on employee level</td><td>Map(posted_at -> 5 days ago, salary -> 42,532–78,988 a year, schedule_type -> Full-time)</td><td>List(5 days ago, 42,532–78,988 a year, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyOiBBZ2lsZSBEZXYgVGVhbSBNZW1iZXIgSVYiLCJjb21wYW55X25hbWUiOiJDYXBnZW1pbmkiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InFZVlFqNTFEM2VzZ0lZSExBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=qYVQj51D3esgIYHLAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNsQrCMBBAce0nON0ooq0ILnUSFW3RrbiWS3umkeQuJEHqd_mD2uUNb3gv-86yuuKnBIfJdAjn5gYnepMVT6GEgzaWJgENoYM7OUUBqgesoRYFkTB0AwjDRURbmu-HlHwsiyJGm-uYpmjeiSuESclYvETFCW0cMJC3mKjd7jZj7lkvF0f0mpxhA4bh-ukpoMJ-9X9bZI2MK6i4N_gDJid_bLYAAAA&shmds=v1_AdeF8KgutCk9f1YpFiMW9LRjDjlQvTE3YabFZ1dOEi5X0_1qMA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=qYVQj51D3esgIYHLAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9a071e98c03386e2d66cc6420deb3c41916471f950b524ff0c.png</td><td>Informatica ETL Developer: Agile Dev Team Member IV</td><td>Aijobs.net</td></tr><tr><td>List(Map(title -> S&P Global Apply, link -> https://careers.spglobal.com/jobs/316835?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Cutshort, link -> https://cutshort.io/job/Backend-Developer-and-Educator-Mumbai-Hyderabad-School-of-Accelerated-Learning-pE4X2gRB?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/senior-backend-developer-c-and-azure/waferwire-cloud-technologies/17025642?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/isE_hgbSc52mYanYlWpjDELjiugHsHZI7DhttbF2oMfer94PvJqoUg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/senior-etl-and-backend-developer-salesforce-%C2%A0-at-s-p-global-4238484393?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Taro, link -> https://www.jointaro.com/jobs/sandp-global/senior-etl-and-backend-developer-salesforce/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/bb31aed9683e64c520547d8fb2da3cd3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/senior-etl-and-backend-developer-salesforce-s-and-p-global-JV_IC2865319_KO0,43_KE44,58.htm?jl=1009760759098&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>S&P Global</td><td>About the Role:\n",
       "\n",
       "Grade Level (for internal use):\n",
       "10\n",
       "\n",
       "Title: Senior ETL and Backend Developer (Salesforce)\n",
       "\n",
       "Job Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n",
       "\n",
       "The Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n",
       "\n",
       "The team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n",
       "\n",
       "The Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n",
       "\n",
       "Responsibilities:\n",
       "• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n",
       "• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n",
       "• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n",
       "• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n",
       "• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n",
       "• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\n",
       "Monitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n",
       "• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\n",
       "Qualifications:\n",
       "\n",
       "Basic Qualifications:\n",
       "• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n",
       "• A minimum of 8+ years of experience in software engineering & Architecture.\n",
       "• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n",
       "• A minimum of 3+ years of Salesforce development, administration/Integration.\n",
       "• Proficiency in Informatica PowerCenter and other ETL tools.\n",
       "• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n",
       "• Experience with Salesforce integration and administration.\n",
       "• Proficiency in backend development languages (e.g., Java, Python, C#).\n",
       "• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Ability to work independently and as part of a team.\n",
       "• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n",
       "• Experience with other ETL tools and data integration platforms.\n",
       "• Informatica Certified Professional\n",
       "Salesforce Certified Administrator or Developer\n",
       "• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n",
       "• Excellent problem solving, analytical and technical troubleshooting skills.\n",
       "• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n",
       "• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n",
       "\n",
       "About S&P Global Commodity Insights\n",
       "At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n",
       "\n",
       "We’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n",
       "\n",
       "S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n",
       "\n",
       "For more information, visit http://www.spglobal.com/commodity-insights.\n",
       "\n",
       "What’s In It For You?\n",
       "\n",
       "Our Purpose:\n",
       "\n",
       "Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n",
       "\n",
       "Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n",
       "\n",
       "Our People:\n",
       "\n",
       "We're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n",
       "\n",
       "From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n",
       "\n",
       "Our Values:\n",
       "\n",
       "Integrity, Discovery, Partnership\n",
       "\n",
       "At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n",
       "\n",
       "Benefits:\n",
       "\n",
       "We take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n",
       "\n",
       "Our benefits include:\n",
       "• Health & Wellness: Health care coverage designed for the mind and body.\n",
       "• Flexible Downtime: Generous time off helps keep you energized for your time on.\n",
       "• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n",
       "• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n",
       "• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n",
       "• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n",
       "\n",
       "For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n",
       "\n",
       "Global Hiring and Opportunity at S&P Global:\n",
       "\n",
       "At S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "Equal Opportunity Employer\n",
       "\n",
       "S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n",
       "\n",
       "If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n",
       "\n",
       "US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n",
       "\n",
       "Job ID: 316835\n",
       "Posted On: 2025-06-03\n",
       "Location: Hyderabad, Telangana, India</td><td>Map(posted_at -> 19 days ago, schedule_type -> Full-time)</td><td>List(19 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjVGNUtpak5fRjROQXhDcERBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India (+1 other)</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=5F5KijN_F4NAxCpDAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXFsQrCMBAAUFz7CU43iZXaiuCim1Sq4iC0e7kkZ1uNdyEJUn_Kb1SX95LPJClr4kE8HJoLIBvYo37Q75JeZMWRh3mNlsJNvKYUlnAWBYHQ6x6EoRLpLE13fYwubIsiBJt3IWIcdK7lWQiTkrG4iwp_2tCjJ2cxUrverMbccbdI69kVKisKLQwMx7chjwpNBg1Z5A4ZMzixGfALm-qj5LEAAAA&shmds=v1_AdeF8KhDGHxAXJEjotU-fQxkvoBBqCk4nXlFRQEfCz-nQy1hiw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=5F5KijN_F4NAxCpDAAAAAA%3D%3D</td><td>null</td><td>Senior ETL and Backend Developer (Salesforce)</td><td>S&P Global Apply</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/etl-developer-at-zensar-technologies-4222824321?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/fx7vQKu-Lzh03WQHb6X6_XAGbcjI7i6bHcT89FwidlwXvLKBVndzPw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Adzuna, link -> https://www.adzuna.in/details/5192914688?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Recruit.net, link -> https://www.recruit.net/job/etl-developer-jobs/CFB50BF71002C882?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/6425154711267049472?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Zensar Technologies</td><td>Job Description\n",
       "\n",
       "Primary Skill Set\n",
       "• ETL Informatica\n",
       "• SQL\n",
       "• Unix\n",
       "• Realtime Data Integration (CDC) using Power Exchange (Can be optional if candidate good in other skills)\n",
       "\n",
       "Good to Have\n",
       "\n",
       "Experience on working with Mainframe Databases/files\n",
       "\n",
       "ETL Batch Scheduling tools like TWS/Tidal\n",
       "\n",
       "Roles & Responsibilities\n",
       "\n",
       "Informatica PowerCenter, Unix scripting, SQL/PLSQL\n",
       "\n",
       "Knowledge of Informatica Power Exchange is preferred\n",
       "\n",
       "Experience With Mainframe Sources/targets Is Preferred\n",
       "• Bachelor’s degree in Computer Science or similar field or equivalent work experience.\n",
       "• 5-8 years of development experience with Informatica Power Center on Data Integration projects.\n",
       "• Strong analytic, problem-solving and organizational skills.\n",
       "• Excellent SQL knowledge and ability to write the complex queries keeping performance aspect in mind.\n",
       "• Experience with analysis of business requirements, designing and writing technical specifications to design.\n",
       "• Hands-on experience to process mainframe files using Informatica Power Exchange.\n",
       "• Hands-on experience with UNIX shell scripting.\n",
       "• Participate in testing and issue resolution to validate functionality and performance.\n",
       "• Hands-on experience on any job scheduling tool, TWS is preferred.\n",
       "• Good written and verbal communication skills.\n",
       "\n",
       "Location\n",
       "\n",
       "1 st Preference: Noida\n",
       "\n",
       "2 nd Preference: Hyderabad\n",
       "\n",
       "3 rd Preference: Gurgaon</td><td>Map(posted_at -> 11 days ago, schedule_type -> Full-time)</td><td>List(11 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiWmVuc2FyIFRlY2hub2xvZ2llcyIsImFkZHJlc3NfY2l0eSI6Ik1hZGhhdmFyYW0sIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InJTQzlGVjVoRHhXZ1h3QURBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Madhavaram, Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=rSC9FV5hDxWgXwADAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCMBAAUFz7CU43OEltRHHRVRFFt04u5ZoeSSS9C7lQ-il-rrq85VWfRbW6tA8400RREmXYwF16UMJsPQjDVcRFWp58KUmPxqjGxmnBEmxjZTTC1Mts3tLrn049ZkoRC3W7w3ZuErv1_kWsmKEl61miuEAKgeGJg8cJM4717yKyQ8YabjwE_AJS2_7bmwAAAA&shmds=v1_AdeF8Kh9BDoIEsj9vh8KQn3UWTKTBDf_wrqKczqjw9E77y6qlQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=rSC9FV5hDxWgXwADAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9a1ddc478d76ec1ab5d1633c51cfe785421ef417997936cc2b.jpeg</td><td>ETL Developer</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Insight Global, link -> https://insightglobal.com/jobs/find_a_job/job-411040?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs, link -> https://www.amgen.jobs/hyderabad-ind/etl-developer/6CCD8EAF358B4D9AB9D502EEFB667993/job/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Hirist, link -> https://www.hirist.tech/j/etl-developer-1472601?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SmartRecruiters Job Search, link -> https://jobs.smartrecruiters.com/SQUIRCLEITCONSULTINGSERVICESPVTLTD/87728117-etl-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/VZteELnCIFig_PpewT1H4Xi550ZVJ7LxPCKfGowAKkMlN6qBv9WgZQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://unisys.wd5.myworkdayjobs.com/zh-CN/External/job/Hyderabad-Waverock/ETL-Developer_REQ565515?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> FirstDCS, link -> https://firstdcs.in/jobs/etl-developer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Adzuna, link -> https://www.adzuna.in/details/5220807061?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Insight Global</td><td>We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\n",
       "Responsibilities:\n",
       "Lead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\n",
       "Collaborate with developers to ensure a smooth transition and integration of the upgraded system.\n",
       "Enhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\n",
       "Provide support and guidance to the ETL development lead.\n",
       "Engage with end-users to ensure their needs are met during and after the upgrade process.\n",
       "Utilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\n",
       "Requirements:\n",
       "Minimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\n",
       "Proven experience with SQL Server upgrades, particularly from 2012 to 2022.\n",
       "Strong SQL Server skills, including building tables, stored procedures, views, and functions.\n",
       "Experience with .NET development is highly desirable to be leveraged for other projects\n",
       "Excellent problem-solving skills and attention to detail.\n",
       "Ability to work effectively in a team environment and communicate with end-users.\n",
       "Preferred Qualifications:\n",
       "Experience with SQL Server 2016.\n",
       "Experience with Oracle\n",
       "Familiarity with the latest features and enhancements in SQL Server 2022.\n",
       "Strong understanding of database architecture and migration strategies.\n",
       "\n",
       "We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>Map(schedule_type -> Contractor, qualifications -> No degree mentioned)</td><td>List(Contractor, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJJbnNpZ2h0IEdsb2JhbCIsImNvbXBhbnlfbmFtZSI6Ikluc2lnaHQgR2xvYmFsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=cEmBeIcvfhfX5HK2AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_13NMQ7CMAwAQLH2CUxeWFCbICQWeEApM3vlJFYSlNpVnKG8gw8jVpZbr_vsusPEmmNqMBZxWGCAhzhQwuoTCMMoEgvtb6m1Va_WqhYTtWHL3nhZrDA52exLnP6YNWGltWCj-Xw5bWbleBz-jsxwfweq6DD08KSCHJGxh4lDxi-RVypzlgAAAA&shmds=v1_AdeF8KhmLbrgMzBBwyUeoUQL_4hKnzeejX-h_elIeYvsdlgyPg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=cEmBeIcvfhfX5HK2AAAAAA%3D%3D</td><td>null</td><td>Insight Global</td><td>Insight Global</td></tr><tr><td>List(Map(title -> Built In, link -> https://builtin.com/job/etl-developer-hands-microsoft-sql-ssis-etl-and-t-sql-pune-location/2942610?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Fiserv</td><td>Calling all innovators – find your future at Fiserv.\n",
       "\n",
       "We’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n",
       "\n",
       "Job Title\n",
       "\n",
       "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n",
       "\n",
       "Work Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n",
       "• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n",
       "• Participates in client kickoff activities including requirements and data gathering sessions.\n",
       "• Ability to analyze and migrate incoming external data into Fiserv solutions.\n",
       "• Strong working knowledge of ETL processes and best practices.\n",
       "• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n",
       "• Documents business processes and identifies opportunities for process redesign.\n",
       "• Reports system defects and identifies opportunities for system enhancements.\n",
       "• Performs as a technical consulting resource for new clients during the implementation process.\n",
       "• Supports several project managers during the pre and post go-live activities\n",
       "• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n",
       "• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n",
       "• Provides technical and analytical guidance to the project team.\n",
       "• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n",
       "• The position requires working in shifts, 2 PM to 11 PM IST\n",
       "\n",
       "What you will need to have:\n",
       "• Minimum of 5 – 10 years of experience in software development.\n",
       "• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n",
       "• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n",
       "• Self-starter who can work effectively, both independently and in a team environment.\n",
       "• Strong analytical, organizational, and problem-solving skills.\n",
       "• Excellent verbal and written communication skills.\n",
       "• Ability to maintain a professional attitude and demeanor in high pressure situations.\n",
       "• Ability to multi-task and manage multiple projects simultaneously.\n",
       "• Weekend/evening availability and support (10% - 25%).\n",
       "\n",
       "What would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n",
       "\n",
       "Thank you for considering employment with Fiserv. Please:\n",
       "• Apply using your legal name\n",
       "• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n",
       "\n",
       "What you should know about us:\n",
       "\n",
       "Fiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n",
       "\n",
       "Our commitment to Diversity and Inclusion:\n",
       "\n",
       "We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n",
       "\n",
       "Warning about fake job posts:\n",
       "\n",
       "Please be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n",
       "\n",
       "Any communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n",
       "\n",
       "If you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=j_liFo3VQHqrD7_BAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMuwrCQBQEUGzT2FtNqZKHCDba-opEUGIvm801WYl7w941-GF-oGszxXBmou8oqne3AlsaqOOeHKZHZWsBW5yNdiz88CivRYyyzMsYAccIArcktJghweVtCQVr5U1YJThxBSHldPt_OTA3HU02rfe9rLNMpEsb8QHrVPMrY0sVf7InV_KPu7TKUd8pT_flavFJe9vMx3sj5AYYi9zWRv0AHMymQbcAAAA&shmds=v1_AdeF8KhckDUoX9CilVCOaJqU36weofSX83Pgps7flcqZRoh66g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=j_liFo3VQHqrD7_BAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9a6d0a97f3acb9c752fe1afaa4e525c4c085b61f46d0d02b02.png</td><td>ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location</td><td>Built In</td></tr><tr><td>List(Map(title -> Hirist, link -> https://www.hirist.tech/j/epam-etl-developer-ssis-ssrs-1495428?ref=rl&pref=rl&jobPos=4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/jdp/777603120491457089?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/df2976441b528bff274847b68dd6d964?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Swathi V</td><td>Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n",
       "\n",
       "Experience : 5+ Years\n",
       "\n",
       "Location : Hyderabad\n",
       "\n",
       "Job Description :\n",
       "\n",
       "We are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n",
       "\n",
       "You will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n",
       "\n",
       "Key Responsibilities :\n",
       "\n",
       "- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n",
       "\n",
       "- Develop, deploy, and maintain SSIS packages for ETL processes.\n",
       "\n",
       "- Work on data warehouse design, development, and maintenance.\n",
       "\n",
       "- Ensure high performance and reliability of data integration workflows.\n",
       "\n",
       "- Troubleshoot SQL queries and ETL issues; optimize database performance.\n",
       "\n",
       "- Collaborate with cross-functional teams to define data solutions.\n",
       "\n",
       "- Work with complex data structures including JSON and XML.\n",
       "\n",
       "- Understand and implement different data models (star, snowflake, etc.).</td><td>Map(schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IlN3YXRoaSBWIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ2ekQ3VTVqbHNoc2EtN3luQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=vzD7U5jlshsa-7ynAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHQQrCMBBAUdz2CK5mJ0htRBBBV4JFKwpiiluZtEMaiZmQBK2n8arWzef97DvKVuVle4YZlPUJdvQiy57C8FJWUkh5lYOPrCAShqYDdrBn1pbGmy4lH9dCxGgLHRMm0xQNPwU7UtyLB6v4zz12GMhbTHRfLOd94Z2eTuQbU2fgBsbB4dNSQIVtDjVZdBod5lC51uAPq3kl_aIAAAA&shmds=v1_AdeF8KhoyQuQp8opvuZyeZS3uH3V1UEvP3N1O5LDE48Myg1k8Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=vzD7U5jlshsa-7ynAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9ab5436031f86f8495670d9227c5f49d66c2ea8c68acaec26e.png</td><td>EPAM - ETL Developer - SSIS/SSRS</td><td>Hirist</td></tr><tr><td>List(Map(title -> Apna, link -> https://apna.co/job/gurgaon/etl-developer-1587516655?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>American Express Global Business Travel</td><td>ETL Developer\n",
       "\n",
       "Amex GBT is a place where colleagues find inspiration in travel as a force for good and – through their work – can make an impact on our industry. We’re here to help our colleagues achieve success and offer an inclusive and collaborative culture where your voice is valued.\n",
       "\n",
       "We are looking for an experienced Data ETL Developer / BI Engineer who loves solving complex problems across a full spectrum of data & technologies. You will lead the building effort of GBT's new BI platform and manage the legacy platform to seamlessly support our business function around data and analytics. You will create dashboards, databases, and other platforms that allow for the efficient collection and evaluation of BI data.\n",
       "\n",
       "What You’ll Do on a Typical Day:\n",
       "• Design, implement, and maintain systems that collect and analyze business intelligence data.\n",
       "• Design and architect an analytical data store or cluster for the enterprise and implement data pipelines that extract, transform, and load data into an information product that helps the organization reach strategic goals.\n",
       "• Create physical and logical data models to store and share data that can be easily consumed for different BI needs.\n",
       "• Develop Tableau dashboards and features.\n",
       "• Create scalable and high-performance data load and management process to make data available near real-time to support on-demand analytics and insights.\n",
       "• Translate complex technical and functional requirements into detailed designs.\n",
       "• Investigate and analyze alternative solutions to data storing, processing, etc., to ensure the most streamlined approaches are implemented.\n",
       "• Serve as a mentor to junior staff by conducting technical training sessions and reviewing project outputs\n",
       "• Design & develop, and maintain a data model implementing ETL processes.\n",
       "• Manage and maintain the database, warehouse, & cluster with other dependent infrastructure.\n",
       "• Work closely with data, products, and another team to implement data analytic solutions.\n",
       "• Support production application and Incident management.\n",
       "• Help define data governance policies and support data versioning processes\n",
       "• Maintain security and data privacy by working closely with the Data Protection Officer internally.\n",
       "• Analyze a vast number of data stores and uncover insights\n",
       "\n",
       "What We’re Looking For:\n",
       "• Degree in computer sciences or engineering\n",
       "• Overall, 3-5 years of experience in data & data warehouse, ETL, and data modeling.\n",
       "• 2+ years of experience working and managing large data stores, complex data pipelines, and BI solutions.\n",
       "• Strong experience in SQL and writing complex queries.\n",
       "• Hands-on experience with Tableau development.\n",
       "• Hands-on working experience on Redshift, data modeling, data warehouse, ETL tool, Python, and Shell scripting.\n",
       "• Understanding of data warehousing and data modeling techniques\n",
       "• Strong data engineering skills on the AWS Cloud Platform are essential.\n",
       "• Knowledge of Linux, SQL, and any scripting language\n",
       "• Good interpersonal skills and a positive attitude\n",
       "• Experience in travel data would be a plus.\n",
       "\n",
       "Location\n",
       "Gurgaon, India\n",
       "\n",
       "The #TeamGBT Experience\n",
       "\n",
       "Work and life: Find your happy medium at Amex GBT.\n",
       "• Flexible benefits are tailored to each country and start the day you do. These include health and welfare insurance plans, retirement programs, parental leave, adoption assistance, and wellbeing resources to support you and your immediate family.\n",
       "• Travel perks: get a choice of deals each week from major travel providers on everything from flights to hotels to cruises and car rentals.\n",
       "• Develop the skills you want when the time is right for you, with access to over 20,000 courses on our learning platform, leadership courses, and new job openings available to internal candidates first.\n",
       "• We strive to champion Inclusion in every aspect of our business at Amex GBT. You can connect with colleagues through our global INclusion Groups, centered around common identities or initiatives, to discuss challenges, obstacles, achievements, and drive company awareness and action.\n",
       "• And much more!\n",
       "\n",
       "All applicants will receive equal consideration for employment without regard to age, sex, gender (and characteristics related to sex and gender), pregnancy (and related medical conditions), race, color, citizenship, religion, disability, or any other class or characteristic protected by law.\n",
       "\n",
       "Click Here for Additional Disclosures in Accordance with the LA County Fair Chance Ordinance.\n",
       "\n",
       "Furthermore, we are committed to providing reasonable accommodation to qualified individuals with disabilities. Please let your recruiter know if you need an accommodation at any point during the hiring process. For details regarding how we protect your data, please consult the Amex GBT Recruitment Privacy Statement.\n",
       "\n",
       "What if I don’t meet every requirement? If you’re passionate about our mission and believe you’d be a phenomenal addition to our team, don’t worry about “checking every box;\" please apply anyway. You may be exactly the person we’re looking for!\n",
       "Experience Level\n",
       "Mid Level\n",
       "\n",
       "More about this Data ETL Developer / BI Engineer job\n",
       "\n",
       "American Express Global Business Travel is aggressively hiring for the job profile of Data ETL Developer / BI Engineer at undefined in null locality. Kindly go through the FAQs below to get all answers related to the given job.\n",
       "\n",
       "1. How much salary can I expect as a Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n",
       "\n",
       "Ans. You can expect a minimum salary of 0 INR. The salary offered will depend on your skills, experience and performance in the interview.\n",
       "\n",
       "2. What is the eligibility criteria to apply for Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n",
       "\n",
       "Ans. The candidate should have completed undefined degree and people who have 3 to 5 years are eligible to apply for this job. You can apply for more jobs in undefined to get hired quickly.\n",
       "\n",
       "3. Is there any specific skill required for this job?\n",
       "\n",
       "Ans. The candidate should have undefined skills and sound communication skills for this job.\n",
       "\n",
       "4. Who can apply for this job?\n",
       "\n",
       "Ans. Both Male and Female candidates can apply for this job.\n",
       "\n",
       "5. Is it a work from home job?\n",
       "\n",
       "Ans. No, it’s not a work from home job and can’t be done online. You can explore and apply for other work from home jobs in undefined at apna.\n",
       "\n",
       "6. Are there any charges or deposits required while applying for the role or while joining?\n",
       "\n",
       "Ans. No work-related deposit needs to be made during your employment with the company.\n",
       "\n",
       "7. How can I apply for this job?\n",
       "\n",
       "Ans. Go to the apna app and apply for this job. Click on the apply button and call HR directly to schedule your interview.\n",
       "\n",
       "8. What is the last date to apply?\n",
       "\n",
       "Ans. The last date to apply for this job is .\n",
       "\n",
       "For more details, download apna app and find Full Time jobs in undefined. Through apna, you can find jobs in 74 cities across India. Join NOW!</td><td>Map(posted_at -> 6 days ago, schedule_type -> Full-time)</td><td>List(6 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVUTCBEZXZlbG9wZXIgLyBCSSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkFtZXJpY2FuIEV4cHJlc3MgR2xvYmFsIEJ1c2luZXNzIFRyYXZlbCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJnY3VjckFjcm5KVmRmdjRxQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=gcucrAcrnJVdfv4qAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOsQrCMBBAce0nON0s2BRBBJ0sLaXi2F0u8UgjaS7kovST_Ezj8uBN71XfTXXqMCP00x06-pDnSAkUtCP0wbpAxfZwYw1CmMwMHGBgtp62lznnKGelRHxtJWN2pja8KA6keVUv1vLHQ2ZMFD1mehyOzVrHYHfNdaHkDAbo15hIBAbPGj20bynV4lPCsgMuwBieDn-GsVkLqwAAAA&shmds=v1_AdeF8KgXR3CZtx-nmpFMr3mb_bKvair4NEYKX_11OW5JMbkBCQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=gcucrAcrnJVdfv4qAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9abbdaf665a0013fc4549112c9395a972e123709ddc360f0b1.jpeg</td><td>Data ETL Developer / BI Engineer</td><td>Apna</td></tr><tr><td>List(Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=01e515f9108f20d8&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Renovision Automation Services Pvt.Ltd.</td><td>Experience - 6+ yearsWork Mode - HybridJob Summary : We are seeking a skilled Informatica ETL Developer with 5+ years of experience in ETL and Business Intelligence projects. The ideal candidate will have a strong background in Informatica PowerCenter, a solid understanding of data warehousing concepts, and hands-on experience in SQL, performance tuning, and production support. This role involves designing and maintaining robust ETL pipelines to support digital transformation initiatives for clients in manufacturing, automotive, transportation, and engineering domains.Key Responsibilities : - Design, develop, and maintain ETL workflows using Informatica PowerCenter.- Troubleshoot and optimize ETL jobs for performance and reliability.- Analyze complex data sets and write advanced SQL queries for data validation and transformation.- Collaborate with data architects and business analysts to implement data warehousing solutions.- Apply SDLC methodologies throughout the ETL development lifecycle.- Support production environments by identifying and resolving data and performance issues.- Work with Unix shell scripting for job automation and scheduling.Required Skills : - 35 years of hands-on experience with Informatica PowerCenter.- Proficiency in SQL and familiarity with NoSQL platforms.- Experience in ETL performance tuning and troubleshooting.- Solid understanding of Unix/Linux environments and scripting.- Excellent verbal and written communication skills.Preferred Qualifications : - AWS Certification or experience with cloud-based data integration is a plus.- Exposure to data modeling and data governance practices. (ref: hirist.tech)</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(4 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyIC0gU1FML1Bvd2VyIENlbnRlciIsImNvbXBhbnlfbmFtZSI6IlJlbm92aXNpb24gQXV0b21hdGlvbiBTZXJ2aWNlcyBQdnQuTHRkLiIsImFkZHJlc3NfY2l0eSI6IlRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlVqTnczajVrcnJpS21VQjdBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=UjNw3j5krriKmUB7AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2OuwrCQBBFsfUTrKYW3RXBRitRESWFr14mcUxW1pmwM0b_zl8zsbkcbnE4_W-vv9rxXdITLRQIm0sGa2ooSk0JxnA-Zv4g75ZXxPa_9pKDEqaiAmHYipSRBovKrNa596rRlWqdzRXy9MKUy8c_JNdurlphojqi0XU6m3xczeVwcSKWJmhofcuXSdfS4plSEwpSODTmMrs5CAwXisglMo5gx7eAP-LlOHnCAAAA&shmds=v1_AdeF8KiGqykFvEjWsWwQECdNjsOUTEn9RtZl-sAMt92jVTfR6w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=UjNw3j5krriKmUB7AAAAAA%3D%3D</td><td>null</td><td>Informatica ETL Developer - SQL/Power Center</td><td>Indeed</td></tr><tr><td>List(Map(title -> Analytics Vidhya, link -> https://jobsnew.analyticsvidhya.com/jobs/etl-developer-hyderabad-2-3-years-of-experience/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>A Client of Analytics Vidhya</td><td>Role Summary:\n",
       "\n",
       "•ETL Developer is responsible for Design and Development of ETL Jobs which follow standards, best practices and are maintainable, modular and reusable.\n",
       "\n",
       "•ETL Developer will analyze and review complex object and data models and the metadata repository in order to structure the processes and data for better management and efficient access.\n",
       "\n",
       "•Working on multiple projects, and delegating work to Junior Analysts to deliver projects on time.\n",
       "\n",
       "•Training and mentoring Junior Analysts and building their proficiency in the ETL process.Job Duties:\n",
       "\n",
       "•Preparing mapping document to extract, transform, and load data ensuring compatibility with all tables and requirement specifications.\n",
       "\n",
       "•Experience in ETL system design and development with Talend / Pentaho PDI is essential.\n",
       "\n",
       "•Create quality rules in Talend.\n",
       "\n",
       "•Tune Talend jobs for performance optimization.\n",
       "\n",
       "•Write relational and multidimensional database queries.\n",
       "\n",
       "•Functional Knowledge of Talend Adminstration Center, Job Servers & Load balancing setup, and all its administrative fucntions.\n",
       "\n",
       "•Develop, maintain, and enhance unit test suites to verify the accuracy of ETL processes, dimensional data, OLAP cubes and various forms of BI content including reports, dashboards, and analytical models.\n",
       "\n",
       "•Exposure in Map Reduce components of Talend / Pentaho PDI.\n",
       "\n",
       "•Creating and deploying Talend / Pentaho custom components is an add-on advantage.\n",
       "\n",
       "Job Specification:\n",
       "\n",
       "•BE, B.Tech / MS Degree in Computer Science, Engineering or a related subject.\n",
       "\n",
       "•Having an experience of 2 – 3+ years.\n",
       "\n",
       "•Comprehensive understanding and working knowledge in Data Warehouse loading, tuning, and maintenance.\n",
       "\n",
       "•Proficiency with Talend or related tools like Pentaho Data Integration / Kettle.\n",
       "\n",
       "•Working knowledge of relational database theory and dimensional database models.\n",
       "\n",
       "•Ability to write complex SQL database queries.\n",
       "\n",
       "•Ability to work independently.</td><td>Map(salary -> ₹1.5M a year, schedule_type -> Full-time)</td><td>List(₹1.5M a year, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyLSBIeWRlcmFiYWQgKDItMysgWWVhcnMgb2YgRXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJBIENsaWVudCBvZiBBbmFseXRpY3MgVmlkaHlhIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJYOGlLR3lLZ0p6MDQ5UkdIQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=X8iKGyKgJz049RGHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WOuw6CUBBEY8snWG3pg4fR2EhFlKjE0phYmQVWuOa6S9gbAz_odwmVzTQnc2a878TL0usFDvQhKw21AZz6klrMsYTZOtgs4U7YKsgT0m7ghrigOQSQSQ46oKIGYTiKVJamce1co7soUrVhpQ6dKcJC3pEw5dJFL8l1jIfW2FJj0dFjvV11YcPVIk5gbwe_G8cSRtsPbYWbKesewfD_mQ9XssgVMvpw5tLgD84obTrJAAAA&shmds=v1_AdeF8KhNH4dOhsM_ohqTFHeygbi7_Rs650WHWh26bXMFYZ5jQQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=X8iKGyKgJz049RGHAAAAAA%3D%3D</td><td>null</td><td>ETL Developer- Hyderabad (2-3+ Years of Experience)</td><td>Analytics Vidhya</td></tr><tr><td>List(Map(title -> JobNet, link -> https://www.jobnet.com.au/in/en/search-jobs-in-Maharashtra,-India/ETL-DEVELOPER-EFD64B886A6B4BB7C2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> WhatJobs, link -> https://en-in.whatjobs.com/jobs/data-transformation?id=153796242&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Expertini, link -> https://in.expertini.com/jobs/job/etl-developer-borivali-panzer-technologies-pvt-ltd-2950-9852891/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Recruit.net, link -> https://www.recruit.net/job/etl-developer-jobs/6CD83997F05151D4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Luxoft</td><td>Project Description:\n",
       "\n",
       "Our client is an EU subsidiary of a Global Financial Bank working in multiple markets and asset classes.\n",
       "\n",
       "DWH/ETL developer will work closely with the Development Lead to design, build interfaces and integrate data from a variety from internal and external data sources into the new Enterprise Data Warehouse environment. The ETL Developer will be responsible for developing ETL primarily utilizing Microsoft & Azure technologies within industry recognized ETL standards, architecture, and best practices.\n",
       "\n",
       "Responsibilities:\n",
       "• Act as a technical expert in the designing, coding, unit testing, supporting, and debugging of data warehouse software components in all aspects of SDLC\n",
       "• Apply cloud and ETL engineering skills to solve problems and design approaches\n",
       "• Troubleshoot and debug ETL pipelines and creating unit tests for ETL pipelines.\n",
       "• Assess query performance and actively contribute to optimizing the code\n",
       "• Write technical documentation and specifications\n",
       "• Support internal audit by submitting required evidence\n",
       "• Create reports and dashboards in the BI portal\n",
       "• Work with Development Lead, DWH Architect and QA Engineers to plan, implement and deliver best ETL strategies\n",
       "• Work with business analysts to understand requirements to create technical design specifications, gaining a sound understanding of business processes for related applications so that integration processes fulfill the end-user requirements\n",
       "• Communicate effectively in a collaborative, complex and high performing team environment as per Agile principles\n",
       "\n",
       "Mandatory Skills Description:\n",
       "• Proven work experience as an ETL Developer\n",
       "• Advanced knowledge of relational databases and dimensional Data Warehouse modelling concepts\n",
       "• Good understanding of physical and logical data modeling\n",
       "• Very good understanding of modern SaaS/PaaS data solutions in a cost conscious approach\n",
       "• Expert level of knowledge of Microsoft Data stack\n",
       "• Experience in developing and deploying data oriented solutions in Cloud (Azure/Synapse Analytics/Fabric)\n",
       "• Experience in designing and implementing data transformation and ETL layers using Data Factory, Notebooks\n",
       "• Experience with PowerBI for report & dashboard creation. PowerQuery and/or DAX is an advantage.\n",
       "• Experience in/understanding of Azure Data Lake Storage\n",
       "• Knowledge/use of CI/CD tools and principles, preferably Azure DevOps or Bamboo\n",
       "• Strong SQL knowledge, able to create complex SQL queries and good understanding of stored procedures, views, indexes, functions, etc.\n",
       "• Good working knowledge of at least one Scripting language. Python is an advantage.\n",
       "• Experience with GIT repositories and working with branches. GitHub, Azure DevOps or Bitbucket experience are preferable.\n",
       "• Ability to troubleshoot and solve complex technical problems\n",
       "• Good understanding of software development best practices\n",
       "• Working experience in Agile projects; preferably using JIRA\n",
       "• Experience in working in high priority projects preferably greenfield project experience\n",
       "• Able to communicate complex information clearly and concisely.\n",
       "• Able to work independently and also to collaborate across the organization\n",
       "• Highly developed problem-solving skills with minimal supervision\n",
       "• Understanding of data governance and enterprise concepts preferably in banking environment\n",
       "• Verbal and written communication skills in English are essential.\n",
       "\n",
       "Nice-to-Have Skills Description:\n",
       "• Microsoft Fabric\n",
       "• Snowflake\n",
       "• Background in SSIS/SSAS/SSRS\n",
       "• Azure DevTest Labs, ARM templates\n",
       "• Azure PurView\n",
       "• Banking/finance experience</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(4 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiTHV4b2Z0IiwiYWRkcmVzc19jaXR5IjoiTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJlZF9YUGZ2UVNVcTZXdXltQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Maharashtra, India</td><td>ETL Developer</td><td>https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=ed_XPfvQSUq6WuymAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCMBAAUFz7CeJwg5NoI4KLXRVR6uZeLvFsIjEXcqdk9c_V5S2v-Uya-eHaw57eFDlTgRWc2YIQFueBExyZx0jTzqtm2RkjEttRFDW41vHTcCLL1TzYyp9BPBbKEZWGzXZd25zGxax_Vb4rhAQX_D2K14JLOKVbwC8_vPQqhAAAAA&shmds=v1_AdeF8KjMEQiuWVFBnZYKg-t8P774XOcFwvkoOM2R-qPEBeYQMQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=ed_XPfvQSUq6WuymAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9aca07e30835060ab20487250180fd7e64e147a75b89f26c9d.png</td><td>ETL Developer</td><td>JobNet</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/data-engineer-hadoop-spark-scala-hive-at-visa-4251105891?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> PowerToFly, link -> https://powertofly.com/jobs/detail/2317944?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/155049377/bcy532-data-engineer-hadoop-spark-scala-hive-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Visa</td><td>Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n",
       "\n",
       "Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n",
       "\n",
       "Job Description\n",
       "\n",
       "Translate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\n",
       "\n",
       "Good to have GenAI Exposure and Agentic AI Knowledge.\n",
       "\n",
       "Work with business partners directly to seek clarity on requirements.\n",
       "\n",
       "Define solutions in terms of components, modules, and algorithms.\n",
       "\n",
       "Design, develop, document, and implement new programs and subprograms, as well as enhancements, modifications and corrections to existing software.\n",
       "\n",
       "Create technical documentation and procedures for installation and maintenance.\n",
       "\n",
       "Write Unit Tests covering known use cases using appropriate tools.\n",
       "\n",
       "Integrate test frameworks in the development process.\n",
       "\n",
       "Work with operations to get the solutions deployed.\n",
       "\n",
       "Take ownership of production deployment of code.\n",
       "\n",
       "Come up with Coding and Design best practices.\n",
       "\n",
       "Thrive in a self-motivated, internal-innovation driven environment.\n",
       "\n",
       "Adapt quickly to new application knowledge and changes.\n",
       "\n",
       "This is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Basic Qualifications\n",
       "\n",
       "-Minimum of 6 months of work experience or a Bachelor's Degree\n",
       "\n",
       "Preferred Qualifications\n",
       "\n",
       "-Bachelor degree in Computer Science.\n",
       "\n",
       "-Minimum of 1 plus years of software development experience in Hadoop using\n",
       "\n",
       "Spark, Scala, Hive.\n",
       "\n",
       "-Expertise in Object Oriented Programming Language Java, Python.\n",
       "\n",
       "-Experience using CI CD Process, version control and bug tracking tools.\n",
       "\n",
       "-Result-oriented with strong analytical and problem-solving skills.\n",
       "\n",
       "-Experience with automation of job execution, validation and comparison of data\n",
       "\n",
       "files on Hadoop Environment at the field level.\n",
       "\n",
       "-Experience in leading a small team and being a team player.\n",
       "\n",
       "-Strong communication skills with proven ability to present complex ideas and\n",
       "\n",
       "document them in a clear and concise way.\n",
       "\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>Map(posted_at -> 5 days ago, schedule_type -> Full-time)</td><td>List(5 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChIYWRvb3AsIFNwYXJrLCBTY2FsYSwgSGl2ZSkiLCJjb21wYW55X25hbWUiOiJWaXNhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImVULTZKTlc2NHVzTUxhdS1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=eT-6JNW64usMLau-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXFsQrCMBAAUFwL_oDTjSq1EcFFJ0Gxugqu5ZoeSTTehVyQ_oW_rC7vVZ9JdThiQTixC0yUYd7iIJJquCXMz18WI9bQhjctYAVX6UEJs_UgDGcRF2m296Uk3RmjGhunBUuwjZWXEaZeRvOQXv906jFTilio22zXY5PYLaf3oAiB4cJDwC9Wqp1ikgAAAA&shmds=v1_AdeF8KhYrHXm7wogwEwbFZ9W4s61HkTq4FBK03n19qF8gZ_PUA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=eT-6JNW64usMLau-AAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f7e4a0d0dc158f67fd721978ef95aab8a775788a042a09089d.jpeg</td><td>Data Engineer (Hadoop, Spark, Scala, Hive)</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/data-engineer-spark-python-at-etelligens-technologies-4230192247?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/data-engineer-spark-python/etelligens-technologies/17263944?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/149564571/vq345-data-engineer-spark-python-pune/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Findjob24h.com, link -> https://in.findjob24h.com/it/big-data-engineer-sparkpython-job26347?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Etelligens Technologies</td><td>Job Description\n",
       "\n",
       "We are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n",
       "• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n",
       "• Perform data cleansing and migration from diverse sources to target systems.\n",
       "• Collaborate with stakeholders to understand requirements and convert them into technical\n",
       "\n",
       "solutions.\n",
       "• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n",
       "• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n",
       "• Support, troubleshoot, and maintain data pipelines and workflows.\n",
       "• Participate in all phases of software development lifecycle including unit testing, integration\n",
       "\n",
       "testing, and performance testing.\n",
       "• Contribute to the modernization of data platforms and analytics tools on Azure.\n",
       "• Ensure data quality and integrity across all pipelines and systems.\n",
       "\n",
       "Required Skills & Qualifications\n",
       "• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n",
       "• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n",
       "• 1+ year of experience in Python and Spark scripting.\n",
       "• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n",
       "• Good understanding of Data Warehousing (DWH) concepts.\n",
       "• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n",
       "• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n",
       "• Hands-on experience in data cleansing, transformation, and migration projects.\n",
       "• Ability to work independently and within a team environment.\n",
       "• Microsoft Certified : Azure Data Engineer Associate\n",
       "• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n",
       "\n",
       "Location : Artha SEZ, Greater Noida West\n",
       "\n",
       "(ref:hirist.tech)</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=Dc6DSNLMiNYCEl95AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHsQrCMBAAUFz7CU43CzaiuOhqEZ0E3cs1HklqvAu5G-p_-MHW5cFrvotmd0JD6DgkJqqwhnvB-nK3j0XhuVcZQAmrjzD_LBIyLY_RrOjBOdXcBjW05FsvbydMg0xulEH_9BqxUslo1G_3m6ktHFbQGeWcArHCg3xkyRISKSSGCz8T_gDku5DQlwAAAA&shmds=v1_AdeF8KhUS3kDcBE04kKw4L-SXFj_kSbNYqcLjnZpQzO2TV5B5A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=Dc6DSNLMiNYCEl95AAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f79c16e836afec51432ee9e157dc4b5b6344f07d3b2165e389.jpeg</td><td>Data Engineer - Spark/Python</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Iitjobs, link -> https://www.iitjobs.com/job/spark-engineer-usa-staffingine-llc-78519?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Staffingine LLC</td><td>- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred</td><td>Map(posted_at -> 1 day ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(1 day ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3NsQrCMBAA0L1fIE63uAgmIrjoKCJKt35AucRrkhrvQi5DJ79ddHnr6z7dZihYX3DlkJiowg4e4kAJq48gDDeRkGl9jq0VPVmrmk3Qhi154-VthcnJYmdx-mPUiJVKxkbj4bhfTOGwXQ0Npyn9C-j7CySGOz8TfgHTIbRdgAAAAA&shmds=v1_AdeF8KjUl41s9rD9xNd7RywARJoJQ3Nel1qruGSZXxbjxJAYWA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f775d6990ffc61dcc6da45082c8a36a2aa06bfab96684f1183.jpeg</td><td>Spark Engineer</td><td>Iitjobs</td></tr><tr><td>List(Map(title -> PyJobs, link -> https://www.pyjobs.com/job/staff-data-engineer-spark-python-hadoop-oMy9olMy?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Visa</td><td>Company Description\n",
       "\n",
       "Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n",
       "\n",
       "When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n",
       "\n",
       "Join Visa: A Network Working for Everyone.\n",
       "Job Description\n",
       "\n",
       "The Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n",
       "\n",
       "Are you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n",
       "\n",
       "As a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n",
       "\n",
       "Essential Functions\n",
       "• Work with manager and clients to fully understand business requirements and desired business outcomes\n",
       "• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n",
       "• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n",
       "• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n",
       "• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n",
       "• Execute data engineering projects ranging from small to large either individually or as part of a project team\n",
       "• Ensure project delivery within timelines and budget requirements\n",
       "• Provide coaching and mentoring to junior team members\n",
       "\n",
       "This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\n",
       "Qualifications\n",
       "\n",
       "• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n",
       "• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n",
       "• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n",
       "• Minimum of 4 years of hands-on expertise with Java or Scala\n",
       "• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n",
       "• Experience working in an Agile and Test Driven Development environment.\n",
       "• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n",
       "• Experience with SAS as a statistical package is preferred\n",
       "• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFwLvoDTjSq1EcFFN1H8mYSCa7m2aRKNdyF3Q30LH1n8hq_4TopDrTgMcERFOJELZG2GeZ0wv0q4f9QzlXDBnjktYAU3bkEs5s4DE5yZXbSzvVdNsjNGJFZOFDV0Vcdvw2RbHs2TW_nXiMdsU0S1zWa7HqtEbjl9BEEIBFfqA_4AG-QyopMAAAA&shmds=v1_AdeF8KjOBn1yJUpUCbIoZfiL9XOirLPsVpNYluZLK1C-IlMUqQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f7a5cc450c7c7438378f704be88128d8f54c6129c21b3ecd44.jpeg</td><td>Staff Data Engineer (Spark, Python, Hadoop)</td><td>PyJobs</td></tr><tr><td>List(Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/databricks-engineer-spark-pyspark-enkefalos-technologies-llp-JV_KO0,33_KE34,60.htm?jl=1009780925122&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=b95950f98d5b6678&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Levels.fyi, link -> https://levels.fyi/jobs?jobId=99571824638993094&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/6215885900103024640?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/153480629/c-941-data-engineer-etl-sql-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Enkefalos Technologies LLP</td><td>Databricks Engineer – Spark / PySpark\n",
       "\n",
       "Location : Remote / Mysore\n",
       "\n",
       "Joining : Immediate\n",
       "\n",
       "Experience : 5+ years\n",
       "\n",
       "Responsibilities :\n",
       "\n",
       "Will implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n",
       "\n",
       "Requirements:\n",
       "• PySpark (RDDs, DataFrames, performance tuning)\n",
       "• Building gold‐layer data models for financial reporting\n",
       "• Experience with complex joins, aggregations, GL hierarchies\n",
       "• Version handling (Actuals vs Budget), currency conversions\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500,395.35 - ₹1,840,348.25 per year\n",
       "\n",
       "Benefits:\n",
       "• Flexible schedule\n",
       "• Paid sick time\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Application Question(s):\n",
       "• Have you worked on ADF/ADLS ?\n",
       "• Do you have hands-on experience of Spark / PySpark\n",
       "\n",
       "Experience:\n",
       "• Databricks Engineering: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>Map(salary -> ₹500,395–₹1,840,348 a year, qualifications -> No degree mentioned, schedule_type -> Full-time, work_from_home -> true, posted_at -> 6 days ago)</td><td>List(6 days ago, ₹500,395–₹1,840,348 a year, Work from home, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Anywhere</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=k4Rb92L6YIBechULAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXGsQrCMBAAUFz7CU4HbkIbEVzU0SJKh4Luco1nEhvvQi5D_Rs_VcTl8arPrNofsOCQgx0VWnaBiTLUcEmYRzDQv_-r4SwDKGG2HoThKOIizXe-lKRbY1Rj47RgCbax8jLCNMhknjLoj5t6zJQiFrqtN6upSeyWi5ZHemAUhStZzxLFBVLouh4Cw4nvAb-C4XW0owAAAA&shmds=v1_AdeF8KizOswJH_SyEGgjpgJ7bj5Cwqu4B0GS78yPFj9XWFNoVQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=k4Rb92L6YIBechULAAAAAA%3D%3D</td><td>null</td><td>Databricks Engineer - Spark / PySpark</td><td>Glassdoor</td></tr><tr><td>List(Map(title -> Hirist, link -> https://www.hirist.tech/j/pi-square-technologies-spark-and-scala-engineer-1486469?ref=kp_prm&jobPos=5&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/jdp/-2093773132764585923?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Hirist, link -> https://www.hirist.com/j/pi-square-technologies-spark-scala-engineer-1486469.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/3286396871957807104?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/149843579/square-technologies-spark-scala-engineer-tfe-344-india/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Recruit.net, link -> https://www.recruit.net/job/pi-square-technologies-spark-scala-jobs/3F001AE1CE143060?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>sandeep raja</td><td>Job Summary :\n",
       "\n",
       "We are seeking a skilled and motivated Spark & Scala Engineer with 46 years of hands-on experience in building scalable data processing applications. The ideal candidate will have a strong background in big data ecosystems and a solid understanding of distributed computing using Apache Spark with Scala.\n",
       "\n",
       "Roles and Responsibilities :\n",
       "\n",
       "- Design, build, and maintain efficient, reusable, and reliable Apache Spark applications.\n",
       "\n",
       "- Optimize Spark applications for maximum speed and scalability.\n",
       "\n",
       "- Implement data ingestion and ETL processes.\n",
       "\n",
       "- Collaborate with data scientists and architects to implement complex big data solutions.\n",
       "\n",
       "- Debug and resolve issues in Spark applications.\n",
       "\n",
       "- Stay up to date with the latest trends in big data technologies and Apache Spark.\n",
       "\n",
       "- Write clean, readable, and maintainable code.\n",
       "\n",
       "- Participate in code reviews and contribute to team knowledge sharing.\n",
       "\n",
       "Required Skills :\n",
       "\n",
       "- 46 years of experience working with Apache Spark (core, SQL, streaming).\n",
       "\n",
       "- Strong proficiency in Scala programming.\n",
       "\n",
       "- Experience in building and optimizing data pipelines and ETL workflows.\n",
       "\n",
       "- Proficient in working with big data tools and frameworks (e.g., Hive, HDFS, Kafka).</td><td>Map(posted_at -> 27 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(27 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJQaSBTcXVhcmUgVGVjaG5vbG9naWVzIC0gU3BhcmsgXHUwMDI2IFNjYWxhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoic2FuZGVlcCByYWphIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlFMNDBNZDljZDlqQkxBX3BBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=QL40Md9cd9jBLA_pAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNoQ7CMBCA4WDnMahTCBJWQoIBPQgokuGXW3dpO8pd6ZVkD8PDMsxvPvFX30V1vgdo3x_MBA-yniWKC6SwhTZhfsIaWosRoWEXmCjPcJMelDBbD8JwEXGRVidfStKjMaqxdlqwBFtbeRlh6mUyo_T6T6d-fqWIhbr9YTfVid1mqcgDUYKMI0JguPIQ8AfNb9jFnwAAAA&shmds=v1_AdeF8KiZSTs7t7WCPWgpQK5yqbyREzWZSeEDIFaZ99Xx8ZkX-g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=QL40Md9cd9jBLA_pAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f7f322242bb49fd97df7d7ea982022c5843da4afdce119aefc.jpeg</td><td>Pi Square Technologies - Spark & Scala Engineer</td><td>Hirist</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/spark-developer-at-infosys-4228309581?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/spark-developer/infosys/17011039?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/186f1d5c41ccc611ca87f0b144a7e5ae?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobgether, link -> https://jobgether.com/offer/6836bc7c448ef06f94b4759c-spark-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobindian.in, link -> https://jobindian.in/ko/spark-developer-job100000?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Indianjob24h.in, link -> https://indianjob24h.in/es/spark-developer-job100000?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Infosys</td><td>• Primary skills:Technology->Big Data - Data Processing->Spark\n",
       "\n",
       "A day in the life of an Infoscion\n",
       "• As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\n",
       "• You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\n",
       "• You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\n",
       "• You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\n",
       "• You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n",
       "• Knowledge of more than one technology\n",
       "• Basics of Architecture and Design fundamentals\n",
       "• Knowledge of Testing tools\n",
       "• Knowledge of agile methodologies\n",
       "• Understanding of Project life cycle activities on development and maintenance projects\n",
       "• Understanding of one or more Estimation methodologies, Knowledge of Quality processes\n",
       "• Basics of business domain to understand the business requirements\n",
       "• Analytical abilities, Strong Technical Skills, Good communication skills\n",
       "• Good understanding of the technology and domain\n",
       "• Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\n",
       "• Awareness of latest technologies and trends\n",
       "• Excellent problem solving, analytical and debugging skills</td><td>Map(posted_at -> 16 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(16 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBEZXZlbG9wZXIiLCJjb21wYW55X25hbWUiOiJJbmZvc3lzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkZ2VmRvVC1NQnFvWHRRUmJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=FvVdoT-MBqoXtQRbAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQoCMQwA0P0mZ6dMDoKtCC66CnKufsCR1thWaxKaouffi294w2dYXRXbE070pipKDTZwkQBG2GIGYTiLpErLY-5d7eC9WXXJOvYSXZSXF6Ygs39IsH-TZWykFTtNu_12dsppvRj5LvY1KAwj3wr-AKcB2QF5AAAA&shmds=v1_AdeF8Kh9NNVFIzcDYAUyzgZze6PxeWepYB2Nz99wOAFvlj190A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=FvVdoT-MBqoXtQRbAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f7288e0093cef41baca5f816735e5c318463cf69b421b2cd83.jpeg</td><td>Spark Developer</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Workday, link -> https://citi.wd5.myworkdayjobs.com/en-US/2/job/Data-Engineer---AVP_24785645?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Indeed, link -> https://in.indeed.com/viewjob?jk=481bb9445cf687c8&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Hirist, link -> https://www.hirist.tech/j/celonis-data-engineer-etlsql-10-12-yrs-1479538?ref=rl&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobgether, link -> https://jobgether.com/offer/67969bf4c58f91dcb9676b4e-ria---data-engineer-with-q-a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/veersa-technologies-data-engineer-informatica-mdm-at-veersa-technologies-4243113652?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/jdp/3467413193916249586?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/data-engineer-2-5-yrs-exp-guardian-management-services-JV_KO0,25_KE26,54.htm?jl=1009784233452&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Foundit.in, link -> https://www.foundit.in/job/debut-infotech-data-engineer-python-scala-debut-infotech-pvt-ltd-india-34609162?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>12542 Citicorp Services India Private Limited</td><td>The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting</td><td>Map(schedule_type -> Full-time)</td><td>List(Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=lBQYlRnJjd-RRHF9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWLTQ_BQBBA4-onOM0RqS4NDpzqI0IcJE04yrQd7eja2exu8Nf8OxWXd3nvdT-d7mqDAWFrKjZEDvqZkddNY0MRZBZdE0F6yQYwgvR8anmQHDyhK2oQAzuRSlNvWYdg_UIp73Vc-YCBi7iQhxJDubzVXXL_w9XX6MhqDHRNZuN3bE01nE-S2TSBNbeTOAsZuScX5GFvSkY4OX62PRz5wYFKYPMXX3VpMJW8AAAA&shmds=v1_AdeF8Kis8dqR1eNIZfspzvlHRHi-cHVVT9hIYJVFIboxveOlTA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=lBQYlRnJjd-RRHF9AAAAAA%3D%3D</td><td>null</td><td>Data Engineer (Snowflake, Spark, AWS) - AVP</td><td>Workday</td></tr><tr><td>List(Map(title -> PowerToFly, link -> https://powertofly.com/jobs/detail/2317939?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs.weekday.works, link -> https://jobs.weekday.works/visa-sw-engineer-(java-and-bigdata%2Fhadoop%2Fspark)-1yr?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/155049294/v777-engineer-java-and-bigdata-hadoop-spark-1yr-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>VISA</td><td>Job Description\n",
       "\n",
       "This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. They will be an integral part of the Payment Products Development team, focusing on the development of software solutions that leverage data to address business challenges. They will be extensively involved in hands-on activities including proof of concepts, development, and testing. They should demonstrate adaptability and readiness to change tasks according to the team's requirements.\n",
       "\n",
       "Key Responsibilities\n",
       "• Deliver high-quality products and features, taking a holistic view from the initial idea to final delivery. This involves design, development, testing, and accountability for release deployments as well as providing support post-deployment.\n",
       "• Understand the product in its entirety and ensure the timely delivery of your assigned module within an agile team setting.\n",
       "• Accurately scope efforts, identify risks, and clearly communicate trade-offs with team members and other stakeholders.\n",
       "• Coordinate and participate in Continuous Integration activities, testing automation frameworks, and other related tasks, in addition to contributing to core product code.\n",
       "• Effectively communicate status updates, issues, and potential risks accurately and promptly.\n",
       "• Perform other tasks related to data governance and system infrastructure as required.\n",
       "\n",
       "This is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Basic Qualifications\n",
       "\n",
       "-Bachelor's degree in Computer Science or equivalent field\n",
       "\n",
       "-Relevant working experience of up to 2 years in the industry\n",
       "\n",
       "-Proven experience in software development, particularly in data-centric\n",
       "\n",
       "projects, demonstrating adherence to standard development best practices\n",
       "\n",
       "-Strong understanding and practical experience with data structures and\n",
       "\n",
       "algorithms, with a passion for tackling complex problems\n",
       "\n",
       "-Proficiency in Java programming\n",
       "\n",
       "-Hands-on experience with Big Data technologies such as Hadoop, Spark, and\n",
       "\n",
       "Hive\n",
       "\n",
       "-Strong knowledge of Unix/Linux operating systems and Shell Scripting\n",
       "\n",
       "-Proficiency in working with RDBMS and SQL\n",
       "\n",
       "-Basic knowledge of manual and automated testing\n",
       "\n",
       "-Familiarity with version control systems, specifically Git\n",
       "\n",
       "-Awareness of and experience with software design patterns\n",
       "\n",
       "-Experience working within an Agile framework\n",
       "\n",
       "Preferred Qualifications\n",
       "\n",
       "-Proficiency in Scala & Kafka programming is a good to have\n",
       "\n",
       "-Experience with Airflow for workflow management\n",
       "\n",
       "-Familiarity with AI concepts and tools, including GitHub Copilot for code\n",
       "\n",
       "development\n",
       "\n",
       "-Exposure to AI/ML development is an added advantage\n",
       "\n",
       "-Proficiency in working with In-memory Databases like Redis\n",
       "\n",
       "-Good knowledge of API development is highly advantageous\n",
       "\n",
       "-Strong verbal and written communication skills, with a proactive and self-\n",
       "\n",
       "motivated approach to improving existing processes to enable faster\n",
       "\n",
       "iterations.\n",
       "\n",
       "-Demonstrated intellectual and analytical rigor, with a keen attention to detail\n",
       "\n",
       "-Team-oriented, energetic, and collaborative approach to work, coupled with a\n",
       "\n",
       "diplomatic and adaptable style\n",
       "\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>Map(posted_at -> 5 days ago, schedule_type -> Full-time)</td><td>List(5 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTVyBFbmdpbmVlciAoSmF2YSBhbmQgQmlnZGF0YS9IYWRvb3AvU3BhcmspIDF5ciIsImNvbXBhbnlfbmFtZSI6IlZJU0EiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoidHV1TkJsMDF3amRIc0xnSEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=tuuNBl01wjdHsLgHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFwLvoDTjSrYqOBSJwV_2rWgY7k2RxqtdyEXpL6HDyx-w5d9J9m5vsOJnWeiCPMK3wjIFo7eWUxormhFgqkDxucCNp8IK6ikBSWMXQ_CcBFxA832fUpBC2NUh9xpwuS7vJOXEaZWRvOQVv812mOkMGCiZrtbj3lgt5zeyvoAnqFk6_EHGOnmHZcAAAA&shmds=v1_AdeF8KjVjVJowf_Si6rhmDvQLwvNY8s0nZa1Y6FrC7MYGcN41w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=tuuNBl01wjdHsLgHAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f7f2857876d96d4a19482a79d8d87bbea86705b552fa82bf54.jpeg</td><td>SW Engineer (Java and Bigdata/Hadoop/Spark) 1yr</td><td>PowerToFly</td></tr><tr><td>List(Map(title -> Iitjobs, link -> https://www.iitjobs.com/job/big-data-lead-lead-data-engineerspark-tech-lead-mclean-va-usa-tanisha-systems-inc-3899?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> TechGig, link -> https://www.techgig.com/jobs/Tech-Lead-Big-Data/70663711?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Tanisha Systems  Inc</td><td>Job Description:Mandatory:10+ years of experience in solution, design and development of applications using Java 8+/J2EE, Spring, Spring-Boot, Micro Services, RESTful Services and with experience in Big Data and with experience working in heavy data background needed.Develop, program, and maintain applications using the Apache Spark open source frameworkWork with different aspects of the Spark ecosystem, including Spark SQL, Data Frames, Datasets, and streaming.Spark Developer must have strong programming skills in Java, Scala, or PythonFamiliar with big data processing tools and techniquesProven experience as a Spark Developer or a related roleStrong programming skills in Java, Scala, or PythonFamiliarity with big data processing tools and techniquesExperience with the Hadoop ecosystemGood understanding of distributed systemsExperience with streaming data platformsMust have strong experience in Big Data and with experience working in heavy data background needed.Must be strong in Cloud AWS event-based architecture, Kubernetes, ELK (Elasticsearch, Logstash & Kibana)Must have excellent experience in designing and Implementing cloud-based solutions in various AWS Services (: s3, Lambda, Step Function, AMQ, SNS, SQS, CloudWatch Events, etc.)Must be well experienced in design and development of Microservice using Spring-Boot and REST API and with GraphQLMust have solid knowledge and experience in NoSQL (MongoDB)Good knowledge and experience in any Queue based implementations.Strong knowledge/experience in ORM Framework - JPA / HibernateGood knowledge in technical concepts – Security, Transaction, Monitoring, PerformanceShould we well have versed with TDD/ATDDShould have experience on Java, Python and Spark2+ years of experience in designing and Implementing cloud-based solutions in various AWS ServicesStrong experience in DevOps tool chain (Jenkins, Artifactory, Ansible/Chef/Puppet/Spinnaker, Maven/Gradle, Atlassian Tool suite)Very Good knowledge and experience in Non-Functional (Technical) Requirements like Security, Transaction, Performance, etc.Excellent analytical and problem-solving skills Nice to have:· Experience in Experience with OAuth implementation using Ping Identity· Experience in API Management (Apigee) or Service Mesh (Istio)· Good knowledge and experience in Queue/Topic (Active-MQ) based implementations.· Good knowledge and experience in Scheduler and Batch Jobs· Experience with scripting languages using Unix· Preferably certified in AWS</td><td>Map(posted_at -> 4 hours ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(4 hours ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJCaWcgRGF0YSBMZWFkLyBMZWFkIERhdGEgRW5naW5lZXIvU3BhcmsgVGVjaCBMZWFkIiwiY29tcGFueV9uYW1lIjoiVGFuaXNoYSBTeXN0ZW1zICBJbmMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQmRWaDV5cnl3aTREUFNOT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>India</td><td>Spark Engineer</td><td>https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=BdVh5yrywi4DPSNOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWJuwrCQBBFsc0nCMLUglkRbLSTiA_skj5MNsNmNZlZdraIP-R3Gk1zLvec7LPIrifvoMCE8CBszZ_zP7PzTBRNGTC-oCLbzXUDd2lACeNkhOEi4npaHruUgh6MUe1zpwmTt7mVwQhTI6N5SqM_1NphpNBjonq33455YLdeVch-ClC-NdGgADe24Hma1uMXP5iqRqkAAAA&shmds=v1_AdeF8KgvLioLXzdCbGcM30mO5raEHL9pykXDnB4l0r3zd6_bmQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=BdVh5yrywi4DPSNOAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f70b7e87e564a84e4f2b75a0e90adaeea18f77af68fffe6ec4.jpeg</td><td>Big Data Lead/ Lead Data Engineer/Spark Tech Lead</td><td>Iitjobs</td></tr><tr><td>List(Map(title -> Novartis, link -> https://www.novartis.com/careers/career-search/job/details/req-10055119-data-insights-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://novartis.wd3.myworkdayjobs.com/en-US/Novartis_Careers/job/Data-Insights-Analyst_REQ-10055119-1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/3b5171d6da89c919a74528de8a507f49?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Unstop, link -> https://unstop.com/jobs/data-insights-analyst-acko-general-insurance-1485517?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Apna, link -> https://apna.co/job/hyderabad/data-insights-analyst-1566740135?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> TechGig, link -> https://www.techgig.com/jobs/Data-Insights-Analyst/71084590?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-3150-9c03d04ebc9df7a060c6901ca379a236?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>IN10 (FCRS = IN010) Novartis Healthcare Private Limited</td><td>Responsible for leveraging Google Analytics 4 (GA4) to extract, analyze, and interpret data to support business objectives and decision-making processes. Requires proficient understanding and translating business requirements into actionable insights and recommendations derived from GA4 data.Key Responsibilities:1.Data Extraction & Analysis: Extract and analyze GA4 data to provide meaningful insights into user behavior, website performance, and digital marketing effectiveness.2.Reporting & Dashboarding: Develop custom reports, dashboards, and data visualizations using GA4 data to monitor key metrics and performance indicators.3.Insights and Recommendations: Providing actionable insights and recommendations to optimize website performance, user experience, and marketing campaigns.4.Customization & Configurations: Configure GA4 data streams, events, and conversions to align with business goals and KPIs (key performance indicators). Utilize custom dimensions, metrics, and attribution models to capture and analyze data relevant to unique business needs.5.Pharma knowledge: Understanding of the Pharma industry landscape, including regulatory compliance, patient journey mapping, and healthcare data privacy considerationsMinimum Requirement:·2-4 Years of experience in digital analytics, hands on experience on Google Analytics 4.·Proficiency in Google Analytics and Google Tag Manager.·Strong analytical skills and ability to interpret data.·Understanding of digital marketing concepts and website optimization techniques.·Excellent communication skills to present findings and recommendations effectively·GA4 Certification is strongly preferred</td><td>Map(posted_at -> 5 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(5 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEluc2lnaHRzIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJJTjEwIChGQ1JTID0gSU4wMTApIE5vdmFydGlzIEhlYWx0aGNhcmUgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImFiMU5uYTFGN1dHbFd4eC1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=ab1Nna1F7WGlWxx-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOwQqCQBCAYbr6CJ3mWFFqQZciKIrKCIl6ABnXxd1Yd2RnEHuo3jG7_LcP_ug7iuYnFITMs62NMBw8ug8LLOBGJbDGoAyQhwtR7fR4a0Ra3iQJs4trFhSrYkVNQl6X1CdvKvmfgg0G3ToUXazWaR-3vp7ts3yZwuR8fL5gB1meLtMp5NRhEMtw1ejEqIHBI9hukHC3jRVdgfXDYGXxBzQioVWwAAAA&shmds=v1_AdeF8Ki5IovOpeRqUkuegClUSkUiOKvtd7O7Wkq4pvGULDQ7Nw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=ab1Nna1F7WGlWxx-AAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550f217f2561801770bc1323d74230147da3ed24178c6aa37305.png</td><td>Data Insights Analyst</td><td>Novartis</td></tr><tr><td>List(Map(title -> Wells Fargo, link -> https://www.wellsfargojobs.com/en/jobs/r-467470/senior-data-management-analyst/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/senior-data-management-analyst-wells-fargo-JV_IC2865319_KO0,30_KE31,42.htm?jl=1009782512404&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/senior-data-management-analyst/6335579?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jooble, link -> https://in.jooble.org/jdp/908886558276547854?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/senior-data-management-analyst-at-wells-fargo-4253125411?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> BeBee, link -> https://in.bebee.com/job/14a3ffa859552460b9db99b4a442d49c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> AnitaB.org Job Board, link -> https://jobs.anitab.org/companies/wells-fargo/jobs/51859236-senior-data-management-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobaaj, link -> https://www.jobaaj.com/job/wells-fargo-senior-data-management-analyst-hyderabad-telangana-india-3-to-5-years-322854?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Wells Fargo</td><td>About this role:\n",
       "\n",
       "Wells Fargo is seeking a Senior Data Management Analyst\n",
       "\n",
       "In this role, you will:\n",
       "• Lead or participate in moderately complex programs and initiatives for data quality, governance, and metadata activities\n",
       "• Design and conduct moderately complex analysis to identify and remediate data quality, data integrity, process, and control gaps\n",
       "• Analyze, assess, and test data controls and data systems to ensure quality and risk compliance standards are met and adhere to data governance standards and procedures\n",
       "• Identify data quality metrics and execute data quality audits to benchmark the state of data quality\n",
       "• Develop recommendations for optimal approaches to resolve data quality issues and implement plans for assessing the quality of new data sources leveraging domain expertise and data, business, or process analysis to inform and support solution design\n",
       "• Lead project teams and mentor less experienced staff members\n",
       "• Drive planning and coordination on moderately complex remediation efforts acting as central point of contact\n",
       "• Consult with clients to assess the current state of data and metadata quality within area of assigned responsibility\n",
       "• Participate in cross-functional groups to develop companywide data governance strategies\n",
       "• Provide input into communication routines with stakeholders, business partners, and experienced leaders\n",
       "\n",
       "Required Qualifications:\n",
       "• 4+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n",
       "\n",
       "Desired Qualifications:\n",
       "• Experience in large enterprise data initiatives\n",
       "• Contact center business or technology experience\n",
       "• Experience implementing, managing, or utilizing big data environments (Hadoop, Big Query, etc.)\n",
       "• Experience using standard BI tools (Tableau, Power BI, MicroStrategy, etc.) preferably from big data environments\n",
       "\n",
       "Posting End Date:\n",
       "24 Jun 2025\n",
       "• Job posting may come down early due to volume of applicants.\n",
       "\n",
       "We Value Equal Opportunity\n",
       "\n",
       "Wells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n",
       "\n",
       "Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n",
       "\n",
       "Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n",
       "\n",
       "Applicants with Disabilities\n",
       "\n",
       "To request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n",
       "\n",
       "Drug and Alcohol Policy\n",
       "\n",
       "Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n",
       "\n",
       "Wells Fargo Recruitment and Hiring Requirements:\n",
       "\n",
       "a. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n",
       "\n",
       "b. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(4 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBNYW5hZ2VtZW50IEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJXZWxscyBGYXJnbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEFWbXpFcV9ySTNSUG1faUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=xAVmzEq_rI3RPm_iAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOvQrCQAwAYFz7CE6ZS-2J0EUnQfwDJwXHkmvD9eSalEuG-jY-qrp881d8FkVzJ46S4YCGcEPGQCOxwZ4xvdVgBVfxoIS5G0AYTiIh0XI3mE26dU411UENLXZ1J6MTJi-ze4nXP60OmGlKaNRumvVcTxzK8kkpKRwxB4HIcH73lNFjX8GDEnL4NSq4cB_xCz8NyUCjAAAA&shmds=v1_AdeF8Kh4C0j69LKLY7UW2LTRMhZnqjSHPg5xPB9oEh8vmo3emw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=xAVmzEq_rI3RPm_iAAAAAA%3D%3D</td><td>null</td><td>Senior Data Management Analyst</td><td>Wells Fargo</td></tr><tr><td>List(Map(title -> D. E. Shaw India, link -> https://www.deshawindia.com/careers/associate-analyst-data-analytics-4672?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/Vi8-jUJ3J6G_26K9tGu-RlIWdVx9IVaiKRgGRiYJpIOy241Gn8r41Q?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Hirewand, link -> https://www.hirewand.com/apply/job/detail?jid=1694148sid%3D5af414cf6ed33ca5454ecbdb&src=jobpost&cpid=1694&uid=85694&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-3396-c87d6663e26180f8f37f411234282c84?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs.weekday.works, link -> https://jobs.weekday.works/coursevita-tutor---data-analytics?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/8884656763974975488?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> WhatJobs, link -> https://en-in.whatjobs.com/jobs/business-intelligence?id=147270418&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Post Job, link -> https://callcenterjob.co.in/associateanalyst-data-analytics-hyderabad_1067056?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>D. E. Shaw India</td><td>The Financial Research (FinRes) group provides research and analysis support to the entire investment life cycle of the firm’s global proprietary trading strategies. The group caters to almost all the quantitative and qualitative strategies of the firm and its research support covers multiple asset classes, sectors, and geographies.</td><td>Map(posted_at -> 14 hours ago, schedule_type -> Full-time)</td><td>List(14 hours ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUvQW5hbHlzdCAtIERhdGEgQW5hbHl0aWNzIiwiY29tcGFueV9uYW1lIjoiRC4gRS4gU2hhdyBJbmRpYSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiak9McEJja1dWaXhiWjJ4c0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=jOLpBckWVixbZ2xsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNzQrCMBCE8dpH8LRnqYkIHtRToeLPVe9lk4YkJe6WbsD2kXxLq14G5psPpngvin0lwjZidroiTJNkWEONGeFXc7QygxsbEIeDDcAEZ2af3PIYcu7loLVIUl4yzrKy_NRMzvCoOzbyjUYCDq5P80ez3W1G1ZNf6VrBScE94Auu1EaESHCZWjegwbaEh0tIHgnL__wBfMXaI6wAAAA&shmds=v1_AdeF8KiGYFSm74ttc37WRtV6BAmPdDCIXraWSL4lzvQKbqdoJQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=jOLpBckWVixbZ2xsAAAAAA%3D%3D</td><td>null</td><td>Associate/Analyst - Data Analytics</td><td>D. E. Shaw India</td></tr><tr><td>List(Map(title -> BMS Careers - Bristol Myers Squibb, link -> https://jobs.bms.com/careers/job/137468587991-senior-analyst-data-risk-office-hyderabad-ts-in?domain=bms.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://bristolmyerssquibb.wd5.myworkdayjobs.com/BMS/job/Hyderabad---TS---IN/Senior-Analyst--Data-Risk-Office_R1592258?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/senior-analyst-data-risk-office-at-bristol-myers-squibb-4250791425?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/senior-analyst-data-risk-office-bristol-myers-squibb-JV_IC2865319_KO0,31_KE32,52.htm?jl=1009783451547&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> SimplyHired, link -> https://www.simplyhired.co.in/job/LBe73mWfRZmlfdY6A1WI6A2BVaIPe0pBYiqd0yrkBfRdvI_f8gJ8GA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Sands Capital Job Board, link -> https://jobs.sandscapitalventures.com/companies/karuna-therapeutics/jobs/52872727-senior-analyst-data-risk-office?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Job Referral, link -> https://getmereferred.com/job-listing/senior-analyst-data-risk-office-bristol-myers-squibb-hyderabad-5-to-10-years-experience-afe03867-553a-46ae-8e65-7e5c0565e5c2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Adzuna, link -> https://www.adzuna.in/details/5258440582?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Bristol Myers Squibb</td><td>Working with Us\n",
       "Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n",
       "\n",
       "Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n",
       "\n",
       "Roles & Responsibilities\n",
       "\n",
       "Functional and Technical\n",
       "• Execution and monitoring of data privacy office key activties.\n",
       "• Assist in the development and implementation of data privacy policies and procedures to ensure compliance with international, federal, and state regulations, including GDPR, CCPA, and HIPAA.\n",
       "• Work closely with legal, IT, HR, and other departments to address data privacy concerns and implement best practices.\n",
       "• Assist in the development and delivery of data privacy training and awareness programs across the organization. Keeps up to date with evolution of regulations impacting privacy, ethics, and data.\n",
       "• Experienced in configuring and working with various industry leading data risk and privacy tools like OneTrust, TrustArc, Microsoft Purview etc.\n",
       "• Demonstrate critical thinking around interpreting business and industry challenges and recommending best practices-based solutions to improve products, processes, systems and reduce risk.\n",
       "• Implement organizational IT controls in accordance with applicable regulations and the ability to evaluate and understand the impact of new regulations and requirements.\n",
       "\n",
       "People Management:\n",
       "• Responsible for training and mentoring junior staff to meet BMS standards.\n",
       "• Preferred experience with working in a multi-cultural, multi-location and diverse environments.\n",
       "\n",
       "Qualifications & Experience\n",
       "• B.E./B.Tech. or equivalent in computer science, engineering, life science field\n",
       "• Recognized privacy/DLP certifications and experience preferred.\n",
       "• At least years of privacy program management, compliance, or strong operations management experience (regulated or healthcare or tech preferred).\n",
       "• Knowledge of HIPAA, GDPR, CPRA, PIPL etc., and other privacy regulations is a must.\n",
       "• Ability to make decisions that impact own work and other groups/teams and works under minimal supervision.\n",
       "• Demonstrates openness to learning and developing. Takes a responsibility for their own and team’s development and growth.\n",
       "• Demonstrates an understanding of factors driving team performance and how they contribute to the team's overall success.\n",
       "• Excellent English Oral and written communication skills including the ability to deliver clear and articulate presentations.\n",
       "• Ability to use PowerPoint, Excel, Word, or other technologies to communicate complex topics to stakeholders, manage personal workload, and track projects and issues.\n",
       "\n",
       "If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n",
       "\n",
       "Uniquely Interesting Work, Life-changing Careers\n",
       "With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n",
       "\n",
       "On-site Protocol\n",
       "\n",
       "BMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n",
       "\n",
       "Site-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n",
       "\n",
       "BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n",
       "\n",
       "BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n",
       "\n",
       "BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n",
       "\n",
       "If you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n",
       "\n",
       "Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time)</td><td>List(4 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHlzdC0gRGF0YSBSaXNrIE9mZmljZSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJsWm9Rdlc1dnAyWjZyYVlMQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=lZoQvW5vp2Z6raYLAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEzQqCQBAAYLr6CJ3mHKlRRFCnIugHIsjuMruOurXN2M4G-ky9ZPQdvuQ7SlYFsZMAW0Y_aExhjxHh5vQJ17p2liCFsxhQwmBbEIaDSONpvGlj7HSd56o-azRidDaz8sqFyUifP8Tov1JbDNR5jFTOl7M-67iZLHbBaRQPl4GCQvH-OGPAMRyHigIarKZwJ4_cIOMUTlw5_AHauiYbrgAAAA&shmds=v1_AdeF8KiC1TH2oVRja-VY93SlEgKpT1ldb1bxA3z1PjzhBfDP2g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=lZoQvW5vp2Z6raYLAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550f6f29ce77ef26d8d11fbeba5f6e8402d56f4e9d4b9a38c143.png</td><td>Senior Analyst- Data Risk Office</td><td>BMS Careers - Bristol Myers Squibb</td></tr><tr><td>List(Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/data-analyst-ii-%E2%80%93-product-information-capabilities-digital-technology-at-general-mills-india-4253777713?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs Trabajo.org, link -> https://in.trabajo.org/job-3396-15a6a47b27f82dc8a87ff91fa314b933?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>General Mills India</td><td>India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.\n",
       "\n",
       "Position Title\n",
       "\n",
       "Software Engineer II – Product Information Capability\n",
       "\n",
       "Function/Group\n",
       "\n",
       "Digital & Technology\n",
       "\n",
       "Location\n",
       "\n",
       "Mumbai\n",
       "\n",
       "Shift Timing\n",
       "\n",
       "Regular\n",
       "\n",
       "Role Reports to\n",
       "\n",
       "D&T Manager – Product Information Capability\n",
       "\n",
       "Remote/Hybrid/in-Office\n",
       "\n",
       "Hybrid\n",
       "\n",
       "About General Mills\n",
       "\n",
       "We make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and Häagen-Dazs, we’ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\n",
       "\n",
       "How we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\n",
       "\n",
       "us into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\n",
       "\n",
       "General Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\n",
       "\n",
       "With our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital & Technology (D&T) Innovation, Technology & Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy & Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\n",
       "\n",
       "We advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\n",
       "\n",
       "Job Overview\n",
       "\n",
       "Function Overview\n",
       "\n",
       "The Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The team's expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\n",
       "\n",
       "The team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the \"Work with Heart\" philosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the following Link\n",
       "\n",
       "Purpose of the role\n",
       "\n",
       "This is an exciting time to work in General Mills' Supply Chain & ITQ Organization! We are accelerating Digital Transformation of our Product Information Capabilities (PIC) capabilities to provide a competitive advantage to our business. To this end, we are seeking a skilled and motivated STIBO Developer to join our Product Information Capabilities team. As a STIBO Developer, you will play a crucial role in designing, developing, and implementing solutions within our STIBO STEP platform to support our product information management (PIM) and master data management (MDM) processes. You will collaborate with cross-functional teams to understand business requirements, translate them into technical specifications, and deliver high-quality solutions that meet our business needs.\n",
       "\n",
       "Key Accountabilities\n",
       "• Design, develop, and maintain STIBO STEP solutions to support product information management processes.\n",
       "• Write and maintain code for business rules to ensure data quality and consistency.\n",
       "• Configure outbound and inbound integrations to exchange data with other systems.\n",
       "• Configure gateway endpoints for seamless data flow.\n",
       "• Develop and maintain data models within STIBO STEP to accurately represent product information.\n",
       "• Build web UI screens for data entry, validation, and reporting.\n",
       "• Develop solutions based on documented requirements and specifications.\n",
       "• Participate in agile project development, including sprint planning, daily stand-ups, and retrospectives.\n",
       "• Collaborate with business analysts, data architects, and other developers to ensure solutions align with business needs and technical standards.\n",
       "• Troubleshoot and resolve issues related to STIBO STEP implementations.\n",
       "• Stay up-to-date with the latest STIBO STEP features and best practices.\n",
       "• Create and maintain technical documentation for STIBO STEP solutions.\n",
       "\n",
       "Minimum Qualifications\n",
       "• Education – Full time graduation from an accredited university (Mandatory- Note: This is the minimum education criteria which cannot be altered)\n",
       "• Experience with tools and concepts related to MDM and/or Digital Product Publishing, such as data governance, data quality, data integration and data exporting\n",
       "• Exposure to Product Information Management Systems (PIM/MDM)\n",
       "• Technical expertise into Stibo platform\n",
       "• Experience with Data Syndication partners such as 1WorldSync, Syndigo and Salsify.\n",
       "• Exposure to GDSN Standards\n",
       "• Strong team player and collaborator with core focus on solution making; Effective analytical and technical skills; Ability to work in a cross functional team environment\n",
       "\n",
       "Preferred Qualifications\n",
       "• Product Information Management / Master Data Management\n",
       "• STIBO STEP certification\n",
       "• Business Analysis skills\n",
       "• SQL, Cloud GCP\n",
       "• Agile / SCRUM Delivery\n",
       "• Familiarity with Service Bus Integration\n",
       "• Preferably experience in Consumer Goods industry.</td><td>Map(posted_at -> 2 days ago, schedule_type -> Full-time)</td><td>List(2 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUkg4oCTIFByb2R1Y3QgSW5mb3JtYXRpb24gQ2FwYWJpbGl0aWVzIHwgRGlnaXRhbCBcdTAwMjYgVGVjaG5vbG9neSIsImNvbXBhbnlfbmFtZSI6IkdlbmVyYWwgTWlsbHMgSW5kaWEiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTWplT1ByZWI4UVZPLVpDVUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=MjeOPreb8QVO-ZCUAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWOMYpCQRBEMfUIYtDRBoL_i7DJGi0KoiAYbC49Yzu_l3b6M92CgoF32APtXfYkjmxSVAWvqoa_g2FYoSN8ZpSbOWw28Pf4gX3R4yXWmE9azuisGZbYY2BhZzK4w4oTOwq8wRfFLqtousEUthrACEvsoDJr1SQ0WnTuvX20rZk0ybwWxibqudVMQa_ttwZ7ycE6LNQLOh3m77Nr0-c0Ga8pU6lLOxaxeunICJz_zRMwu8VYwgAAAA&shmds=v1_AdeF8KgX9-NPInj0R6woa0p_BPSRuDiPPFd-gvZ8aIgLjy7J9Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=MjeOPreb8QVO-ZCUAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550fd832cbd45dde8873d2028e59e4230f6145f1024b5313cd32.jpeg</td><td>Data Analyst II – Product Information Capabilities | Digital & Technology</td><td>LinkedIn</td></tr><tr><td>List(Map(title -> Wells Fargo, link -> https://www.wellsfargojobs.com/en/jobs/r-467474/lead-data-management-analyst/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://sanofi.wd3.myworkdayjobs.com/SanofiCareers/job/Hyderabad/Data-Management-Analyst_R2720815?utm_source=Correlation+Ventures+job+board&utm_medium=getro.com&gh_src=Correlation+Ventures+job+board&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/lead-data-management-analyst/6458888?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/lead-data-management-analyst-at-wells-fargo-4253126114?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> JobzMall, link -> https://www.jobzmall.com/sanofi/job/data-management-analyst-13?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> AnitaB.org Job Board, link -> https://jobs.anitab.org/companies/wells-fargo/jobs/48461478-data-management-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobaaj, link -> https://www.jobaaj.com/job/wells-fargo-data-management-analyst-hyderabad-2-to-4-years-662803?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Expoint, link -> https://expoint.co/job/933024495?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Wells Fargo</td><td>About this role:\n",
       "\n",
       "Wells Fargo is seeking a Lead Data Management Analyst\n",
       "\n",
       "In this role, you will:\n",
       "• Organize and lead complex companywide initiatives to ensure that data quality is maintained so that data can effectively support business processes\n",
       "• Oversee analysis and reporting in support of regulatory requirements\n",
       "• Identify and recommend analysis of data quality or integrity issues\n",
       "• Evaluate data quality metrics and data quality audits to benchmark the state of data quality\n",
       "• Make decisions in complex and multi-faceted situations requiring solid understanding of data governance standards and procedures\n",
       "• Identify new data sources and develop recommendations for assessing the quality of new data\n",
       "• Lead project teams and mentor less experienced staff members\n",
       "• Recommend remediation of process or control gaps that align to management strategy\n",
       "• Serve as relationship manager for a line of business\n",
       "• Consult with and provide recommendations to senior management regarding assessments of the current state of data and metadata quality within area of assigned responsibility\n",
       "• Represent client in cross-functional groups to develop companywide data governance strategies\n",
       "• Strategically collaborate and consult with peers, colleagues, and mid-level to senior managers to coordinate and drive collaboration on solution design and remediation execution\n",
       "\n",
       "Required Qualifications:\n",
       "• 5+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n",
       "\n",
       "Desired Qualifications:\n",
       "• Experience in Data Management, Business Analysis, Analytics, Project Management.\n",
       "\n",
       "Posting End Date:\n",
       "24 Jun 2025\n",
       "• Job posting may come down early due to volume of applicants.\n",
       "\n",
       "We Value Equal Opportunity\n",
       "\n",
       "Wells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n",
       "\n",
       "Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n",
       "\n",
       "Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n",
       "\n",
       "Applicants with Disabilities\n",
       "\n",
       "To request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n",
       "\n",
       "Drug and Alcohol Policy\n",
       "\n",
       "Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n",
       "\n",
       "Wells Fargo Recruitment and Hiring Requirements:\n",
       "\n",
       "a. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n",
       "\n",
       "b. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(4 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgTWFuYWdlbWVudCBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiV2VsbHMgRmFyZ28iLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ik5YbnE1ZlFtc3M5WXNVQ0JBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=NXnq5fQmss9YsUCBAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOvQrCQAwAYFz7CE6ZS-2J4qKTIP6hm-BYcr1wrVyTcslgX8ZnVZdv_orPrFjfCAMc0BDuyBhpIDbYM6ZJDRZwFQ9KmNsOhOEkEhPNd53ZqFvnVFMd1dD6tm5lcMLk5e1e4vVPox1mGhMaNavN8l2PHMvySSkpHDFHgZ7hPAXK6DFU8KCEHH-NCi4cevwCvzbU-KEAAAA&shmds=v1_AdeF8KhS9H58A4V380Akejd_Z20iYBTasKKVtt4Fa7d0Hh4WNg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=NXnq5fQmss9YsUCBAAAAAA%3D%3D</td><td>null</td><td>Lead Data Management Analyst</td><td>Wells Fargo</td></tr><tr><td>List(Map(title -> Wellfound, link -> https://wellfound.com/jobs/3320043-senior-data-analyst-marketing-science?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Built In, link -> https://builtin.com/job/senior-marketing-science-analyst/6457491?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/senior-data-analyst-marketing-science-at-crunchyroll-4251557667?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Crunchyroll</td><td>About the role\n",
       "\n",
       "We are looking for a hands-on Senior Marketing Science Analyst with a passion for understanding data, tracking business trends, and storytelling to join the Center for Data and Insights team. This position will report to the Vice President, Business Intelligence & Analytics, and will collaborate with data engineers, campaign managers, and marketing leaders to investigate campaign trends, build business insights, and recommend proactive measures to enable better business decisions every day. The right person for this position is a service-minded, empathetic problem-solver who will be motivated by the opportunity to build a centralized insights service team from the ground up!\n",
       "• Partner with Branch/ Adjust MMPs, direct response, and media buying agencies to define and implement campaign measurement of integrated marketing campaigns, including TV, OOH, Digital, and Social Media across a wide range of business lines (theatrical, streaming, e-commerce, etc.)\n",
       "• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n",
       "• Evaluating media measurement leveraging MMM (Marketing Mix Modeling), MTA (Multi-Touch Attribution), and Incrementality A/B Testing\n",
       "• Lead / Assist in development of testing roadmaps, measurement plans, KPI & KLI reporting based on set marketing objectives linked to key campaigns and always-on digital marketing tactics.\n",
       "• Deliver ad hoc analysis for client stakeholders throughout marketing, working with the channel leads, planning, and client service departments.\n",
       "• Partner with paid marketing teams in building weekly insight decks for the Marketing leadership team by collaborating effectively across teams, documenting the impact of strategic initiatives on benchmarks, and tracking the progress towards company goals.\n",
       "• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n",
       "• Connect ideas into cohesive, well-grounded recommendations, using creative, structured, and analytical thinking with the help of effective data visualization\n",
       "• Work with offshore and onsite teams and lead the sprint planning/management\n",
       "• Maintain a culture of high-quality output and outstanding customer service by effectively communicating at all levels, ensuring that work gets done, and responding effectively to\n",
       "\n",
       "About You\n",
       "• 5+ years of experience with data analysis, paid campaign analytics, statistics, experimentation, and optimization.\n",
       "• 3+ years of experience in writing complex SQL queries, experience in marketing data automation.\n",
       "• 3+ years of experience with data visualization tools like Tableau, Superset, etc.\n",
       "• Understanding of install and user-level paid campaign tracking to support multi-touch attribution via UTMs and MMP data for all major marketing channels\n",
       "• Proficiency in data analysis, including defining critical metrics, statistical and predictive modeling concepts, descriptive statistics, and experimental design\n",
       "• Experience in Marketing analytics tools like Google Analytics, Adjust, Braze, Branch, etc.\n",
       "• Experience working with large data sets (Terabytes of data/ billions of records).\n",
       "• Deep expertise in measuring marketing performance against lifetime value metrics.\n",
       "• Outstanding teamwork skills: You have excellent interpersonal skills. You’re a good listener. You place the success of the company ahead of any particular idea. Proven ability to work across a globally, matrixed organization.\n",
       "• BS in Statistics, Computer Science, Information Systems, or a related field\n",
       "\n",
       "About the Team\n",
       "\n",
       "The Center for Data and Insights (CDI) is the centralized team of data engineering, BI, analytics, and data science experts, passionate about servicing the organization with timely and certified reports and insights! The mission of the group is to inspire, support, and guide our stakeholders to be data-aware as well as build out the systems of intelligence to discover insights and act on them.\n",
       "\n",
       "Why you will love working at Crunchyroll\n",
       "\n",
       "In addition to getting to work with fun, passionate and inspired colleagues, you will also enjoy the following benefits and perks:\n",
       "• Best-in class medical, dental, and vision private insurance healthcare coverage\n",
       "• Access to counseling & mental health sessions 24/7 through our Employee Assistance Program (EAP)\n",
       "• Free premium access to Crunchyroll\n",
       "• Professional Development\n",
       "• Company's Paid Parental Leave\n",
       "• up to 26 weeks for birthing parents\n",
       "• up to 12 weeks for non-birthing parents\n",
       "• Hybrid Work Schedule\n",
       "• Paid Time Off\n",
       "• Flex Time Off\n",
       "• 5 Yasumi Days\n",
       "• Half-Day Fridays during the summer\n",
       "• Winter Break\n",
       "\n",
       "#LifeAtCrunchyroll #LI-Hybrid</td><td>Map(posted_at -> 4 days ago, schedule_type -> Full-time)</td><td>List(4 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBBbmFseXN0LCBNYXJrZXRpbmcgU2NpZW5jZSIsImNvbXBhbnlfbmFtZSI6IkNydW5jaHlyb2xsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiI3QU16TGktYmRTM1gybEV4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=7AMzLi-bdS3X2lExAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFz7CE43l9qI4KI4iII_4KR7uaRHEo13JTmhfSMfU_yGr_rOqt2dOEqGIyrCnjFNRRu4YX6RRvZwd5HYESzgKhYKYXYBhOEk4hPNt0F1KBtjSkmtL4oaXevkbYTJymieYsu_rgTMNCRU6lbr5dgO7Ov6kD_swpQlJYgM56mnjBb7Bh6UkD0yNnDhPuIPsgWr7asAAAA&shmds=v1_AdeF8KgdWeAg2CAGkFkBzRphX7RkPGLpS9TVp0CfVLA3JykpGQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=7AMzLi-bdS3X2lExAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550f6f73161419591ea7d7c57bacc38d027cbd58da61246d96d7.jpeg</td><td>Senior Data Analyst, Marketing Science</td><td>Wellfound</td></tr><tr><td>List(Map(title -> EWorker, link -> https://eworker.co/job/principal-data-analyst-14/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Storable</td><td>About the Role:\n",
       "We’re seeking a strategic, data-driven professional to lead our Business Analysis initiatives for our Marketplace product within the Self-Storage vertical. This is a ‘hands on keyboard’ high-impact role that bridges business strategy, analytical rigor, and data visualization —ideal for someone who thrives on translating data into actionable insights that drive performance. As the Principal Data Analyst, you will own our marketplace performance metrics, lead the development of world-class dashboards and reporting frameworks, and partner closely with Product, Marketing, Operations, and Finance teams to inform strategic decisions. You’ll drive both hands-on analysis and organizational data literacy while mentoring a team of analysts and collaborating with engineering on data infrastructure.\n",
       "Key Responsibilities:\n",
       "\n",
       "Own and continuously optimize key Marketplace KPIs (conversion, revenue yield, inventory, pricing, etc.).\n",
       "Serve as the steward of metric definitions and business logic—ensuring consistency, accuracy, and alignment across stakeholders.\n",
       "Have a deep understanding of how to run a BI environment. Proactive, insightful, curious.\n",
       "Build and scale world class self-service dashboards and Tableau reports for internal and executive stakeholders.\n",
       "Lead cross-functional analysis projects to uncover trends, diagnose problems, and identify growth and efficiency opportunities.\n",
       "Translate complex data into clear, actionable insights and concise narratives for business and executive audiences.\n",
       "Drive rigorous SQL-based analysis from raw datasets, ensuring high-quality, reproducible outputs.\n",
       "Guide and collaborate with junior analysts in other groups and collaborate with engineering to shape data pipelines and definitions.\n",
       "Collaborate with cross functional partners and teams (Product,Marketing, Finance, Business Insights) to test hypotheses, evaluate feature/campaign performance, and measure outcomes.\n",
       "Identify data gaps or integrity risks and work with stakeholders to improve reliability and accuracy.\n",
       "Champion a culture of curiosity, experimentation, and evidence-based decision-making.\n",
       "Proactively keep other teams in the loop on new data capabilities unlocked and changes being made.\n",
       "\n",
       "Requirements:\n",
       "\n",
       "5-7 years of experience in business analysis, data analytics, or related roles, ideally with at least 2+ years in a leadership capacity.\n",
       "Advanced proficiency in SQL with experience working with complex data models and the ability to query complex relational datasets.\n",
       "Other data engineering experience is a significant plus to facilitate sourcing/formating of data.\n",
       "Deep understanding of performance metrics and business levers in a supply/demand or marketplace environment.\n",
       "Experience leading analytics efforts in partnership with cross-functional teams including Product, Marketing, Finance, and Engineering.\n",
       "Proven track record of developing intuitive, scalable dashboards using Tableau (or similar BI platforms).\n",
       "Strong business acumen with the ability to connect the dots between business objectives and analytical frameworks.\n",
       "Exceptional communication skills with the ability to distill technical findings for non-technical audiences.\n",
       "Capable of influencing and informing executive stakeholders with clear, concise insights.\n",
       "Demonstrated experience owning KPIs, building data products, and drive insights into action in a fast paced environment.\n",
       "Ability to navigate ambiguity, manage and prioritize competing needs, and work cross-functionally.\n",
       "\n",
       "Preferred Qualifications:\n",
       "\n",
       "Experience in the storage, real estate, or marketplace industries strongly preferred\n",
       "Familiarity with modern data stack tools such as Snowflake, dbt, or similar Experience analyzing marketplace dynamics or supply/demand business models\n",
       "Exposure to experimentation frameworks, A/B testing, or uplift modeling\n",
       "Prior exposure to high-growth SaaS or Marketplace operations\n",
       "Data engineering capabilities—especially in sourcing, transforming, or centralizing disparate datasets.\n",
       "\n",
       "About Us:\n",
       "At Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\n",
       "We leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.\n",
       "Important Notice: Protect Yourself from Fraudulent Activities Targeting Job Seekers\n",
       "We’ve been made aware of fraudulent activities where an individual or group is pretending to represent Storable, attempting to deceive job seekers by using our company’s good name and stellar reputation.\n",
       "To protect yourself, please consider the following guidelines:\n",
       "– Official Communication: All genuine communication from Storable will come from official email addresses, ending in “@storable.com.” Be cautious of any communication that doesn’t match this criteria.– No Unsolicited Offers: We do not extend job offers without a formal interview process. If you receive an unsolicited job offer claiming to be from Storable or any of its representatives, it’s a red flag.– Verification: If you’re uncertain about the legitimacy of any job offer or communication claiming to be from Storable, please directly contact our HR department directly at POps@storable.com for verification.\n",
       "Your security and trust are paramount to us. If you suspect you’ve been contacted by someone falsely claiming to be from Storable or using our company’s name for any dubious purpose, please immediately report the incident to POps@storable.com\n",
       "We’re committed to ensuring a transparent and secure hiring process.\n",
       "Thank you for your vigilance and interest in joining our team.</td><td>Map(posted_at -> 18 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(18 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiU3RvcmFibGUiLCJhZGRyZXNzX2NpdHkiOiJTZXJpbGluZ2FtcGFsbGUgKE0pLCBIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InljaTdhWW0wWnR2MEMzbkZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>Serilingampalle (M), Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=yci7aYm0Ztv0C3nFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNvQrCMBRGce0jON1RpW1EcFEXQfAHBKHu5aa9pJH03pBkaF_JpzQu33DOga_4LorqFSx31qODCyaEM6ObY4IKHqIhEoZuAGG4ihhHy-OQko8HpWJ0tYkJk-3qTkYlTFom9REd_9PGAQN5h4na3X471Z7N5tQkCagdgWVoKFhn2eCYvzNaPdcl3OaecoF9CW9ymC1jCXfuLf4Ag_mA060AAAA&shmds=v1_AdeF8KjCBpWl289vZv2SnP2YCKXhew82EyOgaZRf9mt6vq4tDw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=yci7aYm0Ztv0C3nFAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550f8f526436192f16ed0a38f7cdb2e7bb6d317729673fd2f965.jpeg</td><td>Principal Data Analyst</td><td>EWorker</td></tr><tr><td>List(Map(title -> PowerToFly, link -> https://powertofly.com/jobs/detail/2319977?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Kit Job, link -> https://www.kitjob.in/job/156094017/ilc024-data-analyst-1-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobs In India. New Jobs, New Recruitment And Fastest 2025 - Findjob24h.com, link -> https://in.findjob24h.com/de/data-analyst-1-job228081?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>UnitedHealth Group</td><td>At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.\n",
       "\n",
       "Primary Responsibilities:\n",
       "• Validate data with administrative source systems (source of truth)\n",
       "• Analyze complex datasets\n",
       "• Generate actionable insights and recommendations based on data analysis\n",
       "• Database Management:\n",
       "• Develop and maintain data models, data dictionaries, and other documentation\n",
       "• Troubleshoot and resolve database-related issues\n",
       "• Data Extraction and Transformation:\n",
       "• Support capital project that will, in part, import and transform data from various sources for broker bonus calculations\n",
       "• Ensure data integrity and quality through rigorous validation and testing\n",
       "• Data Visualization and Reporting:\n",
       "• Create visually appealing and informative dashboards and reports\n",
       "• Present findings and insights to both technical and non-technical stakeholders in a clear and concise manner\n",
       "• Continuous Learning and Improvement:\n",
       "• Stay up to date with the latest data analysis techniques and tools\n",
       "• Identify opportunities to improve data analysis processes and methodologies\n",
       "• Actively participate in knowledge sharing and mentoring within the team\n",
       "• Comply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n",
       "\n",
       "Required Qualifications:\n",
       "• Undergraduate degree or equivalent experience\n",
       "• 4+ years Experience as SAS Data Analyst\n",
       "• Experience with relational database management systems (e.g., MySQL, Oracle, SQL Server)\n",
       "• Experience with statistical analysis\n",
       "• Familiarity with data visualization tools (e.g., Tableau, Power BI)\n",
       "• Proven excellent problem-solving and critical thinking skills\n",
       "• Proven solid communication and presentation skills to effectively convey complex data analysis findings to both technical and non-technical stakeholders\n",
       "• Proven ability to work independently and collaboratively in a fast-paced, deadline-driven environment\n",
       "• Proven detail-oriented with a focus on accuracy and data integrity\n",
       "\n",
       "At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone - of every race, gender, sexuality, age, location and income - deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.\n",
       "\n",
       "#NTRQ</td><td>Map(posted_at -> 3 days ago, schedule_type -> Full-time)</td><td>List(3 days ago, Full-time)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IlVuaXRlZEhlYWx0aCBHcm91cCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJhdHFPMEswckdadGwwUy1xQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=atqO0K0rGZtl0S-qAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7Cbrc4iKYqOCikyBU3Z3LpT2alHgXcifU0T8X3_Ca76JZX9EQLoz5owZ72MJDAihh7SMIQysyZlqeo1nRk_eq2Y1qaKl3vby8MAWZ_SRB_3UasVLJaNQdjrvZFR43qycno-FGmC1CW-VdIDHceUj4A48vSB6EAAAA&shmds=v1_AdeF8KgRRYJHoDYbXc7fjK1BSe6PTzGUlRHkX-gsGlpdfX6DEA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=atqO0K0rGZtl0S-qAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550f061c5dcfce02185763416e4255437348fd02f62c016e5bce.jpeg</td><td>Data Analyst 1</td><td>PowerToFly</td></tr><tr><td>List(Map(title -> Built In, link -> https://builtin.com/job/data-analyst-competitive-benchmarking-reporting/4372058?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Shine, link -> https://www.shine.com/jobs/data-reporting-analyst/hitachi-careers/17179165?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Workday, link -> https://reputation.wd1.myworkdayjobs.com/en-US/External/job/Data-Analyst---Competitive-Benchmarking---Reporting_JR101313?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> LinkedIn, link -> https://in.linkedin.com/jobs/view/data-analyst-%E2%80%93-competitive-benchmarking-reporting-at-reputation-4175824635?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Glassdoor, link -> https://www.glassdoor.co.in/job-listing/data-analyst-%E2%80%93-competitive-benchmarking-and-reporting-reputation-JV_IC2865319_KO0,53_KE54,64.htm?jl=1009664407483&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic), Map(title -> Jobrapido.com, link -> https://in.jobrapido.com/jobpreview/1314205064239251456?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic))</td><td>Reputation</td><td>Why Work at Reputation?\n",
       "• Reputation reached over $100m in Annual Recurring Revenue (ARR) in 2022 and continues to grow worldwide.\n",
       "• We've raised over $200 million in funding from A-list venture capital firms such as Bessemer Venture Partner and Kleiner Perkins, including $150 million in equity financing from Marlin Equity Partners in January 2022.\n",
       "• Reputation is trusted by more than 250 partners, including Google, Facebook, Salesforce, J.D. Power, Amazon and Web.com.\n",
       "• Our industry leading platform has been recognized by Forrester and Gartner as a vendor of choice in Voice of the Customer, Customer Feedback Management, and Social Suites research reports.\n",
       "• The platform is used by 10+ major automotive OEMs and 16,000 auto dealerships, more than 250 healthcare systems, and over 100 leading property management firms.\n",
       "• Our executive management team is committed to building a performance-based culture where excellence is rewarded and careers are developed.\n",
       "• Who thrives at Reputation? Managers who embody a player-coach mentality. Employees who value teamwork and cross-functional collaboration. People who emphasize perseverance and hustle over quick wins and luck.\n",
       "• Our Mission: We exist to forge relationships between companies and communities.\n",
       "\n",
       "We are seeking a Data Analyst with expertise in data stewardship and analysis to manage our competitive benchmark data tenants and produce customer-facing reports for marketing and sales. The ideal candidate is proficient in BigQuery (SQL), detail-oriented, and capable of working independently on complex data projects.\n",
       "\n",
       "Responsibilities:\n",
       "• Data Stewardship & Governance: Ensure data accuracy, integrity, and accessibility by managing competitive benchmark data tenants, implementing verification processes, and maintaining governance best practices.\n",
       "• Data Pipeline Management: Develop and maintain data ingestion pipelines, including API integrations and web scraping, ensuring timely updates and scalability.\n",
       "• Data Validation & Quality Control: Oversee large-scale data input and validation, balancing hands-on data manipulation with automation strategies. Collaborate with data owners to resolve quality issues.\n",
       "• Advanced Analytics & Reporting: Conduct in-depth data analysis to uncover trends, patterns, and insights that drive business decisions. Scope, design, and execute analysis projects with minimal supervision.\n",
       "• Industry & Market Insights: Develop data-driven industry reports for marketing and sales, leveraging analytics to provide insights on market trends and competitive landscapes.\n",
       "• Cross-Functional Collaboration: Work closely with product managers, engineers, marketing, and leadership to resolve data issues, provide training, and ensure alignment on data best practices.\n",
       "• Tool & Process Optimization: Design scalable internal tools and reporting solutions to enhance data accessibility, automation, and usability across teams.\n",
       "• Stakeholder Communication: Translate complex data insights into actionable recommendations for key stakeholders, including non-technical audiences.\n",
       "\n",
       "Qualifications:\n",
       "• 5+ years of proven experience in data analysis, data stewardship, and managing large-scale datasets, with a strong focus on ensuring data accuracy, integrity, and accessibility.\n",
       "• Must be proficient with SQL and NoSQL (BigQuery), as well as have the ability to thrive in an environment that often requires patience and creativity to query, extract and validate data in which there are high levels of uncertainty.\n",
       "• Strong prior professional experience managing databases and using applicable tools is required.\n",
       "• Experience with and knowledge of ETL processes and data migration.\n",
       "• Understanding of and prior experience with General Data Protection Regulation.\n",
       "• Demonstrated experience and proficiency in using Python in real-world, business applications are substantial advantages to candidates.\n",
       "• Highly articulate with outstanding verbal and written communication skills; you are able to explain data-driven insights and analyses intuitively to clients, executives, and technical and non-technical coworkers in both large and small groups.\n",
       "• You should be comfortable taking feedback from everyone – Product Managers, Engineers, customers, the CEO, etc. – to help understand business objectives and develop the most powerful analyses and tools possible.\n",
       "• Proven ability to operate in a fast-paced, data-driven environment.\n",
       "\n",
       "When you join Reputation, you can expect:\n",
       "• Flexible working arrangements.\n",
       "• Career growth with paid training tuition opportunities.\n",
       "• Active Employee Resource Groups (ERGs) to engage with.\n",
       "• An equitable work environment.\n",
       "\n",
       "Our employees say it best:\n",
       "\n",
       "According to Glassdoor, 94% of our employees approve of our CEO, Joe Burton.\n",
       "\n",
       "Our employees highlight our:\n",
       "• Ample Opportunities- “There are many opportunities to learn and grow. Many open roles are replaced with internal promotions.”\n",
       "• Positive Culture- \"Great opportunity and exceptional culture.\" “You will never have a better culture anywhere else. Period.”\n",
       "• Training and Tools- “All managers truly want you to succeed, and you are given great tools and training to be successful in your role.”\n",
       "• Balance- “Great work life balance and awesome team environment!”\n",
       "\n",
       "Diversity Programs & Initiatives:\n",
       "\n",
       "Our Reputation Nation spans around the world. This global perspective allows us to intentionally unlock the magic that comes from diversity of experience to contribute to our success.\n",
       "\n",
       "At Reputation, we believe in:\n",
       "• Diversity: Embracing a culture that values uniqueness.\n",
       "• Inclusion: Inviting diverse groups to take part in company life.\n",
       "• Belonging: Helping each individual feel accepted for who they are.\n",
       "\n",
       "\"At Reputation, we see diversity and inclusion as the foundation for an equitable workplace. Our goal is to empower all of our employees, regardless of their background, to make an impact in their work each and every day.\" - Joe Burton, CEO, Reputation\n",
       "\n",
       "Additionally, we offer a variety of benefits and perks, such as:\n",
       "• Health Insurance & Wellness Benefits: Group Health Insurance: Medical Insurance with floater policy of up to 10,00,000 for employee + spouse + 2 dependent children + 2 parents / parent-in-laws\n",
       "• Maternity Benefits: Medical insurance up to 75,000 INR, 26 weeks of leave for birth, adoption or surrogacy\n",
       "• Life Insurance: Insurance at 3x annual cost to the company (Term Insurance, GPA)\n",
       "• Accident/Disability Insurance: Insured at 3x base salary for permanent total disability, permanent partial disability and temporary total disability (GPA)\n",
       "• OPD: of 7500 per annum per employee\n",
       "\n",
       "Leaves\n",
       "• 10 Company observed holidays a year (Refer to the Holiday Calendar for the Year)\n",
       "• 12 Casual/Sick leaves (Pro-rata calculated)\n",
       "• 02 Earned Leaves per Month (Pro-rata calculated)\n",
       "• 04 Employee Recharge days (aka company holiday/office closed)\n",
       "• Maternity & Paternity (6 months)\n",
       "• Bereavement Leave (10 Days)\n",
       "\n",
       "Car Lease:\n",
       "Reputation offers Car Lease Program that allows employees to lease a car with no upfront cost or down payment. They benefit from a fixed monthly lease rental and 20-30% tax savings.\n",
       "\n",
       "We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n",
       "\n",
       "To learn more about how we handle the personal data of applicants, visit our Candidate Privacy Notice.\n",
       "\n",
       "Applicants only - No 3rd party agency candidates.</td><td>Map(posted_at -> 6 days ago, schedule_type -> Full-time, qualifications -> No degree mentioned)</td><td>List(6 days ago, Full-time, No degree mentioned)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3Qg4oCTIENvbXBldGl0aXZlIEJlbmNobWFya2luZyBcdTAwMjYgUmVwb3J0aW5nIiwiY29tcGFueV9uYW1lIjoiUmVwdXRhdGlvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiT0NPR3UtU1dxWjh4X0gzS0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>Hyderabad, Telangana, India</td><td>Data Analyst</td><td>https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=OCOGu-SWqZ8x_H3KAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMOwrCQBBAsc0RrKYSlJiIYKOVH_CDldjLJBk2q5uZZXeU2HkH7-KBPImmebzmveTTS44bVIQlo3tGhe_rDWtpPKlV-yBYEZd1g-Fm2cAATuQlaOdjOEgBkTCUNQjDVsQ46i9qVR_neR6jy0xUVFtmpTS5MBXS5lcpYodLrDGQd6h0mc4mbebZjIb__b1L_j_LsHtWFLDAKoUzOWSDjCnsubL4A5xdZbm5AAAA&shmds=v1_AdeF8KgqJaK4z_DrbsmmMwvzImCqZmsYLB7D-WOnGPBr39JPlQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=OCOGu-SWqZ8x_H3KAAAAAA%3D%3D</td><td>https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550fc40b2f11b139c5c1588093d189c8c58e29d87754dd6f59e2.jpeg</td><td>Data Analyst – Competitive Benchmarking & Reporting</td><td>Built In</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          {
           "link": "https://careers.astrazeneca.com/job/chennai/lead-consultant-technical-lead-fullstack-data-engineer/7684/83185188576?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "AstraZeneca Careers"
          },
          {
           "link": "https://astrazeneca.wd3.myworkdayjobs.com/en-US/broadbean_external/job/Lead-Consultant---Technical-Lead---Fullstack-Data-Engineer_R-230153-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://in.jooble.org/jdp/8526236387968584391?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/lead-consultant-technical-lead-fullstack-data-engineer-at-astrazeneca-4258753290?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.iitjobs.com/job/technical-lead-data-chennai-tamil-nadu-india-preludesys-1900?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Iitjobs"
          },
          {
           "link": "https://in.bebee.com/job/e2f20069a8e55a3ce4c21ba26ac31b52?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/lead-consultant-technical-lead-fullstack-data-engineer-astrazeneca-JV_IC2833209_KO0,54_KE55,66.htm?jl=1009795683395&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://jobs.bwam.network/companies/citi-2-db90f7f6-7bfb-4128-83f9-bb534f36fcb7/jobs/49956080-big-data-program-lead-svp-chennai?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "The BWAM Job Board - Black Women In Asset Management"
          }
         ],
         "AstraZeneca",
         "Job Title: Lead Consultant - Technical Lead - Data, Analytics & AI\nCareer Level: E\n\nIntroduction to role: Are you ready to redefine an industry and change lives? Join our Operations IT organization, where your work will have a direct impact on patients by redefining our ability to develop life-changing medicines. We empower the business to perform at its peak, combining innovative science with leading digital technology platforms and data. As part of the Data Analytics and AI (DA&AI) group, you'll deliver innovative solutions that drive the transformation of medicine development. As the Technical Lead, you'll oversee the technical delivery of products and projects aligned with business objectives, tapping into your expertise in Data and Cloud Engineering, Software Engineering, AI, and more. Collaborate with stakeholders to ensure seamless delivery and maximum business impact!\n\nAccountabilities:\n• Bridge business needs with technical solutions by leading IT application design and implementation.\n• Collaborate with stakeholders to define and deliver requirements, translating them into detailed specifications.\n• Own the technical vision of products and compile detailed technical designs, refining user Epics/stories.\n• Optimize performance, mitigate risks, and ensure alignment to timelines and resource allocations.\n• Advise on industry trends and standard methodologies to enhance performance and business outcomes.\n• Provide technical direction and guidance to IT teams and business units.\n• Contribute to Data & Software Engineering standards and best practices.\n• Research new technologies to boost system performance and scalability.\n• Lead and mentor technical teams (Data Engineering, Cloud Engineering, Software Engineering) and work with AI/GenAI leads to foster collaboration and innovation.\n• Ensure platform stability, scalability, and simplicity while adhering to regulatory requirements and data security standards.\n• Foster continuous improvement and innovation.\n• Supervise technical leadership throughout the software development lifecycle and provide hands-on support in development, technical delivery, review, and testing while collaborating with global teams.\n• Apply central Enterprise Data Platforms and guide DevOps, DataOps, and MLOps teams to ensure standard methodologies.\n• Ensure data solutions align with FAIR principles and support end-to-end data science and machine learning.\n\nEssential Skills/Experience:\n• Minimum 10 years of experience in the design, development, and delivery of software and data engineering solutions.\n• Extensive technical expertise in Data Engineering, Software Engineering, and Cloud Engineering.\n• Strong foundational knowledge of AI Engineering principles and practices.\n• Deep understanding of DevOps, MLOps, and DataOps methodologies with practical implementation experience.\n• Demonstrated success in product development and/or product management, delivering complex solutions end-to-end.\n• Demonstrable ability to provide technical thought leadership across Data, Analytics, and AI domains.\n• Exceptional communication, customer management, and multi-functional collaboration skills.\n• Robust analytical and problem-solving abilities with a collaborative, team-oriented approach.\n• Hands-on experience driving innovation throughout the full product development lifecycle.\n• Solid understanding of Data Mesh and Data Product concepts and architectures.\n• Proficiency in Agile methodologies and facilitating iterative, multi-functional team delivery.\n• Hands-on experience designing, implementing, and optimizing data pipelines using leading ETL tools.\n• Skilled in architecting, deploying, and managing scalable, secure AWS cloud environments.\n• Proficient in workflow orchestration tools such as Apache Airflow.\n• Practical experience implementing DataOps practices with tools like DataOps.Live.\n• Strong expertise in data storage and analytics platforms such as Snowflake.\n• Ability to deliver actionable insights through business intelligence tools, including Power BI.\n• Extensive full-stack development experience, including backend proficiency with Node.js and Python and frontend expertise with ReactJS or NextJS.\n• Experience designing and deploying Generative AI solutions.\n• Hands-on implementation of AI/ML models using platforms such as Amazon SageMaker.\n• Advanced programming skills, especially in Python.\n• Solid knowledge of both SQL and NoSQL database technologies.\n• Familiarity with agile ways of working and iterative development environments.\n• Experience working in large, multinational organizations or pharmaceutical environments is highly desirable.\n• Demonstrated leadership and mentoring skills, with a demonstrable ability to develop high-performing technical teams.\n\nDesirable Skills/Experience:\n• Bachelor's or master's degree in health sciences, Life Sciences, Data Management, IT, or a related field.\n• Experience in the pharmaceutical industry or a similar multinational environment.\n• AWS Cloud or relevant data/software engineering certifications.\n• Awareness of use case specific GenAI tools available in the market and their application in day-to-day work scenarios.\n• Possess working knowledge of basic prompting techniques and continuously improve these skills.\n• Stay up to date with developments in AI and GenAI, applying new insights to work-related situations.\n\nWhen we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That's why we work, on average, a minimum of three days per week from the office. But that doesn't mean we're not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\n\nAt AstraZeneca, we are at a crucial stage of our journey to become a digital and data-led enterprise. Our commitment to innovation empowers us to make the impossible possible by building partnerships and ecosystems that drive scale and speed for exponential growth. With investment backing us all the way, we are focused on disrupting the industry while making a meaningful impact through our work. Here you'll find countless opportunities to learn, grow, and contribute to developing life-changing medicines.\n\nReady to make a difference? Apply now to join our team!\n\nDate Posted\n30-Jun-2025\n\nClosing Date\n\nAstraZeneca embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills. We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements.",
         {
          "posted_at": "3 days ago",
          "schedule_type": "Full-time"
         },
         [
          "3 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJMZWFkIENvbnN1bHRhbnQgLSBUZWNobmljYWwgTGVhZCAtIEZ1bGxzdGFjayBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiQXN0cmFaZW5lY2EiLCJhZGRyZXNzX2NpdHkiOiJDaGVubmFpLCBUYW1pbCBOYWR1LCBJbmRpYSIsImh0aWRvY2lkIjoiRGVXa2pFZDgxNXcwSUpJY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Chennai, Tamil Nadu, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=DeWkjEd815w0IJIcAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3LuwrCQBBAUWz9BKtplTxEsNFKfKGIIKSyCZPNkF1dZ0JmAv6bP2e0uc3hjj-j8e1CWMNWWPtoyAYpFOQ8B4cR_pbCoY9RDd0TdmgIe24CE3WDnKUCJeycB2E4ijSRJmtv1uoqz1Vj1gyjBZc5eeXCVMk7f0ilv5TqsaM2olG5WM7fWcvNbLpR6_BOTA4hMGw9MWNIoMBXiHDFuk_gxHXALxpOsRa-AAAA&shmds=v1_AdeF8KgBYGRmUf76k78qAEmFyfl_BrGpLC_L6zaXvWx_bHB8DQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=DeWkjEd815w0IJIcAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f200a1978d9d4338da/images/56fc5e069c9f2d48c59a9b83fcb6628a5563c2c8e003cb81494faae7f495eb69.png",
         "Lead Consultant - Technical Lead - Fullstack Data Engineer",
         "AstraZeneca Careers"
        ],
        [
         [
          {
           "link": "https://careers.cognizant.com/us-en/jobs/00063792912/aws-data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Cognizant Careers"
          },
          {
           "link": "https://careers.services.global.ntt/global/en/job/15cb4c0c4158ad0/AWS-Redshift-administrator-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "NTT DATA Careers"
          },
          {
           "link": "https://careers-inc.nttdata.com/job/Bangalore-AWS-Redshift-administrator-Engineer-KA/1290126800/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "NTT DATA Careers"
          },
          {
           "link": "https://www.foundit.in/job/aws-data-engineer-mount-talent-consulting-bengaluru-bangalore-34974804?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Foundit"
          },
          {
           "link": "https://www.shine.com/jobs/aws-data-engineer/capgemini-technology-services-india-limited/17091071?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://jobs.smartrecruiters.com/Datazymes/744000051989725-aws-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SmartRecruiters Job Search"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/aws-data-engineer-usefulbi-JV_IC2940587_KO0,17_KE18,26.htm?jl=1009205219719&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://cutshort.io/job/Data-Engineer-AWS-Bengaluru-Bangalore-Kloud9-Technologies-MxGnCgGy?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Cutshort"
          }
         ],
         "Cognizant",
         "Job Summary:\n\nExperience : 4 - 8 years\n\nLocation : Bangalore\n\nThe Data Engineer will contribute to building state-of-the-art data Lakehouse platforms in AWS, leveraging Python and Spark. You will be part of a dynamic team, building innovative and scalable data solutions in a supportive and hybrid work environment. You will design, implement, and optimize workflows using Python and Spark, contributing to our robust data Lakehouse architecture on AWS. Success in this role requires previous experience of building data products using AWS services, familiarity with Python and Spark, problem-solving skills, and the ability to collaborate effectively within an agile team.\n\nMust Have Tech Skills:\n\n· Demonstrable previous experience as a data engineer.\n• Technical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\n\n· Proficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nNice To Have Tech Skills:\n\n· Familiar with data services in a Lakehouse architecture.\n\n· Familiar with technical design practices, allowing for the creation of scalable, reliable data products that meet both technical and business requirements\n\n· A master’s degree or relevant certifications (e.g., AWS Certified Solutions Architect, Certified Data Analytics) is advantageous\n\nKey Accountabilities:\n• Writes high quality code, ensuring solutions meet business requirements and technical standards.\n• Works with architects, Product Owners, and Development leads to decompose solutions into Epics, assisting the design and planning of these components.\n• Creates clear, comprehensive technical documentation that supports knowledge sharing and compliance. Experience in decomposing solutions into components (Epics, stories) to streamline development.\n• Actively contributes to technical discussions, supporting a culture of continuous learning and innovation.\n\nKey Skills:\n• Proficient in Python and familiar with a variety of development technologies.\n• Previous experience of implementing data pipelines, including use of ETL tools to streamline data ingestion, transformation, and loading.\n• Solid understanding of AWS services and cloud solutions, particularly as they pertain to data engineering practices. Familiar with AWS solutions including IAM, Step Functions, Glue, Lambda, RDS, SQS, API Gateway, Athena.\n• Proficient in quality assurance practices, including code reviews, automated testing, and best practices for data validation.\n• Experienced in Agile development, including sprint planning, reviews, and retrospectives\n\nEducational Background:\n• Bachelor’s degree in computer science, Software Engineering, or related essential.\n\nBonus Skills:\n• Financial Services expertise preferred, working with Equity and Fixed Income asset classes and a working knowledge of Indices.\n• Familiar with implementing and optimizing CI/CD pipelines. Understands the processes that enable rapid, reliable releases, minimizing manual effort and supporting agile development cycles.",
         {
          "posted_at": "6 days ago",
          "schedule_type": "Full-time"
         },
         [
          "6 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJBV1MgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkNvZ25pemFudCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiWjYyVVNZZkhtUHk0dHdwNkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Hyderabad, Telangana, India (+1 other)",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Z62USYfHmPy4twp6AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCQAwAYFz7CE4Zi9SeCC46FRV_VgXHkmvD9eRMyiVD9S18Y_EbvuI7K8rmcYMDGsKRQ2SiDEu4igclzN0AwnASCYnmu8Fs1K1zqqkOamixqzt5OWHyMrmneP3X6oCZxoRG7XqzmuqRw6LcS-D4QTaIDOd3Txk99hXcKSEHZKzgwn3EH5RzejuUAAAA&shmds=v1_AdeF8KgvQshfbWQnT9KEkSQ9vQxlf8gOj45PNHhqexzNlz8RDw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Z62USYfHmPy4twp6AAAAAA%3D%3D",
         null,
         "AWS Data Engineer",
         "Cognizant Careers"
        ],
        [
         [
          {
           "link": "https://mycareer.verizon.com/jobs/r-1074436/engineer-iii-consultant-data-engineering/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Verizon Careers"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/engineer-iii-specialist-ai-ml-engineering-verizon-JV_IC2940587_KO0,41_KE42,49.htm?jl=1009592483182&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://in.jooble.org/jdp/6894517318855681925?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/engineer-iii-specialist-ai-ml-engineering-at-verizon-4235356641?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://flexa.careers/jobs/verizon-engineer-iii-consultant-data-engineering-682c1bc407733afef4fc1f03?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Flexa Careers"
          },
          {
           "link": "https://www.simplyhired.co.in/job/OU3ePsc3DL68Xm2pEJ66r4zCD12Qw89bAnDUvRVQAnOPV9aRh_ZBIw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          }
         ],
         "Verizon",
         "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat you’ll be doing…\n\nWe are looking for data engineers who can work with world class team members to help drive telecom business to its full potential. We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n\nAs a data engineer you will be collaborating with business product owners, coaches, industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n• Understanding the business requirements and converting them to technical design.\n• Working on Data Ingestion, Preparation and Transformation.\n• Developing data streaming applications.\n• Debugging the production failures and identifying the solution.\n• Working on ETL/ELT development.\n• Understanding devops process and contributing for devops pipelines\n\nWhat we’re looking for...\n\nYou’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n\nYou’ll need to have…\n• Bachelor’s degree or four or more years of work experience.\n• Four or more years of relevant work experience.\n• Experience with Data Warehouse concepts and Data Management life cycle.\n• Experience in Big Data technologies - GCP/Hadoop/Spark/Composer/DataFlow/Bigquery.\n• Experience in complex SQL.\n• Experience working on Streaming ETL pipelines\n• Expertise in Java\n• Experience with MemoryStore / Redis / Spanner\n• Experience in troubleshooting the data issues.\n• Experience with data pipeline and workflow management & Governance tools.\n• Knowledge of Information Systems and their applications to data management processes.\n\nEven better if you have one or more of the following…\n• Three or more years of relevant experience.\n• Any relevant Certification on ETL/ELT developer.\n• Certification in GCP-Data Engineer.\n• Accuracy and attention to detail.\n• Good problem solving, analytical, and research capabilities.\n• Good verbal and written communication.\n• Experience presenting to and influence stakeholders.\n• Experience in driving a small team of 2 or more members for technical delivery\n\n#AI&D\n\nWhere you’ll be working\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJFbmdpbmVlciBJSUkgQ29uc3VsdGFudC1EYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiVmVyaXpvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEhtc2E1cThWc3RlSGhKaUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Hyderabad, Telangana, India (+2 others)",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=xHmsa5q8VsteHhJiAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zXMsQrCMBCAYVz7CE43OYhtRHDRwUFF4yyu5dIeaSTeleSE6gP5nNbB5V9--IrPpNgd2QcmSmCthb1wfkZF1vKAivCfgT2UcBEHmTA1HQjDScRHmm471T5vjMk5Vj4ramiqRh5GmJwM5i4u_1LnDhP1EZXq1Xo5VD37-ew22u8RCwznV0sJHbYLuFJE9si4AMttwC8rSRd-qQAAAA&shmds=v1_AdeF8KiZ7TXW6uRH9Apq6vbBfICpbvSKG8yQ27xEOPJ9xgguaA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=xHmsa5q8VsteHhJiAAAAAA%3D%3D",
         null,
         "Engineer III Consultant-Data Engineering",
         "Verizon Careers"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/data-engineer-python-pyspark-and-azure-databricks-4-6-years-at-emids-4259163468?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.instahyre.com/job-367026-azure-data-enginner-f2f-drive-at-hcl-technologies-bangalore/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Instahyre"
          },
          {
           "link": "https://www.drjobpro.com/job/view/MC5MAW4C083H9Q4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Dr. Job Pro"
          },
          {
           "link": "https://www.sercanto.in/detail/a/data-engineering-azure-data-engineering-databricks-powershell_bengaluru_35748259?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Sercanto"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/7954895824423485440?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          }
         ],
         "Emids",
         "Hi All,\n\nGreetings for the day!!\n\nWe are currently hiring for Data Engineer (Python, Pyspark, and Azure Databricks) for Emids(MNC) at Bangalore location.\n\nRole: Data Engineer\n\nExp: 5 to 8 Years\n\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\n\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\nNote: Candidate Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.\n\nRole Overview:\n\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\n• Lead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\n• Architect scalable data streaming and processing solutions to support healthcare data workflows.\n• Develop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\n• Ensure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\n• Collaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\n• Troubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\n• Mentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\n• Stay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\nRequired Skills & Qualifications:\n• 4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\n• Expertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\n• Experience with Azure Databricks (or willingness to learn and adopt it quickly).\n• Hands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\n• Proficiency in SQL, NoSQL databases, and data modeling for big data processing.\n• Knowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\n• Experience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\n• Strong analytical skills, problem-solving mindset, and ability to lead complex data projects.\n• Excellent communication and stakeholder management skills.\n\nNote: This is not a contract position, this will be a permanent position with Emids.\n\nInterested candidates Can Share Your Updated Profile with details for below Email.\n\nNAME:\n\nCCTC:\n\nECTC:\n\nNotice Period:\n\nOffers in Hand :\n\nEmail ID: Ravi.chekka@emids.com",
         {
          "posted_at": "8 hours ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "8 hours ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChQeXRob24sIFB5c3BhcmssIGFuZCBBenVyZSBEYXRhYnJpY2tzKSB8fCAoNCAtIDYgWWVhcnMpIiwiY29tcGFueV9uYW1lIjoiRW1pZHMiLCJhZGRyZXNzX2NpdHkiOiJCZW5nYWx1cnUsIEthcm5hdGFrYSwgSW5kaWEiLCJodGlkb2NpZCI6Im9TMEEyREhLeU9wd0lVdnVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Bengaluru, Karnataka, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=oS0A2DHKyOpwIUvuAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOu27CQBBFRcsnUN0iBYn8iKKQAiqioIikoU0RoVl7tF5sz1g7awkQX5cvi2mudI5Oced_s_nvByXCTnwQ5ojl4ZIalQyHiw0U2wwkNbbXMTLupYuhau0RtxuWr8jxhh-mOIkcX-pgE1QNVPCp6jtebJqUBluXpVlXeEuUQlVU2pcq7PRcntTZfY7WUOSho8THl9XzuRjEPz3s-lAbguCdxVM3xjHDN0WZjrSUYS91oH_1b8lDxQAAAA&shmds=v1_AdeF8KhLqsOGlWBbjd2BCVC5tFwl-_UblBRFb7FpYzcHeQH24g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=oS0A2DHKyOpwIUvuAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f200a1978d9d4338da/images/56fc5e069c9f2d48e9cafb9aee8ab0386edd45db1d4eb8a4eb227fed4079aa51.jpeg",
         "Data Engineer (Python, Pyspark, and Azure Databricks) || (4 - 6 Years)",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://careers.mastercard.com/us/en/job/R-245980/Senior-Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Mastercard Careers"
          },
          {
           "link": "https://careers.avalara.com/careers-home/jobs/15360?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Avalara Careers"
          },
          {
           "link": "https://jobs.citi.com/job/pune/senior-data-engineer-avp/287/81288712640?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Citi Careers"
          },
          {
           "link": "https://search.jobs.barclays/job/pune/senior-data-engineer/13015/82501694448?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Barclays Careers"
          },
          {
           "link": "https://www.iitjobs.com/job/senior-data-engineer-at-pune-pune-maharashtra-india-iitjobs-inc-92196?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Iitjobs"
          },
          {
           "link": "https://jobs.lever.co/5xdata/d6d32a93-a965-4c91-811f-4610446c683f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Lever"
          },
          {
           "link": "https://www.foundit.in/job/senior-data-engineer-foundit-bengaluru-bangalore-35050149?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Foundit"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/senior-data-engineer-mastercard-JV_IC2856202_KO0,20_KE21,31.htm?jl=1009711720715&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          }
         ],
         "Mastercard",
         "Job Title:\n\nSenior Data Engineer\n\nOverview:\n\nPosition Overview:\n\nThe Senior Data Engineer, MyMPA will be part of GBSC’s Automation & Engineering Team, responsible for implementing enhancements and periodic refreshes of an enterprise-wide data platform.\n\nThis role will also work closely with the VP of Analytics & Metrics and Director of FP&A and gather requirements for changes and enhancements to the application and contribute to the technology platform's evolution as it grows to support the rapidly expanding Mastercard business.\n\nThe ideal candidate will have hands-on development skills combined with an ability to analyze and understand end user requirements that are critical success factors within this role. This role requires the skills and desire to work as an individual contributor as well as, collaborate cross functionally with various business constituents.\n\n1. Have you ever worked on an enterprise-wide reporting solution that relied heavily on your own knowledge and abilities to build and maintain the solution?\n2. Are you constantly hungry to learn? Do you have the “growth mindset” as opposed to the “fixed mindset”?\n3. Do you love working with people, helping them, and turning their requirements into something that can make a difference?\n\nRole:\n\n• Skilled at writing performant and reliable SQL queries to source data from Centralized data repositories such as a Data Warehouse.\n• Proficient at designing and developing Relational and Multi-dimensional Databases to host the sourced data and work with Tableau and Power BI developers within and outside the team to support reporting and dashboarding needs of the organization.\n• Develop ETL workflows and macros in Alteryx to source data for projects and build high-level and detailed data validations to ensure accuracy of the sourced data\n• Contribute to initiatives aimed at automating the data extraction, application of data quality checks to ensure datasets are released on time and with 100% accuracy.\n• Develop technical components to ensure department’s compliance with audit requirements such as SOX and other statutory/audit requirements as applicable from time to time.\n• Organize and lead discussions with customers to brainstorm on data quality issues and contribute to discussions to devise business rules to address data quality issues\n• Use MS-Excel and MS-PPT to capture findings and present to customers in an easy-to-understand and impactful manner\n• Liaison with the internal groups in MasterCard Operations and Technology to ensure our solutions remain in compliance with MasterCard technical standards. Navigate O&T requirements around change management and new development.\n\nAll About You:\n\n• Strong understanding of Windows and Linux server.\n• Good understanding of SQL Server or Oracle DB.\n• Solid understanding of Essbase technology – understand how this technology works, for both BSO\nand ASO cubes.\n• Develop BSO and ASO cubes with a strong eye for performance.\n• Strong commitment to quality and error testing code you develop. Strong ability to step in and analyze the code of others on the team.\n• Be able to work within an Agile environment that is highly responsive to the business. Our team is part of the Finance organization – you must be comfortable with working as part of the business with a strong “roll up your sleeves” mentality.",
         {
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6Ik1hc3RlcmNhcmQiLCJhZGRyZXNzX2NpdHkiOiJQdW5lLCBNYWhhcmFzaHRyYSwgSW5kaWEiLCJodGlkb2NpZCI6IlV1SE5zREk1eXZ2ckFfdVBBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Pune, Maharashtra, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=UuHNsDI5yvvrA_uPAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCQAwAUFz7CU6ZHLT2RHDRVRGFguAHlPQa7k5qUpIIfodfrC5vetVnVq3uxEUUjugIJ06FiRTWcJUejFBjBmE4i6SR5ofsPtk-BLOxSeboJTZRnkGYenmHh_T2p7OMStOITt12t3k3E6flokVz0og6QGG4vZhqaPE30bIr1nDhoeAXlqvX1JUAAAA&shmds=v1_AdeF8Kjilf5bxnv3Bc7iM6HGmbYmIZhgjXWK3XVsMCSyUs2b_A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=UuHNsDI5yvvrA_uPAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f200a1978d9d4338da/images/56fc5e069c9f2d48097f40d761b5eca5cc8e962f017d44ef1d011b40189d9eb4.png",
         "Senior Data Engineer",
         "Mastercard Careers"
        ],
        [
         [
          {
           "link": "https://www.pepsicojobs.com/main/jobs/365762?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "PepsiCo Careers"
          },
          {
           "link": "https://careers.gevernova.com/global/en/job/GVXGVWGLOBALR5003613EXTERNALENGLOBAL/Sr-Staff-Software-Engineer-Architect?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "GE Vernova Careers"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/sr-staff-software-engineer-architect-ge-vernova-JV_IC2865319_KO0,36_KE37,47.htm?jl=1009653004807&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.shine.com/jobs/architect-data-engineer/pepsico/17195234?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://www.themuse.com/jobs/gevernova/srstaff-software-engineer-architect?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "The Muse"
          },
          {
           "link": "https://www.hirist.tech/j/amgen-information-system-engineer-0-7-yrs-1463997?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://gevernova.wd5.myworkdayjobs.com/it-IT/Vernova_ExternalSite/job/Hyderabad/SrStaff-Software-Engineer---Product-Architect_R5003613/apply/applyManually?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/sr-staff-software-engineer-architect-at-ge-vernova-4169907901?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          }
         ],
         "PepsiCo",
         "Overview\n\nProvide the job title you would like to be displayed on the job posting:\n\nData Platform Engineer – Transformation & Modernization\n\nJob Overview: A Data Platform Engineer to be a key player in our transformation and modernization programs, leading the migration of applications from legacy systems to Azure-based architectures. This role involves designing, implementing, and optimizing scalable, cloud-native data solutions using Databricks, Azure DevOps (ADO), and Agile development methodologies.\n\nAs an active contributor to code development, you will help drive automation, operational excellence, and data quality across our platforms. You will collaborate with data science and product teams to create solutions that enhance our data-driven decision-making capabilities.\n\nResponsibilities\n\nResponsibilities: • Lead the migration and modernization of data platforms, moving applications and pipelines to Azure-based solutions.\n• Actively contribute to code development in projects and services.\n• Manage and scale data pipelines from internal and external data sources to support new product launches and ensure high data quality.\n• Develop automation and monitoring frameworks to capture key metrics and operational KPIs for pipeline performance.\n• Implement best practices around systems integration, security, performance, and data management.\n• Collaborate with internal teams, including data science and product teams, to drive solutioning and proof-of-concept (PoC) discussions.\n• Develop and optimize procedures to transition data into production.\n• Define and manage SLAs for data products and operational processes.\n• Prototype and build scalable solutions for data engineering and analytics.\n• Research and apply state-of-the-art methodologies in data and Platform engineering.\n• Create and maintain technical documentation for knowledge sharing.\n• Develop reusable packages and libraries to enhance development efficiency.\n\nQualifications\n\nQualifications: • Bachelor’s degree in Computer Science, MIS, Business Management, or related field\n• 10 + years’ experience in Information Technology\n• 4 + years of Azure, AWS and Cloud technologies\n• Experience in data platform engineering, with a focus on cloud transformation and modernization.\n• Strong knowledge of Azure services, including Databricks, Azure Data Factory, Synapse Analytics, and Azure DevOps (ADO).\n• Proficiency in SQL, Python, and Spark for data engineering tasks.\n• Hands-on experience building and scaling data pipelines in cloud environments.\n• Experience with CI/CD pipeline management in Azure DevOps (ADO).\n• Understanding of data governance, security, and compliance best practices.\n• Experience working in an Agile development environment.\n• Prior experience in migrating applications from legacy platforms to the cloud.\n• Knowledge of Terraform or Infrastructure-as-Code (IaC) for cloud resource management.\n• Familiarity with Kafka, Event Hubs, or other real-time data streaming solutions.\n• Experience with lagacy RDBMS (Oracl, DB2, Teradata)\n• Background in supporting data science models in production.\n\nDoes the person hired for this job need to be based in a PepsiCo office, or can they be remote?: Employee must be based in a Pepsico office\n\nPrimary Work Location: Hyderabad HUB-IND\n\nIs this role approved for relocation?: No\n\nWould you like to initially post this job internally-only or both internally and externally?: Post both internally and externally",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJBcmNoaXRlY3QgLSBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiUGVwc2lDbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNzVXaWFLWW1Xa1V2Vk5VY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Hyderabad, Telangana, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=75WiaKYmWkUvVNUcAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKPQrCQBBAYWxzBKupLCRmg2Cjlaj4U1nYh9nNsFlZZ8LOFPEkXldtHnzwqs-savclDMkoGKzgiIZw4piYqPx8Ew9K-DtAGM4iMdN8N5iNunVONTdRDS2FJsjLCZOXyT3F6z-dDlhozGjUrTft1Iwcl4s7jZoOAonh8u6poMe-hgdl5IiMNVy5T_gFzhsgGJoAAAA&shmds=v1_AdeF8KgYr74ZFWV4KJEaWZHMkZ0LC87cwjr0f-JrKRTAYyHS8g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=75WiaKYmWkUvVNUcAAAAAA%3D%3D",
         null,
         "Architect - Data Engineer",
         "PepsiCo Careers"
        ],
        [
         [
          {
           "link": "https://www.okta.com/company/careers/business-technology/senior-analytics-data-engineer-6716722/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Okta"
          },
          {
           "link": "https://wellfound.com/jobs/3267194-senior-data-analytics-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Wellfound"
          },
          {
           "link": "https://builtin.com/job/senior-data-analytics-engineer/4679310?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/senior-data-analytics-engineer-ethos-life-JV_IC2940587_KO0,30_KE31,41.htm?jl=1009705371850&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://prudential.wd3.myworkdayjobs.com/ko-KR/prudential/job/Senior-Data-Analytics-Engineer_23120122?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://www.simplyhired.co.in/job/vIJWmyry5Lf7NASB70kqBc-8xTYRfcbx8ePcNVXpKj9vYXmwX_LxVQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://www.antaltechjobs.in/job/senior-data-analytics-engineer-8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Antal Tech Jobs"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/senior-analytics-data-engineer-at-okta-4231155979?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          }
         ],
         "Okta, Inc.",
         "Get to know OktaOkta is The World’s Identity Company. We free everyone to safely use any technology, anywhere, on any device or app. Our flexible and neutral products, Okta Platform and Auth0 Platform, provide secure access, authentication, and automation, placing identity at the core of business security and growth.At Okta, we celebrate a variety of perspectives and experiences. We are not looking for someone who checks every single box - we’re looking for lifelong learners and people who can make us better with their unique experiences. Join our team! We’re building a world where Identity belongs to you.Senior Analytics Engineer\nWe are looking for an experienced Analytics Engineer to join Okta’s enterprise data team. This analyst will have strong background in SaaS subscription and product analytics, a passion for providing customer usage insights to internal stakeholders, and experience organizing complex data into consumable data assets. In this.",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHl0aWNzIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJPa3RhLCBJbmMuIiwiYWRkcmVzc19jaXR5IjoiQmVuZ2FsdXJ1LCBLYXJuYXRha2EsIEluZGlhIiwiaHRpZG9jaWQiOiJhZHlINU5lRGFEQWwyUW1aQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Bengaluru, Karnataka, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=adyH5NeDaDAl2QmZAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2NMQrCQBAAsc0TrLZV4p0IabRSFFELCx8gm3O5nDl3w-0F4mv8qtFmmmGY4jMpqhtxkARbxvjOwSnsMSMc2AcmSrCAs9SghMk1IAxHER9pumly7nRtrWo0XjOOqXHyssJUy2CfUusPd20wURcx031VLQfTsZ_Prm3GEk7sDASGHbHH2Ke-hAsmHvft3z4CfgG5SMGUogAAAA&shmds=v1_AdeF8KhyLWW_m3EB7N8ESd9cJTImIS9eRNMNaNNrFoYMaBtpYA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=adyH5NeDaDAl2QmZAAAAAA%3D%3D",
         null,
         "Senior Analytics Data Engineer",
         "Okta"
        ],
        [
         [
          {
           "link": "https://careers.cencora.com/ca/fr/job/R2424901/Lead-Data-Engineer-Data-Engineering?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Cencora Careers"
          },
          {
           "link": "https://myhrabc.wd5.myworkdayjobs.com/ro-RO/Global/job/Lead-Data-Engineer---Data-Engineering_R2424901?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://in.jooble.org/jdp/-4848617790076881756?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://apna.co/job/bengaluru/lead-software-engineer-java-dev-and-data-engineeri-1128030568?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Apna"
          },
          {
           "link": "https://www.bayt.com/en/india/jobs/lead-software-engineer-bi-data-platform-lead-72980898/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Bayt.com"
          },
          {
           "link": "https://in.expertini.com/jobs/job/cgi-lead-software-engineersenior-software-engineer-data-engineering-india-cgi-315-46444987/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Expertini"
          },
          {
           "link": "https://www.kitjob.in/job/148315132/n273-cgi-lead-software-engineer-senior-software-engineer-data-engineering-hyderabad/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "Cencora",
         "Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\n\nJob Details\n\nPRIMARY DUTIES AND RESPONSIBILITIES:\n• Leads design, development, and automation of scalable Analytic data engineering products leveraging Machine Learning, cloud infrastructure\n• Leads the design, build and operationalization of monitoring and tracking of data quality and data flow dynamics\n• Optimizes existing data processes and implements best-in-class data transformation capabilities\n• Leverages advanced statistical and computational methodologies to deliver insights and identify strategic opportunities\n• Implements Data Ingestion Framework and other re-usable components leveraged for delivering data pipeline and supporting ad-hoc data ingestions\n• Assists with development and storage of analytics-ready data for development of analytic deliverables\n• Recommends data products to solve business problems meeting multiple stakeholder requirements\n• Drives project planning processes, delegates non-complex tasks to junior team members\n• Mentors other team members and assists them with priority setting and issue resolution\n• Maintains data and information environment that provides consistent and accurate reporting of the client’s data\n• Leverages Machine Learning to enhance the developed solution\n• Collaborates with Business and Client representatives to target strategic analytics opportunities and define deliverable scope\n• Facilitates the resolution of issues regarding projects being worked on to include proper documentation of recommendations for those issues\n• Manages multiple initiatives in parallel partnering with other Analytics team members and conducts review meetings with stakeholders to drive prioritization effort\n• Analyzes model errors and design strategies to overcome them\n• Implements best practices for data engineering to ensure quality delivery of enterprise solutions\n• Brings expert knowledge of data visualization tools and techniques to drive business analytics and semantic data access requirements.\n• Work closely with business users, vendors, and delivery teams to understand the business requirements that drive the analysis and design of business analytics and reporting solutions.\n• Conceptualize, design, and develop data visualization solutions that synthesize data concepts into clear communications for key business stakeholders.\n• Drives business stakeholder adoption of insights-driven decision making and/or business process innovation.\n• Leads knowledge transfer around using data visualizations to business stakeholders.\n• Assist in developing best practices for data presentation and sharing across the organization.\n• Ensures data visualization standards are maintained and implemented.\n• Demonstrate an ability to reduce data to the bare minimum of what is needed to optimally communicate a message.\n• Provides technical leadership, coaching and mentoring to team members and business users.\n• Participates in POC projects and provides business analytics solutions recommendations.\n• Evaluates new visualization tools and performs research on best practices.\n• Contributes to strategic planning meetings and provides guidance and expertise on system options, risk, cost vs. benefits, and impacts on business processes and goals.\n• Has expertise in multiple technical environments and possesses business knowledge that spans multiple business areas.\n• Responsible for BI Tool administration & security functions as designated\n\n.\n\nEDUCATIONAL QUALIFICATIONS:\n\nBachelor’s Degree in Statistics, Computer Science, Information Technology or any other related discipline or equivalent related experience.\n\nPreferred Certifications:\n• Advanced Data Analytics Certifications\n• AI and ML Certifications\n• SAS Statistical Business Analyst Professional Certification\n\nWORK EXPERIENCE:\n6+ years of directly-related or relevant experience, preferably in healthcare data analytics or data engineering.\n\nWorking Hours:\n\n7PM IST to 2AM IST; Hybrid Working Model\n\nSKILLS & KNOWLEDGE:\n\nBehavioral Skills:\n• Conflict Resolution\n• Creativity & Innovation\n• Decision Making\n• Planning\n• Presentation Skills\n• Risk-taking\n\nTechnical Skills:\n• Advanced Data Visualization Techniques\n• Advanced Statistical Analysis\n• Big Data Analysis Tools and Techniques\n• Data Governance\n• Data Management\n• Data Modelling\n• Data Quality Assurance\n• Machine Learning and AI Fundamentals\n• Programming languages like SQL, R, Python\n\nTools Knowledge:\n• Business Intelligence Software like Tableau, Power BI, Alteryx, QlikSense\n• Data Visualization Tools\n• Microsoft Office Suite\n• Statistical Analytics tools (SAS, SPSS3)\n\nWhat Cencora offers\n\n​Benefit offerings outside the US may vary by country and will be aligned to local market practice. The eligibility and effective date may differ for some benefits and for team members covered under collective bargaining agreements.\n\nFull time\n\nAffiliated Companies\nAffiliated Companies: CENCORA INDIA TECHNOLOGY SERVICES PRIVATE LIMITED\n\nEqual Employment Opportunity\n\nCencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\n\nThe company’s continued success depends on the full and effective utilization of qualified individuals. Therefore, harassment is prohibited and all matters related to recruiting, training, compensation, benefits, promotions and transfers comply with equal opportunity principles and are non-discriminatory.\n\nCencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com. We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned",
         {
          "posted_at": "3 days ago",
          "schedule_type": "Full-time"
         },
         [
          "3 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgLSBEYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiQ2VuY29yYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJHdzRzcW9FbWlPdEVnTDAwQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Gw4sqoEmiOtEgL00AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_1XIsQoCMQyAYVxvcnbKLNiK4KJuKqL4DkfaC70eNSlNhnsJ31kdXX74_u696E5PwgEuaAhXTpmJGmz-nTl910MCKGGLIwjDTSQVWh1Hs6oH71WLS2poObooLy9MQWY_SdBfeh2xUS1o1O_229lVTuvlmThKQ8gMdx4yfgDb05UQkAAAAA&shmds=v1_AdeF8Kgf1ac1-PLaIiWlr3bT-tlzdySgj_wz-q2WLGlnsUEgdg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Gw4sqoEmiOtEgL00AAAAAA%3D%3D",
         null,
         "Lead Data Engineer - Data Engineering",
         "Cencora Careers"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/azure-data-engineer-%E2%80%93-azure-databricks-at-aiprus-software-private-limited-4260810830?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.antaltechjobs.in/job/microsoft-azure-data-engineer-apac?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Antal Tech Jobs"
          },
          {
           "link": "https://www.jobaaj.com/job/accenture-microsoft-azure-modern-data-platform-data-platform-engineer-bengaluru-karnataka-india-9-to-68-years-289678?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobaaj"
          },
          {
           "link": "https://expoint.co/job/433094843?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Expoint"
          },
          {
           "link": "https://in.trabajo.org/job-2927-4b20954cde7d8d75f85d90daefcf05f4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/8651830065782325248?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          }
         ],
         "Aiprus Software Private Limited",
         "Job Title: Azure Data Engineer – Azure Databricks\n\nLocation: Bangalore, India\n\nExperience: 5 to 10 Years\n\nJob Summary:\n\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n\nKey Responsibilities:\n• Design and develop scalable data pipelines using Azure Databricks and PySpark.\n• Transform raw data into actionable insights through advanced data engineering techniques.\n• Build, deploy, and maintain machine learning models using MLlib, TensorFlow, and MLflow.\n• Optimize data integration workflows from Azure Blob Storage, Data Lake, and SQL/NoSQL sources.\n• Execute large-scale data processing using Spark Pools, fine-tuning configurations for performance and cost-efficiency.\n• Collaborate with data scientists, analysts, and business stakeholders to deliver robust data solutions.\n• Maintain and enhance Databricks notebooks and Delta Lake architectures.\n\nRequired Skills & Qualifications:\n• Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field.\n• 7+ years of experience in data engineering, with at least 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n• Strong proficiency in:\n• Python, PySpark, Pandas, NumPy, SciPy\n• Spark SQL, DataFrames, RDDs\n• Delta Lake, Databricks Notebooks, MLflow\n• Hands-on experience with:\n• Azure Data Lake, Blob Storage, Synapse Analytics\n• Excellent problem-solving and communication skills.\n• Ability to work independently and in a collaborative team environment.\n\nPreferred Qualifications:\n• Experience with CI/CD pipelines for data workflows.\n• Familiarity with data governance and security best practices in Azure.\n• Knowledge of real-time data processing and streaming technologies.",
         {
          "posted_at": "6 hours ago",
          "schedule_type": "Full-time"
         },
         [
          "6 hours ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJBenVyZSBEYXRhIEVuZ2luZWVyIOKAkyBBenVyZSBEYXRhYnJpY2tzIiwiY29tcGFueV9uYW1lIjoiQWlwcnVzIFNvZnR3YXJlIFByaXZhdGUgTGltaXRlZCIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiek5tRUxOMG5Iam5zazl6TkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Bengaluru, Karnataka, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=zNmELN0nHjnsk9zNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WOO27CUBBFlZYlpJoaERsh0SRSEAiE-BRILACN7eF5wMxYM8-AqNgD-2ExWQmmSnOLc05xO8-Pzmh8a4xgihFhJoGFyODv_oB_nhnnR4cvWGoGTmh5CSowVw0Vff6UMdb-nabuVRI8YuQ8yfWUqlCm1_Sgmb9n5yUa1RVG2g2G_WtSS-j-jrm2xmGr-3hpNWyMz20Baz5xpAJYYEISsGqs6cEKTdpDR-zBQgrGF9MrPYTBAAAA&shmds=v1_AdeF8KgIVBow-Ns54zCqcTfwOEP5aEU5Cn9QBz40t-r_Q8lxhg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=zNmELN0nHjnsk9zNAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f200a1978d9d4338da/images/56fc5e069c9f2d485f3524bedd4cd584c799aed00abeba663015fe2649128562.jpeg",
         "Azure Data Engineer – Azure Databricks",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://www.splunk.com/en_us/careers/jobs/principle-software-engineer-for-31866.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Splunk"
          },
          {
           "link": "https://builtin.com/job/software-engineer-ii-backend-data-platform/6337823?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://www.antaltechjobs.in/job/software-engineer---reference-data-platform?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Antal Tech Jobs"
          },
          {
           "link": "https://www.instahyre.com/job-297629-software-engineer-data-platform-at-walmart-global-tech-india-bangalore/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Instahyre"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/software-engineer-ii-backend-data-platform-at-abnormal-ai-4240981999?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://in.jooble.org/rjdp/-4348083940499712143?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://www.simplyhired.co.in/job/88ZuUX6jSORTKI7TUZv6WxDbh8f1BmoPtIK0E8CBjU8d3oBbYFfUxw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://www.themuse.com/jobs/abnormalsecurity/software-engineer-ii-backend-data-platform-c9ec20?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "The Muse"
          }
         ],
         "Splunk",
         "Splunk, a Cisco company, is building a safer and more resilient digital world with an end-to-end full stack platform made for a hybrid, multi-cloud world. Leading enterprises use our unified security and observability platform to keep their digital systems secure and reliable. Our customers love our technology, but it's our caring employees that make Splunk stand out as an amazing career destination. No matter where in the world or what level of the organization, we approach our work with kindness. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Come help organizations be their best, while you reach new heights with a team that has your back.Role SummarySplunk AppDynamics, a leader in observability solutions for both cloud and on-premises environments, empowers customers with end-to-end visibility across their entire application ecosystems, cloud, on-premises, or hybrid. Our team is dedicated to developing, maintaining, and supporting SaaS and on-premises offerings within the observability product suite.We're looking for a Senior Software Engineer to build upon our application and data platform as we continue to innovate on application observability. We move fast and iterate quickly. We are passionate about solving customers’ problems. We have ambitious goals to build best-of-its-kind products. You will help the team win in a fast-growing market. If you are passionate about innovation and embrace the challenge of working on highly scalable systems that handle large volumes of data, this position is for you.Meet the Team (Optional)Data platform is the heart of the AppDynamics architecture. We process more than 100 billion data packets per day, more than 5 peta byte of data flows through our platform. We are in 8 regions having 40 clusters of 7000 nodes. We ingest, process and store all the data that power the dashboards that our customers rely on for observing their infrastructure and their applications.What you'll get to do\n• Design and build highly scalable solutions\n• Work with a team of exceptionally capable and dedicated peers, all the way from engineering to product management and customer support\n• Work in an open environment, work together to get things done and adapt to the team's changing needs\n• Leverage technologies including Kafka, ElasticSearch, Docker, and Kubernetes across different cloud environments like AWS and Azure\n• lead critical initiatives for the organisation\nMust-have Qualifications\n• 15+ years of full-stack developer experience in designing and developing highly scalable, distributed applications, products, and services.\n• Expertise in Java programming language.\n• Strong proficiency in data structures, algorithms, threads, concurrent programming\n• Extensive knowledge of SQL and at least one relational database engine: MySQL. Hands on experience in RDS or NoSQL (Dynamo, MongoDB) is a big plus\n• Experience of building applications using microservices architecture with expertise in Dockers and Kubernetes.\n• Strong communication skills, both verbal and written. Ability to multi-task and adapt quickly to changing requirements, scope, and priorities.\n• Optimize data pipelines, storage systems, and query engines for performance and efficiency.\n• Work closely with product teams, data engineers, software developers, and product managers to deliver on business goals.\n• Mentor team members in architecture principles, coding best practices, and system design.\n• Ensure robust monitoring, logging, and alerting systems for proactive issue detection and resolution.\n• Support CI/CD processes and automate testing for data systems\n• Stay abreast of emerging trends in big data, machine learning, and distributed systems to recommend innovative solutions.\nNice-to-have QualificationsWe’ve taken special care to separate the must-have qualifications from the nice-to-haves. “Nice-to-have” means just that: Nice. To. Have. So, don’t worry if you can’t check off every box. We’re not hiring a list of bullet points–we’re interested in the whole you.\n• Added advantage of having an experience in working on Cloud Observability Space.\n• experience of other languages like python, etc\n• experience of front-end technologies\nSplunk is an Equal Opportunity EmployerSplunk, a Cisco company, is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.\n\nNote:",
         {
          "posted_at": "9 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "9 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJQcmluY2lwbGUgU29mdHdhcmUgRW5naW5lZXIgZm9yIERhdGEgUGxhdGZvcm0gLSAzMTg2NiIsImNvbXBhbnlfbmFtZSI6IlNwbHVuayIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiY2k4emNxSnNNanNSemNYUEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Bengaluru, Karnataka, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=ci8zcqJsMjsRzcXPAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOOw7CMBBERZsjUG1DgyDhIyIEHQIhPkUkDoA2YeOYOLuW7YicixNimhm9Zt4k31FyL5zmSltD8JA6fNARnFhpJnJQi4MjBoTCYIjQwRzWy22ex75KCZ7QVQ0Iw1lEGRrvmxCs32WZ9yZVPmDQVVpJlwlTKUP2ltL_4-mbKLJxlZ6rzWJILavp5GFNzy1ohgOxQtO7fgY3dBwvtDiDC780_gAY7Jx1tQAAAA&shmds=v1_AdeF8Kg4BT_m5sIxVWF_oYIBoa0Kgrztog4xYjhPZHAegoh_Tg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=ci8zcqJsMjsRzcXPAAAAAA%3D%3D",
         null,
         "Principle Software Engineer for Data Platform - 31866",
         "Splunk"
        ],
        [
         [
          {
           "link": "https://group.bnpparibas/en/careers/job-offer/software-developer-python?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Groupe BNP Paribas"
          },
          {
           "link": "https://in.indeed.com/viewjob?jk=6709c3557b007841&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/sr-software-developer-asp-net-midas-it-services-JV_KO0,29_KE30,47.htm?jl=1006209888609&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/software-developer-sase-at-check-point-software-4253111300?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.foundit.in/job/software-developer-banking-sector-ambitiwiz-india-34842371?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Foundit"
          },
          {
           "link": "https://apna.co/job/hyderabad/software-developer-android-1699992987?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Apna"
          },
          {
           "link": "https://www.techgig.com/jobs/Software-Developer-PHP-technologies/70786114?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "TechGig"
          },
          {
           "link": "https://www.jobaaj.com/job/larsen-toubro-software-developer-web-applications-greater-chennai-area-5-to-10-years-686697?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobaaj"
          }
         ],
         "BNP Paribas India Solutions",
         "About BNP Paribas India Solutions:\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n\nAbout BNP Paribas Group:\n\nBNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n\nCommitment to Diversity and Inclusion\n\nAt BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n\nAbout Business line/Function:\n\nThe Intermediate Holding Company (“IHC”) program structured at the U.S. level across poles of activities of BNP Paribas provides guidance, supports the analysis, impact assessment and drives adjustments of the U.S. platform’s operating model due to the drastic changes introduced by the Enhanced Prudential Standards (“EPS”) for Foreign Banking Organizations (“FBOs”) finalized by the Federal Reserve in February 2014, implementing Section 165 of U.S. Dodd-Frank Act.\n\nThe IT Transversal Team is part of the Information Technology Group which works simultaneously on a wide range of projects arising from business, strategic initiatives, and regulatory changes and reengineering of existing applications to improve functionality and efficiency.\n\nJob Title:\n\nPython Developer\n\nDate:\n\nJune-25\n\nDepartment:\n\nITG- Fresh\n\nLocation:\n\nChennai, Mumbai\n\nBusiness Line / Function:\n\nFinance Dedicated Solutions\n\nReports to:\n\n(Direct)\n\nGrade:\n\n(if applicable)\n\n(Functional)\n\nNumber of Direct Reports:\n\nNA\n\nDirectorship / Registration:\n\nNA\nPosition Purpose\n\nThe Python Developer will play a critical role in building and maintaining financial applications and tools that support data processing, analysis, and reporting within a fast-paced financial services environment. This position involves developing scalable and secure systems. The developer will collaborate with business analysts, finance users/or finance BA to translate complex business requirements into efficient, high-quality software solutions.\n\nA strong understanding of financial concepts, data integrity, and regulatory compliance is essential. The detailed responsibilities are mentioned below.\n\nResponsibilities\n\nDirect Responsibilities\n\n- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n\n- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n\n- Expertise in PySpark for large-scale data processing and loading into databases.\n\n- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n\n- Strong communication skills to effectively collaborate with team members and stakeholders.\n\n- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\nTechnical & Behavioral Competencies\n\n- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n\n- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n\n- Expertise in PySpark for large-scale data processing and loading into databases.\n\n- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n\n- Strong communication skills to effectively collaborate with team members and stakeholders.\n\n- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n\n- Good analytical, problem solving, & communication skills\n\n- Engage in technical discussions and to help in improving the system, process etc\n\nNice to Have\n\n- Familiarity with Plotly and Matplotlib for data visualization of large datasets.\n\n- Skilled in API programming, handling JSON, CSV, and other unstructured data from various systems.\n\n- Familiarity with JavaScript, CSS, and HTML.\n\n- Experience with cloud architecture applications such as Dataiku or Databricks; competency with ETL tools.\n\n- Knowledge of regulatory frameworks, RISK, CCAR, and GDPR.\nSpecific Qualifications (if required)\n\nSkills Referential\n\nBehavioural Skills: (Please select up to 4 skills)\n\nAbility to collaborate / Teamwork\n\nCritical thinking\n\nAbility to deliver / Results driven\n\nCommunication skills - oral & written\n\nTransversal Skills: (Please select up to 5 skills)\n\nAnalytical Ability\n\nAbility to develop and adapt a process\n\nAbility to understand, explain and support change\n\nAbility to develop others & improve their skills\n\nChoose an item.\n\nEducation Level:\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 5 years\n\nOther/Specific Qualifications (if required)",
         {
          "posted_at": "7 days ago",
          "schedule_type": "Full-time"
         },
         [
          "7 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBEZXZlbG9wZXItIFB5dGhvbiIsImNvbXBhbnlfbmFtZSI6IkJOUCBQYXJpYmFzIEluZGlhIFNvbHV0aW9ucyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ6VTdCX1R2emRzczk0M0ZoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=zU7B_Tvzdss943FhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWMMQrCQBAAsc0TrLawEpJTwUY7EUQLCeQBYS-uuZPz9rhdNX7E9xqxGZgppvhMimXDV31hJtjTkwInyiXUb3UcoYQTWxDC3DkY_cDcB5punWqSjTEioepFUX1XdXw3HMnyYG5s5YdW3PhNAZXa1XoxVCn289nuXEON2VsUOMaLR2g4PNRzFPDxn74MpNGLmQAAAA&shmds=v1_AdeF8KhddRLnKmMGyyu5XSrgUY0KL77JpalABEFKQDRF04WXHg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=zU7B_Tvzdss943FhAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f3e6590e63b71be3be/images/7ca52d7311167a4b2e85f52b4c2b84195050eda524e990896d525b1805249c20.jpeg",
         "Software Developer- Python",
         "Groupe BNP Paribas"
        ],
        [
         [
          {
           "link": "https://in.indeed.com/viewjob?jk=f610807b1c0a08e1&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/freelance-python-developer-teqlawn-JV_KO0,26_KE27,34.htm?jl=1009797935274&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          }
         ],
         "Teqlawn",
         "We’re looking for an experienced Python Developer (4–7 years) to build scalable web and application solutions, with opportunities to integrate AI capabilities. Experience in AI integration is a must.\n\nResponsibilities:\n• Develop scalable web and application solutions using Python, with integration of AI/ML components\n• Collaborate with clients to understand project goals and technical requirements\n• Write clean, maintainable, and well-documented code\n• Troubleshoot, debug, and optimize applications for performance and reliability\n• Ensure timely and efficient delivery of milestones and final deliverables\n• Participate in code reviews and contribute to maintaining coding standards and best practices\n• Work with relevant frameworks and libraries such as Django, Flask, FastAPI, NumPy, pandas, and scikit-learn\n\nNote: Please share the link to your portfolio along with your application.\n\nJob Types: Full-time, Contractual / Temporary, Freelance\nContract length: 2 months\n\nPay: ₹50,000.00 - ₹80,000.00 per month\n\nBenefits:\n• Work from home\n\nLocation Type:\n• Remote\n\nSchedule:\n• Day shift\n• Monday to Friday\n\nExperience:\n• Python Development: 4 years (Required)\n\nWork Location: Remote",
         {
          "posted_at": "1 day ago",
          "qualifications": "No degree mentioned",
          "salary": "₹50K–₹80K a month",
          "schedule_type": "Full-time",
          "work_from_home": "true"
         },
         [
          "1 day ago",
          "₹50K–₹80K a month",
          "Work from home",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlRlcWxhd24iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiY20xcUU4aHhjbFQ3dnVyT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Anywhere",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJMQ9BMRAA4FjfZDbdLNEisbAKYTLYX651aftSd9VreFa_HMu3fN1n0q0OlSgje4LLu0Vh2NOTshSqsICzOFDC6iP85igSMs12sbWiW2tVswnasCVvvNytMDkZ7SBO__QasVLJ2Khfb5ajKRzm0ys9Mr4YEsOJbwm_PB-WVoUAAAA&shmds=v1_AdeF8KgBybPFrWw15GHBBGaXtzfcSVMmfp01tpCXCitzSPYcjg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D",
         null,
         "Freelance Python Developer",
         "Indeed"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/python-developer-%E2%80%94-full-time-1-2-years-exp-in-office-bangalore-at-serp-hawk-4257156470?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://network.mattwallaert.com/companies/capgemini-2-af6e920e-79b5-48e9-88db-41d4adb84606/jobs/41903824-sb-author-4-6-years-bangalore?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Matt Wallaert Job Board"
          },
          {
           "link": "https://www.kitjob.in/job/156848102/python-developer-fast-years-bangalore-wfo-kfy791-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "SERP Hawk",
         "\uD83D\uDE80 We’re Hiring: Python Developer\n\nSERP Hawk is looking for a talented Python Developer to join our dynamic team in Bangalore!\n\n\uD83C\uDF1F About Us\n\nSERP Hawk is a leading digital marketing and technology solutions company based in Bangalore. We specialize in building robust backend systems, scalable applications, and AI-powered solutions for clients across various industries.\n\n\uD83D\uDCCD Location:B-1, Bannerghatta Slip Road, KEB Colony, New Gurappana Palya, 1st Stage, BTM Layout 1, Bengaluru, Karnataka 560029\n\n\uD83C\uDF10 Website: www.serphawk.com\n\n\uD83D\uDCBC What You’ll Do\n• Design and develop scalable backend architectures.\n• Write clean, efficient Python code.\n• Integrate APIs and databases.\n• Implement CI/CD pipelines and automated tests.\n• Ensure high performance, security, and reliability.\n\n✅ What We’re Looking For\n\n✔️ 1–2 years of experience in Python development.\n\n✔️ Proficiency in frameworks like Django, Flask, or FastAPI.\n\n✔️ Strong understanding of APIs and databases.\n\n✔️ Experience with CI/CD tools and best practices.\n\n✔️ Excellent problem-solving skills and a collaborative mindset.\n\n\uD83D\uDCA1 Nice to Have\n\n⭐ Experience with AI/chatbots.\n\n⭐ Knowledge of cloud services and containerization.\n\n\uD83D\uDCB0 Salary\n• ₹20,000 – ₹25,000 per month (based on skills and experience).\n\n\uD83D\uDCCC Additional Details\n\n\uD83D\uDCBC This is a full-time, in-office role in Bangalore.\n\n\uD83C\uDFE2 Candidates must report to the office daily.\n\n\uD83C\uDF10 Should be flexible to work and attend meetings or calls as per client time zones.\n\n✨ Apply now and grow with us!",
         {
          "posted_at": "5 hours ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "5 hours ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAlCBGdWxsLVRpbWUgfCAxLTIgeWVhcnMgRXhwIHwgSW4tT2ZmaWNlIHwgQmFuZ2Fsb3JlIiwiY29tcGFueV9uYW1lIjoiU0VSUCBIYXdrIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjNBMWtiUmowZFhsRVZsWnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=3A1kbRj0dXlEVlZyAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNvQrCMBSFce3o6HRnIa0WXHQT699iUVcpabxto2luSKK24OBD-CA-k09iXQ4cvsN3gk8vOKWtr0jDAu-oyKCF7-sNy5tS7ChrhCeMWQwtcusgaUzXN5rtikKKP5tzXXJFFoHBlnJw3U5U0PlWRKXCwazy3rhpFDmnwtJ57qUIBdURacypiS6Uu39kruIWjeIes3gyakKjy2H_kOxTWPPHFaTufs-S_wDzm2IWswAAAA&shmds=v1_AdeF8KjN42J0FJmp3krgNjBjj45F10uMxQlIRBdqv0myDo7lHw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=3A1kbRj0dXlEVlZyAAAAAA%3D%3D",
         null,
         "Python Developer — Full-Time | 1-2 years Exp | In-Office | Bangalore",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/sql-%2B-python-at-wissen-technology-4259166199?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.foundit.in/job/sdet-etl-testing-python-sql-anoma-india-34924961?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Foundit"
          },
          {
           "link": "https://www.shine.com/jobs/python-no-sql/revolite-infotech-pvt-ltd/17358425?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/173300637608443904?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          }
         ],
         "Wissen Technology",
         "Wissen Technology is Hiring for SQL With Python\n\nAbout Wissen Technology:\n\nWissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges.\n\nRole Overview:\n\nWe are looking for a skilled and detail-oriented candidate with a strong foundation in SQL, Python, and data processing techniques. The ideal candidate is passionate about transforming raw data into meaningful insights and has hands-on experience across the data pipeline—from data wrangling to visualization.\n\nExperience: 3-7 Years\n\nLocation: Bengaluru\n\nRequired Skills:\n• Strong experience with SQL (e.g., joins, subqueries, CTEs, window functions).\n• Proficiency in Python for data manipulation (e.g., pandas, NumPy).\n• Experience working with relational databases like MySQL, PostgreSQL, SQL Server, or Oracle.\n• Hands-on experience in data wrangling, cleaning, and feature engineering.\n• Understanding of ETL processes and tools.\n• Familiarity with version control systems like Git.\n• Knowledge of data visualization techniques and tools.\n• Strong problem-solving and analytical skills.\n\nThe Wissen Group was founded in the year 2000. Wissen Technology, a part of Wissen Group, was established in the year 2015. Wissen Technology is a specialized technology company that delivers high-end consulting for organizations in the Banking & Finance, Telecom, and Healthcare domains. We help clients build world class products.\n\nWe offer an array of services including Core Business Application Development, Artificial Intelligence & Machine Learning, Big Data & Analytics, Visualization & Business Intelligence, Robotic Process Automation, Cloud Adoption, Mobility, Digital Adoption, Agile & DevOps, Quality Assurance & Test Automation.\n\nOver the years, Wissen Group has successfully delivered $1 billion worth of projects for more than 20 of the Fortune 500 companies. Wissen Technology provides exceptional value in mission critical projects for its clients, through thought leadership, ownership, and assured on-time deliveries that are always ‘first time right’.\n\nThe technology and thought leadership that the company commands in the industry is the direct result of the kind of people Wissen has been able to attract. Wissen is committed to providing them with the best possible opportunities and careers, which extends to providing the best possible experience and value to our clients.\n\nWe have been certified as a Great Place to Work® company for two consecutive years (2020-2022) and voted as the Top 20 AI/ML vendor by CIO Insider. Great Place to Work® Certification is recognized world over by employees and employers alike and is considered the ‘Gold Standard’. Wissen Technology has created a Great Place to Work by excelling in all dimensions - High-Trust, High-Performance Culture, Credibility, Respect, Fairness, Pride and Camaraderie.\n\nWebsite: www.wissen.com\n\nLinkedIn: https://www.linkedin.com/company/wissen-technology\n\nWissen Leadership: https://www.wissen.com/company/leadership-team/\n\nWissen Live: https://www.linkedin.com/company/wissen-technology/posts/feedView=All\n\nWissen Thought Leadership: https://www.wissen.com/articles/\n\nEmployee Speak:\n\nhttps://www.ambitionbox.com/overview/wissen-technology-overview\n\nhttps://www.glassdoor.com/Reviews/Wissen-Infotech-Reviews-E287365.htm\n\nGreat Place to Work:\n\nhttps://www.wissen.com/blog/wissen-is-a-great-place-to-work-says-the-great-place-to-work-institute-india/\n\nhttps://www.linkedin.com/posts/wissen-infotech_wissen-leadership-wissenites-activity-6935459546131763200-xF2k\n\nAbout Wissen Interview Process:\n\nhttps://www.wissen.com/blog/we-work-on-highly-complex-technology-projects-here-is-how-it-changes-whom-we-hire/\n\nLatest in Wissen in CIO Insider:\n\nhttps://www.cioinsiderindia.com/vendor/wissen-technology-setting-new-benchmarks-in-technology-consulting-cid-1064.html",
         {
          "posted_at": "7 hours ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "7 hours ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJTUUwgKyBQeXRob24iLCJjb21wYW55X25hbWUiOiJXaXNzZW4gVGVjaG5vbG9neSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJITW5vZUwxWlF1QWVma0hlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=HMnoeL1ZQuAefkHeAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHsQrCMBAA0L2f0Omgm2Iigot-gCgOFQXHksQjSYl3oXdDu_jt6vLgNZ-mu9-usIZ-0cQEG7iwB0E3hQS_n5hjwfaYVKscrBUpJoo6zcEEflsm9Dzbkb38GSS5CWtxisNuv51Npbhqn1kECR4YEnHhuEAmONMruy_V8I7vgAAAAA&shmds=v1_AdeF8Kg816FNs6a2t4PUxzzK9j5QqwX8eJyEBns2tE281vSbJA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=HMnoeL1ZQuAefkHeAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f3e6590e63b71be3be/images/7ca52d7311167a4bb7fce52e6e3afcd63870bf70216e5e0689db804b95796ad4.jpeg",
         "SQL + Python",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://in.indeed.com/viewjob?jk=506261436cb2c2f5&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://www.shine.com/jobs/backend-python-developer-freelance-only/increscent-software/17280744?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://in.jooble.org/rjdp/-6935223859228097491?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/30083578733789184?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://www.kitjob.in/job/158839731/dv559-python-developer-part-time-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          },
          {
           "link": "https://www.prepintro.com/job/python-developer-part-time-quick-compare/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "PrepIntro"
          }
         ],
         "Variance Technologies Private Limited",
         "Job Opportunity: Python Developer at Variance Technologies Private Limited!\n\nRole: Python Developer\n\nDuration: 1 Months\n\nLocation: Hybrid / Remote\n\nResponsibilities:\n\nCollaborate with our development team to develop and maintain Python-based applications tailored for finance-related tasks\n\nImplement object-oriented programming principles to ensure the scalability and maintainability of codebase\n\nGain hands-on experience with Pandas for data manipulation and analysis within finance contexts\n\nSupport integration of REST and WebSocket APIs for seamless communication with financial data providers and platforms\n\nAssist in automating financial modeling and analysis workflows using Python, including Excel automation for enhanced efficiency\n\nRequirements:\n\nCurrently pursuing or recently completed a degree in Computer Science, Finance, or a related field\n\nBasic proficiency in Python programming language, with a strong willingness to learn and grow\n\nExceptional attention to detail and proactive attitude towards problem-solving\n\nGenuine interest in the intersection of finance and technology\n\nBonus Skills:\n\nFamiliarity with fundamental financial concepts and markets\n\nExposure to Python libraries such as Pandas, NumPy, or SciPy\n\nDemonstrated interest in financial data analysis and visualization techniques\n\nBasic understanding of REST and WebSocket APIs\n\nPerks:\n\nHands-on experience working on real-world projects at the forefront of finance and technology\n\nMentorship and guidance from seasoned professionals in the field\n\nNetworking opportunities with industry experts to expand your professional connections\n\nFlexible scheduling to accommodate academic commitments\n\nPotential for transition to a full-time position based on exceptional performance and availability\n\nReady to kickstart your career with a prestigious 1-month internship at Variance Technologies Private Limited? Apply now by sending your resume and a tailored cover letter expressing your interest and qualifications to careers@variancefintech.com. Join our team and embark on a journey towards shaping the future of finance through technology!\n\nVariance Technologies Private Limited is committed to fostering a diverse and inclusive workplace where all individuals are empowered to thrive. We welcome applicants from all backgrounds and identities to apply.\n\nJob Type: Full-time\n\nPay: From ₹35,000.00 per month\n\nBenefits:\n• Work from home\n\nSchedule:\n• Monday to Friday\n\nEducation:\n• Bachelor's (Preferred)\n\nExperience:\n• Python: 1 year (Preferred)\n• total work: 1 year (Preferred)\n\nWork Location: Remote",
         {
          "posted_at": "5 days ago",
          "schedule_type": "Full-time",
          "work_from_home": "true"
         },
         [
          "5 days ago",
          "Work from home",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIEZ1bGwgdGltZSIsImNvbXBhbnlfbmFtZSI6IlZhcmlhbmNlIFRlY2hub2xvZ2llcyBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiaXZCN3JUQTN5YnkxRzV2MUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Anywhere",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=ivB7rTA3yby1G5v1AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFz7CU43CzYquOgqiuLQQVxLmh7JSZoLubPUv_FTrcubXvVdVNvmo4ETnHDEyBkLnN8xgtKAsIYbdyBoiwswnwuzj7g8BtUsB2NEYu1FrZKrHQ-GE3Y8mRd38qeVYAvmaBXb3X4z1Tn5Vf20hWxyCA90IXFkTyjQFBrnB3caSLEHSnBNPdkfzRn8NKMAAAA&shmds=v1_AdeF8Ki_eVNJcL3hCOFzq-36japCXlxDofyZJcWoB2ZGzGRkKA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=ivB7rTA3yby1G5v1AAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f3e6590e63b71be3be/images/7ca52d7311167a4bbcf9496c8d47b104687ec43b62a624497a9763b0ee63de1f.png",
         "Python Developer Full time",
         "Indeed"
        ],
        [
         [
          {
           "link": "https://www.glassdoor.com/job-listing/python-developer-%E2%80%93-ai-and-llm-integrations-discover-webtech-JV_KO0,42_KE43,59.htm?jl=1009797088926&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://in.trabajo.org/job-3396-b3df6a9d9d368e7d40f9d409ed265e6b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          }
         ],
         "Discover WebTech Private Limited",
         "We are hiring a skilled Python Developer with a minimum of 3 years of experience, who has hands-on expertise in developing AI modules, building intelligent agents using LLMs, and working with cutting-edge frameworks such as LangChain, LangGraph, and LangSmith.\n\nThe ideal candidate should be capable of designing, developing, and deploying backend services as well as intelligent AI-driven tools and systems.\n\nKey Responsibilities\n• Design and implement intelligent agents using LLM-based frameworks like LangChain, LangGraph, and LangSmith.\n• Build backend systems using Python (Django, FastAPI, or Flask).\n• Develop and integrate APIs, third-party tools, and cloud services.\n• Create AI modules that interact with knowledge bases, APIs, and perform multi-step reasoning.\n• Implement prompt engineering, memory chains, and agent behavior logic.\n• Collaborate with cross-functional teams to deliver robust AI features.\n• Optimize code for scalability, performance, and reliability.\n\nRequired Skills and Qualifications\n• 3+ years of hands-on experience with Python.\n• Proficiency in LangChain, LangGraph, or LangSmith.\n• Strong experience in working with LLMs like OpenAI, Cohere, or Anthropic.\n• Deep understanding of prompt engineering and agent orchestration.\n• Experience with APIs, JSON, and external integrations.\n• Knowledge of data storage systems (PostgreSQL, MongoDB).\n• Familiarity with Docker, Git, and CI/CD tools.\n• Excellent problem-solving and debugging skills.\n\nPreferred Qualifications\n• Knowledge of vector databases (e.g., Pinecone, Weaviate, FAISS).\n• Experience with microservices, asynchronous programming, and message queues (Celery, RabbitMQ).\n• Familiarity with frontend technologies (React.js or Vue.js) is a plus.\n• Exposure to cloud platforms such as AWS, GCP, or Azure.\n\nJob Types: Full-time, Permanent\n\nPay: ₹30,000.00 - ₹70,000.00 per month\n\nBenefits:\n• Health insurance\n\nSchedule:\n• Day shift\n\nWork Location: In person",
         {
          "posted_at": "3 days ago",
          "schedule_type": "Full-time"
         },
         [
          "3 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBBSSBcdTAwMjYgTExNIEludGVncmF0aW9ucyIsImNvbXBhbnlfbmFtZSI6IkRpc2NvdmVyIFdlYlRlY2ggUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Im1JUlJqRWJSV0pCSkdQR0VBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=mIRRjEbRWJBJGPGEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOMQrCQBBFsc0RrKYSFExEsNEqEJBIhBSCZdjdDJuVZCfsDCF23sFjeCtP4tr86v33f_JZJHn9lI48FDhhTyMG-L7ekJewgqq6QukFbVDiyDNs4UIaGFUwHcTOmcj2uDx1IiMfs4y5Ty1LpE1qaMjIo6Y5e5DmfzTcqYBjrwSb_WE3p6O3m3Xh2NAUd--obxjFdXBTRKBygxNswfn4onXqB38mJRuuAAAA&shmds=v1_AdeF8KjaSOxt1sFNPUIlTSZP3rzQxsP1gUV27BAuF8V580Z6aw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=mIRRjEbRWJBJGPGEAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f3e6590e63b71be3be/images/7ca52d7311167a4b2428c1372d3bee3b8c018c5c58d9bae8bd00bd66393bc89c.jpeg",
         "Python Developer – AI & LLM Integrations",
         "Glassdoor"
        ],
        [
         [
          {
           "link": "https://careers.hitachi.com/jobs/16246064-full-stack-developer-python-slash-react-js?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hitachi Careers"
          },
          {
           "link": "https://www.shine.com/jobs/full-stack-developer-python-react-js/hitachi-careers/17318841?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://builtin.com/job/full-stack-developer-appian-java/6385724?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://startup.jobs/full-stack-developer-python-react-js-hitachi-vantara-corporation-6843457?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Startup Jobs"
          },
          {
           "link": "https://www.jobleads.com/in/job/full-stack-developer-python-react-js--india--e5f404c1e83327d5061e6410ccf06c525?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "JobLeads"
          }
         ],
         "Hitachi Careers",
         "Our Company\n\nWe're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n\nOur group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n\nImagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n\nThe team\n\nWe are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n\nThe role: Full Stack Developer/Specialist\n\nResponsibilities:\n• Design, develop, and maintain applications.\n• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n• Optimise applications for performance, scalability, and user experience.\n• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n• Provide training and support to end-users and IT staff on functionalities and best practices.\n• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n• Participate in project planning, execution, and post-implementation support.\n• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n• What you'll bring\nQualifications:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n• Expertise in development and customisation.\n• Proficiency in Python and React JS\n• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n• Experience with RESTful APIs and web services.\n• Excellent problem-solving skills and attention to detail.\n• Strong communication and collaboration skills.\n• Ability to work independently and as part of a team in a fast-paced environment.\n• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n• Prior experience in building AI applications is a plus.\n• Prior experience with automation tools like UIPath is a plus.\n• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\nPreferred Skills\n• Certification in\n• Experience with other low-code/no-code platforms.\n• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n• Experience with Agile development methodologies.\nAbout us\n\nWe're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n\n#LI-MS3\n\nChampioning diversity, equity, and inclusion\n\nDiversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n\nHow we look after you\n\nWe help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\nWe're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",
         {
          "posted_at": "14 days ago",
          "schedule_type": "Full-time"
         },
         [
          "14 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=gHUO0z59qIifBLiVAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNPQ7CMAxAYbH2BIjJIyDRICQWGBhA_HRCcIDKDVYTCHEUG1RuwnEpy1u-4RXfQbHZv0KAq6J9wI7eFDhRhvH5o44jGLgQWoXqOoEZVNyAEGbroLcDcxtotHaqSVbGiISyFUX1trT8NByp4c7cuZF_anGYKQVUqhfLeVem2E6HR9-fnYdtb5QFfIRTvHn8AQLWDNibAAAA&shmds=v1_AdeF8KiBbHp5HoJs2LSbE900z5gBevA-S4BK5odgvTSsP5in2w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=gHUO0z59qIifBLiVAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f3e6590e63b71be3be/images/7ca52d7311167a4b2adaf1e3020b5d6aed08e2ec45299500cc6be0568fcaacfb.png",
         "Full Stack Developer (Python / React JS)",
         "Hitachi Careers"
        ],
        [
         [
          {
           "link": "https://jobs.goodyear.com/job/Gachibowli-Hyderabad-Python-Back-End-Developer-TG/1295198900/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Goodyear Jobs"
          },
          {
           "link": "https://in.indeed.com/viewjob?jk=50601a8fed987a6b&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/python-back-end-developer-impactqa-JV_KO0,25_KE26,34.htm?jl=1009707951248&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/python-back-end-developer-at-the-goodyear-tire-rubber-company-4240997333?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://in.bebee.com/job/d459ad4d3fd04946ece63568728abb4d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://www.glassdoor.com/job-listing/python-back-end-developer-impactqa-JV_KO0,25_KE26,34.htm?jl=1009707951248&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.kitjob.in/job/147722131/c345-python-back-end-developer-hyderabad/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "Goodyear",
         "Location: IN - Hyderabad Telangana\n\nGoodyear Talent Acquisition Representative: M Bhavya Sree\n\nSponsorship Available: No\n\nRelocation Assistance Available: No\n\nDuties and Responsibilities:\n\n• Develop and support Data Driven applications\n\n• Help design and develop back-end services and APIs for data-driven applications and simulations.\n\n• Work with our technical partners to collaborate on system requirements and data integration needs for our new applications.\n\n• Support the deployment and scaling of new back-end technologies and cloud-native architectures within the organization.\n\n• Work closely with our data scientists to support model deployment into production environments.\n\n• Develop and maintain server-side components for digital tools and products using Python or other modern back-end technologies and frameworks. Build scalable, secure, and efficient services that support a seamless experience across multiple platforms.\n\n• Design, implement, and maintain robust database systems (SQL and NoSQL), ensuring high availability and performance for critical applications.\n\n• Contribute to DevOps practices including CI/CD pipelines, infrastructure as code, containerization (Docker), and orchestration (Kubernetes).\n\n• Learn about the tire industry and tire manufacturing processes from subject matter experts.\n\n• Be a part of cross-functional teams working together to deliver impactful results.\n\nSkills Required:\n\n• Significant experience in server-side development using Python\n\n• Strong understanding of RESTful API design, microservices architecture, and service-oriented design\n\n• Experience with relational and non-relational databases such as PostgreSQL, MySQL, MongoDB, or DynamoDB\n\n• Application of software design skills and methodologies (algorithms, data structures, design patterns, software architecture and testing)\n\n• Hands-on experience working with cloud platforms such as AWS, Microsoft Azure, or Google Cloud Platform\n\n• Good teamwork skills - ability to work in a team environment and deliver results on time.\n\n• Strong communication skills - capable of conveying information concisely to diverse audiences.\n\n• Exposure to DevOps practices including CI/CD pipelines (e.g., GitHub Actions, Jenkins), containerization (e.g., Docker), and orchestration tools (e.g., Kubernetes)\n\n• Familiarity with front-end technologies like React, HTML, CSS, and JavaScript for API integration purposes\n\nGoodyear is an Equal Employment Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to that individual's race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender identity, age, physical or mental disability, ethnicity, citizenship, or any other characteristic protected by law.\n\nGoodyear is one of the world’s largest tire companies. It employs about 68,000 people and manufactures its products in 53 facilities in 20 countries around the world. Its two Innovation Centers in Akron, Ohio and Colmar-Berg, Luxembourg strive to develop state-of-the-art products and services that set the technology and performance standard for the industry. For more information about Goodyear and its products, go to www.goodyear.com/corporate\n\n#Li-Hybrid",
         {
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJQeXRob24gQmFjay1FbmQgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiR29vZHllYXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoid0VrdmdmMVREWEhlTFhuQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=wEkvgf1TDXHeLXnAAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBCAYVy7uTrdLDQpgotuoohOvkG5JEdSjbmQC9KuPrlx-aefr_uuuuGx1MAJTmhf_SU5ONOHImcq0MOdDQhhsQHacmX2kTbHUGuWg9YiUXmpWCerLL81JzI86ycb-WeUgIVyxErjbj_MKie_XTfELY2EKcEtuQl_JVU7c4UAAAA&shmds=v1_AdeF8KgHgMxjKE_DbEzp4XIf1w6Su6mkNvoSraz2PSMk2_EXfA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=wEkvgf1TDXHeLXnAAAAAAA%3D%3D",
         null,
         "Python Back-End Developer",
         "Goodyear Jobs"
        ],
        [
         [
          {
           "link": "https://in.indeed.com/viewjob?jk=0c13aa90c829c820&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://www.glassdoor.com/job-listing/generative-ai-and-backend-developer-python-intellypod-JV_KO0,42_KE43,53.htm?jl=1009799100097&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          }
         ],
         "Intellypod",
         "Job Description (JD) For Gen Ai with Python:\n\nWe're Hiring: GenAI & Backend Developer (Python)\n\nWork Location: Remote (Work From Home)\n\nExperience: 2+ Years\n\nImmediate Joiners Preferred\n\nCompany: IntellyPod\n\nApply at: hrd@intellypod.com | hr@intellypod.com\n\nAbout the Role:\n\nIntellyPod is looking for a passionate Backend Developer with hands-on experience in GenAI, LLMs, and Python. If you’re excited about building scalable APIs, integrating AI models, and working with the latest in backend and GenAI technologies — we’d love to connect!\n\nKey Responsibilities:\n\n· Develop and maintain Python-based backend services.\n\n· Design and implement RESTful APIs.\n\n· Integrate GenAI/LLM solutions into applications.\n\n· Manage and optimize SQL/NoSQL databases.\n\n· Collaborate with cross-functional tech teams.\n\nMust-Have Skills:\n\n· 2+ years of experience in backend development (Python).\n\n· Experience with GenAI and Large Language Models (e.g., GPT, LLaMA).\n\n· Strong knowledge of REST APIs and database design.\n\n· Familiarity with Git and backend architecture best practices.\n\nNeed to Have:\n\n· Experience with AWS/GCP/Azure.\n\n· Docker, Kubernetes, or CI/CD exposure.\n\n· Familiarity with vector databases (e.g., Pinecone, FAISS).\n\n· Prompt engineering or LLM fine-tuning knowledge.\n\nWhy Join Us?\n\n· 100% Remote – Flexible work setup\n\n· Work on next-gen AI products\n\n· Fast-growing, collaborative tech team\n\n· Opportunity to innovate with emerging AI tools\n\nReady to build the future with us? Send your resume to: hrd@intellypod.com | hr@intellypod.com #GenAI #BackendDeveloper #PythonJobs #LLM #RemoteJob #HiringNow #IntellyPod\n\nJob Type: Full-time\n\nPay: Up to ₹70,000.00 per month\n\nLocation Type:\n• Remote\n\nApplication Question(s):\n• Are an immediate joiner -\n\nAre on notice period if yes [Then how many days]\n• Write YES or NO\n\n1) Need to ask have you worked on LLM based project -\n\n2) Have you worked on chatbot types apps -\n\n3) Have you strong knowleged of OOps and Python basic -\n\n4) Have you knowledge of Rest APi development -\n\nExperience:\n• 5G: 3 years (Required)\n\nWork Location: Remote",
         {
          "posted_at": "5 hours ago",
          "schedule_type": "Full-time",
          "work_from_home": "true"
         },
         [
          "5 hours ago",
          "Work from home",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJHZW5lcmF0aXZlIEFJIFx1MDAyNiBCYWNrZW5kIERldmVsb3BlcihweXRob24pIiwiY29tcGFueV9uYW1lIjoiSW50ZWxseXBvZCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiI4ekp1Sk9ZUFlTbWtOcFRuQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Anywhere",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=8zJuJOYPYSmkNpTnAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFy7ujndJCrYiOBSFxWh1J8oaXo00XgXckdp_8MPVpc3vuKzKC41EmarYUS4NrCGm3UvpB7uOGLkhHmTZvVMW9jDgzsQtNl5YIKaeYi4OnvVJJUxIrEcRH-XKx2_DRN2PJknd_KnFW8zpmgV2-PpMJWJht2yIcUY58Q9BIKG-mC_XT6-kZcAAAA&shmds=v1_AdeF8KhFSQIHvN7Byhuea57DUPzza7X2-vxxKhhpa_88YSEXhg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=8zJuJOYPYSmkNpTnAAAAAA%3D%3D",
         null,
         "Generative AI & Backend Developer(python)",
         "Indeed"
        ],
        [
         [
          {
           "link": "https://jobs.ashbyhq.com/dehazelabs/3fc045c8-c43d-4523-b258-80e35b2930ed/application?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/junior-python-developer-dehazelabs-JV_KO0,23_KE24,34.htm?jl=1009433325415&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://wellfound.com/jobs/3280338-junior-python-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Wellfound"
          },
          {
           "link": "https://builtinpune.in/job/junior-python-developer/6461539?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In Pune"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/junior-python-developer-delhi-at-gradxpert-4256193297?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://diveintopython.org/jobs/junior-python-developer-17232?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Dive Into Python"
          },
          {
           "link": "https://phlex.my.salesforce-sites.com/recruit/fRecruit__ApplyJob?vacancyNo=VN838&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Salesforce"
          },
          {
           "link": "https://builtin.com/job/junior-python-developer/6539360?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          }
         ],
         "Dehazelabs",
         "Location: Onsite, Kokapet, Hyderabad, Telangana.\n\nJob Type: Full-Time\n\nAbout Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n\nResponsibilities:\n• Develop and maintain Python applications and services\n• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n• Write clean, efficient, and well-documented code\n• Troubleshoot, debug, and optimize applications for performance and scalability\n• Participate in code reviews and maintain coding standards\n• Stay up-to-date with the latest industry trends and technologies\n\nRequirements:\n• Ability to code in Python and SQL\n• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n• Knowledge of version control systems (e.g., Git)\n• Excellent problem-solving skills and attention to detail\n• Strong communication and teamwork abilities.\n\nPreferred Qualifications:\n• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n• Knowledge of RESTful APIs and microservices architecture.",
         {
          "qualifications": "No degree mentioned",
          "salary": "₹216K–₹420K a year",
          "schedule_type": "Full-time",
          "work_from_home": "true"
         },
         [
          "₹216K–₹420K a year",
          "Work from home",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Anywhere",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=LYbcWfU4skOE2uhgAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQoCMQwAUFxvdXPKLNiK4KLrgXiTf3CkNVwrNSlNlNPRL1eXt7zus-jc8OAsDS4vS8LQ05OKVGqwgUECKGGLCX5zEpkKrY7JrOrBe9XiJjW0HF2UuxemILO_SdA_oyZsVAsajbv9dnaVp_Wyp4RvKhgUMsOZrxm_tV-eZYUAAAA&shmds=v1_AdeF8KiLjAS30ie4ZRkW1kEru4d1GwgKEP1yzRfISnv7NJRViQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=LYbcWfU4skOE2uhgAAAAAA%3D%3D",
         null,
         "Junior Python Developer",
         "Jobs"
        ],
        [
         [
          {
           "link": "https://www.recruit.net/job/etl-developer-jobs/1EFDB05A4E7926F2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Recruit.net"
          }
         ],
         "Vivid Resourcing",
         "Job Title:\nData Engineer / ETL Developer\n\nLocation:\nUS, remote from India\n\nDepartment:\nIT / Data & Analytics\n\nReports To:\nHead of Data & Analytics\n\nEmployment Type:\nContract, 12 months\n\nPay:\n$28-35 per hour payrolled or self-employed\n\nAbout the Role\n\nWe are seeking a Data Engineer / ETL Developer to help design, build, and maintain data pipelines that support our internal reporting and analytics needs. This role will focus on integrating data from systems such as ERP, MES , and production databases into a centralized data platform to enable reliable and timely insights across the business. You will work alongside BI developers, IT staff, and business users to ensure the smooth flow of data and contribute to the foundation of a modern business intelligence environment.\n\nKey Responsibilities\n\nETL & Data Pipeline Development\n• Develop and maintain ETL/ELT pipelines to move data from source systems (ERP, MES, SQL databases, flat files) into our centralized platform.\n• Use tools such as Azure Data Factory , SSIS , or similar to orchestrate and automate data workflows.\n\nData Modeling & Integration\n• Assist in designing and building data models and data marts optimized for Power BI dashboards.\n• Support the creation and maintenance of dataflows and datasets in Power BI Service.\n\nData Quality & Documentation\n• Implement data validation, transformation, and cleansing logic to ensure high-quality, reliable data.\n• Document data processes, business rules, and data mappings to support knowledge sharing and governance.\n\nCross-Functional Collaboration\n• Work with internal teams (production, quality, operations, finance) to understand data requirements and reporting needs.\n• Collaborate with BI developers to ensure the data pipeline supports efficient and user-friendly reporting.\n\nRequired Qualifications\n• Bachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\n• 2+ years of experience in data engineering or ETL development roles.\n• Solid experience with SQL and ETL tools (e.G., Azure Data Factory , SSIS , Informatica , etc.).\n• Familiarity with Power BI , including working with datasets, dataflows, or basic DAX.\n• Understanding of data integration, transformation, and warehousing concepts.\n• Experience working with or integrating manufacturing systems (ERP, MES) is a strong plus.\n\nPreferred Skills\n• Experience with cloud data platforms (especially Microsoft Azure ).\n• Exposure to Python or scripting for automation.\n• Familiarity with data governance and documentation practices.\n• Experience with manufacturing environments or industrial data is beneficial.\n\nSoft Skills\n• Strong attention to detail and a logical, structured approach to problem-solving.\n• Willingness to learn and grow in a fast-paced environment.\n• Good communication and collaboration skills across technical and non-technical teams.\n• Proactive and solutions-oriented mindset.",
         {
          "posted_at": "14 hours ago",
          "schedule_type": "Contractor"
         },
         [
          "14 hours ago",
          "Contractor"
         ],
         "eyJqb2JfdGl0bGUiOiJFdGwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiVml2aWQgUmVzb3VyY2luZyIsImFkZHJlc3NfY2l0eSI6IkJpbGFzcHVyLCBDaGhhdHRpc2dhcmgsIEluZGlhIiwiaHRpZG9jaWQiOiJueDl3cW1oMXpfcnBSX2dFQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Bilaspur, Chhattisgarh, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=nx9wqmh1z_rpR_gEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOw7CMAwAULH2CEwemBA0gMQCGx8hGBlYK7e1EqMQR7Fb9SScF_GGV31n1eJqES40UpRMBdbwkBaUsHQBJMFNxEeaH4NZ1oNzqrH2amjc1Z18nCRqZXJvafVfowEL5YhGzW6_meqc_HL74pF7eJLKUDpOHjjBiSNqHsoKziGgGavHElZwTz3jD2cHAICZAAAA&shmds=v1_AdeF8KioJG0pd9n2Q01rAjESLIj0HIja7A9i_QyV6EIckPzASg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=nx9wqmh1z_rpR_gEAAAAAA%3D%3D",
         null,
         "Etl Developer",
         "Recruit.net"
        ],
        [
         [
          {
           "link": "https://careers.spglobal.com/jobs/316835?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "S&P Global"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/senior-etl-and-backend-developer-salesforce-sp-global-JV_IC2935226_KO0,43_KE44,53.htm?jl=1009760759089&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.simplyhired.co.in/job/EDeu4j5BnY0WMfXpapbv1XDfoizam8v2C0MIvUs-eAQ_RxwZzld-Sw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://spglobalcareers.dejobs.org/ahmedabad-ind/senior-etl-and-backend-developer/A24C5AF22D7C4078A2550A706263A7F5/job/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/senior-etl-and-backend-developer-salesforce-%C2%A0-at-s-p-global-4238481789?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://in.expertini.com/jobs/job/senior-etl-and-backend-developer-ahmedabad-sp-global-spglobal-316835ahmedabadindia/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Expertini"
          },
          {
           "link": "https://in.trabajo.org/job-3150-0be8b3a1a781f9d4e53e1881178412e0?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          },
          {
           "link": "https://www.talentify.io/job/senior-software-developer-ahmedabad-gujarat-in-sp-global-316835?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Talentify"
          }
         ],
         "S&P Global",
         "About the Role:\n\nGrade Level (for internal use):\n10\n\nTitle: Senior ETL and Backend Developer (Salesforce)\n\nJob Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n\nThe Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n\nThe team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n\nThe Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n\nResponsibilities:\n• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\nMonitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\nQualifications:\n\nBasic Qualifications:\n• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n• A minimum of 8+ years of experience in software engineering & Architecture.\n• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n• A minimum of 3+ years of Salesforce development, administration/Integration.\n• Proficiency in Informatica PowerCenter and other ETL tools.\n• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n• Experience with Salesforce integration and administration.\n• Proficiency in backend development languages (e.g., Java, Python, C#).\n• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n• Excellent problem-solving skills and attention to detail.\n• Ability to work independently and as part of a team.\n• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n\nPreferred Qualifications:\n• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n• Experience with other ETL tools and data integration platforms.\n• Informatica Certified Professional\nSalesforce Certified Administrator or Developer\n• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n• Excellent problem solving, analytical and technical troubleshooting skills.\n• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat’s In It For You?\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nWe're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n\nOur benefits include:\n• Health & Wellness: Health care coverage designed for the mind and body.\n• Flexible Downtime: Generous time off helps keep you energized for your time on.\n• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring and Opportunity at S&P Global:\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\n-----------------------------------------------------------\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n-----------------------------------------------------------\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n\nJob ID: 316835\nPosted On: 2025-06-03\nLocation: Hyderabad, Telangana, India",
         {
          "posted_at": "30 days ago",
          "schedule_type": "Full-time"
         },
         [
          "30 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjJlUWpKemNOclM4MXR6QzhBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India (+1 other)",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=2eQjJzcNrS81tzC8AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXFsQrCMBAAUFz7CU43iZXaiuCim1Sq4iC0e7kkZ1uNdyEJUn_Kb1SX95LPJClr4kE8HJoLIBvYo37Q75JeZMWRh3mNlsJNvKYUlnAWBYHQ6x6EoRLpLE13fYwubIsiBJt3IWIcdK7lWQiTkrG4iwp_2tCjJ2cxUrverMbccbdI69kVKisKLQwMx7chjwpNBg1Z5A4ZMzixGfALm-qj5LEAAAA&shmds=v1_AdeF8KhDGHxAXJEjotU-fQxkvoBBqCk4nXlFRQEfCz-nQy1hiw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=2eQjJzcNrS81tzC8AAAAAA%3D%3D",
         null,
         "Senior ETL and Backend Developer (Salesforce)",
         "S&P Global"
        ],
        [
         [
          {
           "link": "https://www.recruit.net/job/senior-etl-developer-jobs/644A9040253E845D?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Recruit.net"
          }
         ],
         "Vivid Resourcing",
         "Job Title:\nSenior Data Engineer / ETL Developer\n\nLocation:\nUS, from India\n\nDepartment:\nIT / Data & Analytics\n\nReports To:\nDirector of Data & Analytics\n\nEmployment Type:\nContract, 12 months\n\nPay:\n$30-38 per hour payrolled or self-employed\n\nAbout the Role\n\nWe are seeking a skilled and motivated Senior Data Engineer / ETL Developer to lead the design and implementation of end-to-end data integration solutions powering this new platform. You will work closely with IT, business stakeholders, and BI developers to build scalable data pipelines, robust data models, and a high-performance reporting environment centered around Power BI.\n\nThis role is a key contributor in shaping our data infrastructure and delivering timely, accurate insights across the organization—from the shop floor to the boardroom.\n\nKey Responsibilities\n\nData Engineering & Integration\n• Design, build, and maintain scalable ETL/ELT pipelines to extract, transform, and load data from diverse manufacturing systems (ERP, MES, IoT sensors, SQL databases, flat files, APIs).\n• Develop and manage data workflows using tools such as Azure Data Factory, SQL Server Integration Services (SSIS), or other modern data orchestration platforms.\n\nPlatform & Architecture Support\n• Contribute to the design and architecture of the new internal analytics platform, ensuring flexibility, scalability, and cost-efficiency.\n• Design and optimize data lake and data warehouse solutions using cloud-native tools (preferably Microsoft Azure).\n\nPower BI Enablement\n• Build and optimize semantic data models, dataflows, and datasets for use in Power BI.\n• Collaborate with Power BI developers and business analysts to ensure data models meet reporting requirements, including DAX performance tuning and efficient data structure design.\n\nData Governance & Quality\n• Implement data validation, cleansing, and monitoring processes to ensure data quality, reliability, and accuracy.\n• Contribute to data governance policies, including documentation, lineage tracking, and security controls.\n\nCollaboration & Mentorship\n• Work closely with cross-functional teams (IT, production, quality, finance, supply chain) to gather data requirements and deliver meaningful solutions.\n• Provide technical leadership and mentor junior developers or analysts in best practices for data engineering and analytics.\n\nRequired Qualifications\n• Bachelor’s or Master’s degree in Computer Science, Information Systems, Engineering, or a related field.\n• 5+ years of experience in data engineering, ETL development, or database architecture roles.\n• Proven experience with ETL tools such as Azure Data Factory, SSIS, or Informatica.\n• Advanced SQL skills, including performance tuning, stored procedures, and query optimization.\n• Strong experience with Power BI, including DAX, data modeling, and dataset optimization.\n• Experience with data warehousing (e.G., Azure Synapse, SQL Server, Snowflake) and cloud platforms (preferably Azure).\n• Understanding of manufacturing systems and data (e.G., ERP, MES, shop floor data, SCADA, historians).\n\nPreferred Skills\n• Experience with Python or other scripting languages for automation and data manipulation.\n• Familiarity with time-series data and integration from IoT or edge devices.\n• Knowledge of DevOps practices, CI/CD for data pipelines, and version control using Git.\n• Exposure to data governance frameworks and tools like Purview, Alation, or Collibra.\n• Power BI Service administration experience and integration with Power Platform (e.G., Power Automate, Power Apps)\n\nKey Competencies\n• Strong analytical and problem-solving skills with attention to detail.\n• Excellent communication skills, with the ability to bridge technical and business domains.\n• Self-starter with the ability to lead initiatives and work independently or collaboratively.\n• A passion for continuous improvement and innovation in a manufacturing setting.",
         {
          "posted_at": "14 hours ago",
          "schedule_type": "Contractor"
         },
         [
          "14 hours ago",
          "Contractor"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRXRsIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlZpdmlkIFJlc291cmNpbmciLCJhZGRyZXNzX2NpdHkiOiJCaWxhc3B1ciwgQ2hoYXR0aXNnYXJoLCBJbmRpYSIsImh0aWRvY2lkIjoiMnQxYXo2bnUzRU1abUlLLUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Bilaspur, Chhattisgarh, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=2t1az6nu3EMZmIK-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFz7CE43a21UcNHNH0RHBdeSpkdyEnMhdy19Gd9V_Iav-s6q5RMTcYGLRjjjiJEzFljBnTsQtMUF4ARXZh9xfgiqWfbGiMTGi1ol1zj-GE7Y8WTe3Mm_VoItmKNVbLe79dTk5BebF43UwwOFh-IoeaAER4pW8lBqOIVgVUm8LaGGW-rJ_gAo5AAooAAAAA&shmds=v1_AdeF8KgmUPxQQ3or2dZeuo-n_D7oSa5c3VpT-ZiKwta1oTJcNA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=2t1az6nu3EMZmIK-AAAAAA%3D%3D",
         null,
         "Senior Etl Developer",
         "Recruit.net"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/etl-developer-%E2%80%93-ibm-datastage-at-tata-consultancy-services-4256166032?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://in.bebee.com/job/4a2af5f029af9a8d94f2bdca1fd2d9e0?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://in.trabajo.org/job-169-44aa45285a4aa3fccf6f03ea6fc9cf38?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          },
          {
           "link": "https://jobs.jobg8.com/jobs/etl-developer-ibm-datastage-in-hyderabad-andhra-pradesh-india/5673-2972096984D8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobg8"
          },
          {
           "link": "https://en-in.whatjobs.com/jobs/software-developer?id=156858651&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "WhatJobs"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/1046235732449951744?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://www.recruit.net/job/etl-developer-ibm-datastage-jobs/C9D3D0220324FB4B?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Recruit.net"
          },
          {
           "link": "https://www.learn4good.com/jobs/hyderabad/india/info_technology/4340112163/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Learn4Good"
          }
         ],
         "Tata Consultancy Services",
         "Job Title: ETL Developer – IBM DataStage\n\nExperience: 5 to 10 years\n\nLocation: Hyderabad, Chennai, Mumbai, Bangalore, Ahmedabad, Indore\n\nEmployment Type: Full-time\n\nJob Summary:\n\nWe are seeking a skilled ETL Developer with hands-on experience in IBM DataStage to join our dynamic data engineering team. The ideal candidate will have a strong background in ETL development, data warehousing concepts, and performance optimization. This role involves designing and implementing robust ETL solutions, collaborating with cross-functional teams, and ensuring high data quality and integrity.\n\nKey Responsibilities:\n• Design, develop, and implement ETL processes using IBM DataStage.\n• Work across the full project lifecycle including low-level design, development, testing, and deployment.\n• Develop parallel jobs using various DataStage stages such as Copy, Join, Merge, Lookup, Funnel, Filter, Sort, Remove Duplicates, Aggregator, Change Capture, and Transformer.\n• Integrate data from heterogeneous sources including Oracle, SQL Server, and flat files.\n• Optimize and troubleshoot existing ETL processes for performance improvements.\n• Collaborate with data analysts and architects to understand business requirements and translate them into technical solutions.\n• Ensure data quality and integrity across multiple data sources.\n• Create and maintain technical documentation for ETL processes.\n• Participate in code reviews and adhere to ETL best practices.\n• Work in Agile environments and use tools like JIRA for tracking tasks and issues.\n• Demonstrate strong problem-solving skills and the ability to troubleshoot technical issues effectively.\n• Understand and support operational requirements as part of business delivery.\n\nRequired Skills:\n• Strong experience with IBM DataStage for ETL development and migration.\n• Solid understanding of database and data warehousing concepts.\n• Proficiency in SQL and UNIX.\n• Experience working with large datasets and complex data transformations.\n• Familiarity with Agile methodologies and tools like JIRA.\n• Excellent communication and collaboration skills.",
         {
          "posted_at": "7 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "7 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIOKAkyBJQk0gRGF0YVN0YWdlIiwiY29tcGFueV9uYW1lIjoiVGF0YSBDb25zdWx0YW5jeSBTZXJ2aWNlcyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoibkdQWmp0dFNpdlVKUWVEYkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Hyderabad, Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=nGPZjttSivUJQeDbAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMOw6CQBAA0NhyBKupDYIx0Rjt_EQxWklPhmWyYNYZsrMa6LyDB_FOnkRtXvmi9yCa7_ITbOlBTlry8Hm-IFufYYsBLwEtwRiOUoISelODMOxFrKPhqg6h1WWaqrrEasDQmMTILRWmUrr0KqX-KbRGT63DQMV0NumSlu1okf962Ajr3QVk08OF_KMxpNAwHPqKPJZYxZCTQ7bIGEPGVYNfx2UgOrIAAAA&shmds=v1_AdeF8KhYUGEHFHlN-1EsA9uwB1Zzbk12mKgso4NzzISq5LIJHw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=nGPZjttSivUJQeDbAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f6b9fc5f72634052b6/images/b0a1e06c4ca31a9abdb7ee0d6152dbca2ffa1facb3dd8dd0abf7d1cc766aba59.jpeg",
         "ETL Developer – IBM DataStage",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://insightglobal.com/jobs/find_a_job/job-411040?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Insight Global"
          },
          {
           "link": "https://firstdcs.in/jobs/etl-developer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "FirstDCS"
          },
          {
           "link": "https://jobs.smartrecruiters.com/SQUIRCLEITCONSULTINGSERVICESPVTLTD/87728117-etl-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SmartRecruiters Job Search"
          },
          {
           "link": "https://www.hirist.tech/j/etl-developer-1472601?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/etl-developer-cgi-JV_IC2865319_KO0,13_KE14,17.htm?jl=1009777977370&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.simplyhired.co.in/job/jTrE3sYd8ESsw_aMfgsEoiKUf2JLjAdYHf4v4N1Qrjh_vf6-R8mNrw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://www.iitjobs.com/job/etl-developer-hyderabad-telangana-india-talent-join-77101?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Iitjobs"
          },
          {
           "link": "https://unisys.wd5.myworkdayjobs.com/zh-CN/External/job/Hyderabad-Waverock/ETL-Developer_REQ565515?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          }
         ],
         "Insight Global",
         "We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\nResponsibilities:\nLead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\nCollaborate with developers to ensure a smooth transition and integration of the upgraded system.\nEnhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\nProvide support and guidance to the ETL development lead.\nEngage with end-users to ensure their needs are met during and after the upgrade process.\nUtilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\nRequirements:\nMinimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\nProven experience with SQL Server upgrades, particularly from 2012 to 2022.\nStrong SQL Server skills, including building tables, stored procedures, views, and functions.\nExperience with .NET development is highly desirable to be leveraged for other projects\nExcellent problem-solving skills and attention to detail.\nAbility to work effectively in a team environment and communicate with end-users.\nPreferred Qualifications:\nExperience with SQL Server 2016.\nExperience with Oracle\nFamiliarity with the latest features and enhancements in SQL Server 2022.\nStrong understanding of database architecture and migration strategies.\n\nWe are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         {
          "qualifications": "No degree mentioned",
          "schedule_type": "Contractor"
         },
         [
          "Contractor",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ikw2UEZlVW5YOU90VExsNnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=L6PFeUnX9OtTLl6yAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU43OEnbiOCiq1Irjt3LpT2SlHgXckHqb_jF4hte9d1Uu9vwhCu9KUqiDA08xIIS5smDMHQiLtL24ktJejZGNbZOC5YwtZO8jDBZWc0iVv-N6jFTilhoPJ4Oa5vY7ZueNThfoItiMUJguH9mymhxrmGgiOyQsYae54A__sKURpUAAAA&shmds=v1_AdeF8KiTpOWlgVKEBN36SqjGjguWk7ZGmxLJNUkotMGGo1lIug&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=L6PFeUnX9OtTLl6yAAAAAA%3D%3D",
         null,
         "ETL Developer",
         "Insight Global"
        ],
        [
         [
          {
           "link": "https://www.glassdoor.com/job-listing/etl-developer-sonata-software-JV_IC2865319_KO0,13_KE14,29.htm?jl=1009683443468&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.simplyhired.co.in/job/VZteELnCIFig_PpewT1H4Xi550ZVJ7LxPCKfGowAKkMlN6qBv9WgZQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          }
         ],
         "sonataOne",
         "Real-time data Ingestion, Streaming data, Kafka, AWS Cloud streaming tools, ETL, Semi-structured data formats like JSON, XML\n\nTools: Talend, Kafka, AWS Event Bridge, Lamda and and Strong SQL & Python",
         {
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoic29uYXRhT25lIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=cEmBeIcvfhfX5HK2AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFz7CE43OIjURAQXXRV_EFy6l0t7JJF4F3JB6kP4zuI3fM131ixO3R2O9KYkmQqs4SYOlLAMAYThLOITzQ-h1qx7a1WT8VqxxsEM8rLC5GSyT3H6r9eAhXLCSv12t5lMZr9aqjBWfDBBZLh8RirocGyho4TskbGFK48Rf42DLLCQAAAA&shmds=v1_AdeF8KjYvob7TkJsNO7njAvFnKCDIIe6qNw9Ivsd5EiD_jwNFg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=cEmBeIcvfhfX5HK2AAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f6b9fc5f72634052b6/images/b0a1e06c4ca31a9ab5436031f86f8495670d9227c5f49d66c2ea8c68acaec26e.png",
         "ETL Developer",
         "Glassdoor"
        ],
        [
         [
          {
           "link": "https://www.hirist.tech/j/epam-etl-developer-ssis-ssrs-1495428?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://in.jooble.org/jdp/777603120491457089?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://in.bebee.com/job/df2976441b528bff274847b68dd6d964?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://in.expertini.com/jobs/job/epam-etl-developer-ssisssrs-hyderabad-epam-systems-india-private-limited-315-46314721/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Expertini"
          },
          {
           "link": "https://www.jobted.in/job/49ed391f8cb580e15d52ff7e58ad3743?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobted"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/5315199303575142400?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          }
         ],
         "EPAM Systems India Private Limited",
         "Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n\nExperience : 5+ Years\n\nLocation : Hyderabad\n\nJob Description :\n\nWe are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n\nYou will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n\nKey Responsibilities :\n\n- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n\n- Develop, deploy, and maintain SSIS packages for ETL processes.\n\n- Work on data warehouse design, development, and maintenance.\n\n- Ensure high performance and reliability of data integration workflows.\n\n- Troubleshoot SQL queries and ETL issues; optimize database performance.\n\n- Collaborate with cross-functional teams to define data solutions.\n\n- Work with complex data structures including JSON and XML.\n\n- Understand and implement different data models (star, snowflake, etc.).",
         {
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IkVQQU0gU3lzdGVtcyBJbmRpYSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InZ6RDdVNWpsc2hzYS03eW5BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=vzD7U5jlshsa-7ynAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNQQrCMBBFcdsjuJq11EYEEXQlKFpREOO-TNshjaSZkAmiR_N2Vt183oMHP3uPsuXusjnDFHa3E2zpQY4DxcG1LrXS-qoHPnINQhibDtjDntk4Gq-7lIKslBJxhZGEyTZFw71iTzU_1Z1r-U4lHUYKDhNV88XsWQRvJpvfq35Jol6g9K1FuET7GCI42d4masF6OLxailhjm8ONHHqDHvN__gHRy8qtvAAAAA&shmds=v1_AdeF8Khi9AUFim2i7dQRHCiMZmDc-jtbDVoLgUBJzi2wIeCrhA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=vzD7U5jlshsa-7ynAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f6b9fc5f72634052b6/images/b0a1e06c4ca31a9abbdaf665a0013fc4549112c9395a972e123709ddc360f0b1.png",
         "EPAM - ETL Developer - SSIS/SSRS",
         "Hirist"
        ],
        [
         [
          {
           "link": "https://builtin.com/job/etl-developer-hands-microsoft-sql-ssis-etl-and-t-sql-pune-location/2942610?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          }
         ],
         "Fiserv",
         "Calling all innovators – find your future at Fiserv.\n\nWe’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n\nJob Title\n\nETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n\nWork Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n• Participates in client kickoff activities including requirements and data gathering sessions.\n• Ability to analyze and migrate incoming external data into Fiserv solutions.\n• Strong working knowledge of ETL processes and best practices.\n• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n• Documents business processes and identifies opportunities for process redesign.\n• Reports system defects and identifies opportunities for system enhancements.\n• Performs as a technical consulting resource for new clients during the implementation process.\n• Supports several project managers during the pre and post go-live activities\n• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n• Provides technical and analytical guidance to the project team.\n• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n• The position requires working in shifts, 2 PM to 11 PM IST\n\nWhat you will need to have:\n• Minimum of 5 – 10 years of experience in software development.\n• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n• Self-starter who can work effectively, both independently and in a team environment.\n• Strong analytical, organizational, and problem-solving skills.\n• Excellent verbal and written communication skills.\n• Ability to maintain a professional attitude and demeanor in high pressure situations.\n• Ability to multi-task and manage multiple projects simultaneously.\n• Weekend/evening availability and support (10% - 25%).\n\nWhat would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n\nThank you for considering employment with Fiserv. Please:\n• Apply using your legal name\n• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n\nWhat you should know about us:\n\nFiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n\nOur commitment to Diversity and Inclusion:\n\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n\nWarning about fake job posts:\n\nPlease be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n\nAny communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n\nIf you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=j_liFo3VQHqrD7_BAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMuwrCQBQEUGzT2FtNqZKHCDba-opEUGIvm801WYl7w941-GF-oGszxXBmou8oqne3AlsaqOOeHKZHZWsBW5yNdiz88CivRYyyzMsYAccIArcktJghweVtCQVr5U1YJThxBSHldPt_OTA3HU02rfe9rLNMpEsb8QHrVPMrY0sVf7InV_KPu7TKUd8pT_flavFJe9vMx3sj5AYYi9zWRv0AHMymQbcAAAA&shmds=v1_AdeF8KhckDUoX9CilVCOaJqU36weofSX83Pgps7flcqZRoh66g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=j_liFo3VQHqrD7_BAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f6b9fc5f72634052b6/images/b0a1e06c4ca31a9a99b750505837edd957d2221342d6b3cee8ec7a41d0cbd4c8.png",
         "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location",
         "Built In"
        ],
        [
         [
          {
           "link": "https://talent500.com/jobs/hsbc/etl-developersenior-consultant-specialist-india-T500_HB_267031/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Talent500"
          },
          {
           "link": "https://www.kitjob.in/job/148979890/fvy-143-etl-workflow-specialist-mumbai/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "HSBC",
         "Some careers shine brighter than others.If you're looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.HSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.We are currently seeking an experienced professional to join our team in the role of Senior Consultant Specialist.In this role, you will:- Communicating effectively with senior stakeholders.- Work with a team of technologists, ensuring prioritization of tasks and supporting removal of blockers.- Working on burning the product backlog and oversee product demo's- Understanding complex business requirements and new technologies and understand how to influence teams to adopt new practice and embrace automation.- Managing of risks and issues\n\nRequirements\n• name : HSBC\n• location : India, IN",
         {
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyL1NlbmlvciBDb25zdWx0YW50IFNwZWNpYWxpc3QiLCJjb21wYW55X25hbWUiOiJIU0JDIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRIYmhtVUVOWmF3OGh3Q0lBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=THbhmUENZaw8hwCIAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFwLvoDTzYKJCC46aRV_cKt7ucajjcS7kDulb-Eri9_wVd9JtTveb3CgDyXJVHxDHKVALazvZMgGTaYQMUU1WMBVOlDCEgYQhpNIn2i2HcyybrxXTa5XQ4vBBXl5Yepk9E_p9F-rAxbKCY3a1Xo5usz9fHpu9jVEhgs_Iv4A6wTUP5IAAAA&shmds=v1_AdeF8KgnDfRiHj39w5Z-ucwcYFT3-FJ0Z_lyEKOroyTU0w9PgA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=THbhmUENZaw8hwCIAAAAAA%3D%3D",
         null,
         "ETL Developer/Senior Consultant Specialist",
         "Talent500"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/senior-informatica-developer-at-everestdx-inc-4258304922?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          }
         ],
         "EverestDX Inc",
         "About the Company:\n\nEverest DX – We are a Digital Platform Services company, headquartered in Stamford. Our Platform/Solution includes Orchestration, Intelligent operations with BOTs’, AI-powered analytics for Enterprise IT. Our vision is to enable Digital Transformation for enterprises to deliver seamless customer experience, business efficiency and actionable insights through an integrated set of futuristic digital technologies.\n\nDigital Transformation Services - Specialized in Design, Build, Develop, Integrate, and Manage cloud solutions and modernize Data centers, build a Cloud-native application and migrate existing applications into secure, multi-cloud environments to support digital transformation. Our Digital\n\nPlatform Services enable organizations to reduce IT resource requirements and improve productivity, in addition to lowering costs and speeding digital transformation.\n\nDigital Platform - Cloud Intelligent Management (CiM) - An Autonomous Hybrid Cloud Management Platform that works across multi-cloud environments. helps enterprise Digital Transformation get most out of the cloud strategy while reducing Cost, Risk and Speed.\n\nTo know more please visit: http://www.everestdx.com\n\nResponsibilities:\n• Candidate should hands-on experience on ETL and SQL.\n• Design, develop, and optimize ETL workflows using Informatica PowerCenter.\n• Implement cloud-based ETL solutions using Informatica IDMC and IICS.\n• Should have expertise on all transformations in Power Center and IDMC/IICS.\n• Should have experience or knowledge on the PC to IICS migration using CDI PC tool or some other tool.\n• Lead data migration projects, transitioning data from on-premise to cloud environments.\n• Write complex SQL queries and perform data validation and transformation.\n• Conduct detailed data analysis to ensure accuracy and integrity of migrated data.\n• Troubleshoot and optimize ETL processes for performance and error handling.\n• Collaborate with cross-functional teams to gather requirements and design solutions.\n• Create and maintain documentation for ETL processes and system configurations.\n• Implement industry best practices for data integration and performance tuning.\n\nRequired Skills:\n• Hands-on experience with Informatica Power Center, IDMC and IICS.\n• Strong expertise in writing complex SQL queries and database management.\n• Experience in data migration projects (on-premise to cloud).\n• Strong data analysis skills for large datasets and ensuring accuracy.\n• Solid understanding of ETL design & development concepts.\n• Familiarity with cloud platforms (AWS, Azure).\n• Experience with version control tools (e.g., Git) and deployment processes.\n\nPreferred Skills:\n• Experience with data lakes, data warehousing, or big data platforms.\n• Familiarity with Agile methodologies.\n• Knowledge of other ETL tools",
         {
          "posted_at": "2 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "2 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgSW5mb3JtYXRpY2EgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiRXZlcmVzdERYIEluYyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNDdnQ200aEh5aHpSWlhDOEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Hyderabad, Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=47gCm4hHyhzRZXC8AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNuw7CMAwAxdpPYPKMSoNALLAW8VhhYKuc1qRBqR3FUVX-hk-lLLecTld8F8XuTuwlwZVfkgbMvkWoaaQgkRKs4SYWlDC1PQjDWcQFWh77nKMejFENldP8z6pWBiNMVibzFqt_NNpjohgwU7Pdb6YqsluVp5ESaa6f87QFz3D5dJTQYlfCgwKyQ8Zylp3HH2BwtZmjAAAA&shmds=v1_AdeF8Kh2tiS2TqlfE8K6mB5AqUFgHR-MKCtcsj-IXlL7YHB-6g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=47gCm4hHyhzRZXC8AAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f6b9fc5f72634052b6/images/b0a1e06c4ca31a9aca07e30835060ab20487250180fd7e64e147a75b89f26c9d.jpeg",
         "Senior Informatica Developer",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://www.iitjobs.com/job/spark-engineer-usa-staffingine-llc-78519?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Iitjobs"
          }
         ],
         "Staffingine LLC",
         "- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred",
         {
          "posted_at": "4 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "4 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3NsQrCMBAA0L1fIE63uAgmIrjoKCJKt35AucRrkhrvQi5DJ79ddHnr6z7dZihYX3DlkJiowg4e4kAJq48gDDeRkGl9jq0VPVmrmk3Qhi154-VthcnJYmdx-mPUiJVKxkbj4bhfTOGwXQ0Npyn9C-j7CySGOz8TfgHTIbRdgAAAAA&shmds=v1_AdeF8KjUl41s9rD9xNd7RywARJoJQ3Nel1qruGSZXxbjxJAYWA&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f765f7018a3bebbd7b/images/51c80805102136f7e4a0d0dc158f67fd721978ef95aab8a775788a042a09089d.jpeg",
         "Spark Engineer",
         "Iitjobs"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/data-engineer-spark-python-at-etelligens-technologies-4230192247?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.shine.com/jobs/data-engineer-spark-python/etelligens-technologies/17263944?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://www.kitjob.in/job/149564571/vq345-data-engineer-spark-python-pune/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          },
          {
           "link": "https://in.findjob24h.com/it/big-data-engineer-sparkpython-job26347?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Findjob24h.com"
          }
         ],
         "Etelligens Technologies",
         "Job Description\n\nWe are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n\nKey Responsibilities\n• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n• Perform data cleansing and migration from diverse sources to target systems.\n• Collaborate with stakeholders to understand requirements and convert them into technical\n\nsolutions.\n• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n• Support, troubleshoot, and maintain data pipelines and workflows.\n• Participate in all phases of software development lifecycle including unit testing, integration\n\ntesting, and performance testing.\n• Contribute to the modernization of data platforms and analytics tools on Azure.\n• Ensure data quality and integrity across all pipelines and systems.\n\nRequired Skills & Qualifications\n• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n• 1+ year of experience in Python and Spark scripting.\n• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n• Good understanding of Data Warehousing (DWH) concepts.\n• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n• Hands-on experience in data cleansing, transformation, and migration projects.\n• Ability to work independently and within a team environment.\n• Microsoft Certified : Azure Data Engineer Associate\n• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n\nLocation : Artha SEZ, Greater Noida West\n\n(ref:hirist.tech)",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=Dc6DSNLMiNYCEl95AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHsQrCMBAAUFz7CU43CzaiuOhqEZ0E3cs1HklqvAu5G-p_-MHW5cFrvotmd0JD6DgkJqqwhnvB-nK3j0XhuVcZQAmrjzD_LBIyLY_RrOjBOdXcBjW05FsvbydMg0xulEH_9BqxUslo1G_3m6ktHFbQGeWcArHCg3xkyRISKSSGCz8T_gDku5DQlwAAAA&shmds=v1_AdeF8KhUS3kDcBE04kKw4L-SXFj_kSbNYqcLjnZpQzO2TV5B5A&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=Dc6DSNLMiNYCEl95AAAAAA%3D%3D",
         null,
         "Data Engineer - Spark/Python",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://www.pyjobs.com/job/staff-data-engineer-spark-python-hadoop-oMy9olMy?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "PyJobs"
          }
         ],
         "Visa",
         "Company Description\n\nVisa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n\nWhen you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n\nJoin Visa: A Network Working for Everyone.\nJob Description\n\nThe Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n\nAre you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n\nAs a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n\nEssential Functions\n• Work with manager and clients to fully understand business requirements and desired business outcomes\n• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n• Execute data engineering projects ranging from small to large either individually or as part of a project team\n• Ensure project delivery within timelines and budget requirements\n• Provide coaching and mentoring to junior team members\n\nThis is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\nQualifications\n\n• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n• Minimum of 4 years of hands-on expertise with Java or Scala\n• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n• Experience working in an Agile and Test Driven Development environment.\n• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n• Experience with SAS as a statistical package is preferred\n• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFwLvoDTjSq1EcFFN1H8mYSCa7m2aRKNdyF3Q30LH1n8hq_4TopDrTgMcERFOJELZG2GeZ0wv0q4f9QzlXDBnjktYAU3bkEs5s4DE5yZXbSzvVdNsjNGJFZOFDV0Vcdvw2RbHs2TW_nXiMdsU0S1zWa7HqtEbjl9BEEIBFfqA_4AG-QyopMAAAA&shmds=v1_AdeF8KjOBn1yJUpUCbIoZfiL9XOirLPsVpNYluZLK1C-IlMUqQ&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f765f7018a3bebbd7b/images/51c80805102136f775d6990ffc61dcc6da45082c8a36a2aa06bfab96684f1183.jpeg",
         "Staff Data Engineer (Spark, Python, Hadoop)",
         "PyJobs"
        ],
        [
         [
          {
           "link": "https://in.indeed.com/viewjob?jk=b95950f98d5b6678&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/databricks-engineer-spark-pyspark-enkefalos-technologies-llp-JV_KO0,33_KE34,60.htm?jl=1009780925122&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/6215885900103024640?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://www.kitjob.in/job/153480629/c-941-data-engineer-etl-sql-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "Enkefalos Technologies LLP",
         "Databricks Engineer – Spark / PySpark\n\nLocation : Remote / Mysore\n\nJoining : Immediate\n\nExperience : 5+ years\n\nResponsibilities :\n\nWill implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n\nRequirements:\n• PySpark (RDDs, DataFrames, performance tuning)\n• Building gold‐layer data models for financial reporting\n• Experience with complex joins, aggregations, GL hierarchies\n• Version handling (Actuals vs Budget), currency conversions\n\nJob Type: Full-time\n\nPay: ₹500,395.35 - ₹1,840,348.25 per year\n\nBenefits:\n• Flexible schedule\n• Paid sick time\n• Provident Fund\n• Work from home\n\nApplication Question(s):\n• Have you worked on ADF/ADLS ?\n• Do you have hands-on experience of Spark / PySpark\n\nExperience:\n• Databricks Engineering: 4 years (Required)\n\nWork Location: Remote",
         {
          "posted_at": "15 days ago",
          "qualifications": "No degree mentioned",
          "salary": "₹500,395.34–₹1,840,348.20 a year",
          "schedule_type": "Full-time",
          "work_from_home": "true"
         },
         [
          "15 days ago",
          "₹500,395.34–₹1,840,348.20 a year",
          "Work from home",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Anywhere",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=k4Rb92L6YIBechULAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXGsQrCMBAAUFz7CU4HbkIbEVzU0SJKh4Luco1nEhvvQi5D_Rs_VcTl8arPrNofsOCQgx0VWnaBiTLUcEmYRzDQv_-r4SwDKGG2HoThKOIizXe-lKRbY1Rj47RgCbax8jLCNMhknjLoj5t6zJQiFrqtN6upSeyWi5ZHemAUhStZzxLFBVLouh4Cw4nvAb-C4XW0owAAAA&shmds=v1_AdeF8KizOswJH_SyEGgjpgJ7bj5Cwqu4B0GS78yPFj9XWFNoVQ&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=k4Rb92L6YIBechULAAAAAA%3D%3D",
         null,
         "Databricks Engineer - Spark / PySpark",
         "Indeed"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/senior-pyspark-data-engineer-big-data-cloud-data-solutions-python-at-synechron-4258672529?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://jobs.towardsai.net/job/nagarro-senior-staff-engineer-big-data-xrl0?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Towards AI Jobs"
          }
         ],
         "Synechron",
         "Job Summary\n\nSynechron is seeking a skilled PySpark Data Engineer to design, develop, and optimize data processing solutions leveraging modern big data technologies. In this role, you will lead efforts to build scalable data pipelines, support data integration initiatives, and work closely with cross-functional teams to enable data-driven decision-making. Your expertise will contribute to enhancing business insights and operational efficiency, positioning Synechron as a pioneer in adopting emerging data technologies.\n\nSoftware Requirements\n\nRequired Software Skills:\n• PySpark (Apache Spark with Python) – experience in developing data pipelines\n• Apache Spark ecosystem knowledge\n• Python programming (versions 3.7 or higher)\n• SQL and relational database management systems (e.g., PostgreSQL, MySQL)\n• Cloud platforms (preferably AWS or Azure)\n• Version control: GIT\n• Data workflow orchestration tools like Apache Airflow\n• Data management tools: SQL Developer or equivalent\n\nPreferred Software Skills:\n• Experience with Hadoop ecosystem components\n• Knowledge of containerization (Docker, Kubernetes)\n• Familiarity with data lake and data warehouse solutions (e.g., AWS S3, Redshift, Snowflake)\n• Monitoring and logging tools (e.g., Prometheus, Grafana)\n\nOverall Responsibilities\n• Lead the design and implementation of large-scale data processing solutions using PySpark and related technologies\n• Collaborate with data scientists, analysts, and business teams to understand data requirements and deliver scalable pipelines\n• Mentor junior team members on best practices in data engineering and emerging technologies\n• Evaluate new tools and methodologies to optimize data workflows and improve data quality\n• Ensure data solutions are robust, scalable, and aligned with organizational data governance policies\n• Stay informed on industry trends and technological advancements in big data and analytics\n• Support production environment stability and performance tuning of data pipelines\n• Drive innovative approaches to extract value from large and complex datasets\n\nTechnical Skills (By Category)\n\nProgramming Languages:\n• Required: Python (PySpark experience minimum 2 years)\n• Preferred: Scala (for Spark), SQL, Bash scripting\n\nDatabases/Data Management:\n• Relational databases (PostgreSQL, MySQL)\n• Distributed storage solutions (HDFS, cloud object storage like S3 or Azure Blob Storage)\n• Data warehousing platforms (Snowflake, Redshift – preferred)\n\nCloud Technologies:\n• Required: Experience deploying and managing data solutions on AWS or Azure\n• Preferred: Knowledge of cloud-native services like EMR, Data Factory, or Azure Data Lake\n\nFrameworks and Libraries:\n• Apache Spark (PySpark)\n• Airflow or similar orchestration tools\n• Data processing frameworks (Kafka, Spark Streaming – preferred)\n\nDevelopment Tools and Methodologies:\n• Version control with GIT\n• Agile management tools: Jira, Confluence\n• Continuous integration/deployment pipelines (Jenkins, GitLab CI)\n\nSecurity Protocols:\n• Understanding of data security, access controls, and GDPR compliance in cloud environments\n\nExperience Requirements\n• Minimum of 5+ years in data engineering, with hands-on PySpark experience\n• Proven track record of developing, deploying, and maintaining scalable data pipelines\n• Experience working with data lakes, data warehouses, and cloud data services\n• Demonstrated leadership in projects involving big data technologies\n• Experience mentoring junior team members and collaborating across teams\n• Prior experience in financial, healthcare, or retail sectors is beneficial but not mandatory\n\nDay-to-Day Activities\n• Develop, optimize, and deploy big data pipelines using PySpark and related tools\n• Collaborate with data analysts, data scientists, and business teams to define data requirements\n• Conduct code reviews, troubleshoot pipeline issues, and optimize performance\n• Mentor junior team members on best practices and emerging technologies\n• Design solutions for data ingestion, transformation, and storage\n• Evaluate new tools and frameworks for continuous improvement\n• Maintain documentation, monitor system health, and ensure security compliance\n• Participate in sprint planning, daily stand-ups, and project retrospectives to align priorities\n\nQualifications\n• Bachelor’s or Master’s degree in Computer Science, Information Technology, or related discipline\n• Relevant industry certifications (e.g., AWS Data Analytics, GCP Professional Data Engineer) preferred\n• Proven experience working with PySpark and big data ecosystems\n• Strong understanding of software development lifecycle and data governance standards\n• Commitment to continuous learning and professional development in data engineering technologies\n\nProfessional Competencies\n• Analytical mindset and problem-solving acumen for complex data challenges\n• Effective leadership and team management skills\n• Excellent communication skills tailored to technical and non-technical audiences\n• Adaptability in fast-evolving technological landscapes\n• Strong organizational skills to prioritize tasks and manage multiple projects\n• Innovation-driven with a passion for leveraging emerging data technologies\n\nS YNECHRON’S DIVERSITY & INCLUSION STATEMENT\n\nDiversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.\n\nAll employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.\n\nCandidate Application Notice",
         {
          "posted_at": "1 day ago",
          "schedule_type": "Full-time"
         },
         [
          "1 day ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgUHlTcGFyayBEYXRhIEVuZ2luZWVyIChCaWcgRGF0YSwgQ2xvdWQgRGF0YSBTb2x1dGlvbnMsIFx1MDAyNiBQeXRob24pIiwiY29tcGFueV9uYW1lIjoiU3luZWNocm9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjJYOEpqZUhEQmxiRndHQklBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=2X8JjeHDBlbFwGBIAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWMvQrCMBRGce3o6HQnUamtCC66-YPoJGS3pOklicZ7Q5JC-1i-oVWXbziH82XvUXYXSJYD3HrhZXjCUSYJJ9KWEAPM9lb_UA4Hx23z14JdmyxTzGE6hMkwzWEJV64hogzKABOcmbXDyc6k5OO2LGN0hY5JJqsKxa-SCWvuygfX8TtVNDKgdzJhtd6susKTXoxFT6hMGO4swYUaKz-GmguwtAAAAA&shmds=v1_AdeF8Kh_6lQF_zZmfGWNPm5MUF0xae5qgHPDxabaDnydWNTtyA&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=2X8JjeHDBlbFwGBIAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f765f7018a3bebbd7b/images/51c80805102136f792392bb11dadac654bb21ac87a136a842fd1b92d1d91915d.jpeg",
         "Senior PySpark Data Engineer (Big Data, Cloud Data Solutions, & Python)",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://goto.wd5.myworkdayjobs.com/GoToCareers/job/Bangalore-KA-IN/Senior-Software-Engineer---Big-Data_R25-1293?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/senior-data-engineer-delta-lake-spark-unity-catalog-at-goto-4255173621?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.kitjob.in/job/150484214/senior-sdet-engineer-analytics-kk159-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "306 - GoTo Technologies India Private Limited",
         "Job Description Where you’ll work: India (Remote) Engineering at GoTo We’re the trailblazers of remote work technology. We build powerful, flexible work software that empowers everyone to live their best life, at work and beyond. And blaze even more trails along the way. There’s ample room for growth – so you can blaze your own trail here too. When you join a GoTo product team, you’ll take on a key role in this process and see your work be used by millions of users worldwide. Your Day to Day As a Senior Data Engineer, you would be: Design, develop, and maintain robust, scalable, and efficient ETL/ELT data pipelines to process structured and unstructured data from diverse sources. Architect and implement cloud-native data solutions using AWS services including S3, EMR, Lambda, and EKS. Build and optimize large-scale data processing workflows using Apache Spark and Databricks, with hands-on experience implementing and managing Delta Lake tables for high-volume batch and streaming use cases. Manage and govern data using Unity Catalog, maintaining strong data lineage, access controls, and metadata management. Develop and maintain data models (relational and dimensional) to support analytics, reporting, and machine learning use cases. Schedule, monitor, and orchestrate workflows using Apache Airflow or similar orchestration tools. Implement data quality checks, logging, monitoring, and alerting to ensure reliability and visibility of data pipelines. Collaborate with analysts, data scientists, and business stakeholders to deliver high-quality, trusted data for downstream applications and enable self-service BI tooling. Ensure adherence to best practices in data governance, security, and compliance. Mentor junior engineers and contribute to engineering standards, including CI/CD, automated testing, and documentation What We’re Looking For As an Senior Data Engineer, your background will look like: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. 5+ years of relevant experience in data engineering or software development, with a proven track record designing and maintaining production-grade data pipelines. Proficiency in Python programming and SQL for data transformation and analytics. Strong hands-on experience with Apache Spark and Databricks, including direct exposure to Delta Lake for data lake management, ACID transactions, schema enforcement and evolution, and time travel functionalities. Experience managing and organizing data access with Unity Catalog. In-depth experience with AWS services—specifically S3, EMR, Lambda, and EKS—and an excellent understanding of cloud-based data architecture and security best practices. Strong data modeling skills (dimensional, normalized) and understanding of data warehousing and lakehouse paradigms. Hands-on experience orchestrating workflows using Apache Airflow or similar orchestration frameworks. Familiarity with BI and data visualization tools (e.g., Tableau, Power BI). Experience establishing data quality processes, monitoring, and observability for pipelines. Exceptional communication, collaboration, and problem-solving skills, with the ability to thrive in an agile, multicultural team environment. At GoTo, authenticity and inclusive culture are key to our thriving workplace, where diverse perspectives drive innovation and growth. Our team of GoGetters is passionate about learning, exploring, and working together to achieve success while staying committed to delivering exceptional experiences for our customers. We take pride in supporting our employees with comprehensive benefits, wellness programs, and global opportunities for professional and personal development. By maintaining an inclusive environment, we empower our teams to do their best work, make a meaningful impact, and grow their career. Learn more. At GoTo, we’re bold enough to imagine a world of work without limits—where curiosity and AI-driven innovation fuel our constant growth. As the leader in cloud communications and IT, we solve real-world challenges through practical, cutting-edge solutions and an unwavering customer-first mindset. Our culture is rooted in inclusion, ownership, and transparency, fueling an environment where every voice contributes to both personal and collective achievement. Here, collaboration sparks bold ideas, and authenticity is celebrated—empowering you to adapt, evolve, and make a real impact. Join GoTo, and help shape the future of work while accelerating your own growth alongside exceptional people who are redefining what’s possible.",
         {
          "posted_at": "10 days ago",
          "schedule_type": "Full-time"
         },
         [
          "10 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAoRGVsdGEgTGFrZSwgU3BhcmsgXHUwMDI2IFVuaXR5IENhdGFsb2cpIiwiY29tcGFueV9uYW1lIjoiMzA2IC0gR29UbyBUZWNobm9sb2dpZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlJxSFNWX2s0aVZkRWhBNGRBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=RqHSV_k4iVdEhA4dAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNwWoCQRBEydVPyKlOoqK7EtGDHlXEICioZ-ldm9nWsXuZGYL5wXyXI7kUPOpR1fn76OyPrGIBK0qEtTpR5oDein3mHd15iGNL4Y4uzirpF8ssenN9jPBtFSJTqBuYYmPmPH8umpTaOC_LGH3hYqIkdVHbozTlyp7lzar4jktsKHDrKfHlazp-Fq26wWwynuXhjZ0MJ64btXwlHLHVqxAOQX6yj508JPEVov_FC83n_Q_JAAAA&shmds=v1_AdeF8Kg5aF4RzwNE8-lh6QnTWkwzZMZ_FT7aljYehDjX-BkuNQ&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=RqHSV_k4iVdEhA4dAAAAAA%3D%3D",
         null,
         "Senior Data Engineer (Delta Lake, Spark & Unity Catalog)",
         "Workday"
        ],
        [
         [
          {
           "link": "https://jobgether.com/offer/66f5f6104841e0ee22e41ea5-cloud-data-engineer--spark-databricks?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobgether"
          }
         ],
         "Brighttier",
         "This a Full Remote job, the offer is available from: India\n\nJob Title: Cloud Engineer – Spark/Databricks Specialist\nLocation: Remote\nJob Type: Contract\nIndustry: IT/Cloud Engineering\nJob Summary:\nWe are looking for a highly skilled Cloud Engineer with a specialization in Apache Spark and Databricks to join our dynamic team. The ideal candidate will have extensive experience working with cloud platforms such as AWS, Azure, and GCP, and a deep understanding of data engineering, ETL processes, and cloud-native tools. Your primary responsibility will be to design, develop, and maintain scalable data pipelines using Spark and Databricks, while optimizing performance and ensuring data integrity across diverse environments.\nKey Responsibilities:\nDesign and Development:\n• Architect, develop, and maintain scalable ETL pipelines using Databricks, Apache Spark (Scala, Python), and other cloud-native tools such as AWS Glue, Azure Data Factory, and GCP Dataflow.\n• Design and build data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n• Implement efficient data ingestion, transformation, and processing workflows with Spark and Databricks.\n• Optimize the performance of ETL processes for faster data processing and lower costs.\n• Develop and manage data pipelines using other ETL tools such as Informatica, SAP Data Intelligence, and others as needed.\nData Integration and Management:\n• Integrate structured and unstructured data sources (relational databases, APIs, ERP systems) into the cloud data infrastructure.\n• Ensure data quality, validation, and integrity through rigorous testing.\n• Perform data extraction and integration from SAP or ERP systems, ensuring seamless data flow.\nPerformance Optimization:\n• Monitor, troubleshoot, and enhance the performance of Spark/Databricks pipelines.\n• Implement best practices for data governance, security, and compliance across data workflows.\nCollaboration and Communication:\n• Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders, to define data requirements and deliver scalable solutions.\n• Provide technical guidance and recommendations on cloud data engineering processes and tools.\nDocumentation and Maintenance:\n• Document data engineering solutions, ETL pipelines, and workflows.\n• Maintain and support existing data pipelines, ensuring they operate effectively and align with business goals.\nQualifications:\nEducation:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\nExperience:\n• 7+ years of experience in cloud data engineering or similar roles.\n• Expertise in Apache Spark and Databricks for data processing.\n• Proven experience with cloud platforms like AWS, Azure, and GCP.\n• Experience with cloud-native ETL tools such as AWS Glue, Azure Data Factory, Kafka, GCP Dataflow, etc.\n• Hands-on experience with data platforms like Redshift, Snowflake, Azure Synapse, and BigQuery.\n• Experience in extracting data from SAP or ERP systems is preferred.\n• Strong programming skills in Python, Scala, or Java.\n• Proficient in SQL and query optimization techniques.\nSkills:\n• In-depth knowledge of Spark/Scala for high-performance data processing.\n• Strong understanding of data modeling, ETL/ELT processes, and data warehousing concepts.\n• Familiarity with data governance, security, and compliance best practices.\n• Excellent problem-solving, communication, and collaboration skills.\nPreferred Qualifications:\n• Certifications in cloud platforms (e.g., AWS Certified Data Analytics, Google Professional Data Engineer, Azure Data Engineer Associate).\n• Experience with CI/CD pipelines and DevOps practices for data engineering.\n• Exposure to Apache Hadoop, Kafka, or other data frameworks is a plus.\n\nThis offer from \"Brighttier\" has been enriched by Jobgether.com and got a 74% flex score.",
         {
          "schedule_type": "Contractor",
          "work_from_home": "true"
         },
         [
          "Work from home",
          "Contractor"
         ],
         "eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyLSBTcGFyayBcdTAwMjYgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJyaWdodHRpZXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTzdzcjNWUGpVa2l1R0VpZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Anywhere",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=O7sr3VPjUkiuGEigAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3MPQvCMBCAYVy7ujnd5CC0EcFFoYMfiK7-gHJJjzQ25sJdhP4Of7Efyzs8w1u9Z1V7jPzq4YQF4Zx8SERSwz2jjLD8s5XgRoUabmxBCcUNwAkuzD7SYj-UknVnjGpsvBYswTWOn4YTWZ7Mg63-0umAQjlioW6zXU9NTn41P0jw30EggZDgmvqAH8LOuxqVAAAA&shmds=v1_AdeF8KiYkYThhxCHaf21kIvz7iAYkbibpbTZilPvLl1NaSnZKQ&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=O7sr3VPjUkiuGEigAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f765f7018a3bebbd7b/images/51c80805102136f7288e0093cef41baca5f816735e5c318463cf69b421b2cd83.png",
         "Cloud Data Engineer- Spark & Databricks",
         "Jobgether"
        ],
        [
         [
          {
           "link": "https://citi.wd5.myworkdayjobs.com/en-US/2/job/Data-Engineer---AVP_24785645?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://in.indeed.com/viewjob?jk=481bb9445cf687c8&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://wellfound.com/jobs/2924371-data-engineer-intern-rolling-2027-28-grad?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Wellfound"
          },
          {
           "link": "https://www.hirist.tech/j/celonis-data-engineer-etlsql-10-12-yrs-1479538?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://jobgether.com/offer/67969bf4c58f91dcb9676b4e-ria---data-engineer-with-q-a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobgether"
          },
          {
           "link": "https://www.foundit.in/job/debut-infotech-data-engineer-python-scala-debut-infotech-pvt-ltd-india-34609162?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Foundit"
          },
          {
           "link": "https://in.bebee.com/job/fe79db0a9583dea16798df933003c829?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/data-engineer-2-5-yrs-exp-guardian-management-services-JV_KO0,25_KE26,54.htm?jl=1009784233452&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          }
         ],
         "12542 Citicorp Services India Private Limited",
         "The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=lBQYlRnJjd-RRHF9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWLTQ_BQBBA4-onOM0RqS4NDpzqI0IcJE04yrQd7eja2exu8Nf8OxWXd3nvdT-d7mqDAWFrKjZEDvqZkddNY0MRZBZdE0F6yQYwgvR8anmQHDyhK2oQAzuRSlNvWYdg_UIp73Vc-YCBi7iQhxJDubzVXXL_w9XX6MhqDHRNZuN3bE01nE-S2TSBNbeTOAsZuScX5GFvSkY4OX62PRz5wYFKYPMXX3VpMJW8AAAA&shmds=v1_AdeF8Kis8dqR1eNIZfspzvlHRHi-cHVVT9hIYJVFIboxveOlTA&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=lBQYlRnJjd-RRHF9AAAAAA%3D%3D",
         null,
         "Data Engineer (Snowflake, Spark, AWS) - AVP",
         "Workday"
        ],
        [
         [
          {
           "link": "https://in.indeed.com/viewjob?jk=fd4bd619675010e2&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://www.adzuna.in/details/5082710850?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Adzuna"
          }
         ],
         "Siemens Healthineers",
         "jobid\n• 460574\n\njobfamily\n• Research & Development\n\ncompany\n• Siemens Healthcare Private Limited\n\norganization\n• Siemens Healthineers\n\njobType\n• Full-time\n\nexperienceLevel\n• Experienced Professional\n\ncontractType\n• Permanent\n\nAs a Data Engineer , you are required to:\n\nDesign, build, and maintain data pipelines that efficiently process and transport data from various sources to storage systems or processing environments while ensuring data integrity, consistency, and accuracy across the entire data pipeline.\n\nIntegrate data from different systems, often involving data cleaning, transformation (ETL), and validation. Design the structure of databases and data storage systems, including the design of schemas, tables, and relationships between datasets to enable efficient querying. Work closely with data scientists, analysts, and other stakeholders to understand their data needs and ensure that the data is structured in a way that makes it accessible and usable.\n\nStay up-to-date with the latest trends and technologies in the data engineering space, such as new data storage solutions, processing frameworks, and cloud technologies. Evaluate and implement new tools to improve data engineering processes.\n\nQualification : Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Science, Engineering is desirable.\n\nExperience level : At least 3 - 5 years hands-on experience in Data Engineering\n\nDesired Knowledge & Experience :\n• Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\n• Knowing Spark internals: Catalyst/Tungsten/Photon\n• Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\n• IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\n• Test: pytest, Great Expectations\n• CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\n• Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\n• Languages: Python/Functional Programming (FP)\n• SQL : TSQL/Spark SQL/HiveQL\n• Storage : Data Lake and Big Data Storage Design\n\nadditionally it is helpful to know basics of:\n• Data Pipelines : ADF/Synapse Pipelines/Oozie/Airflow\n• Languages: Scala, Java\n• NoSQL : Cosmos, Mongo, Cassandra\n• Cubes : SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\n• SQL Server : TSQL, Stored Procedures\n• Hadoop : HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\n• Data Catalog : Azure Purview, Apache Atlas, Informatica\n\nRequired Soft skills & Other Capabilities :\n\nGreat attention to detail and good analytical abilities.\n\nGood planning and organizational skills\n\nCollaborative approach to sharing ideas and finding solutions\n\nAbility to work independently and also in a global team environment.",
         {
          "posted_at": "8 days ago",
          "schedule_type": "Full-time"
         },
         [
          "8 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVB5dGhvbixQeVNwYXJrLFNRTCAsU3BhcmsgQXJjaGl0ZWN0dXJlLEF6dXJlIERhdGFicmlja3MiLCJjb21wYW55X25hbWUiOiJTaWVtZW5zIEhlYWx0aGluZWVycyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJXVkxrZFF1QU4tSGFXQjIzQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=WVLkdQuAN-HaWB23AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2OuwrCQBBFsfUTBGFqiYkINloJig8sImmFMFmH7GqcCTsjGD_Nr1PTHG5xLpzhZzC8bNAQtlwHJorTvDMvnORd0WK8J8X5BEk_YR2dD0bOnpGS9ftH-F-rGNxdYQpHqUAJfxYIw06kbmi08matLrNMtUlrNbTgUiePTJgqeWU3qfSPUj1Gahs0KueL2SttuZ6Mi0APYoU9YWO-D1QIDAe-BvwCc1oX1b0AAAA&shmds=v1_AdeF8KgUpMOygYFmUQXSA0j_a1MHh8_TRHWepqUL3g9i4LSfLw&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=WVLkdQuAN-HaWB23AAAAAA%3D%3D",
         "https://serpapi.com/searches/686540f765f7018a3bebbd7b/images/51c80805102136f7f2857876d96d4a19482a79d8d87bbea86705b552fa82bf54.jpeg",
         "Data Engineer-Python,PySpark,SQL ,Spark Architecture,Azure Databricks",
         "Indeed"
        ],
        [
         [
          {
           "link": "https://www.antaltechjobs.in/job/cloud-data-engineer-spark-databricks-1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Antal Tech Jobs"
          },
          {
           "link": "https://in.trabajo.org/job-3150-67d1a486ddf45f6afb09b190cee713d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/8580629334899818496?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          }
         ],
         "Antal Job Board",
         "Vacancy No\nVN1228\n\nBusiness Unit\nEMEA\n\nJob Location\nIndia\n\nEmployment Type\nFull Time\n\nJob Details and Responsibilities\nWe are seeking an experienced Cloud Data Engineer with a strong background in AWS, Azure, and GCP. The ideal candidate will have extensive experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, and other ETL tools like Informatica, SAP Data Intelligence, etc. You will be responsible for designing, implementing, and maintaining robust data pipelines and building scalable data lakes. Experience with various data platforms like Redshift, Snowflake, Databricks, Synapse, Snowflake and others is essential. Familiarity with data extraction from SAP or ERP systems is a plus.\n\nKey Responsibilities:\n\nDesign and Development:\n• Design, develop, and maintain scalable ETL pipelines using cloud-native tools (AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.).\n• Architect and implement data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n• Develop and optimize data ingestion, transformation, and loading processes using Databricks, Snowflake, Redshift, BigQuery and Azure Synapse.\n• Implement ETL processes using tools like Informatica, SAP Data Intelligence, and others.\n• Develop and optimize data processing jobs using Spark Scala.\nData Integration and Management:\n• Integrate various data sources, including relational databases, APIs, unstructured data, and ERP systems into the data lake.\n• Ensure data quality and integrity through rigorous testing and validation.\n• Perform data extraction from SAP or ERP systems when necessary.\nPerformance Optimization:\n• Monitor and optimize the performance of data pipelines and ETL processes.\n• Implement best practices for data management, including data governance, security, and compliance.\nCollaboration and Communication:\n• Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\n• Collaborate with cross-functional teams to design and implement data solutions that meet business needs.\nDocumentation and Maintenance:\n• Document technical solutions, processes, and workflows.\n• Maintain and troubleshoot existing ETL pipelines and data integrations.\n\nQualifications\n\nEducation:\n\nBachelor’s degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n\nExperience:\n• 7+ years of experience as a Data Engineer or in a similar role.\n• Proven experience with cloud platforms: AWS, Azure, and GCP.\n• Hands-on experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.\n• Experience with other ETL tools like Informatica, SAP Data Intelligence, etc.\n• Experience in building and managing data lakes and data warehouses.\n• Proficiency with data platforms like Redshift, Snowflake, BigQuery, Databricks, and Azure Synapse.\n• Experience with data extraction from SAP or ERP systems is a plus.\n• Strong experience with Spark and Scala for data processing.\n\nSkills:\n• Strong programming skills in Python, Java, or Scala.\n• Proficient in SQL and query optimization techniques.\n• Familiarity with data modeling, ETL/ELT processes, and data warehousing concepts.\n• Knowledge of data governance, security, and compliance best practices.\n• Excellent problem-solving and analytical skills.\n• Strong communication and collaboration skills.\n\nPreferred Qualifications:\n• Experience with other data tools and technologies such as Apache Spark, or Hadoop.\n• Certifications in cloud platforms (AWS Certified Data Analytics – Specialty, Google Professional Data Engineer, Microsoft Certified: Azure Data Engineer Associate).\n• Experience with CI/CD pipelines and DevOps practices for data engineering\n• Selected applicant will be subject to a background investigation, which will be conducted and the results of which will be used in compliance with applicable law.\n\nWhat we offer in return:\n• Remote Working: Lemongrass always has been and always will offer 100% remote work\n• Flexibility: Work where and when you like most of the time\n• Training: A subscription to A Cloud Guru and generous budget for taking certifications and other resources you’ll find helpful\n• State of the art tech: An opportunity to learn and run the latest industry standard tools\n• Team: Colleagues who will challenge you giving the chance to learn from them and them from you\n\nLemongrass Consulting is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate on the basis of race, religion, color, national origin, religious creed, gender, sexual orientation, gender identity, gender expression, age, genetic information, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics\n\nAbout Lemongrass\nLemongrass (lemongrasscloud.com) is a global leader in SAP consulting, focused on helping organizations transform their business processes through innovative solutions and technologies. With a strong commitment to customer success, Lemongrass partners with companies to drive their digital transformation journeys, enabling them to unlock the full potential of their SAP investments.\n\nWe do this with our continuous innovation, automation, migration and operation, delivered on the world's most comprehensive cloud platforms – AWS, Azure and GCP and SAP Cloud ERP. We have been working with AWS and SAP since 2010 and we are a Premier Amazon Partner Network (APN) Consulting Partner. We are also a Microsoft Gold Partner, a Google Cloud Partner and an SAP Certified Silver Partner.\n\nOur team is what makes Lemongrass exceptional and why we have the excellent reputation in the market that we enjoy today. At Lemongrass, you will work with the smartest and most motivated people in the business. We take pride in our culture of innovation and collaboration that drives us to deliver exceptional benefits to our clients every day.",
         {
          "posted_at": "28 days ago",
          "schedule_type": "Full-time"
         },
         [
          "28 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyIChTcGFyay9EYXRhYnJpY2tzKSIsImNvbXBhbnlfbmFtZSI6IkFudGFsIEpvYiBCb2FyZCIsImFkZHJlc3NfY2l0eSI6Ik5hZ3B1ciwgTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJCQXpseTdfU0lsQjByTmF2QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Nagpur, Maharashtra, India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=BAzly7_SIlB0rNavAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2MSwrCMBQAcdsjuHpLlbYRwY3iwh-ioBsPUF7SkMTGvJCXQu_kJbVuZjEwU3wmxe7oqW_hhBnhHIwLWieYPSOmToxSJqc6nkMFN5LAGpOyQAEuRMbr6dbmHHkjBLOvDWfMTtWK3oKCljSIF0ke0bDFpKPHrJvVejnUMZhFtQ8Z_X98IEwtuAAPNLFPJdzxFyDbnLCEa2gdfgGeXekBrgAAAA&shmds=v1_AdeF8KjAGOLnsK3rAfqRDABw5KppbKxIXlgUH4D1KfdIwL1zUw&shem=sdl1p&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=BAzly7_SIlB0rNavAAAAAA%3D%3D",
         null,
         "Cloud Data Engineer (Spark/Databricks)",
         "Antal Tech Jobs"
        ],
        [
         [
          {
           "link": "https://jobs.bms.com/careers/job/137468822836-data-analyst-iii-hyderabad-ts-in?domain=bms.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BMS Careers - Bristol Myers Squibb"
          },
          {
           "link": "https://bristolmyerssquibb.wd5.myworkdayjobs.com/en-US/BMS/job/Hyderabad---TS---IN/Data-Analyst-III_R1593008?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/data-analyst-iii-at-bristol-myers-squibb-4259495625?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/data-analyst-iii-bristol-myers-squibb-JV_IC2865319_KO0,16_KE17,37.htm?jl=1009795686545&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://builtin.com/job/data-analyst-iii/6537119?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://www.jobaaj.com/job/bristol-myers-squibb-data-analyst-iii-hyderabad-telangana-india-5-to-7-years-356224?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobaaj"
          },
          {
           "link": "https://www.simplyhired.co.in/job/J-Cd6cs1eJL1KIfnTPhq8N20XLmjDINnr97u-VtK5tbrR3KwMhLnJA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://www.glassdoor.com/job-listing/data-analyst-iii-bristol-myers-squibb-JV_IC2865319_KO0,16_KE17,37.htm?jl=1009795686545&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          }
         ],
         "Bristol Myers Squibb",
         "Working with Us\nChallenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n\nThe US Commercial Analytics, Market Access Data Analyst will play a crucial role in supporting data-driven decision-making processes, generating insights, and providing strategic guidance to optimize our Market Access & Pricing activities in the US. This position requires a deep expertise in secondary analytics and statistical analytic techniques, along with a firm grasp of the US healthcare landscape, especially market access and pricing dynamics. The Business Analyst III will typically be responsible for leading support for multiple therapeutic areas in Market Access and will collaborate with cross-functional teams to enhance understanding of business performance and drive growth.\n\nRoles and Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Perform statistical analyses, data mining and predictive modelling to uncover trends and drivers of performance\n• Collaborate with stakeholders (US based Market Access Business Insights & Analytics team and US Market Access team members) to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects including payer, PBM and patient level subnational & claims data analyses, formulary and policy level analysis, and ad-hoc analytics based on secondary data to support US Market Access strategy\n• Prepare reports, dashboards and presentations to communicate findings, insights and recommendations to stakeholders\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up-to-date with industry trends (US access and health care landscape), best practices, and emerging technologies\n• Partner with US based Market Access BIA/Commercial team members (for a given project scope)\n• Provide overall project oversight and training/mentorship to junior analysts and team members as required\n• Supports culture of fact-based decision making through application of best-in-class, innovative, and appropriate methodologies to address key business questions\n\nSkills & Competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions, around US pricing and reimbursement strategies\n• Proficiency in statistical analysis techniques and applications of those to US commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and strategic thinking, with the ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome Candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• 5-8 years prior Pharmaceutical industry experience with 1-2 years within US Market Access & Pricing Analytics\n• Experience working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims, medical policy and formulary data), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with reimbursement and regulatory requirements and compliance in the US biopharma industry\n• Certification or training in relevant analytics or business intelligence tools is a plus\n\nIf you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n\nUniquely Interesting Work, Life-changing Careers\nWith a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n\nOn-site Protocol\n\nBMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n\nSite-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n\nBMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n\nIf you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",
         {
          "posted_at": "3 days ago",
          "schedule_type": "Full-time"
         },
         [
          "3 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUlJIiwiY29tcGFueV9uYW1lIjoiQnJpc3RvbCBNeWVycyBTcXVpYmIiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZHU3JGRGo4OTcybHdnQWpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=VGSrFDj8972lwgAjAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCQAwAUFz7CU7ZBNGeKC46KYJWcNK95NpwPTmTeonQfos_K77hFd9JMTuhIRwY06gGVVXBEq7iQQlz04EwnEVCoum-M-t155xqKoMaWmzKRl5OmLwM7ile_9XaYaY-oVG93q6Gsucw3xxzVJMEt5Gywv39id5DZLiMLWX02C7gQQk5IOMCKm4j_gDVdrxangAAAA&shmds=v1_AdeF8KimfTfXd_kjZHggUmeLzGSdHyGfC3ozzFChVrXaynaLjA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=VGSrFDj8972lwgAjAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540fc1f758ae0067cfba0/images/fada94f4e909550f217f2561801770bc1323d74230147da3ed24178c6aa37305.png",
         "Data Analyst III",
         "BMS Careers - Bristol Myers Squibb"
        ],
        [
         [
          {
           "link": "https://www.novartis.com/careers/career-search/job/details/req-10045492-data-science-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Novartis"
          },
          {
           "link": "https://jobs.jobvite.com/ispschools/job/ovzqwfwK?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobvite"
          },
          {
           "link": "https://www.iitjobs.com/job/data-science-analyst-usa-tech-inspiron-90887?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Iitjobs"
          },
          {
           "link": "https://in.bebee.com/job/4b491748d6b9a8ee939c83831871113d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://apna.co/job/bengaluru/data-science-analyst-1504543814?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Apna"
          },
          {
           "link": "https://unstop.com/jobs/data-science-analyst-infogain-1507456?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Unstop"
          },
          {
           "link": "https://builtin.com/job/data-science-analyst/3295763?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://www.adzuna.in/details/5260944043?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Adzuna"
          }
         ],
         "IN10 (FCRS = IN010) Novartis Healthcare Private Limited",
         "About the Role:Within Insights and Commercial Solutions, below are the key service groups:  Marketing Effectiveness (Business Analytics, Forecasting, Competitive Intelligence)  Field Force Operations  Commercial Consulting  Brand Insights  Patient access services (Pricing and market access, etc.)  Primary Market Research Provide analytics support to Novartis internal customers (CPOs & Regional marketing and sales teams) on various projects • Support and facilitate data enabled decision making for Novartis internal customers using data analysis and data science techniques/methods on internal and external (3rd party) data to solve business problems • Support I&CS business in building capabilities by involving in various initiatives like knowledge sharing, on-boarding and training support, support in all business-related tasks/activities, building process documentation and knowledge repositories.Analyst, Data ScienceLocation – Hyderabad #LI Hybrid Key Responsibilities:Delivering projects and managing internal customer expectations across multi-channel marketing analytics, Portfolio Analytics, Targeting and Segmentation, Predictive Analytics, Resource Allocation and Optimization, Other ad-hoc requests.Create and deliver customer requirements as agreed SLAs (timeliness, accuracy, quality, etc.) and drive excellent customer satisfaction. Deliver advanced analytical and statistical solutions for various projects related to promotion evaluation, multi-channel marketing (MCM) campaign design, return on investment (ROI) analysis, resource allocation, segmentation, targeting, and other ad-hoc business questionsSupport exploratory research to identify new areas of application of advanced analytics/data science in providing enhanced decision-making support. Deliver services through a structured project management approach with appropriate documentation and communication throughout the delivery of servicesAutomation of project codes and development of front-end delivery solutions. Support in creation and maintenance of standard operating procedures (SOPs), quality checklists that will enable excellent quality outputs within the function.Support in developing and maintaining knowledge repositories that captures qualitative and quantitative reports of brands, disease areas, macroeconomic trends of Novartis operating markets, etc.Comply with all internal functional operating procedures like time tracking, KPI tracking and reporting, and other internal systems and processes. Comply to all Novartis operating procedures as per legal/IT/HR requirementsEssential Requirements:Technical Skills: Statistical Modeling Experience (Covering but not limited to, Regression, Classification, Supervised and Unsupervised Learning Methods, Optimization techniques, Predictive Models, Deep learning Models, e.g. Neural Networks, Natural Language Processing, Text Mining etc.)Programming languages/tools – R, SAS, Python, SQLVisualization tools – Qlikview, Qliksense, TableauWorking knowledge of MS-Office (MS Excel, VBA, PowerPoint, Access)Pharmaceutical industry domain/datasets knowledge – desirableEducation:Graduation/Post Graduation in Business Administration, Mathematics, Economics, Statistics, Engineering or Quantitative streamsExperience: Experience (Minimum 2years) in analytics in marketing analytics, experience in pharma industry is preferableShould have strong quantitative and systems backgroundStrong analytical thinking with problem solving approachShould have good ability to understand new data sources in short timeframe and embed them into standard analysesA good understanding of pharmaceutical domain and data would be ideal. Should have worked in an international company with exposure to working in a cross-cultural environmentStrong and proactive business results focus, and proven ability to provide insights that increase productivityWhy Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook. https://www.novartis.com/careers/benefits-rewards Commitment to Diversity and Inclusion: Novartis is committed to building an outstanding, inclusive work environment and diverse teams' representative of the patients and communities we serve. Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network.",
         {
          "posted_at": "7 days ago",
          "schedule_type": "Full-time"
         },
         [
          "7 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVuY2UgQW5hbHlzdCIsImNvbXBhbnlfbmFtZSI6IklOMTAgKEZDUlMgPSBJTjAxMCkgTm92YXJ0aXMgSGVhbHRoY2FyZSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiejA4Y3NEMUZUdmdvZExfWkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=z08csD1FTvgodL_ZAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLzQqCQBAAYLr6CJ3m2A-pBV2KoCgqIyTyAWRcB91Yd8QZxN6ph6wu3-0LPqNgfkJFyIwlbwgOHt1bFBZw4wKEsDM1sIcLc-VovK1VW9lEkYgLK1FUa0LDTcSeCh6iFxfyJ5caO2odKuWrdTyEra9m-yRdxjA5H58Z7CBJ42U8hZR77NQKXAmd1ubX4NHZ_jfhbhurVIL1kPjS4hdRPmF7rwAAAA&shmds=v1_AdeF8KitnZKbCILU3LI82mL07zTe7Dv_v83dUFqGICYpHkWx8w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=z08csD1FTvgodL_ZAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540fc1f758ae0067cfba0/images/fada94f4e909550f21c750561ff1b21c265ac6278579d4b78e5ef5cb44208d2c.png",
         "Data Science Analyst",
         "Novartis"
        ],
        [
         [
          {
           "link": "https://www.glassdoor.com/job-listing/data-sr-modeler-data-analyst-immediate-joiner-the-talent-quest-JV_IC2865319_KO0,45_KE46,62.htm?jl=1009798098764&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.simplyhired.co.in/job/s9r4eMpYNfeYtngj8mAjRCkDSPZL87Zzwc5Y_lpLOO4rT9VOfcRAZw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          }
         ],
         "The Talent Quest",
         "Vacancy with a company focused on digital transformation, specializing in intelligent automation, digitalization, data science & analytics, and mobile enablement. They help businesses improve cost efficiency, productivity, and agility by reducing turnaround time and errors. The company provides services and solutions including operations digital transformation consulting, next-gen shared services setup consulting, cognitive RPA deployment, and AI-enabled CX enhancement. Founded in 2020 ;with HQ in Gurugram, India; the Company is now operating from Noida, Mumbai, Hyderabad, and Bengaluru as well.\n\nJob Role:We are seeking a highly skilled and detail-oriented Data Modeler to join our Data\n\nManagement team. The ideal candidate will be responsible for designing and\n\nimplementing logical and physical data models to support enterprise data\n\ninitiatives. This role requires close collaboration with business stakeholders, data\n\narchitects, and engineers to ensure data is structured and accessible for analytics,\n\nreporting, and operational needs.\n\nThe successful candidate will:\n\nProvides technical expertise in needs identification, data modelling, data\n\nmovement and transformation mapping (source to target), automation and testing\n\nstrategies, translating business needs into technical solutions with adherence to\n\nestablished data guidelines and approaches from a business unit or project\n\nperspective.\n\n7-10 Years industry implementation experience with one or more data\n\nmodelling tools such as Erwin, ERStudio, PowerDesigner etc.\n\n Minimum of 8 years of data architecture, data modelling or similar\n\nexperience\n\n 5-7 years of management experience required\n\n 5-7 years consulting experience preferred\n\n Experience working with dimensionally modelled data\n\n Bachelor’s degree or equivalent experience, Master’s Degree Preferred\n\n Understanding of cloud (Azure, AWS, GCP, Snowflake preferred) and on\n\npremises architectures\n\nJob Types: Full-time, Permanent\n\nPay: Up to ₹3,000,000.00 per year\n\nBenefits:\n• Cell phone reimbursement\n• Internet reimbursement\n• Life insurance\n• Paid sick time\n• Paid time off\n• Work from home\n\nWork Location: In person",
         {
          "posted_at": "2 days ago",
          "schedule_type": "Full-time"
         },
         [
          "2 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIFNyLk1vZGVsZXIvRGF0YSBBbmFseXN0KCBJbW1lZGlhdGUgSm9pbmVyKSIsImNvbXBhbnlfbmFtZSI6IlRoZSBUYWxlbnQgUXVlc3QiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ino0YkdrY3RHOUdZdmFTRVVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=z4bGkctG9GYvaSEUAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2OwQqCUBBFaesntJplhfkiaFOrICqDFpF7GXVQ4zkjbybQH-v7sjaXczaHG31m0fmEhvAMyV0q8hTc34-MflRbQNp1VLVoBDdpmcIS1hMVoIShbEAYLiK1p_mhMet175yqT2o1tLZMSumcMBUyuJcU-ptcGwzU-ymZb3ebIem5XrmsIcjQExs83qQGLcN1rChggVUMGXnkGhljSHm68wWOhHMouQAAAA&shmds=v1_AdeF8KixMp25pqJo0NTjPByEzl4f3AmNgLfXodeVjxPNvUpjNg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=z4bGkctG9GYvaSEUAAAAAA%3D%3D",
         null,
         "Data Sr.Modeler/Data Analyst( Immediate Joiner)",
         "Glassdoor"
        ],
        [
         [
          {
           "link": "https://insightglobal.com/jobs/find_a_job/job-422244?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Insight Global"
          }
         ],
         "Insight Global",
         "Project Background:\nMosaic is our financial planning and reporting transformation program, bringing an aligned business process and toolset to all key business units enabling us to get consistent and standardized financial metrics across geographies. Started in 2019 the program is already rolled out to Europe and most of North America and continues across the world.\nThere are three key components to the program:\n1. A standardized planning tool IBM Cognos TM1\n2. A global data warehouse based on Azure Cloud know as SPOT (single point of truth). The SPOT team is made up of functional data analysts & data engineers\n3. An extensive suite of dashboards known as Cockpit using Tableau. The Cockpit team are generally finance analysts with Tableau skills\nRole Background:\nWe are looking for an additional functional/data analyst to join the existing team of functional/data analysts working on the SPOT data warehouse.\nThe role of the functional/data analyst is to be the bridge between the Cockpit team, Data Engineering team, and Cognos TM1 team. The analyst understands the business data requirements and translates these into data definition and data transformation documents that are then build by the data engineers. The analyst can also be involved in testing, data validation and any sustain activity related to the flow and accuracy of the data in SPOT & Cockpit.\nTypically, the functional data analyst receives requirements for new and/or amended data pipelines from the Cockpit team or works with the Cognos TM1 team on new data sets that need to flow to SPOT & Cockpit.\nThe current functional data analysts come from a range of backgrounds from data engineers to business analysts. What unites the team is a passion for data, attention to detail, ability to navigate and understand complex data flows, and a customer-centric mindset. This is a high profile and at times high pressure transformation program and everyone on the team must be responsive to our stakeholders needs.\nKey Accountabilities:\nThis role will primarily work with the North America Cockpit team primarily based in Mexico City, but as part of the team they will be expected to build a good knowledge of all data pipelines and work on any area as needed. The key elements of the role are:\nDevelop and maintain SPOT solution design & data architecture:\no Ensure SPOT solution design & data model is up to date with latest business requirements\no Contribute to the delivery of current technical roadmap that can support the predicted growth in data volumes\nTranslate and communicate business requirements across all IT delivery teams and/or partners:\no Collaborate closely with SPOT Cockpit teams to ensure business requirements received from Global & Regional Planning teams are well-understood and translated into the SPOT solution design (functional & data model)\nAct as an owner in the diagnosis and resolution of any data issues raised with the team, partnering with other teams as needed.\n\nWe are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         {
          "posted_at": "6 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Contractor"
         },
         [
          "6 days ago",
          "Contractor",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBJTlRMIC0gTWV4aWNvIG9yIEluZGlhIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Il96ZU5oSUE0Ujg3ak1KZHNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=_zeNhIA4R87jMJdsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWOsQrCMBBAce0nON0sbSOCi7oIQq2oU_dySUMSiXcll6H9Jb_Sist78KZXfFbF6YIZ4UwYZ8lQQfvs7osedgqGgRO0NARcyo01iMVkPDBBw-yiXR99zqMclBKJtZOMOZja8FsxWc2TerGWH3rxmOwYMdt-t99O9UhuU7UkwfkMTWSNEQLBdR5sQo1DCZ2NSA4Jy__CF9IICi6tAAAA&shmds=v1_AdeF8KhFqSmgnQVrm9cAq5NS80RUOZdWCXZSAECxyvvZZQK5RQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=_zeNhIA4R87jMJdsAAAAAA%3D%3D",
         null,
         "Data Analyst - INTL - Mexico or India",
         "Insight Global"
        ],
        [
         [
          {
           "link": "https://aijobs.net/job/1401617-in-specialist-3-data-analyst-tras-assurance-bangalore/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Aijobs.net"
          },
          {
           "link": "https://www.kitjob.in/job/150285117/ctc889-advanced-system-dynamics-analyst-marine-vessel-analysis-specialist-kota/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "PwC",
         "Line of ServiceAssuranceIndustry/SectorNot ApplicableSpecialismAssuranceManagement LevelSpecialistJob Description & SummaryAt PwC, our people in audit and assurance focus on providing independent and objective assessments of financial statements, internal controls, and other assurable information enhancing the credibility and reliability of this information with a variety of stakeholders. They evaluate compliance with regulations including assessing governance and risk management processes and related controls.In digital assurance at PwC, you will focus on providing assurance services over clients' digital environment, including processes and controls, cyber security measures, data and AI systems, and their associated governance, to help organisations and their stakeholders build trust in their technology while complying with relevant regulations.*Why PWCAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm’s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations. \"Job Description & SummaryABOUT TECHNOLOGY RISK ASSURANCE SERVICES:The Technology Risk Assurance Services practice in the firm, provides both audit and non-audit services related to controls around the financial reporting process, including financial business process and IT management controls.SPA provides:· Financial and operation applications/business process controls reviews· Database security controls reviews· IT general controls reviews· Infrastructure security reviews· Third party assurance and opinion services· Sarbanes-Oxley readiness, process improvement and sustainability services· Due diligence on systems and controls· Pre- and post-implementation systems reviews· Project assurance services· Data services (e.g., CAATs, data quality reviews)· Computer security reviewsResponsibilities (essential functions and responsibilities of the job):The tasks, roles and responsibilities concerned with the job will include the following:Executing various audit engagements as a support to field in-charge andApplying The Firm’s methodology while executing various job rolesApply current knowledge of IT trends and systems to identify security and risk management issues, and other opportunities for improvementAssist in the planning and execution of business process control reviews, information systems audits, and other Assurance offerings across a variety of industriesA good understanding of clients Business Risks, Audit Risks, IT Risks and the mitigating controls in addressing these risksGain strong comprehension of client operations, processes, and business objectives and utilize that knowledge on engagements.Evaluate and test business processes and controls and identify areas of risks.Perform general computer and application controls reviews including ERPs like SAP, Oracle Application, JD Edwards, MFG Pro, etc.Comply with requirements of ICAI Auditing Standards like AAS 29Data Analysis using CAATs tool like Excel, ACL, SQL etc*Mandatory skill sets - Data Analyst*Preferred Skill Sets – Business Analyst*Years of experience required – 0-1 Years*Education Qualification – Post Graduation in Data ScienceEducation (if blank, degree and/or field of study not specified)Degrees/Field of Study required: Postgraduate (Diploma)Degrees/Field of Study preferred:Certifications (if blank, certifications not specified)Required SkillsData AnalyticsOptional SkillsAccepting Feedback, Accepting Feedback, Active Listening, Artificial Intelligence (AI) Platform, Auditing Methodologies, Cloud Engineering, Communication, Compliance and Governance, Compliance and Standards, Compliance Auditing, Corporate Governance, Cybersecurity Governance, Cybersecurity Risk Management, Data Quality, Data Quality Assessment, Data Quality Assurance Testing, Data Validation, Emotional Regulation, Empathy, Governance Framework, Inclusion, Information Assurance, Information Security Governance, Intellectual Curiosity, Internal Controls {+ 11 more}Desired Languages (If blank, desired languages not specified)Travel RequirementsNot SpecifiedAvailable for Work Visa Sponsorship?NoGovernment Clearance Required?NoJob Posting End Date",
         {
          "posted_at": "1 day ago",
          "salary": "29,679–55,120 a year",
          "schedule_type": "Full-time"
         },
         [
          "1 day ago",
          "29,679–55,120 a year",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJJTiBTcGVjaWFsaXN0IDMtIERhdGEgQW5hbHlzdCAtIFRSQVMgLUFzc3VyYW5jZeKAkyBCYW5nYWxvcmUiLCJjb21wYW55X25hbWUiOiJQd0MiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiZTlNdVBjTnNJWHlmb3ZrSkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=e9MuPcNsIXyfovkJAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJMQrCMBSAYVyLJ3B6s9BGFBedqoLUQcS6l9f4SCMxL-RFrJt38AKezZOoyw8ff_YeZHW1hzqQtuisJJjlsMGEUHp0j59zOB3LGvJS5BbRa_o8X7BCb9BxpN_ecQtCGHUH7GHLbByNll1KQRZKibjCSMJkdaH5qthTy726cCv_NNJhpOAwUTOdT_oieDMeHu5rsB4qf7b4BdFQI-WjAAAA&shmds=v1_AdeF8KgHugeD7CzRIEP7YN_reUh11kgJaYmeQyaRo77I08Aw9w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=e9MuPcNsIXyfovkJAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540fc1f758ae0067cfba0/images/fada94f4e909550fd832cbd45dde8873d2028e59e4230f6145f1024b5313cd32.png",
         "IN Specialist 3- Data Analyst - TRAS -Assurance– Bangalore",
         "Aijobs.net"
        ],
        [
         [
          {
           "link": "https://jobs.bms.com/careers/job/137468822835-data-analyst-1-hyderabad-ts-in?domain=bms.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BMS Careers - Bristol Myers Squibb"
          },
          {
           "link": "https://bristolmyerssquibb.wd5.myworkdayjobs.com/en-US/BMS/job/Hyderabad---TS---IN/Data-Analyst-1_R1593138?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://www.jobaaj.com/job/data-analyst-1-hyderabad-telangana-india-3-to-5-years-461411?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobaaj"
          },
          {
           "link": "https://aijobs.net/job/1400726-data-analyst-1/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Aijobs.net"
          },
          {
           "link": "https://in.trabajo.org/job-1940-174b71c3e8dc4cd53918c839b8b9b2ec?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          },
          {
           "link": "https://notify.careers/postings/290a6e16-bf16-4c53-8329-0aaade1c0e57?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Notify"
          },
          {
           "link": "https://in.bebee.com/job/153589814e5a2d7440988ab7dfe948d7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://in.talent.com/view?id=da5efbc05dde&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Talent.com"
          }
         ],
         "Bristol Myers Squibb",
         "The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n\nRoles & Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up to date with industry trends, best practices, and emerging technologies\n\nSkills and competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n• commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• Proven experience (1-3 years) in a similar business analyst role\n• Prior Pharmaceutical industry and/or healthcare consulting experience required\n• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with regulatory requirements and compliance in the US biopharma industry\n\nThe US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n\nRoles & Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up to date with industry trends, best practices, and emerging technologies\n\nSkills and competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n• commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• Proven experience (1-3 years) in a similar business analyst role\n• Prior Pharmaceutical industry and/or healthcare consulting experience required\n• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with regulatory requirements and compliance in the US biopharma industry",
         {
          "posted_at": "16 days ago",
          "schedule_type": "Full-time"
         },
         [
          "16 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ1ZUlVZURnaVJaU1ludkdlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=ueIUeDgiRZSYnvGeAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOwrCQBAAUGxzBKtpbESzfrDRShH8gJX2YTYZNivrTNwZITmKtxVf8YrvqJgc0RD2jGlQgyXM4SoelDDXLQjDSSQkGu9as063zqmmMqihxbqs5eWEyUvvnuL1X6UtZuoSGlWrzaIvOw7T9SFHNUlwGygr3N-f6D1EhvPQUEaPzQwelJADMs7gwk3EH-5fs9ScAAAA&shmds=v1_AdeF8KiIPde6t361nwB3hQNEzTdlaOVGqbQjNngpI_huwKekOA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=ueIUeDgiRZSYnvGeAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540fc1f758ae0067cfba0/images/fada94f4e909550f7835ac31df150746db7b335654baaa715d0cf8b6c3643f43.png",
         "Data Analyst 1",
         "BMS Careers - Bristol Myers Squibb"
        ],
        [
         [
          {
           "link": "https://careers.dupont.com/us/en/job/246328W/Data-Analyst-Finance-VBA-Power-Query-Power-BI-Python-4-years-of-experience?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "DuPont Careers"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/data-analyst-finance-vba-power-query-power-bi-python-4%2B-years-of-experience-at-dupont-4255861626?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://builtin.com/job/data-analyst-finance-vba-power-query-power-bi-python-4-years-experience/6531858?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://jobgether.com/offer/685f49cfde1ebd49c8f53cab-data-analyst-finance-vba-power-query-power-bi-python-4-years-of-experience?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobgether"
          },
          {
           "link": "https://www.talentify.io/job/data-analyst-finance-vba-power-query-power-bi-python-4-years-of-experience-hyderabad-telangana-in-dupont-246328w?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Talentify"
          },
          {
           "link": "https://en-in.whatjobs.com/jobs/business-intelligence?id=157977416&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "WhatJobs"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/6356895448866750464?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://www.recruit.net/job/data-analyst-finance-vba-power-jobs/9BA3CAEB78743536?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Recruit.net"
          }
         ],
         "Dupont",
         "At DuPont, our purpose is to empower the world with essential innovations to thrive. We work on things that matter. Whether it’s providing clean water to more than a billion people on the planet, producing materials that are essential in everyday technology devices from smartphones to electric vehicles, or protecting workers around the world. Discover the many reasons the world’s most talented people are choosing to work at DuPont. Why Join Us | DuPont Careers\n\nJob Summary:\n\nThe Data Analyst/Developer will play a crucial role in developing automated data solutions, creating comprehensive dashboards, and utilizing advanced Excel functionalities. This position will require collaboration with various teams to translate data needs into actionable intelligence.\n\nKey Areas of Expertise and Responsibilities:\n\n1. Visual Basic for Applications (VBA)\n• Responsibilities:\n• Develop and maintain complex VBA applications to automate repetitive tasks.\n• Incorporate SAP Scripting within VBA to optimize business processes.\n• Troubleshoot and debug existing VBA code to ensure smooth functionality.\n• Criteria:\n• Advanced proficiency in VBA programming.\n• Demonstrated experience with SAP interfaces and scripting.\n• Ability to write modular, efficient, and maintainable code.\n• Knowledge of Excel object model and its functionalities.\n\n2. Power Query\n• Responsibilities:\n• Utilize Power Query for data extraction, transformation, and loading (ETL) processes.\n• Develop and maintain data models in Excel to streamline data preparation.\n• Create and optimize Power Query scripts for efficient data processing.\n• Criteria:\n• Intermediate experience with Power Query including M language for data transformation.\n• Familiarity with connecting Power Query to various data sources (e.g., databases, APIs).\n• Ability to perform data cleansing and manipulation through Power Query.\n\n3. Power BI\n• Responsibilities:\n• Create interactive, user-friendly dashboards and reports using Power BI.\n• Collaborate with stakeholders to gather reporting requirements and deliver insights.\n• Optimize Power BI reports for performance and usability.\n• Criteria:\n• Intermediate knowledge of Power BI Desktop and Power BI Service.\n• Ability to create DAX measures and calculated columns for enhanced analytics.\n• Familiarity with data visualization best practices and techniques.\n\n4. Python\n• Responsibilities:\n• Develop Python scripts to automate data manipulation and Excel-related tasks.\n• Utilize libraries such as Pandas and NumPy for data analysis and automation.\n• Collaborate with the data team to integrate Python solutions with existing tools.\n• Criteria:\n• Intermediate proficiency in Python, especially in data manipulation and automation.\n• Experience with libraries like Pandas, NumPy, and openpyxl for Excel automation.\n• Understanding of APIs and ability to retrieve data programmatically.\n\nQualifications:\n• Bachelor’s degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n• A minimum of 4 of experience in a data-related role, focusing on the technologies mentioned.\n• Strong analytical and problem-solving skills with attention to detail.\n• Excellent communication skills and the ability to work collaboratively with diverse teams.\n\nPreferred Skills:\n• Experience with SQL and relational databases for data querying and data management.\n• Familiarity with other business intelligence tools (like Tableau or QlikView) is a plus.\n• Knowledge of machine learning principles is an advantage.\n• Understanding of data warehousing concepts and methodologies.\n\nJoin our Talent Community to stay connected with us!\n\nOn May 22, 2024, we announced a plan to separate our Electronics and Water businesses in a tax-free manner to its shareholders. On January 15, 2025, we announced that we are targeting November 1, 2025, for the completion of the intended separation of the Electronics business (the “Intended Electronics Separation”)*. We also announced that we would retain the Water business. We are committed to ensuring a smooth and successful separation process for the Future Electronics business. We look forward to welcoming new talent interested in contributing to the continued success and growth of our evolving organization.\n\n(1)The separation transactions are subject to satisfaction of customary conditions, including final approval by DuPont's Board of Directors, receipt of tax opinion from counsel, the filing and effectiveness of Form 10 registration statements with the U.S. Securities and Exchange Commission, applicable regulatory approvals, and satisfactory completion of financing.  For further discussion of risks, uncertainties and assumptions that could impact the achievement, expected timing and intended benefits of the separation transactions, see DuPont’s announcement.\n\nDuPont is an equal opportunity employer. Qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability or any other protected class. If you need a reasonable accommodation to search or apply for a position, please visit our Accessibility Page for Contact Information.\n\nDuPont offers a comprehensive pay and benefits package. To learn more visit the Compensation and Benefits page.",
         {
          "posted_at": "2 days ago",
          "schedule_type": "Full-time"
         },
         [
          "2 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgRmluYW5jZSAoVkJBLCBQb3dlciBRdWVyeSwgUG93ZXIgQkksIFB5dGhvbikgNCsgeWVhcnMgb2YgZXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJEdXBvbnQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImNGeEdTMkhJQXpJRW5kb2ZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=cFxGS2HIAzIEndofAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zWLMU7DQBBFRZsjUP0GiQRjI5Q0oUoUAUkFEqKNxvbEu2iZWe1OhPeSOROmoPl6T09_drmauR0ZYSMUSjY8eyHpGLef202FN_3hhPczp_Iv2_1ExZzKHMs7FKaUoSfwGDl5nr5z3OOgLfKUOgcVvKgOga-fnFnM66bJOdRDNjLf1Z1-Nyrc6th8aZv_5pgdJY6BjI-Pq4exjjIsbnbnqGLwgtfSc6KW-gofHEgGEqqwl97TL1wZQbrRAAAA&shmds=v1_AdeF8KjE_yEZOuigr5UIe33mQKphFiyPOSsPa5mAJd61oGdJsA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=cFxGS2HIAzIEndofAAAAAA%3D%3D",
         "https://serpapi.com/searches/686540fc1f758ae0067cfba0/images/fada94f4e909550f6f73161419591ea7d7c57bacc38d027cbd58da61246d96d7.png",
         "Data Analyst Finance (VBA, Power Query, Power BI, Python) 4+ years of experience)",
         "DuPont Careers"
        ],
        [
         [
          {
           "link": "https://jobs.carrier.com/en/job/hyderabad/sustainability-data-analyst/29289/82926538448?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Search Carrier Jobs - Carrier"
          },
          {
           "link": "https://www.terra.do/climate-jobs/job-board/Sustainability-Data-Analyst-Nextracker-8332022/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Terra.do"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/sustainability-data-analyst-carrier-JV_IC2865319_KO0,27_KE28,35.htm?jl=1009789533619&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.saurenergy.com/energy-jobs/sustainability-data-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Saur Energy"
          },
          {
           "link": "https://www.simplyhired.co.in/job/3SsLNieYjqema8Vy7j0dXwbxh9GFa-yltDVlR_r4oBzmPNd0PH7BCQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/sustainability-data-analyst-at-carrier-4255900397?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.jobaaj.com/job/nextracker-inc-sustainability-data-analyst-hyderabad-telangana-india-0-to-15-years-354454?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobaaj"
          },
          {
           "link": "https://www.glassdoor.com/job-listing/sustainability-data-analyst-carrier-JV_IC2865319_KO0,27_KE28,35.htm?jl=1009789533619&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          }
         ],
         "Carrier",
         "Role: Sustainability Data Analyst\n\nLocation: Hyderabad, India\n\nFull/ Part-time: Full time\n\nBuild a career with confidence\n\nCarrier Global Corporation, global leader in intelligent climate and energy solutions is committed to creating solutions that matter for people and our planet for generations to come. From the beginning, we've led in inventing new technologies and entirely new industries. Today, we continue to lead because we have a world-class, diverse workforce that puts the customer at the center of everything we do.\n\nAbout the role\n\nWe are seeking a results-driven Sustainability Data Analyst to join Carrier’s product sustainability analytics team. The ideal candidate will have a strong background in sustainability practices and data analytics, with a focus on analyzing environmental impact and identifying opportunities for improvement. This role involves conducting comprehensive sustainability/energy audits, analyzing data, and developing strategies to enhance sustainability performance.\n\nKey responsibilities:\n• Perform detailed sustainability audits and analyze data to identify trends and areas for improvement.\n• Develop and implement strategies to enhance sustainability and reduce environmental impact.\n• Monitor and evaluate the performance of sustainability initiatives using data analytics.\n• Collaborate with cross-functional teams to optimize sustainability practices.\n• Prepare reports and presentations on sustainability metrics and audit findings.\n• Stay updated on industry trends and best practices in sustainability and data analytics.\n\nMinimum Requirements:\n\nEducation: Bachelor’s or Master’s degree in Mechanical, Energy Engineering, Environmental Science, Sustainability, Data Analytics, or a related field.\n\nExperience: atleast 6 years in sustainability, data analytics, or BI development roles to be able to drive himself towards business goals independently.\n\nKey Skills:\n• Strong analytical skills, attention to detail and ability to think from first principles.\n• Excellent communication and teamwork abilities.\n• Proficiency in data analytics software and tools. Knowledge of python, SQL, Power-BI, would be added advantage.\n• Knowledge of relevant regulations and standards in sustainability.\n• Familiarity with data visualization tools and techniques.\n• Willingness to be flexible, learn new tools, techniques and deliver.\n\nBenefits\n\nWe are committed to offering competitive benefits programs for all of our employees, and enhancing our programs when necessary.\n• Enjoy your best years with our retirement savings plan\n• Have peace of mind and body with our health insurance\n• Make yourself a priority with flexible schedules, parental leave and our holiday purchase scheme\n• Drive forward your career through professional development opportunities\n• Achieve your personal goals with our Employee Assistance Programme.\n\nOur commitment to you\n\nOur greatest assets are the expertise, creativity and passion of our employees. We strive to provide a great place to work that attracts, develops and retains the best talent, promotes employee engagement, fosters teamwork and ultimately drives innovation for the benefit of our customers. We strive to create an environment where you feel that you belong, with diversity and inclusion as the engine to growth and innovation. We develop and deploy best-in-class programs and practices, providing enriching career opportunities, listening to employee feedback and always challenging ourselves to do better. This is The Carrier Way.\n\nJoin us and make a difference.\n\nApply Now!\n\nCarrier is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.\n\nJob Applicant's Privacy Notice:\n\nClick on this link to read the Job Applicant's Privacy Notice",
         {
          "posted_at": "9 days ago",
          "schedule_type": "Full-time"
         },
         [
          "9 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTdXN0YWluYWJpbGl0eSBEYXRhIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJDYXJyaWVyIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJPMkpEMHBVcG0xc3dxYnc1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=O2JD0pUpm1swqbw5AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLvQrCQAwAYFz7CE6ZHKT2pOCiU1HwZ9W95NpwPTmTconQPopvqy7f9hWfRVHf32oYGX1M0WY4oSE0jGlWgw3cxIMS5m4AYTiLhETLw2A26t451VSFX7fYVZ28nDB5mdxTvP5pdcBMY0Kjtt5tp2rksF4dMedIGSLDZe4po8e-hAcl5ICMJVy5j_gFiNgRqpwAAAA&shmds=v1_AdeF8KgSn7fUrNOz8pCz2N5vkRmMXPHGqqIwTzCKcQ0k093PfQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=O2JD0pUpm1swqbw5AAAAAA%3D%3D",
         null,
         "Sustainability Data Analyst",
         "Search Carrier Jobs - Carrier"
        ],
        [
         [
          {
           "link": "https://careers.icims.com/jobs/6072?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "ICIMS Careers"
          }
         ],
         "iCIMS Talent Acquisition",
         "Job Overview\n\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.\n\nAbout Us\n\nWhen you join iCIMS, you join the team helping global companies transform business and the world through the power of talent. Our customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile. As the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent. We’re passionate about helping companies build a diverse, winning workforce and about building our home team. We're dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\n\nResponsibilities\n• Perform various data analysis functions to analyze data from a variety of sources including external labor market data and research and internal data sets from our platforms \n• Incorporate information from a variety of systems to produce comprehensive and compelling narratives for thought-leadership initiatives and customer engagements\n• Demonstrate critical thinking - identify the story in context using multiple data sets, and present results. A strong proficiency in data storytelling will be critical to success in this role.\n• Understand principles of quality data visualization and apply them in Tableau to create and maintain custom dashboards for consumption by other employees\n• Find and investigate data quality issues, root causes and recommend remedies to be implemented by the data scientists and engineers  \n• Liaise with teams around our business to understand their problems, determine how our team can help, then use our database to produce the content they need\n• Identify data mapping and enrichment requirements. Familiarity with SQL, especially the logic behind different types of data joins and writing efficient queries, will be necessary\n• Consistently ensure that business is always conducted with integrity and that behavior aligns with iCIMS policies, procedures, and core competencies\n\nAdditional Job Responsibilities: \n• Produce and adapt data visualizations in response to business requests for internal and external use\n• Shows good judgement in prioritizing their own commitments and those of the larger team, while demonstrating initiative and appropriate urgency when needed\n• Mentor junior team members in best practices for analytics, data visualization, and data storytelling. Exemplify these standards and guide teammates in following them.\n• Think creatively to produce unique, actionable insights from complex datasets, which can deliver value to our business and to our customers.\n\nQualifications\n• 5-10 years professional experience working in an analytics capacity\n• Excellent communication skills, especially with regards to data storytelling – finding insights from complex datasets and sharing those findings with key stakeholders\n• Strong data analytics and visualization skills\n• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n• Advanced knowledge of Excel (Pivot tables, VLOOKUPs, IF statements)\n• Familiarity with data guardrails to ensure compliance with applicable data governance regulations and privacy laws (i.e., GDPR)\n\nPreferred\n• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n\nEEO Statement\n\niCIMS is a place where everyone belongs. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at iCIMS.\n\nWe are proud to be an equal opportunity and affirmative action employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at careers@icims.com.\n\nCompensation and Benefits\n\nCompetitive health and wellness benefits include medical insurance (employee and dependent family members), personal accident and group term life insurance, bonding and parental leave, lifestyle spending account reimbursements, wellness services offerings, sick and casual/emergency days, paid holidays, tuition reimbursement, retirals (PF - employer contribution) and gratuity. Benefits and eligibility may vary by location, role, and tenure. Learn more here: https://careers.icims.com/benefits",
         {
          "posted_at": "21 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "21 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJTci4gRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiaUNJTVMgVGFsZW50IEFjcXVpc2l0aW9uIiwiYWRkcmVzc19jaXR5IjoiUmFpIER1cmcsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImZMUlRBQjlESVFWdFdOeGpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Rai Durg, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=fLRTAB9DIQVtWNxjAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFz7CU63CVITEXTQqViQCi62e7nWkJ7ES82dUH_Gb1WXN77sM8sWdTJQoiIUjOEtCis4xw7EYeoHiAynGH1w88OgOsreWpFgvCgq9aaPDxvZdXGy99jJn1YGTG4MqK7dbNeTGdkvd3SsLjU0GBwrFP3zRUJKv50YrkhQvpLPoXEB2SNjDhXfCL--pAUHoQAAAA&shmds=v1_AdeF8KitaSipmTLybCgiXZCrA1aS7QWwHWuaNBynowT2b-63_w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=fLRTAB9DIQVtWNxjAAAAAA%3D%3D",
         null,
         "Sr. Data Analyst",
         "ICIMS Careers"
        ],
        [
         [
          {
           "link": "https://alfalaval.wd3.myworkdayjobs.com/en-US/Alfa_Laval_jobs/job/Master-Data-Analyst_JR0038405?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://www.iitjobs.com/job/master-data-analyst-ohio-usa-nexpro-technologies-inc-102212?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Iitjobs"
          },
          {
           "link": "https://www.jobringer.com/job/master-data-analyst/d813db76?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "JobRinger"
          },
          {
           "link": "https://aijobs.net/job/1390525-master-data-analyst/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Aijobs.net"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/2298526562489729024?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://talentsjobs.in/job-detail?job-id=161057&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Talents Jobs"
          }
         ],
         "C32511 Alfa Laval India Private Limited",
         "Purpose of the Job: To provide direct support to the Sales and Order Handling teams by managing customer master data and administering the Order Management System. This role ensures data integrity, system readiness, and compliance with internal controls during order booking and execution processes. Key Responsibilities: Customer Master Data Management Manage the full lifecycle of customer codes, including creation, amendment, and deactivation. Conduct background checks using the Global World Check system and coordinate with the Export Control Super User for compliance validation. Ensure customer data is configured correctly in the system for seamless order booking and execution. Maintain data accuracy and cleanliness in line with Alfa Laval’s global data standards. System Administration Act as the system administrator for the Customer Order Management system. Maintain system master data and troubleshoot operational issues. Ensure the system is up-to-date and aligned with business requirements. Order Booking Compliance Perform basic compliance checks during order booking. Verify completeness and correctness of mandatory information. Ensure all required approvals are in place as per the authorization matrix. Process Activation Initiate internal processes required for order execution. Collaborate with cross-functional teams to ensure timely and accurate order processing. Key Competencies: Strong attention to detail and data accuracy Knowledge of compliance and export control processes Proficiency in ERP and order management systems Analytical and problem-solving skills Effective communication and collaboration across teams Qualifications & Experience: Bachelor’s degree in Business Administration, Information Systems, or related field 3+ years of experience in data management or order processing roles Familiarity with data governance and compliance frameworks Experience with customer master data and ERP systems (e.g., SAP) \"We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\" Every day, we get opportunities to make a positive impact – on our colleagues, partners, customers and society. Together, we’re pioneering the solutions of the future and unlocking the full potential of precious resources. Trusted to act on initiative, we challenge conventional thinking to develop world-leading technologies that inspire progress in vital areas, including energy, food, water and shipping. As we push forward, the innovative, open spirit that fuels our 140-year-old start-up culture and rapid growth also drives our personal growth. So, as we shape a more resourceful, less wasteful world, we build our careers too. Meet our dedicated people who always go the extra mile Alfa Laval Career Site Or sign up for Job Alerts to stay in touch Sign up for Job Alerts",
         {
          "posted_at": "6 days ago",
          "schedule_type": "Full-time"
         },
         [
          "6 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJNYXN0ZXIgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiQzMyNTExIEFsZmEgTGF2YWwgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImhJMnc5MHYxS1E3QXVQTkNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=hI2w90v1KQ7AuPNCAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXKwQqCQBCAYbr6CJ3mLOSq4aVOUhCFQW8go066su7IziD2LL1sRZf_8v3RexPFdxSlAGdUhNKje4nCDm7cgBCGdgD2cGHuHW2Pg-osB2NEXNKLoto2aXky7Knh1YzcyC-1DBhodqhU50W6JrPv4_S0z4ssg9I9ESpc0MHVdxbhEezyPaGyk1XqwPo_fADlWYTFngAAAA&shmds=v1_AdeF8Kj2JTzacKV1mRNPu-q1EBnGdhXo_LYLYriehURv_C9NnQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=hI2w90v1KQ7AuPNCAAAAAA%3D%3D",
         null,
         "Master Data Analyst",
         "Workday"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/engr-ii-data-engineering-at-verizon-4254112027?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.adzuna.in/details/5204263702?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Adzuna"
          },
          {
           "link": "https://www.kitjob.in/job/147941185/f-097-engr-ii-data-engineering-tamil-nadu/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          },
          {
           "link": "https://jobindian.in/de/engr-ii-data-engineering-job126726?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobindian.in"
          }
         ],
         "Verizon",
         "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat You’ll Be Doing...\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform and analytics teams, you will understand and enable the required data sets from different sources. This includes both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n• Understanding the business requirements.\n• Transforming technical design.\n• Working on data ingestion, preparation and transformation.\n• Developing the scripts for data sourcing and parsing.\n• Developing data streaming applications.\n• Debugging the production failures and identifying the solution.\n• Working on ETL/ELT development.\n\nWhat We’re Looking For...\n\nYou’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems.\n\nYou'll Need To Have\n• Bachelor’s degree or one or more years of relevant experience required, demonstrated through work experience and/or military experience.\n• Experience with Data Warehouse concepts and Data Management life cycle.\n\nEven better if you have one or more of the following:\n• Any related Certification on ETL/ELT developer.\n• Accuracy and attention to detail.\n• Good problem solving, analytical, and research capabilities.\n• Good verbal and written communication.\n• Experience presenting to and influencing partners.\n\nWhy Verizon?\n\nVerizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.\n• We are a ‘pay for performance’ company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.\n• Your benefits are market competitive and delivered by some of the best providers.\n• You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.\n• We offer generous paid time off benefits to help you manage your work life balance and opportunities for flexible working arrangements*.\n• Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.\n• You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.\n\nYour benefits package will vary depending on the country in which you work.\n• subject to business approval\n\nIf Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.\n\nWhere you’ll be working\n\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.",
         {
          "posted_at": "1 day ago",
          "schedule_type": "Full-time"
         },
         [
          "1 day ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkpaY3lnNzJpTUJGcHpfUTJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=JZcyg72iMBFpz_Q2AAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXMsQoCMQyAYVxvcnbKLFwrgouuitQHcD3SGtpKLylNh8PJR_dcfviXb_huBnvj2MC58YodYZ3MRC1zhBEe4kEJW0ggDHeRWGh3Sb1XPVurWkzUjj0HE2S2wuRlsW_x-s-kCRvVgp2m4-mwmMpxv32u9mfFMoPjV8YfwwKm9YMAAAA&shmds=v1_AdeF8KgbJllH7cXh9TgLLP3wueXhHQRV_Z4prkZgeJwYfCo8Jw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=JZcyg72iMBFpz_Q2AAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c1f32f11d4cca51f4fc/images/56fc5e069c9f2d48c59a9b83fcb6628a5563c2c8e003cb81494faae7f495eb69.jpeg",
         "Engr II-Data Engineering",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://www.pepsicojobs.com/main/jobs/385591?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "PepsiCo Careers"
          },
          {
           "link": "https://www.hirist.tech/j/amgen-associate-data-engineer-4-7-yrs-1467180?ref=kp_prm&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://www.shine.com/jobs/data-engineer-associate/massmutual-india/17177733?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://www.simplyhired.co.in/job/SbR_fqoOTfdMjrCgOyzgqnQbq1eYlbR3kezB3CmrxSWJR44WXnbOXw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/associate-analyst-data-engineer-at-pepsico-4233231137?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://aijobs.net/job/1267375-associate-manager-data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Aijobs.net"
          },
          {
           "link": "https://jobs.vertexventureshc.com/companies/eyebio/jobs/51211685-associate-manager-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Job Board - Vertex Ventures HC"
          },
          {
           "link": "https://www.jobzmall.com/pepsico/job/associate-manager-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "JobzMall"
          }
         ],
         "PepsiCo",
         "Overview\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT.  The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics, and new product development.  PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\n\nWhat PepsiCo Data Management and Operations does:\n\nMaintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\n\nResponsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\n\nWork cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\n\nIncrease awareness about available data and democratize access to it across the company.\n\n \n\n               As a data engineer, you will be the key technical expert building PepsiCo's data products to drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help developing very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n\nResponsibilities\n• Act as a subject matter expert across different digital projects.\n• Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\n• Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\n• Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\n• Responsible for implementing best practices around systems integration, security, performance, and data management.\n• Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\n• Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\n• Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\n• Develop and optimize procedures to “productionalize” data science models.\n• Define and manage SLA’s for data products and processes running in production.\n• Support large-scale experimentation done by data scientists.\n• Prototype new approaches and build solutions at scale.\n• Research in state-of-the-art methodologies.\n• Create documentation for learnings and knowledge transfer.\n• Create and audit reusable packages or libraries.\n\nQualifications\n• 4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n• 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n• 3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n• 2+ years in cloud data engineering experience in Azure.\n• Fluent with Azure cloud services. Azure Certification is a plus.\n• Experience in Azure Log Analytics\n• Experience with integration of multi cloud services with on-premises technologies.\n• Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\n• Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\n• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n• Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\n• Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\n• Experience with Statistical/ML techniques is a plus.\n• Experience with building solutions in the retail or in the supply chain space is a plus.\n• Experience with version control systems like Github and deployment & CI tools.\n• Working knowledge of agile development, including DevOps and DataOps concepts.\n• B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.\n\n Skills, Abilities, Knowledge:\n• Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\n• Strong change manager. Comfortable with change, especially that which arises through company growth.\n• Ability to understand and translate business requirements into data and technical requirements.\n• High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\n• Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\n• Strong organizational and interpersonal skills; comfortable managing trade-offs.",
         {
          "posted_at": "1 month ago",
          "schedule_type": "Full-time"
         },
         [
          "1 month ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUgQW5hbHlzdCAtIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJQZXBzaUNvIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJkdUI4NkhSU1JwRWNPdlBCQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=duB86HRSRpEcOvPBAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXNvwrCMBCAcVz7CE43OUhtRBBEp6Lin8nBvVzSI43Eu5DLYJ_GV7UuH_ymr_rOql2rKi5gIWgZ46gFVnDCgnBmH5goT76LBSXMbgBhuIj4SPPDUErSvTGqsfFasATXOHkbYbLyMS-x-k-nA2ZKcVp0m-360yT2y8WDkoajQGC4jj1ltNjX8KSI7JGxhhv3AX-3W2KaogAAAA&shmds=v1_AdeF8KiLN_MtFDkdScKO8VdsUpBnPWjFPW93bXtnciDsvcO4jw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=duB86HRSRpEcOvPBAAAAAA%3D%3D",
         null,
         "Associate Analyst - Data Engineer",
         "PepsiCo Careers"
        ],
        [
         [
          {
           "link": "https://careers.qualcomm.com/careers/job/446704941923-senior-big-data-engineer-hyderabad-telangana-india?domain=qualcomm.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Qualcomm Careers"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/senior-big-data-engineer-qualcomm-JV_IC2865319_KO0,24_KE25,33.htm?jl=1009710483297&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.simplyhired.co.in/job/p5JHOrXYLuAzJ6DHcAierjARZhkHwHZ_rEUlfxDTE4l9fkr6fj354A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/senior-big-data-engineer-at-grid-dynamics-3971156692?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://jobs.abven.com/companies/nuvia/jobs/49028670-senior-big-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Atlantic Bridge Job Board"
          },
          {
           "link": "https://www.jointaro.com/jobs/qualcomm/senior-big-data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Taro"
          },
          {
           "link": "https://in.bebee.com/job/c771d19594f49f9a197d760525fd19f4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://www.jobaaj.com/job/epam-systems-senior-engineer-big-data-hyderabad-telangana-india-5-to-8-years-289425?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobaaj"
          }
         ],
         "Qualcomm",
         "Company:\nQualcomm India Private Limited\n\nJob Area:\nEngineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\n• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n• 2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary:\n\nPreferred Qualifications\n• 3+ years of experience as a Data Engineer or in a similar role\n• Experience with data modeling, data warehousing, and building ETL pipelines\n• Solid working experience with Python, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n• Experience with Big Data tools, platforms and architecture with solid working experience with SQL\n• Experience working in a very large data warehousing environment, Distributed System.\n• Solid understanding on various data exchange formats and complexities\n• Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n• Strong data visualization skills\n• Basic understanding of Machine Learning; Prior experience in ML Engineering a plus\n• Ability to manage on-premises data and make it inter-operate with AWS based pipelines\n• Ability to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\nEducation\n• Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n• Preferred Qualifications: Masters in CS/ECE with a Data Science / ML Specialization\n\nMinimum Qualifications:\n• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n• 3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n• Completes assigned coding tasks to specifications on time without significant errors or bugs.\n• Adapts to changes and setbacks in order to manage pressure and meet deadlines.\n• Collaborates with others inside project team to accomplish project objectives.\n• Communicates with project lead to provide status and information about impending obstacles.\n• Quickly resolves complex software issues and bugs.\n• Gathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n• Seeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n• Participates in technical conversations with tech leads/managers.\n• Anticipates and communicates issues with project team to maintain open communication.\n• Makes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n• Prioritizes project deadlines and deliverables with minimal supervision.\n• Resolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n• Writes readable code for large features or significant bug fixes to support collaboration with other engineers.\n• Determines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n• Unit tests own code to verify the stability and functionality of a feature.\n\nApplicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n\nQualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n\nTo all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n\nIf you would like more information about this role, please contact Qualcomm Careers.",
         {
          "posted_at": "4 days ago",
          "schedule_type": "Full-time"
         },
         [
          "4 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQmlnIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJRdWFsY29tbSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieVRVTHdrQjB2Ujk5bmJQUUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Hyderabad, Telangana, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=yTULwkB0vR99nbPQAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXKvQrCMBAAYFz7CE63CVIbEVx0K4o_m-heLumRRtK7kItQn8TXVZdv-qrPrDJ34iAZ2uDhgAXhyD4wUYYVXMWCEmY3gDCcRHyk-X4oJenOGNXYeC1YgmucjEaYrEzmKVb_dDpgphSxULfZrqcmsV8ubi-MvzxCYDi_e8posa_hQRHZI2MNF-4DfgEmuzRrmgAAAA&shmds=v1_AdeF8KjxAA1LvW477axq0hGA-OFF2hfFqSX5wKA2PopSDIT20g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=yTULwkB0vR99nbPQAAAAAA%3D%3D",
         null,
         "Senior Big Data Engineer",
         "Qualcomm Careers"
        ],
        [
         [
          {
           "link": "https://jobs.smartrecruiters.com/Blend360/744000066099595-manager-data-engineer-aws-databricks?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SmartRecruiters Job Search"
          },
          {
           "link": "https://builtin.com/job/manager-data-engineer-aws-databricks/6471278?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/manager-data-engineer-aws-databricks-at-blend-4251765113?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://in.bebee.com/job/fca80f9788fd2535e238c0cca3e73eea?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://www.jobnet.com.au/in/en/find-jobs-in-India/DATA-ENGINEER-DATABRICKS-168C894F40A0C5BE28/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "JobNet"
          },
          {
           "link": "https://aijobs.net/job/1364959-manager-data-engineer-aws-databricks/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Aijobs.net"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/2851598743965270016?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://www.recruit.net/job/data-engineer-aws-databricks-platform-jobs/50A2CBFD15AB79EE?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Recruit.net"
          }
         ],
         "Blend360",
         "Company Description\n\nBlend is a premier AI services provider, committed to co-creating meaningful impact for its clients through the power of data science, AI, technology, and people. With a mission to fuel bold visions, Blend tackles significant challenges by seamlessly aligning human expertise with artificial intelligence. The company is dedicated to unlocking value and fostering innovation for its clients by harnessing world-class people and data-driven strategy. We believe that the power of people and AI can have a meaningful impact on your world, creating more fulfilling work and projects for our people and clients. For more information, visit www.blend360.com\n\nJob Description\n\nWe are seeking a seasoned Data Engineering Manager with 8+ years of experience to lead and grow our data engineering capabilities. This role demands strong hands-on expertise in Python, SQL, Spark, and advanced proficiency in AWS and Databricks. As a technical leader, you will be responsible for architecting and optimizing scalable data solutions that enable analytics, data science, and business intelligence across the organization.\n\nKey Responsibilities:\n• Lead the design, development, and optimization of scalable and secure data pipelines using AWS services such as Glue, S3, Lambda, EMR, and Databricks Notebooks, Jobs, and Workflows.\n• Oversee the development and maintenance of data lakes on AWS Databricks, ensuring performance and scalability.\n• Build and manage robust ETL/ELT workflows using Python and SQL, handling both structured and semi-structured data.\n• Implement distributed data processing solutions using Apache Spark/PySpark for large-scale data transformation.\n• Collaborate with cross-functional teams including data scientists, analysts, and product managers to ensure data is accurate, accessible, and well-structured.\n• Enforce best practices for data quality, governance, security, and compliance across the entire data ecosystem.\n• Monitor system performance, troubleshoot issues, and drive continuous improvements in data infrastructure.\n• Conduct code reviews, define coding standards, and promote engineering excellence across the team.\n• Mentor and guide junior data engineers, fostering a culture of technical growth and innovation.\n\nQualifications\n\nRequirements\n• 8+ years of experience in data engineering with proven leadership in managing data projects and teams.\n• Expertise in Python, SQL, Spark (PySpark), and experience with AWS and Databricks in production environments.\n• Strong understanding of modern data architecture, distributed systems, and cloud-native solutions.\n• Excellent problem-solving, communication, and collaboration skills.\n• Prior experience mentoring team members and contributing to strategic technical decisions is highly desirable.",
         {
          "posted_at": "3 days ago",
          "schedule_type": "Full-time"
         },
         [
          "3 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJNYW5hZ2VyIERhdGEgRW5naW5lZXIgLSBBV1MgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJsZW5kMzYwIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJxelhlVGQtWWc2ekE5U3R4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=qzXeTd-Yg6zA9StxAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_x2OPQvCMBBAce1PcLpNkNoURQfFQVH8ACcFx3JpjjQa70qSof4g_6fV5cF708s-g2x9QUZLAXaYEPZsHVNvE9jcr_-mg6ufsQ9n0RAJQ92AMBxErKfhqkmpjUulYvSFjQmTq4taXkqYtHTqITr-UMUGA7UeE1XTedkVLdvxaOuJzWxRgmM4vg0F1GhyuJFHtv1XDic2Dr816jqRqAAAAA&shmds=v1_AdeF8KjB09fTQmZGocZSKjJ3x436LxkG8VsacVRcD1YoflacNg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=qzXeTd-Yg6zA9StxAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c1f32f11d4cca51f4fc/images/56fc5e069c9f2d48e9cafb9aee8ab0386edd45db1d4eb8a4eb227fed4079aa51.png",
         "Manager Data Engineer - AWS Databricks",
         "SmartRecruiters Job Search"
        ],
        [
         [
          {
           "link": "https://insightglobal.com/jobs/find_a_job/illinois/chicago/data-engineer-intl-india-eor-6fb570f8/job-420771/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Insight Global"
          },
          {
           "link": "https://builtin.com/job/corp-tech-im-data-engineer-advanced/6460507?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://www.hirist.tech/j/novastrid-data-engineer-apache-sparkkafkajava-1486693?ref=red_old&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/proclink-data-engineer-looker-snowflake-db-at-proclink-4232324074?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://in.jooble.org/jdp/6463234110532246560?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://in.bebee.com/job/c37234d3474727c2ac29e31d23869997?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://in.expertini.com/jobs/job/data-engineer-pune-hyderabad-bangalore-gurgaon-hyderabad-oncorre-inc-4187-11583867/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Expertini"
          },
          {
           "link": "https://getmereferred.com/job-listing/sw-realtime-data-engineer-iii-data-streaming-ncr-voyix-hyderabad-2-to-5-years-experience-964f4074-1dd3-4725-944b-56208e38609a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Getmereferred.com"
          }
         ],
         "Insight Global",
         "- In this role, you will be building data pipeline solutions by designing, adopting, and applying big data strategies and architectures. You must have knowledge of scalable system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration, data transformation, data modeling, and data cleansing.\n- The Sr. Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for business stakeholders.\n- Design and develop a logical data model for a large multi-tenant application with intent to facilitate consistency and efficiency.\n- Collaborate with analysts and internal clients to understand the goals of the overall system data architecture, and further develop on business needs/issues.\n- Test/troubleshoot problems and conduct root cause analysis.\n- Own communications with technical contacts at software vendors to escalate software issues, recommend enhancements, and find sustainable workarounds to issues.\n- Work in tandem with analysts and other stakeholders to develop and execute necessary processes and controls around the flow of data to meet data governance standards.\n- Verify accuracy of table changes and data transformation processes\n- Deliver fully tested code prior to prod-deployment when appropriate.\n- Recommend and implement enhancements that address system performance requirements, streamline processes and improve data integrity.\n- Create sound technical documentation and train peer developers on this documentation as development completes.\n- Additional duties as assigned to ensure company success.\nThe compensation for this role varies depending on depth and length of experience, and can range from 27-36 LAKH.\n\nWe are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         {
          "posted_at": "4 days ago",
          "schedule_type": "Full-time and Contractor"
         },
         [
          "4 days ago",
          "Full-time and Contractor"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElOVEwgSW5kaWEgLSBFT1IgNmZiNTcwZjgiLCJjb21wYW55X25hbWUiOiJJbnNpZ2h0IEdsb2JhbCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiZUtoaVo5cWozWC1wLXlBVEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Hyderabad, Telangana, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=eKhiZ9qj3X-p-yATAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_yXOwQqCQBCAYbr6CJ3mHKYSWFHQKTEjCsK7zOq4bmwzsrsHe6cesqLLD9_tj96z6HDEgFCwNkzkoLrWF6i4MwhLKG53WPcq32T99suzKPCErh1AGEoRbWm-H0IY_S5NvbeJ9gGDaZNWnqkwKZnShyj_S-MHdDRaDNSs8mxKRtaLZcXe6CFAaUWhBcNwenXkUGEXQ00WWSNj_D_6AJgewVCvAAAA&shmds=v1_AdeF8KjtnRae7jcc6Wnu7qoLy9twbkVc7hyFD6P1jKcU7Ff-aA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=eKhiZ9qj3X-p-yATAAAAAA%3D%3D",
         null,
         "Data Engineer INTL India - EOR 6fb570f8",
         "Insight Global"
        ],
        [
         [
          {
           "link": "https://jobs.sanofi.com/en/job/hyderabad/r-and-d-data-engineer/2649/24948624128?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Sanofi"
          },
          {
           "link": "https://insightglobal.com/jobs/find_a_job/job-416162?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Insight Global"
          },
          {
           "link": "https://zoetis.wd5.myworkdayjobs.com/pt-BR/zoetis_intl/job/Hyderabad/Data-Engineer---Testing_JR00018487/apply/autofillWithResume?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://cutshort.io/job/Big-Data-Engineer-Hyderabad-Jobdost-lJsfLSlo?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Cutshort"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/big-data-engineer-methodhub-software-pvt-ltd-JV_IC2865319_KO0,17_KE18,44.htm?jl=1009753191511&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://wellfound.com/jobs/3298010-product-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Wellfound"
          },
          {
           "link": "https://www.shine.com/jobs/data-engineer-hyd-noida/arrise-solutions-india-pvt-ltd/17050236?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://www.simplyhired.co.in/job/4hTLBb8k83gtypVXlPanXweOFnq7S_xenP6fxizzQ6cQjRbm4VtNnQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          }
         ],
         "Sanofi",
         "Position Title: R&D Data Engineer\n\nAbout the Job\n\nAt Sanofi, we’re committed to providing the next-gen healthcare that patients and customers need. It’s about harnessing data insights and leveraging AI responsibly to search deeper and solve sooner than ever before. Join our R&D Data & AI Products and Platforms Team as an R&D Data Engineer\n\nand you can help make it happen.\n\nWhat you will be doing:\n\nSanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives.\n\nThe R&D Data & AI Products and Platforms Team is a key team within R&D Digital, focused on developing and delivering Data and AI products for R&D use cases. This team plays a critical role in pursuing broader democratization of data across R&D and providing the foundation to scale AI/ML, advanced analytics, and operational analytics capabilities.\n\nAs an R&D Data Engineer, you will join this dynamic team committed to driving strategic and operational digital priorities and initiatives in R&D. You will work as a part of a Data & AI Product Delivery Pod, lead by a Product Owner, in an agile environment to deliver Data & AI Products. As a part of this team, you will be responsible for the design and development of data pipelines and workflows to ingest, curate, process, and store large volumes of complex structured and unstructured data. You will have the ability to work on multiple data products serving multiple areas of the business.\n\nOur vision for digital, data analytics and AI\n\nJoin us on our journey in enabling Sanofi’s Digital Transformation through becoming an AI first organization. This means:\n• AI Factory - Versatile Teams Operating in Cross Functional Pods: Utilizing digital and data resources to develop AI products, bringing data management, AI and product development skills to products, programs and projects to create an agile, fulfilling and meaningful work environment.\n• Leading Edge Tech Stack: Experience build products that will be deployed globally on a leading-edge tech stack.\n• World Class Mentorship and Training: Working with renown leaders and academics in machine learning to further develop your skillsets.\n\nWe are an innovative global healthcare company with one purpose: to chase the miracles of science to improve people’s lives. We’re also a company where you can flourish and grow your career, with countless opportunities to explore, make connections with people, and stretch the limits of what you thought was possible. Ready to get started?\n\nMain Responsibilities:\n\nData Product Engineering:\n• Provide input into the engineering feasibility of developing specific R&D Data/AI Products\n• Provide input to Data/AI Product Owner and Scrum Master to support with planning, capacity, and resource estimates\n• Design, build, and maintain scalable and reusable ETL / ELT pipelines to ingest, transform, clean, and load data from sources into central platforms / repositories\n• Structure and provision data to support modeling and data discovery, including filtering, tagging, joining, parsing and normalizing data\n• Collaborate with Data/AI Product Owner and Scrum Master to share Progress on engineering activities and inform of any delays, issues, bugs, or risks with proposed remediation plans\n• Design, develop, and deploy APIs, data feeds, or specific features required by product design and user stories\n• Optimize data workflows to drive high performance and reliability of implemented data products\n• Oversee and support junior engineer with Data/AI Product testing requirements and execution\n\nInnovation & Team Collaboration:\n• Stay current on industry trends, emerging technologies, and best practices in data product engineering\n• Contribute to a team culture of of innovation, collaboration, and continuous learning within the product team\n\nAbout You:\n\nKey Functional Requirements & Qualifications:\n• Bachelor’s degree in software engineering or related field, or equivalent work experience\n• 3-5 years of experience in data product engineering, software engineering, or other related field\n• Understanding of R&D business and data environment preferred\n• Excellent communication and collaboration skills\n• Working knowledge and comfort working with Agile methodologies\n\nKey Technical Requirements & Qualifications:\n• Proficiency with data analytics and statistical software (incl. SQL, Python, Java, Excel, AWS, Snowflake, Informatica)\n• Deep understanding and proven track record of developing data pipelines and workflows\n\nWhy Choose Us?\n• Bring the miracles of science to life alongside a supportive, future-focused team\n• Discover endless opportunities to grow your talent and drive your career, whether it’s through a promotion or lateral move, at home or internationally\n• Enjoy a thoughtful, well-crafted rewards package that recognizes your contribution and amplifies your impact\n• Take good care of yourself and your family, with a wide range of health and wellbeing benefits including high-quality healthcare, prevention and wellness programs\n\nPursue Progress. Discover Extraordinary.\n\nProgress doesn’t happen without people – people from different backgrounds, in different locations, doing different roles, all united by one thing: a desire to make miracles happen. You can be one of those people. Chasing change, embracing new ideas and exploring all the opportunities we have to offer. Let’s pursue progress. And let’s discover Extraordinary together.\n\nAt Sanofi, we provide equal opportunities to all regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or gender identity.\n\nWatch our ALL IN video and check out our Diversity Equity and Inclusion actions at sanofi.com!",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJSXHUwMDI2RCBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiU2Fub2ZpIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJWUzRuREMyZ2hRajk3SVhDQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=VS4nDC2ghQj97IXCAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXEOw7CMAwAULH2CExeQAiVBCGxwFrEZwT2ykndNCjYVZyhXIIzI97wqu-sWt2XDTRYEE4cIhNl2MBNHChh9gMIw1kkJJofh1JGPVirmkzQgiV64-VthcnJZF_i9F-rA2YaExZqd_vtZEYO68UDWfoIkeHy6Sijw66GJyXkgIw1XLmL-AOLzZUukQAAAA&shmds=v1_AdeF8KiUiGHMq1ccmErcIZrh8u6TiBmwaiEY0ku0I3erwamrWQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=VS4nDC2ghQj97IXCAAAAAA%3D%3D",
         null,
         "R&D Data Engineer",
         "Sanofi"
        ],
        [
         [
          {
           "link": "https://careers.fedex.com/data-engineer-senior-ii/job/P25-195813-1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "FedEx Careers"
          },
          {
           "link": "https://www.kitjob.in/job/149927346/software-engineer-senior-pi604-hyderabad/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "Federal Express Corporation AMEA",
         "Responsible for supporting the development and maintenance of business intelligence and analytics systems, implementing data warehouse architecture, enabling self-service data exploration, designing data models, implementing data security strategies, optimizing production processes, providing user training and support, and overseeing project management activities as a Data Engineer to facilitate data-driven decision-making and generate insights for business stakeholders.\n\n1. Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making.\n2. Implement of business intelligence and analytics systems, ensuring alignment with business requirements.\n3. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets.\n4. Enable self-service data exploration capabilities for users to analyze and visualize data independently.\n5. Develop reporting and analysis applications to generate insights from data for business stakeholders.\n6. Design and implement data models to organize and structure data for analytical purposes.\n7. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information.\n8. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability.\n9. Assist in training and support to users on business intelligence tools and applications.\n10. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions.\n\nEducation: Bachelors’ degree or equivalent in Computer Science, MIS, or similar discipline.\nAccreditation: Specific business accreditation for Business Intelligence.\n\nExperience: Relevant work experience in data engineering based on the following number of years:\nAssociate: Prior experience not required\nStandard I: Two (2) years\nStandard II: Three (3) years\nSenior I: Four (4) years\nSenior II: Five (5) years\n\nKnowledge, Skills and Abilities\n• Fluency in English\n• Analytical Skills\n• Accuracy & Attention to Detail\n• Numerical Skills\n• Planning & Organizing Skills\n• Presentation Skills\n\nPreferred Qualifications:\n\nPay Transparency:\n\nPay:\n\nAdditional Details:\n\nFedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n\nAll qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\nOur Company\n\nFedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World’s Most Admired Companies by \"Fortune\" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\nOur Philosophy\n\nThe People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\nOur Culture\n\nOur culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970’s. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today’s global marketplace.",
         {
          "posted_at": "1 day ago",
          "schedule_type": "Full-time"
         },
         [
          "1 day ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVNlbmlvciBJSSIsImNvbXBhbnlfbmFtZSI6IkZlZGVyYWwgRXhwcmVzcyBDb3Jwb3JhdGlvbiBBTUVBIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJhZDU5RmU0Vl94QzdTVGFqQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=ad59Fe4V_xC7STajAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xWNMQrCQBAAsc0TrLaWeBHBRgsJGjWClfZhkyyXk3P3uL0ifso3mjRTDTPZb5GZMyaEiq1jorh-EjuJUNewhru0oISxG0AYriLW0_IwpBR0XxSq3lhNmFxnOvkUwtTKWLyl1RmNDhgpeEzUbHeb0QS2q-OFeorooRpDJFU4SQwSp8Y0KB9VCY7h9p2dFvscXuSRLTLmUHPv8A_4kYajsQAAAA&shmds=v1_AdeF8KjaG6aMORYGf4051X3suZ-qy95n3Jiwr0ohUmRB9vMyoQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=ad59Fe4V_xC7STajAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c1f32f11d4cca51f4fc/images/56fc5e069c9f2d486126bca3dea35dd097fe2d1a2d8eb4211d8e7c9b3a358a8c.png",
         "Data Engineer-Senior II",
         "FedEx Careers"
        ],
        [
         [
          {
           "link": "https://careers.thomsonreuters.com/us/en/job/JREQ188466/Lead-Data-Engineer-Snowflake-PowerBi?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Thomson Reuters Careers"
          },
          {
           "link": "https://careers.spglobal.com/jobs/312785?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "S&P Global Apply"
          },
          {
           "link": "https://www.tide.co/careers/lead-data-engineersnowflake-dbt-6039463003/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Tide"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/lead-data-engineer-snowflake-dbt-careers-at-tide-JV_IC2865319_KO0,32_KE33,48.htm?jl=1009341668378&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.themuse.com/jobs/thomsonreuters/lead-data-engineersnowflakepowerbi?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "The Muse"
          },
          {
           "link": "https://www.simplyhired.co.in/job/JYBWkXhp61Q0w6JFgWr0APlCESuwOqQe2Xf_ccRFbUHZhbBYmzmXGQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://www.antaltechjobs.in/job/lead-data-scientist-r-247933?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Antal Tech Jobs"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/lead-data-engineer-snowflake-powerbi-at-thomson-reuters-4203795186?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          }
         ],
         "Thomson Reuters",
         "Want to be part of the Data & Analytics organization, whose strategic goal is to create a world-class Data & Analytics company by building, embedding, and maturing a data-driven culture across Thomson Reuters.\n\nAbout The Role\nWe are looking for a highly motivated individual with strong organizational and technical skills for the position of Lead Data Engineer/ Data Engineering Manager (Snowflake). You will play a critical role working on cutting edge of Data Engineering and analytics, leveraging predictive models, machine learning and generative AI to drive business insights and facilitating informed decision-making and help Thomson Reuters rapidly scale data-driven initiatives.\n\nEffectively communicate across various levels, including Executives, and functions within the global organization.\nDemonstrate strong leadership skills with ability to drive projects/tasks to delivering value\nEngage with stakeholders, business analysts and project team to understand the data requirements.\nDesign analytical frameworks to provide insights into a business problem.\nExplore and visualize multiple data sets to understand data available and prepare data for problem solving.\nDesign database models (if a data mart or operational data store is required to aggregate data for modeling).\n\nAbout You\nYou're a fit for the Lead Data Engineer/ Data Engineering Manager (Snowflake), if your background includes:\nQualifications: B-Tech/M-Tech/MCA or equivalent\nExperience: 7-9 years of corporate experience\nLocation: Bangalore, India\nHands-on experience in developing data models for large scale data warehouse/data Lake – Snowflake, BW\nMap the data journey from operational system sources through any transformations in transit to its delivery into enterprise repositories (Warehouse, Data Lake, Master Data, etc.)\nEnabling on the overall master and reference data strategy, including the procedures to ensure the consistency and quality of Finance reference data.\nExperience across ETL, SQL and other emerging data technologies with experience in integrations of a cloud-based analytics environment\nBuild and refine end-to-end data workflows to offer actionable insights\nFair understanding of Data Strategy, Data Governance Process\nKnowledge in BI analytics and visualization tools: Power BI, Tableau\n\n#LI-NR1\n\nWhat’s in it For You?\n• Hybrid Work Model: We’ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\n• Flexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\n• Career Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow’s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\n• Industry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\n• Culture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\n• Social Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\n• Making a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\n\nAbout Us\n\nThomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\n\nWe are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\n\nAs a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\n\nWe also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here.\n\nLearn more on how to protect yourself from fraudulent job postings here.\n\nMore information about Thomson Reuters can be found on thomsonreuters.com.",
         {
          "posted_at": "13 days ago",
          "schedule_type": "Full-time"
         },
         [
          "13 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIoU25vd2ZsYWtlLFBvd2VyQmkpIiwiY29tcGFueV9uYW1lIjoiVGhvbXNvbiBSZXV0ZXJzIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJzSFl5TXBoOXlWbDl4R0pBQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India (+1 other)",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=sHYyMph9yVl9xGJAAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXMOw6CQBAA0NhyBKsp1SAYExu1Mho_sTBKTwYYl9VlhuysAc_kJdXmlS_6DKL1mbCCLQaEHRvLRH50Y-nuDp8UX6Qjv7FjmMJJClBCX9YgDHsR42i4qkNodZmmqi4xGjDYMimlSYWpkD59SKF_cq3RU-swUD5fzPqkZTNJsloa_WVXegXyCpbh8K7IY4FVDBk5ZIOMMRy5svgFQsAuQq4AAAA&shmds=v1_AdeF8Kh8-j-XzHUufgizyl-nmbADY2mWLIPoo7WgtBtGnVAKsw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=sHYyMph9yVl9xGJAAAAAAA%3D%3D",
         null,
         "Lead Data Engineer(Snowflake,PowerBi)",
         "Thomson Reuters Careers"
        ],
        [
         [
          {
           "link": "https://mycareer.verizon.com/jobs/r-1056569/engr-ii-data-engineering/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Verizon Careers"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/engr-ii-data-engineering-verizon-JV_IC2833209_KO0,24_KE25,32.htm?jl=1009742802532&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.simplyhired.co.in/job/7ab9InpGyrsCrGtIuQOt5AEX2zo92xBczrSyhf0x_v6SUgfGwEzs9A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/engr-ii-data-engineering-at-verizon-4234705104?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://flexa.careers/jobs/verizon-engr-ii-data-engineering-682498987cb2829cff4713e1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Flexa Careers"
          },
          {
           "link": "https://analyticshiring.com/job-detail/engr-ii-data-science-verizon-1-3-years-1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Analytics Hiring"
          }
         ],
         "Verizon",
         "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat you’ll be doing…\n\nWe are looking for data engineers who can work with world class team members to help drive telecom business to its full potential . We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n\nAs a data engineer you will be collaborating with business product owners , coaches , industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n\nUnderstanding the business requirements and the technical design.\n\nWorking on Data Ingestion, Preparation and Transformation.\n\nDeveloping data streaming applications.\n\nDebugging the production failures and identifying the solution.\n\nWorking on ETL/ELT development.\n\nWhere you'll be working:\n\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nWhat we’re looking for...\n\nYou’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n\nYou’ll need to have:\n\nBachelor’s degree or one or more years of work experience.\n\nExperience with Data Warehouse concepts and Data Management life cycle.\n\nExperience in any DBMS\n\nExperience in Shell scripting, Spark, Scala.\n\nKnowledge in GCP/BigQuery.\n\nEven better if you have:\n\nTwo or more years of relevant experience.\n\nAny relevant Certification on ETL/ELT developer.\n\nCertification in GCP-Data Engineer.\n\nAccuracy and attention to detail.\n\nGood problem solving, analytical, and research capabilities.\n\nGood verbal and written communication.\n\nExperience presenting to and influence stakeholders.\n\n#AI&D\n\nWhere you’ll be working\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJJZ05hWTZ2NWxkNUFrcWRyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India (+1 other)",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=IgNaY6v5ld5AkqdrAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBCAYVz7CE43OUjbiOCiq6J1FtdyaY40Jd6VXIbqg_i8xuWHf_mq76oyF_YJuq45Y0YoE5goBfbQwF0sKGEaRhCGq4iPtD6NOc96NEY1tl4z5jC0g7yMMFlZzCRW_-l1xERzxEz9_rBb2pn9dvMs9qdggeH2dpTQoqvhQRHZI2MNHbuAP7q54GCZAAAA&shmds=v1_AdeF8Kiuwef0H887FylI9GPjfrbykKYgIDyHUa7VFIs0XgrQfA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=IgNaY6v5ld5AkqdrAAAAAA%3D%3D",
         null,
         "Engr II-Data Engineering",
         "Verizon Careers"
        ],
        [
         [
          {
           "link": "https://www.epam.com/careers/job-listings/job.epamgdo_blte2153bda103f7b14_en-us_Hyderabad_India.senior-data-engineer-data-integration_hyderabad_india?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "EPAM"
          },
          {
           "link": "https://careers.epam-poland.pl/careers/job-listings/job.epamgdo_bltb8357bde43ea0988_en-us_Hyderabad_India.senior-data-engineer_hyderabad_india?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "EPAM"
          },
          {
           "link": "https://www.themuse.com/jobs/epamsystems/senior-data-engineer-data-integration-1fd4c1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "The Muse"
          },
          {
           "link": "https://careers.epam.cn/job-listings/job.epamgdo_blte2153bda103f7b14_en-us_Hyderabad_India.senior-data-engineer-data-integration_hyderabad_india?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Careers.epam.cn"
          },
          {
           "link": "https://in.bebee.com/job/40a4d1721b584eb315ac658dd0f0d8de?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://jobs.fintechaustralia.org.au/companies/epam-systems/jobs/46250365-senior-data-engineer-data-integration?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "FinTech Australia Job Board"
          }
         ],
         "EPAM Systems",
         "EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.\n\nOur company is looking for an experienced Senior Data Engineer to join our team.\n\nAs a Senior Data Engineer, you will be working on a project that focuses on data integration and ETL for cloud-based platforms. You will be responsible for designing and implementing complex data solutions, ensuring that the data is accurate, reliable, and easily accessible.\n\nRESPONSIBILITIES\n• Design and implement complex data solutions for cloud-based platforms\n• Develop ETL processes using SQL, Python, and other relevant technologies\n• Ensure that data is accurate, reliable, and easily accessible for all stakeholders\n• Collaborate with cross-functional teams to understand data integration needs and requirements\n• Develop and maintain documentation, including technical specifications, data flow diagrams, and data mappings\n• Monitor and optimize data integration processes for performance and efficiency, ensuring data accuracy and integrity\n\nREQUIREMENTS\n• Bachelor's degree in Computer Science, Electrical Engineering, or a related field\n• 5-8 years of experience in data engineering\n• Experience with cloud-native or Spark-based ETL tools such as AWS Glue, Azure Data Factory, or GCP Dataflow\n• Strong knowledge of SQL for data querying and manipulation\n• Experience with Snowflake for data warehousing\n• Experience with cloud platforms such as AWS, GCP, or Azure for data storage and processing\n• Excellent problem-solving skills and attention to detail\n• Good verbal and written communication skills in English at a B2 level\n\nNICE TO HAVE\n• Experience with ETL using Python\n\nWE OFFER\n• Opportunity to work on technical challenges that may impact across geographies\n• Vast opportunities for self-development: online university, knowledge sharing opportunities globally, learning opportunities through external certifications\n• Opportunity to share your ideas on international platforms\n• Sponsored Tech Talks & Hackathons\n• Unlimited access to LinkedIn learning solutions\n• Possibility to relocate to any EPAM office for short and long-term projects\n• Focused individual development\n• Benefit package\n• Health benefits\n• Retirement benefits\n• Paid time off\n• Flexible benefits\n• Forums to explore beyond work passion (CSR, photography, painting, sports, etc.)",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAtIERhdGEgSW50ZWdyYXRpb24iLCJjb21wYW55X25hbWUiOiJFUEFNIFN5c3RlbXMiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZnZkJvSVM5OFhDSUw3NWNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India",
         "Data Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=VgfBoIS98XCIL75cAAAAAA%3D%3D&hl=en-CA&shndl=37&shmd=H4sIAAAAAAAA_yWOuwrCQBBFsc0nWE2tMSuCjYIgGHyAIMQ-TJJhs7KZCTtbmF_yK12xOXAOt7jZZ5YdKmInAU4YEUq2jokCrP5-5Ug2YHTCKd2kASUMbQ_JzyLW03zfxzjqzhhVX1iNadwWrQxGmBp5m5c0-kOtPQYaPUaqN9v1uxjZLpbl43iHatJIg4JjuEwdBWywy-FJHtkiY55udA6_8FxceK0AAAA&shmds=v1_AdeF8KipGi_N-DHgUjXB42f5_RXP26KCycnrS-oh3qg8MfFCoQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=VgfBoIS98XCIL75cAAAAAA%3D%3D",
         null,
         "Senior Data Engineer - Data Integration",
         "EPAM"
        ],
        [
         [
          {
           "link": "https://www.glassdoor.com/job-listing/python-developer-%E2%80%93-telegram-bot-integration-and-excel-automation-sanga-and-associates-equidote-JV_KO0,64_KE65,94.htm?jl=1009786902391&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          }
         ],
         "SANGA & ASSOCIATES - EQUIDOTE",
         "Job Title:\n\nPython Developer – Telegram Bot Integration & Excel Automation\n\nJob Description:\n\nWe are seeking a skilled and detail-oriented Python Developer to help automate the process of sending structured trade updates from an Excel file to a Telegram channel using a Telegram bot.\n\nThis is a freelance / part-time project with the potential for ongoing work based on performance.\n\nResponsibilities:\n• Read data from an Excel file that is regularly updated using Python.\n• Format and send messages to a Telegram channel or group using the Telegram Bot API.\n• Implement conditions to filter or trigger messages based on specific columns (e.g., P&L thresholds).\n• Ensure the messages are well-formatted and synchronized.\n• (Optional) Schedule the script to run at regular intervals (e.g., every 5 mins or on update).\n• Ensure error handling, avoid duplicate messages, and maintain clean logs.\n\nRequired Skills:\n• Strong experience with Python scripting\n• Proficiency in using pandas for Excel/CSV handling\n• Working knowledge of the Telegram Bot API\n• Experience with HTTP requests (requests library)\n• Ability to format dynamic messages (Markdown/HTML for Telegram)\n• (Optional but a plus) Familiarity with scheduling tools like schedule, cron jobs, or apscheduler\n\nNice to Have:\n• Understanding of stock market data or options trading (for better context)\n• Experience integrating with trading APIs or using TradingView alerts\n• Basic knowledge of Excel automation or VBA\n\nProject Details:\n• Project Type: One-time setup, with possible ongoing maintenance\n• Location: Remote (India preferred)\n• Start Date: Immediate\n\nHow to Apply:\n\nPlease apply with:\n• A short summary of your experience with Python + Telegram Bots\n• A link to any relevant projects or GitHub repos\n• Your expected rate and estimated time to complete the task\n\nJob Type: Freelance\n\nBenefits:\n• Health insurance\n• Provident Fund\n• Work from home\n\nSchedule:\n• Day shift\n\nSupplemental Pay:\n• Performance bonus\n• Yearly bonus\n\nWork Location: Remote",
         {
          "posted_at": "1 day ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Contractor",
          "work_from_home": "true"
         },
         [
          "1 day ago",
          "Work from home",
          "Contractor",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBUZWxlZ3JhbSBCb3QgSW50ZWdyYXRpb24gXHUwMDI2IEV4Y2VsIEF1dG9tYXRpb24iLCJjb21wYW55X25hbWUiOiJTQU5HQSBcdTAwMjYgQVNTT0NJQVRFUyAtIEVRVUlET1RFIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNBNXdGSEplRUV3c2lQVWJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Anywhere",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=cA5wFHJeEEwsiPUbAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWOwQqCQBRFaesntHorF0EaQZtaTSliiyw0WspoDzXGeeK8wnb9Q__Tx_QljbS5cC4H7nU-E-dyfHJNGgJ8oKIOe_i-3pChwqqXLWyJIdY8AjdWcyEcSlQg7kztv5rDngowKPuyBssRUaVwuqmZO7P2fWOUVxm2cumV1PqksaDBv1FhxshNLXvslGTMl6vF4HW6mrmpOETCrok0TXaxyMLU7oSncxwkWQiNtqeujfwBo0pJkMEAAAA&shmds=v1_AdeF8KgGojAhM22tqdWqg5a2PXRmpF45HPM2Fl94icndi1v0Vg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=cA5wFHJeEEwsiPUbAAAAAA%3D%3D",
         null,
         "Python Developer – Telegram Bot Integration & Excel Automation",
         "Glassdoor"
        ],
        [
         [
          {
           "link": "https://in.indeed.com/viewjob?jk=7922169ef6f54d0f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://in.jooble.org/rjdp/3736138824119121405?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://www.shine.com/jobs/python-developer-remote/algorithmx/17085610?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://www.jobaaj.com/job/vinnovate-solutions-python-developer-trainee-01-remote-greater-kolkata-area-0-to-15-years-486676?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobaaj"
          },
          {
           "link": "https://in.bebee.com/job/8babfc440260f4e75574129a6af82ad7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/remote-python-developer-17852-at-turing-4253263895?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://jobs.journalducoin.com/job/remote-python-developer-17852-StPRG6NYIb-at-turing-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Journal Du Coin Jobs"
          },
          {
           "link": "https://in.expertini.com/jobs/job/remote-python-developer-17852-narela-turing-3121-11626181/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Expertini"
          }
         ],
         "Xpress Health",
         "Job Title: Python Developer\nLocation: Remote\nSalary: Up to ₹12 LPA (based on experience and skillset)\nExperience: 3–6 years (preferred)\nEmployment Type: Full-time\n\nAbout Xpress Health\n\nXpress Health is a healthtech platform transforming clinical workforce management by connecting healthcare professionals to facilities in real time. With operations across Ireland and rapid expansion underway, we’re solving critical staffing challenges in the healthcare sector using advanced technology and automation.\n\nRole Overview\n\nWe are looking for a Python Developer with solid experience in backend development and building scalable, secure systems. You will work closely with cross-functional teams to build backend services, automate operations, and power data-driven features in our real-time healthcare staffing platform. If you're passionate about solving real-world problems through code, this role is for you.\n\nKey Responsibilities\n• Design, develop, and maintain backend services and APIs using Python (preferably with frameworks like Django or Flask).\n• Build scalable systems for real-time scheduling, user management, and analytics.\n• Integrate third-party APIs and internal services securely and efficiently.\n• Work with databases (SQL and NoSQL) to design efficient schemas and queries.\n• Optimize performance and ensure system reliability under scale.\n• Collaborate with frontend, product, and QA teams to deliver complete features.\n• Write clean, maintainable, and well-documented code.\n• Participate in code reviews, system design discussions, and architecture planning.\n\nRequirements\n• 3–6 years of professional experience with Python backend development.\n• Strong knowledge of Django, Flask, or other web frameworks.\n• Proficient in working with relational databases (MySQL/PostgreSQL) and REST APIs.\n• Experience with Git, CI/CD pipelines, and working in an Agile environment.\n• Strong debugging, testing, and problem-solving skills.\n• Good communication and ability to collaborate with remote teams.\n\nPreferred Qualifications\n• Experience in healthcare, staffing, or enterprise SaaS platforms.\n• Familiarity with containerization tools like Docker and orchestration platforms (Kubernetes).\n• Exposure to cloud platforms like AWS, GCP, or Azure.\n• Knowledge of async programming and task queues (e.g., Celery, Redis).\n• Experience working with frontend teams using React/Vue (a plus).\n\nWhat We Offer\n• Competitive salary up to ₹12 LPA, depending on experience.\n• A mission-driven environment working on meaningful, real-world problems.\n• Opportunity to shape a rapidly scaling healthtech product.\n• Flexible work culture with remote options and learning opportunities.\n• Collaborative, cross-functional team with international exposure.\n\nBe part of a product-first team building real-time tech that supports critical healthcare systems. Join Xpress Health as a Python Developer !!!\n\nJob Type: Full-time\n\nPay: Up to ₹1,200,000.00 per year\n\nBenefits:\n• Paid time off\n• Work from home\n\nLocation Type:\n• Remote\n\nSchedule:\n• Evening shift\n• Fixed shift\n• Monday to Friday\n• UK shift\n\nApplication Question(s):\n• What is your current and expected CTC?\n• Are you currently working? If yes, what is your notice period?\n\nExperience:\n• Python : 5 years (Required)\n\nWork Location: Remote",
         {
          "posted_at": "2 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time",
          "work_from_home": "true"
         },
         [
          "2 days ago",
          "Work from home",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIC0gUmVtb3RlIiwiY29tcGFueV9uYW1lIjoiWHByZXNzIEhlYWx0aCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJGWjc0LW4wM2NjU2xGWVVOQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Anywhere",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=FZ74-n03ccSlFYUNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKPQoCMRBAYWz3ABZWUwsmi2CjreBPJVZ2yyQOSSSbCZlB1hN4bdfm8RWv-y66_vbRyAWO9KbMlRps4E4jK824sgMhbD7CvJyYQ6bVIapW2Vsrkk0QRU3eeB4tF3I82Rc7-WeQiI1qRqVhu-snU0tYLx-1kQicCbNGSAUu5ZnwB4FBw8-KAAAA&shmds=v1_AdeF8KjVUwvIIq3UN363ahkdRLQt5573bng1Exift4YiGZG0IQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=FZ74-n03ccSlFYUNAAAAAA%3D%3D",
         null,
         "Python Developer - Remote",
         "Indeed"
        ],
        [
         [
          {
           "link": "https://careers.hitachi.com/jobs/16246064-full-stack-developer-python-slash-react-js?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hitachi Careers"
          },
          {
           "link": "https://www.shine.com/jobs/full-stack-developer-python-react-js/hitachi-careers/17318841?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://builtin.com/job/full-stack-developer-appian-java/6385724?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://startup.jobs/full-stack-developer-python-react-js-hitachi-vantara-corporation-6843457?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Startup Jobs"
          }
         ],
         "Hitachi Careers",
         "Our Company\n\nWe're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n\nOur group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n\nImagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n\nThe team\n\nWe are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n\nThe role: Full Stack Developer/Specialist\n\nResponsibilities:\n• Design, develop, and maintain applications.\n• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n• Optimise applications for performance, scalability, and user experience.\n• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n• Provide training and support to end-users and IT staff on functionalities and best practices.\n• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n• Participate in project planning, execution, and post-implementation support.\n• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n• What you'll bring\nQualifications:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n• Expertise in development and customisation.\n• Proficiency in Python and React JS\n• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n• Experience with RESTful APIs and web services.\n• Excellent problem-solving skills and attention to detail.\n• Strong communication and collaboration skills.\n• Ability to work independently and as part of a team in a fast-paced environment.\n• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n• Prior experience in building AI applications is a plus.\n• Prior experience with automation tools like UIPath is a plus.\n• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\nPreferred Skills\n• Certification in\n• Experience with other low-code/no-code platforms.\n• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n• Experience with Agile development methodologies.\nAbout us\n\nWe're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n\n#LI-MS3\n\nChampioning diversity, equity, and inclusion\n\nDiversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n\nHow we look after you\n\nWe help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\nWe're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",
         {
          "posted_at": "4 days ago",
          "schedule_type": "Full-time"
         },
         [
          "4 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=gHUO0z59qIifBLiVAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNPQ7CMAxAYbH2BIjJIyDRICQWGBhA_HRCcIDKDVYTCHEUG1RuwnEpy1u-4RXfQbHZv0KAq6J9wI7eFDhRhvH5o44jGLgQWoXqOoEZVNyAEGbroLcDcxtotHaqSVbGiISyFUX1trT8NByp4c7cuZF_anGYKQVUqhfLeVem2E6HR9-fnYdtb5QFfIRTvHn8AQLWDNibAAAA&shmds=v1_AdeF8KiBbHp5HoJs2LSbE900z5gBevA-S4BK5odgvTSsP5in2w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=gHUO0z59qIifBLiVAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c217223f45d80e1a169/images/7ca52d7311167a4b971ff2d0af582580ff6d1717ced792605a7b527753be5d04.png",
         "Full Stack Developer (Python / React JS)",
         "Hitachi Careers"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/python-developer-role-at-pitangent-analytics-and-technology-solutions-pvt-ltd-4253593146?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.shine.com/jobs/python-developer-role/pitangent-analytics-and-technology-solutions-pvt-ltd/17329373?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://in.trabajo.org/job-2927-738d23fc657433716975fb6ed90bb670?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          },
          {
           "link": "https://www.prepintro.com/job/python-developer-role-pitangent-analytics-and-technology-solutions-pvt-ltd/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "PrepIntro"
          }
         ],
         "Pitangent Analytics and Technology Solutions Pvt. Ltd.",
         "Overview\n\nPi tangent Group of Companies is an ISO:9001: 2015 Certified, CMMIL-3, Award winning Software Development Company in Eastern India. It caters to areas like AI/ML to Web development to SAAS engineering. The Group is actively seeking out talented, enthusiastic Python Developers as a value addition to the growing team at Kolkata.\n\nKey Responsibilities\n• Design and develop robust backend applications using Python.\n• Collaborate with front-end developers to integrate user-facing elements with server-side logic.\n• Implement RESTful APIs for seamless communication between server and client.\n• Write reusable, testable, and efficient code following best practices.\n• Manage and optimize multiple databases and data storage solutions.\n• Perform unit and integration testing to ensure software reliability.\n• Participate in code reviews and maintain version control in Git.\n• Gather and analyze user requirements to provide optimal solutions\n• Contribute to project documentation and specifications.\n• Collaborate with QA engineers to troubleshoot and resolve issues.\n• Maintain quality assurance processes to ensure best practices are enforced.\n• Engage in agile development practices, participating in sprints and meetings.\n• Mentor junior developers and provide guidance as needed.\n\nRequired Qualifications\n• Bachelor's degree in computer science or related field.\n• 1-2 yrs of experience in Python development.\n• Strong understanding of Django or Flask web frameworks.\n• Proficient with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB).\n• Experience with version control systems, preferably Git.\n• Solid understanding of RESTful API design principles.\n• Familiarity with front-end technologies (HTML, CSS, JavaScript).\n• Experience with containerization tools such as Docker.\n• Strong communication and teamwork abilities.\n• Familiarity with cloud services (AWS, Azure) is a plus.\n• Understanding of security principles and best practices.\n• Experience with Agile/Scrum methodologies.\n• Proven ability to manage multiple tasks and meet deadlines.\n\nSkills: agile,communication,nosql,scrum,aws lambda,docker,sql,azure,html,javascript,mongodb,css,django,aws,git,postgresql,restful apis,python,flask",
         {
          "posted_at": "2 days ago",
          "schedule_type": "Full-time"
         },
         [
          "2 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIFJvbGUiLCJjb21wYW55X25hbWUiOiJQaXRhbmdlbnQgQW5hbHl0aWNzIGFuZCBUZWNobm9sb2d5IFNvbHV0aW9ucyBQdnQuIEx0ZC4iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiLUNHQ2RNdUVNQ2V4X3F2MkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=-CGCdMuEMCex_qv2AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLsQrCMBCAYVz7CE43iyYiuOgggiCKQ1H3krZHEol3oXeW9p18SOvyLx9_8Z0Vy3LUwAQn7DFxxg7unBBWcOUaBF3XBJj4zOwTzvdBNcvOWpFkvKjT2JiG35YJax7si2v5p5LgOszJKVab7XowmfziUEZ15JEUjuTSOL0Cjlp4YhOIE_sRHpw-GpkEyl4N3LQ1EAku1Eb3A0HKY1ivAAAA&shmds=v1_AdeF8KiDxLDc0gfRIaXbzvY14S2B0nHEtBpAt1Jh8t-Bcylozw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=-CGCdMuEMCex_qv2AAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c217223f45d80e1a169/images/7ca52d7311167a4bb7fce52e6e3afcd63870bf70216e5e0689db804b95796ad4.jpeg",
         "Python Developer Role",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/python-and-groovy-framework-developer-at-aptita-4252137179?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.adzuna.in/details/5260946971?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Adzuna"
          },
          {
           "link": "https://in.bebee.com/job/98bb991ce97889cb619dc6d5e2e2f6cb?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/5148408131285417984?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://indianjob24h.in/ko/snmp-and-framework-developer-job100578?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indianjob24h.in"
          },
          {
           "link": "https://jobindian.in/it/snmp-and-framework-developer-job100578?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobindian.in"
          }
         ],
         "Aptita",
         "Urgent Hiring!!!\n\nRole : Python and Groovy Framework Developer\n\nMandatory Skills: Python, Appium, Groovy, Git\n\nExperience: 3 to 8 Years\n\nLocation: Bengaluru\n\nContract - 1Year\n\nJob Description:\n\nQualifications\n\n Bachelor’s or master’s degree in Computer Science, Software Engineering, or a\n\nrelated field\n\n 6 to 8 years of relevant experience in quality assurance, with a proven track record in\n\nWebKit or browser engine testing, including team leadership responsibilities.\n\n Proficiency in CI/CD and test automation frameworks such as Appium, and scripting\n\nlanguages like Python, or Shell.\n\nJob Overview\n\nWe are seeking a dynamic and experienced Lead Python & Groovy Framework developer\n\nto join our team You will be part of a fast-paced, Agile development team and work on a\n\nvariety of projects, from building new tools and solutions to improving existing ones.\n\nIn this role, you will have the chance to grow your skills and take your career to the next\n\nlevel. We offer a supportive, challenging, and exciting work environment, with\n\nopportunities for professional development, training, and advancement.\n\nIf you are a Python & Groovy Framework Developer Engineer with a passion for\n\ntechnology and a drive to continuously improve processes, we want to hear from you!\n\nIf you are passionate about browser engine technologies, performance optimization, and\n\nleadership, we encourage you to apply!\n\nPrimary Skills:\n\n Strong experience in Python Framework development, with the ability to automate\n\nand optimize processes using Jenkins Pipeline script\n\n Good knowledge in Groovy scripting\n\n Expertise in CI/CD tools such as Jenkins, CircleCI, or GitLab\n\n Good understanding of Appium.\n\nStrong Problem solving and debugging skills.\n\n Excellent communication and collaboration skills, both with technical and non-\n\ntechnical stakeholders\n\n Version Control: Familiarity with version control systems such as Git for reviewing\n\nchanges and ensuring test coverage.\n\n Communication: Strong communication and collaboration skills for working with\n\ncross-functional teams.\n\n Agile Methodologies: Experience with Agile Scrum methodologies\n\nNotice Period: Immediate- 30 Days\n\nEmail to : sharmila.m@aptita.com\n\n·",
         {
          "posted_at": "2 days ago",
          "schedule_type": "Contractor"
         },
         [
          "2 days ago",
          "Contractor"
         ],
         "eyJqb2JfdGl0bGUiOiJQeXRob24gYW5kIEdyb292eSBGcmFtZXdvcmsgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQXB0aXRhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Imx1ckt5NFVRSi0wb3JkMi1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=lurKy4UQJ-0ord2-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFy7uDvdLJiI4KIuglh08g_KpT2aaJoLuaO2_-BHq8ubXvVZVKfHrJ4TYOqgLszjDNeCA725vOBCI0XOVGADd3YghKX18Os1cx9pdfSqWQ7WikTTi6KG1rQ8WE7keLJPdvKnEY-FckSlZrffTianfr08Zw2KEBLcUhfwC2SmAnCPAAAA&shmds=v1_AdeF8Kg1C23WGoyyYiHbcJJj5H-E6x8Uv3rxsbNlyZq6zp-KYw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=lurKy4UQJ-0ord2-AAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c217223f45d80e1a169/images/7ca52d7311167a4bbcf9496c8d47b104687ec43b62a624497a9763b0ee63de1f.jpeg",
         "Python and Groovy Framework Developer",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://careers.allianz.com/global/en/job/74590/AI-Python-Developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Allianz"
          },
          {
           "link": "https://www.foundit.in/job/python-ai-developer-antal-international-network-india-34929408?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Foundit.in"
          },
          {
           "link": "https://jobgether.com/offer/663670de270b7304e3768e52-python-ai-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobgether"
          },
          {
           "link": "https://in.bebee.com/job/5931ed0d09d410418c22498c411007aa?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://unstop.com/jobs/ai-python-developer-allianz-technology-india-1502774?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Unstop"
          },
          {
           "link": "https://www.jobaaj.com/job/mogi-i-o-ott-podcast-short-video-apps-for-you-python-ai-developer-india-4-to-8-years-528567?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobaaj"
          },
          {
           "link": "https://en.wizbii.com/company/allianz/job/ai-python-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "WIZBII Jobs"
          },
          {
           "link": "https://in.trabajo.org/job-2927-f587076fe15195988499f1600c4dde9c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          }
         ],
         "Allianz Insurance",
         "We are seeking a skilled AI Developer proficient in Python to support various AI use cases within our dynamic team. The ideal candidate will possess a strong understanding of Python programming and a basic familiarity with PyTest. This role will also provide opportunities to work with cloud computing technologies, particularly Azure, and develop APIs.\n\nKey Responsibilities:\n• Utilize advanced Python skills to support and enhance AI use cases, ensuring the efficient and effective implementation of AI solutions.\n• Develop and execute comprehensive test scripts using PyTest to ensure the reliability and accuracy of AI models and applications.\n• Collaborate with cross-functional teams, including data scientists, engineers, and product managers, to integrate AI capabilities into existing systems and workflows.\n• Analyze and optimize AI algorithms for performance and scalability, ensuring they meet business requirements.\n• Stay updated with the latest advancements in AI and machine learning technologies and apply them to improve existing solutions.\n\nRequirements:\n\nMust-Have\n• Python: Advanced proficiency with extensive experience in writing efficient, maintainable, and scalable code. Demonstrated ability to solve complex problems using Python.\n• PyTest: Basic experience in writing and executing unit tests, with a fundamental understanding of test-driven development practices.\n\nGood-to-Have\n• Cloud Computing: Familiarity with cloud platforms and services, with hands-on experience in deploying and managing AI applications in a cloud environment.\n• Azure: Experience with Microsoft Azure, particularly in using its AI and machine learning services.\n• APIs: Knowledge of designing, developing, and consuming APIs, particularly RESTful APIs, for integrating AI solutions with other systems and applications\n\nAbout Allianz Technology\n\nAllianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 13,000 employees located in 22 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry. We oversee the full digitalization spectrum – from one of the industry’s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n\nD&I statement\n\nAllianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, race or ethnicity, age, nationality, religion, disability, or philosophy of life. Join us. Let´s care for tomorrow.",
         {
          "posted_at": "24 days ago",
          "schedule_type": "Full-time"
         },
         [
          "24 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJBSSBQeXRob24gRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQWxsaWFueiBJbnN1cmFuY2UiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQkg5Z0tqUFM2TjBGeExOb0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=BH9gKjPS6N0FxLNoAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCMBCAYVz7CJ1uLtiI4KJTQSh18g3KJR5JJN6FXJTq7ntblx8-_ua7abphguu7BmE404uSZCqwhYtYUMLiAqxnFPGJ2lOoNevRGNXUe61Yo-udPIwwWVnMXaz-M2vAQjlhpXl_2C19Zt-1Q0oR-QMT67MgO4LIK24Rf5NpGzqIAAAA&shmds=v1_AdeF8KiayGId8JpU67KYeQvNrYxf2FRgPNJcuJ1hpC22xryV5g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=BH9gKjPS6N0FxLNoAAAAAA%3D%3D",
         null,
         "AI Python Developer",
         "Allianz"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/developer-angular-python-azure-at-the-value-maximizer-4253121431?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://startup.jobs/developer-angular-python-azure-qode-6889841?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Startup Jobs"
          },
          {
           "link": "https://www.kitjob.in/job/155673037/n135-developer-angular-python-azure-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          },
          {
           "link": "https://www.prepintro.com/job/developer-angular-python-azure-the-value-maximizer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "PrepIntro"
          }
         ],
         "The Value Maximizer",
         "About the Role :\n\nAs a Senior Developer, you will play a critical role in designing, implementing, and optimizing IoT applications, Salesforce development, and web solutions. This role is ideal for experienced developers eager to lead projects, mentor junior team members, and shape technical strategies across diverse platforms and technologies. You will be an important part of our international project teams, will work closely with team members from the headquarters and if qualified can lead international software development projects.\n\nKey Responsibilities :\n• Central role and expert for development of IoT frontend applications with Angular, Capacitor, and PrimeNG\n• Design and implement IoT backend systems using C#, .Net 8.0, and the Azure platform\n• Management and optimization of CI/CD pipelines using Azure DevOps and Bicep\n• Drive advanced Salesforce development efforts, including customizations, integrations, and automation\n• Manage and enhance the SharePoint Online intranet platform\n• Architect and implement Power Platform solutions tailored to business needs\n• Develop and maintain complex web applications using Django (Python) and PHP\n• Supervise website development and optimization with Typo3, PHP, and SolR, collaborating closely with the marketing department\n• Monitor and ensure the performance, compliance, and SEO optimization of global websites and portals\n\nKey Requirements :\n• 6-8 years of experience\n• Strong expertise in Angular, Python, and C#\n• Advanced experience with CI/CD pipelines and version control systems (e.g., Azure DevOps or Git)\n• Proficiency in web technologies such as PHP, MySQL, and modern frameworks\n• In-depth knowledge of Salesforce, Azure cloud solutions, and SharePoint Online\n• Proven ability to lead technical projects, mentor team members, and collaborate across departments\n• Excellent problem-solving skills, attention to detail, and a proactive approach to innovation\n• Proficiency in German is an advantage but not a requirement. Candidates must, however, have a minimum of B2-level English proficiency to communicate effectively within the international teams and our headquarters",
         {
          "posted_at": "4 days ago",
          "schedule_type": "Full-time"
         },
         [
          "4 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEZXZlbG9wZXItIEFuZ3VsYXIsIFB5dGhvbiBcdTAwMjYgQXp1cmUiLCJjb21wYW55X25hbWUiOiJUaGUgVmFsdWUgTWF4aW1pemVyIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRteXhMclAxTG5oaWFJZEVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=TmyxLrP1LnhiaIdEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXIsQrCMBAAUFz7CeJwk4PYRAQHdSoIoiA4iGu51iOJpLmQS6X2Q_xedXnDKz6TYnugF3mOlEqoguk9piVc39lygDlUY58ISjhzA0KYWgu_PzIbT9O9zTnKTmsRr4xkzK5VLXeaAzU86Cc38qcWi4mix0z1erMaVAxmMbtZgjv6nuCCg-vcSAlcgFN4OPwCbU-Do5kAAAA&shmds=v1_AdeF8KjYHF0YxKON-lXUVNHhoroFrj1uQG5NbM40EoMOFsKyGA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=TmyxLrP1LnhiaIdEAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c217223f45d80e1a169/images/7ca52d7311167a4b2adaf1e3020b5d6aed08e2ec45299500cc6be0568fcaacfb.png",
         "Developer- Angular, Python & Azure",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://in.indeed.com/viewjob?jk=24b103e5e62e2d5e&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/freelance-python-developer-gbim-technologies-JV_KO0,26_KE27,44.htm?jl=1009784121958&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          }
         ],
         "GBIM Technologies Pvt.Ltd.",
         "We’re Hiring – Freelance Python Developer (Experienced)\nWe are currently looking for a freelance Python developer who brings hands-on experience and can help us build and debug efficient backend systems, automate processes, and solve critical issues with speed and precision.\nKey Expertise Required:\n\nPython (Backend Development)\n\nWeb Scraping & Data Extraction\n\nWeb Automation\n\nFlask | Pandas | ETL\n\nAWS (Basic to Intermediate)\n\nGoogle / Meta / LinkedIn / Third-Party API Integration\n\nProblem-solving mindset – quick in identifying & fixing bugs/errors\n\nIf you are a solution-driven individual with a solid track record in handling Python-based backend projects and automation tasks, we’d love to connect with you!\nPlease DM or share your portfolio/work samples at [hr@gbim.com / sarabjeet.mann@gbim.in ].\n#PythonDeveloper #FreelanceOpportunity #WebScraping #Automation #BackendDevelopment #Flask #AWS #APIIntegration #DataExtraction #HiringNow #LinkedInJobs\n\nJob Type: Full-time\n\nPay: ₹500.00 - ₹10,000.00 per hour\n\nLocation Type:\n• Remote\n\nSchedule:\n• Day shift\n• Monday to Friday\n\nWork Location: Remote\n\nSpeak with the employer\n+91-XXXXXXXXXX",
         {
          "posted_at": "3 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time",
          "work_from_home": "true"
         },
         [
          "3 days ago",
          "Work from home",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkdCSU0gVGVjaG5vbG9naWVzIFB2dC5MdGQuIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNtMXFFOGh4Y2xUN3Z1ck9BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Anywhere",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCMBAAUFz7CU4HboKJCi66FbFUFDq4lzQ9kkjMhdxR6of4v-rylld9FtXuUhCjSRahe4unBGecMFLGAhu40gCMplgPv2mIXMTlyYtkPmrNHJVjMRKssvTSlHCgWT9p4D89e1MwRyPY7w_bWeXk1qumbu_wQOsTRXIBGbpJ1E1GBSFBm8ZgvjqIsEWYAAAA&shmds=v1_AdeF8KjwVw1gvcgbXG9h7aNO0NWe7x2POcgCQEOiGyU2sC4Mww&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=cm1qE8hxclT7vurOAAAAAA%3D%3D",
         null,
         "Freelance Python Developer",
         "Indeed"
        ],
        [
         [
          {
           "link": "https://jobs.ashbyhq.com/dehazelabs/3fc045c8-c43d-4523-b258-80e35b2930ed/application?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs"
          },
          {
           "link": "https://in.indeed.com/viewjob?jk=584b970ed8dd3bcc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/junior-python-developer-altheory-technologies-pvt-ltd-JV_KO0,23_KE24,53.htm?jl=1009767622150&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://wellfound.com/jobs/3280338-junior-python-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Wellfound"
          },
          {
           "link": "https://growflex.in/apply/2133/junior-python-developer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Growflex"
          },
          {
           "link": "https://jobs.weekday.works/capricorn-identity-services-private-limited-junior-python-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs.weekday.works"
          },
          {
           "link": "https://talentsjobs.in/job-detail?job-id=20667&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Talents Jobs"
          },
          {
           "link": "https://www.kitjob.in/job/148905208/junior-python-developer-oa591-kozhikode/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "Dehazelabs",
         "Location: Onsite, Kokapet, Hyderabad, Telangana.\n\nJob Type: Full-Time\n\nAbout Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n\nResponsibilities:\n• Develop and maintain Python applications and services\n• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n• Write clean, efficient, and well-documented code\n• Troubleshoot, debug, and optimize applications for performance and scalability\n• Participate in code reviews and maintain coding standards\n• Stay up-to-date with the latest industry trends and technologies\n\nRequirements:\n• Ability to code in Python and SQL\n• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n• Knowledge of version control systems (e.g., Git)\n• Excellent problem-solving skills and attention to detail\n• Strong communication and teamwork abilities.\n\nPreferred Qualifications:\n• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n• Knowledge of RESTful APIs and microservices architecture.",
         {
          "qualifications": "No degree mentioned",
          "salary": "₹216K–₹420K a year",
          "schedule_type": "Full-time",
          "work_from_home": "true"
         },
         [
          "₹216K–₹420K a year",
          "Work from home",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Anywhere",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=LYbcWfU4skOE2uhgAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQoCMQwAUFxvdXPKLNiK4KLrgXiTf3CkNVwrNSlNlNPRL1eXt7zus-jc8OAsDS4vS8LQ05OKVGqwgUECKGGLCX5zEpkKrY7JrOrBe9XiJjW0HF2UuxemILO_SdA_oyZsVAsajbv9dnaVp_Wyp4RvKhgUMsOZrxm_tV-eZYUAAAA&shmds=v1_AdeF8KiLjAS30ie4ZRkW1kEru4d1GwgKEP1yzRfISnv7NJRViQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=LYbcWfU4skOE2uhgAAAAAA%3D%3D",
         null,
         "Junior Python Developer",
         "Jobs"
        ],
        [
         [
          {
           "link": "https://aijobs.net/job/1369160-det-senior-gig-python-developer-gdsnf02/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Aijobs.net"
          }
         ],
         "EY",
         "At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. Senior Python Developer – Client – Merck, H and M, TIAA, MINT Job Description Bachelor’s or master’s degree with 3+ years of strong Python development experience Design, develop, and maintain high-performance web applications using Python and related frameworks. Strong understanding of Python OOPs, Data type, Data Structure and algorithm , Exception handling, Decarotor , Generator, Iterator, Automation. Strong understanding of Python Libraries (Pandas, TensorFlow, Numpy, SciPy) Experience in Cloud Azure / AWS Develop, optimize, and manage complex APIs (RESTful or GraphQL). Collaborate with cross-functional teams to define, design, and ship new features. Troubleshoot and resolve advanced technical issues in development and production environments. Conduct technical evaluations of new tools and frameworks, recommending their adoption when appropriate. Stay ahead of emerging trends in Python development, ensuring the team remains at the forefront of innovation. Advanced proficiency in Python and frameworks like Django,Flask, or FastAPI. Good understanding of Database Postgres / MySQL & ORM Library i.e. SQL Alchemy/ any ORM libraries Understanding of Code Repository tools i.e. GIT , SVN Strong understanding of DevOps principles(Docker, Kubernetes and microservices) EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",
         {
          "posted_at": "18 hours ago",
          "salary": "51,038–94,785 a year",
          "schedule_type": "Full-time"
         },
         [
          "18 hours ago",
          "51,038–94,785 a year",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJERVQtU2VuaW9yIEdJRyBQeXRob24gRGV2ZWxvcGVyLUdEU05GMDIiLCJjb21wYW55X25hbWUiOiJFWSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ4Mk5pUFZHVVJ3cjczaVE1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "Python Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=Python+Developer&htidocid=x2NiPVGURwr73iQ5AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFzrHzjdLLQpBRcFp9ZQBxHq4lSSeiQpMRdyQeov-NXq8qZXfFbFse1u5YDBUQLZS7i-s6UALb7QU8RUyna4nOoGSjiTBkaVJgu_IYmMx83B5hx5LwSzrwxnld1UTfQUFFDTImbS_GdkqxJGrzKOza5eqhjMdt3dwQXow8OpL0RpNWeNAAAA&shmds=v1_AdeF8KgY457QONkoomgpIdSrLr3hsEX7tOwgdKEcCHxjKfOGyQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Python+Developer&htidocid=x2NiPVGURwr73iQ5AAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c217223f45d80e1a169/images/7ca52d7311167a4b433854c3181b5ba3b78dcb1e1d229c20c1d60ca9f1f91f65.png",
         "DET-Senior GIG Python Developer-GDSNF02",
         "Aijobs.net"
        ],
        [
         [
          {
           "link": "https://aijobs.net/job/1354875-informatica-etl-developer-agile-dev-team-member-iv/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Aijobs.net"
          },
          {
           "link": "https://startup.jobs/informatica-etl-developer-agile-dev-team-member-iv-capgemini-6880043?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Startup Jobs"
          },
          {
           "link": "https://in.expertini.com/jobs/job/informatica-etl-developer-agile-dev-team-member-iv-hyderabad-capgemini-1831-15741907/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Expertini"
          },
          {
           "link": "https://in.bebee.com/job/86ca67461e96d6d76960fcd37185e33f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://www.recruit.net/job/informatica-etl-developer-agile-dev-jobs/19527C8BDF086A2E?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Recruit.net"
          },
          {
           "link": "https://in.trabajo.org/job-3396-8964efb4f35b96b661630e25828096b5?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/119202638297300992?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://superjobss.com/jobs/b138ae46-4be7-4444-a591-9ed1308f8e03?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs"
          }
         ],
         "Capgemini",
         "The ideal candidate will be responsible for designing, developing, and deploying scalable ETL processes using Informatica PowerCenter to support our data warehousing and analytics initiatives. You will collaborate with business and technical stakeholders to ensure high data quality, availability, and performance. Key Responsibilities:Design, develop, and maintain ETL workflows and mappings using Informatica PowerCenter or Informatica Intelligent Cloud Services (IICS).Extract, transform, and load data from various source systems (e.g., SQL Server, Oracle, flat files, cloud APIs) into data warehouses or operational data stores.Optimize ETL performance, conduct tuning, and ensure error handling and logging.Collaborate with data architects and analysts to understand data requirements and deliver high-quality data solutions.Work with QA teams to support data validation and testing efforts.Support data integration, migration, and transformation initiatives.Document ETL processes, data flows, and job schedules.Monitor daily ETL jobs and resolve production issues in a timely manner.RequirementsBachelor’s degree in Computer Science, Information Systems, or a related field (or equivalent work experience).3+ years of experience with Informatica PowerCenter or Informatica IICS.Strong SQL skills and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).Solid understanding of data warehousing concepts and dimensional modeling.Experience in performance tuning and troubleshooting ETL processes.Hands-on experience with job scheduling tools (e.g., Autosys, Control-M, Tidal).Familiarity with version control systems and DevOps practices.Preferred Qualifications:Experience with cloud data platforms (e.g., Snowflake, AWS Redshift, Azure Synapse).Exposure to data governance and data quality tools.Knowledge of scripting languages (e.g., Shell, Python).Experience working in Agile/Scrum environments.Familiarity with BI tools (e.g., Tableau, Power BI) is a plus. BenefitsThis position comes with competitive compensation and benefits package: Competitive salary and performance-based bonuses Comprehensive benefits package Home Office model Career development and training opportunities Flexible work arrangements (remote and/or office-based) Dynamic and inclusive work culture within a globally known group Private Health Insurance Pension Plan Paid Time Off Training & Development *Note: Benefits differ based on employee level",
         {
          "posted_at": "5 days ago",
          "salary": "42,532–78,988 a year",
          "schedule_type": "Full-time"
         },
         [
          "5 days ago",
          "42,532–78,988 a year",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyOiBBZ2lsZSBEZXYgVGVhbSBNZW1iZXIgSVYiLCJjb21wYW55X25hbWUiOiJDYXBnZW1pbmkiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InFZVlFqNTFEM2VzZ0lZSExBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=qYVQj51D3esgIYHLAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNsQrCMBBAce0nON0ooq0ILnUSFW3RrbiWS3umkeQuJEHqd_mD2uUNb3gv-86yuuKnBIfJdAjn5gYnepMVT6GEgzaWJgENoYM7OUUBqgesoRYFkTB0AwjDRURbmu-HlHwsiyJGm-uYpmjeiSuESclYvETFCW0cMJC3mKjd7jZj7lkvF0f0mpxhA4bh-ukpoMJ-9X9bZI2MK6i4N_gDJid_bLYAAAA&shmds=v1_AdeF8KgutCk9f1YpFiMW9LRjDjlQvTE3YabFZ1dOEi5X0_1qMA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=qYVQj51D3esgIYHLAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9a071e98c03386e2d66cc6420deb3c41916471f950b524ff0c.png",
         "Informatica ETL Developer: Agile Dev Team Member IV",
         "Aijobs.net"
        ],
        [
         [
          {
           "link": "https://careers.spglobal.com/jobs/316835?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "S&P Global Apply"
          },
          {
           "link": "https://cutshort.io/job/Backend-Developer-and-Educator-Mumbai-Hyderabad-School-of-Accelerated-Learning-pE4X2gRB?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Cutshort"
          },
          {
           "link": "https://www.shine.com/jobs/senior-backend-developer-c-and-azure/waferwire-cloud-technologies/17025642?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://www.simplyhired.co.in/job/isE_hgbSc52mYanYlWpjDELjiugHsHZI7DhttbF2oMfer94PvJqoUg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/senior-etl-and-backend-developer-salesforce-%C2%A0-at-s-p-global-4238484393?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.jointaro.com/jobs/sandp-global/senior-etl-and-backend-developer-salesforce/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Taro"
          },
          {
           "link": "https://in.bebee.com/job/bb31aed9683e64c520547d8fb2da3cd3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/senior-etl-and-backend-developer-salesforce-s-and-p-global-JV_IC2865319_KO0,43_KE44,58.htm?jl=1009760759098&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          }
         ],
         "S&P Global",
         "About the Role:\n\nGrade Level (for internal use):\n10\n\nTitle: Senior ETL and Backend Developer (Salesforce)\n\nJob Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n\nThe Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n\nThe team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n\nThe Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n\nResponsibilities:\n• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\nMonitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\nQualifications:\n\nBasic Qualifications:\n• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n• A minimum of 8+ years of experience in software engineering & Architecture.\n• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n• A minimum of 3+ years of Salesforce development, administration/Integration.\n• Proficiency in Informatica PowerCenter and other ETL tools.\n• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n• Experience with Salesforce integration and administration.\n• Proficiency in backend development languages (e.g., Java, Python, C#).\n• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n• Excellent problem-solving skills and attention to detail.\n• Ability to work independently and as part of a team.\n• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n\nPreferred Qualifications:\n• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n• Experience with other ETL tools and data integration platforms.\n• Informatica Certified Professional\nSalesforce Certified Administrator or Developer\n• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n• Excellent problem solving, analytical and technical troubleshooting skills.\n• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat’s In It For You?\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nWe're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n\nOur benefits include:\n• Health & Wellness: Health care coverage designed for the mind and body.\n• Flexible Downtime: Generous time off helps keep you energized for your time on.\n• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring and Opportunity at S&P Global:\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\n-----------------------------------------------------------\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n-----------------------------------------------------------\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n\nJob ID: 316835\nPosted On: 2025-06-03\nLocation: Hyderabad, Telangana, India",
         {
          "posted_at": "19 days ago",
          "schedule_type": "Full-time"
         },
         [
          "19 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjVGNUtpak5fRjROQXhDcERBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India (+1 other)",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=5F5KijN_F4NAxCpDAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXFsQrCMBAAUFz7CU43iZXaiuCim1Sq4iC0e7kkZ1uNdyEJUn_Kb1SX95LPJClr4kE8HJoLIBvYo37Q75JeZMWRh3mNlsJNvKYUlnAWBYHQ6x6EoRLpLE13fYwubIsiBJt3IWIcdK7lWQiTkrG4iwp_2tCjJ2cxUrverMbccbdI69kVKisKLQwMx7chjwpNBg1Z5A4ZMzixGfALm-qj5LEAAAA&shmds=v1_AdeF8KhDGHxAXJEjotU-fQxkvoBBqCk4nXlFRQEfCz-nQy1hiw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=5F5KijN_F4NAxCpDAAAAAA%3D%3D",
         null,
         "Senior ETL and Backend Developer (Salesforce)",
         "S&P Global Apply"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/etl-developer-at-zensar-technologies-4222824321?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.simplyhired.co.in/job/fx7vQKu-Lzh03WQHb6X6_XAGbcjI7i6bHcT89FwidlwXvLKBVndzPw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://www.adzuna.in/details/5192914688?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Adzuna"
          },
          {
           "link": "https://www.recruit.net/job/etl-developer-jobs/CFB50BF71002C882?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Recruit.net"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/6425154711267049472?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          }
         ],
         "Zensar Technologies",
         "Job Description\n\nPrimary Skill Set\n• ETL Informatica\n• SQL\n• Unix\n• Realtime Data Integration (CDC) using Power Exchange (Can be optional if candidate good in other skills)\n\nGood to Have\n\nExperience on working with Mainframe Databases/files\n\nETL Batch Scheduling tools like TWS/Tidal\n\nRoles & Responsibilities\n\nInformatica PowerCenter, Unix scripting, SQL/PLSQL\n\nKnowledge of Informatica Power Exchange is preferred\n\nExperience With Mainframe Sources/targets Is Preferred\n• Bachelor’s degree in Computer Science or similar field or equivalent work experience.\n• 5-8 years of development experience with Informatica Power Center on Data Integration projects.\n• Strong analytic, problem-solving and organizational skills.\n• Excellent SQL knowledge and ability to write the complex queries keeping performance aspect in mind.\n• Experience with analysis of business requirements, designing and writing technical specifications to design.\n• Hands-on experience to process mainframe files using Informatica Power Exchange.\n• Hands-on experience with UNIX shell scripting.\n• Participate in testing and issue resolution to validate functionality and performance.\n• Hands-on experience on any job scheduling tool, TWS is preferred.\n• Good written and verbal communication skills.\n\nLocation\n\n1 st Preference: Noida\n\n2 nd Preference: Hyderabad\n\n3 rd Preference: Gurgaon",
         {
          "posted_at": "11 days ago",
          "schedule_type": "Full-time"
         },
         [
          "11 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiWmVuc2FyIFRlY2hub2xvZ2llcyIsImFkZHJlc3NfY2l0eSI6Ik1hZGhhdmFyYW0sIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InJTQzlGVjVoRHhXZ1h3QURBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Madhavaram, Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=rSC9FV5hDxWgXwADAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCMBAAUFz7CU43OEltRHHRVRFFt04u5ZoeSSS9C7lQ-il-rrq85VWfRbW6tA8400RREmXYwF16UMJsPQjDVcRFWp58KUmPxqjGxmnBEmxjZTTC1Mts3tLrn049ZkoRC3W7w3ZuErv1_kWsmKEl61miuEAKgeGJg8cJM4717yKyQ8YabjwE_AJS2_7bmwAAAA&shmds=v1_AdeF8Kh9BDoIEsj9vh8KQn3UWTKTBDf_wrqKczqjw9E77y6qlQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=rSC9FV5hDxWgXwADAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9a1ddc478d76ec1ab5d1633c51cfe785421ef417997936cc2b.jpeg",
         "ETL Developer",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://insightglobal.com/jobs/find_a_job/job-411040?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Insight Global"
          },
          {
           "link": "https://www.amgen.jobs/hyderabad-ind/etl-developer/6CCD8EAF358B4D9AB9D502EEFB667993/job/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs"
          },
          {
           "link": "https://www.hirist.tech/j/etl-developer-1472601?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://jobs.smartrecruiters.com/SQUIRCLEITCONSULTINGSERVICESPVTLTD/87728117-etl-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SmartRecruiters Job Search"
          },
          {
           "link": "https://www.simplyhired.co.in/job/VZteELnCIFig_PpewT1H4Xi550ZVJ7LxPCKfGowAKkMlN6qBv9WgZQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://unisys.wd5.myworkdayjobs.com/zh-CN/External/job/Hyderabad-Waverock/ETL-Developer_REQ565515?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://firstdcs.in/jobs/etl-developer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "FirstDCS"
          },
          {
           "link": "https://www.adzuna.in/details/5220807061?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Adzuna"
          }
         ],
         "Insight Global",
         "We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\nResponsibilities:\nLead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\nCollaborate with developers to ensure a smooth transition and integration of the upgraded system.\nEnhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\nProvide support and guidance to the ETL development lead.\nEngage with end-users to ensure their needs are met during and after the upgrade process.\nUtilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\nRequirements:\nMinimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\nProven experience with SQL Server upgrades, particularly from 2012 to 2022.\nStrong SQL Server skills, including building tables, stored procedures, views, and functions.\nExperience with .NET development is highly desirable to be leveraged for other projects\nExcellent problem-solving skills and attention to detail.\nAbility to work effectively in a team environment and communicate with end-users.\nPreferred Qualifications:\nExperience with SQL Server 2016.\nExperience with Oracle\nFamiliarity with the latest features and enhancements in SQL Server 2022.\nStrong understanding of database architecture and migration strategies.\n\nWe are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         {
          "qualifications": "No degree mentioned",
          "schedule_type": "Contractor"
         },
         [
          "Contractor",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJJbnNpZ2h0IEdsb2JhbCIsImNvbXBhbnlfbmFtZSI6Ikluc2lnaHQgR2xvYmFsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=cEmBeIcvfhfX5HK2AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_13NMQ7CMAwAQLH2CUxeWFCbICQWeEApM3vlJFYSlNpVnKG8gw8jVpZbr_vsusPEmmNqMBZxWGCAhzhQwuoTCMMoEgvtb6m1Va_WqhYTtWHL3nhZrDA52exLnP6YNWGltWCj-Xw5bWbleBz-jsxwfweq6DD08KSCHJGxh4lDxi-RVypzlgAAAA&shmds=v1_AdeF8KhmLbrgMzBBwyUeoUQL_4hKnzeejX-h_elIeYvsdlgyPg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=cEmBeIcvfhfX5HK2AAAAAA%3D%3D",
         null,
         "Insight Global",
         "Insight Global"
        ],
        [
         [
          {
           "link": "https://builtin.com/job/etl-developer-hands-microsoft-sql-ssis-etl-and-t-sql-pune-location/2942610?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          }
         ],
         "Fiserv",
         "Calling all innovators – find your future at Fiserv.\n\nWe’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n\nJob Title\n\nETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n\nWork Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n• Participates in client kickoff activities including requirements and data gathering sessions.\n• Ability to analyze and migrate incoming external data into Fiserv solutions.\n• Strong working knowledge of ETL processes and best practices.\n• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n• Documents business processes and identifies opportunities for process redesign.\n• Reports system defects and identifies opportunities for system enhancements.\n• Performs as a technical consulting resource for new clients during the implementation process.\n• Supports several project managers during the pre and post go-live activities\n• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n• Provides technical and analytical guidance to the project team.\n• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n• The position requires working in shifts, 2 PM to 11 PM IST\n\nWhat you will need to have:\n• Minimum of 5 – 10 years of experience in software development.\n• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n• Self-starter who can work effectively, both independently and in a team environment.\n• Strong analytical, organizational, and problem-solving skills.\n• Excellent verbal and written communication skills.\n• Ability to maintain a professional attitude and demeanor in high pressure situations.\n• Ability to multi-task and manage multiple projects simultaneously.\n• Weekend/evening availability and support (10% - 25%).\n\nWhat would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n\nThank you for considering employment with Fiserv. Please:\n• Apply using your legal name\n• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n\nWhat you should know about us:\n\nFiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n\nOur commitment to Diversity and Inclusion:\n\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n\nWarning about fake job posts:\n\nPlease be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n\nAny communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n\nIf you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=j_liFo3VQHqrD7_BAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMuwrCQBQEUGzT2FtNqZKHCDba-opEUGIvm801WYl7w941-GF-oGszxXBmou8oqne3AlsaqOOeHKZHZWsBW5yNdiz88CivRYyyzMsYAccIArcktJghweVtCQVr5U1YJThxBSHldPt_OTA3HU02rfe9rLNMpEsb8QHrVPMrY0sVf7InV_KPu7TKUd8pT_flavFJe9vMx3sj5AYYi9zWRv0AHMymQbcAAAA&shmds=v1_AdeF8KhckDUoX9CilVCOaJqU36weofSX83Pgps7flcqZRoh66g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=j_liFo3VQHqrD7_BAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9a6d0a97f3acb9c752fe1afaa4e525c4c085b61f46d0d02b02.png",
         "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location",
         "Built In"
        ],
        [
         [
          {
           "link": "https://www.hirist.tech/j/epam-etl-developer-ssis-ssrs-1495428?ref=rl&pref=rl&jobPos=4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://in.jooble.org/jdp/777603120491457089?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://in.bebee.com/job/df2976441b528bff274847b68dd6d964?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          }
         ],
         "Swathi V",
         "Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n\nExperience : 5+ Years\n\nLocation : Hyderabad\n\nJob Description :\n\nWe are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n\nYou will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n\nKey Responsibilities :\n\n- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n\n- Develop, deploy, and maintain SSIS packages for ETL processes.\n\n- Work on data warehouse design, development, and maintenance.\n\n- Ensure high performance and reliability of data integration workflows.\n\n- Troubleshoot SQL queries and ETL issues; optimize database performance.\n\n- Collaborate with cross-functional teams to define data solutions.\n\n- Work with complex data structures including JSON and XML.\n\n- Understand and implement different data models (star, snowflake, etc.).",
         {
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IlN3YXRoaSBWIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ2ekQ3VTVqbHNoc2EtN3luQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=vzD7U5jlshsa-7ynAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHQQrCMBBAUdz2CK5mJ0htRBBBV4JFKwpiiluZtEMaiZmQBK2n8arWzef97DvKVuVle4YZlPUJdvQiy57C8FJWUkh5lYOPrCAShqYDdrBn1pbGmy4lH9dCxGgLHRMm0xQNPwU7UtyLB6v4zz12GMhbTHRfLOd94Z2eTuQbU2fgBsbB4dNSQIVtDjVZdBod5lC51uAPq3kl_aIAAAA&shmds=v1_AdeF8KhoyQuQp8opvuZyeZS3uH3V1UEvP3N1O5LDE48Myg1k8Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=vzD7U5jlshsa-7ynAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9ab5436031f86f8495670d9227c5f49d66c2ea8c68acaec26e.png",
         "EPAM - ETL Developer - SSIS/SSRS",
         "Hirist"
        ],
        [
         [
          {
           "link": "https://apna.co/job/gurgaon/etl-developer-1587516655?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Apna"
          }
         ],
         "American Express Global Business Travel",
         "ETL Developer\n\nAmex GBT is a place where colleagues find inspiration in travel as a force for good and – through their work – can make an impact on our industry. We’re here to help our colleagues achieve success and offer an inclusive and collaborative culture where your voice is valued.\n\nWe are looking for an experienced Data ETL Developer / BI Engineer who loves solving complex problems across a full spectrum of data & technologies. You will lead the building effort of GBT's new BI platform and manage the legacy platform to seamlessly support our business function around data and analytics. You will create dashboards, databases, and other platforms that allow for the efficient collection and evaluation of BI data.\n\nWhat You’ll Do on a Typical Day:\n• Design, implement, and maintain systems that collect and analyze business intelligence data.\n• Design and architect an analytical data store or cluster for the enterprise and implement data pipelines that extract, transform, and load data into an information product that helps the organization reach strategic goals.\n• Create physical and logical data models to store and share data that can be easily consumed for different BI needs.\n• Develop Tableau dashboards and features.\n• Create scalable and high-performance data load and management process to make data available near real-time to support on-demand analytics and insights.\n• Translate complex technical and functional requirements into detailed designs.\n• Investigate and analyze alternative solutions to data storing, processing, etc., to ensure the most streamlined approaches are implemented.\n• Serve as a mentor to junior staff by conducting technical training sessions and reviewing project outputs\n• Design & develop, and maintain a data model implementing ETL processes.\n• Manage and maintain the database, warehouse, & cluster with other dependent infrastructure.\n• Work closely with data, products, and another team to implement data analytic solutions.\n• Support production application and Incident management.\n• Help define data governance policies and support data versioning processes\n• Maintain security and data privacy by working closely with the Data Protection Officer internally.\n• Analyze a vast number of data stores and uncover insights\n\nWhat We’re Looking For:\n• Degree in computer sciences or engineering\n• Overall, 3-5 years of experience in data & data warehouse, ETL, and data modeling.\n• 2+ years of experience working and managing large data stores, complex data pipelines, and BI solutions.\n• Strong experience in SQL and writing complex queries.\n• Hands-on experience with Tableau development.\n• Hands-on working experience on Redshift, data modeling, data warehouse, ETL tool, Python, and Shell scripting.\n• Understanding of data warehousing and data modeling techniques\n• Strong data engineering skills on the AWS Cloud Platform are essential.\n• Knowledge of Linux, SQL, and any scripting language\n• Good interpersonal skills and a positive attitude\n• Experience in travel data would be a plus.\n\nLocation\nGurgaon, India\n\nThe #TeamGBT Experience\n\nWork and life: Find your happy medium at Amex GBT.\n• Flexible benefits are tailored to each country and start the day you do. These include health and welfare insurance plans, retirement programs, parental leave, adoption assistance, and wellbeing resources to support you and your immediate family.\n• Travel perks: get a choice of deals each week from major travel providers on everything from flights to hotels to cruises and car rentals.\n• Develop the skills you want when the time is right for you, with access to over 20,000 courses on our learning platform, leadership courses, and new job openings available to internal candidates first.\n• We strive to champion Inclusion in every aspect of our business at Amex GBT. You can connect with colleagues through our global INclusion Groups, centered around common identities or initiatives, to discuss challenges, obstacles, achievements, and drive company awareness and action.\n• And much more!\n\nAll applicants will receive equal consideration for employment without regard to age, sex, gender (and characteristics related to sex and gender), pregnancy (and related medical conditions), race, color, citizenship, religion, disability, or any other class or characteristic protected by law.\n\nClick Here for Additional Disclosures in Accordance with the LA County Fair Chance Ordinance.\n\nFurthermore, we are committed to providing reasonable accommodation to qualified individuals with disabilities. Please let your recruiter know if you need an accommodation at any point during the hiring process. For details regarding how we protect your data, please consult the Amex GBT Recruitment Privacy Statement.\n\nWhat if I don’t meet every requirement? If you’re passionate about our mission and believe you’d be a phenomenal addition to our team, don’t worry about “checking every box;\" please apply anyway. You may be exactly the person we’re looking for!\nExperience Level\nMid Level\n\nMore about this Data ETL Developer / BI Engineer job\n\nAmerican Express Global Business Travel is aggressively hiring for the job profile of Data ETL Developer / BI Engineer at undefined in null locality. Kindly go through the FAQs below to get all answers related to the given job.\n\n1. How much salary can I expect as a Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n\nAns. You can expect a minimum salary of 0 INR. The salary offered will depend on your skills, experience and performance in the interview.\n\n2. What is the eligibility criteria to apply for Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n\nAns. The candidate should have completed undefined degree and people who have 3 to 5 years are eligible to apply for this job. You can apply for more jobs in undefined to get hired quickly.\n\n3. Is there any specific skill required for this job?\n\nAns. The candidate should have undefined skills and sound communication skills for this job.\n\n4. Who can apply for this job?\n\nAns. Both Male and Female candidates can apply for this job.\n\n5. Is it a work from home job?\n\nAns. No, it’s not a work from home job and can’t be done online. You can explore and apply for other work from home jobs in undefined at apna.\n\n6. Are there any charges or deposits required while applying for the role or while joining?\n\nAns. No work-related deposit needs to be made during your employment with the company.\n\n7. How can I apply for this job?\n\nAns. Go to the apna app and apply for this job. Click on the apply button and call HR directly to schedule your interview.\n\n8. What is the last date to apply?\n\nAns. The last date to apply for this job is .\n\nFor more details, download apna app and find Full Time jobs in undefined. Through apna, you can find jobs in 74 cities across India. Join NOW!",
         {
          "posted_at": "6 days ago",
          "schedule_type": "Full-time"
         },
         [
          "6 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVUTCBEZXZlbG9wZXIgLyBCSSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkFtZXJpY2FuIEV4cHJlc3MgR2xvYmFsIEJ1c2luZXNzIFRyYXZlbCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJnY3VjckFjcm5KVmRmdjRxQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=gcucrAcrnJVdfv4qAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOsQrCMBBAce0nON0s2BRBBJ0sLaXi2F0u8UgjaS7kovST_Ezj8uBN71XfTXXqMCP00x06-pDnSAkUtCP0wbpAxfZwYw1CmMwMHGBgtp62lznnKGelRHxtJWN2pja8KA6keVUv1vLHQ2ZMFD1mehyOzVrHYHfNdaHkDAbo15hIBAbPGj20bynV4lPCsgMuwBieDn-GsVkLqwAAAA&shmds=v1_AdeF8KgXR3CZtx-nmpFMr3mb_bKvair4NEYKX_11OW5JMbkBCQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=gcucrAcrnJVdfv4qAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9abbdaf665a0013fc4549112c9395a972e123709ddc360f0b1.jpeg",
         "Data ETL Developer / BI Engineer",
         "Apna"
        ],
        [
         [
          {
           "link": "https://in.indeed.com/viewjob?jk=01e515f9108f20d8&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          }
         ],
         "Renovision Automation Services Pvt.Ltd.",
         "Experience - 6+ yearsWork Mode - HybridJob Summary : We are seeking a skilled Informatica ETL Developer with 5+ years of experience in ETL and Business Intelligence projects. The ideal candidate will have a strong background in Informatica PowerCenter, a solid understanding of data warehousing concepts, and hands-on experience in SQL, performance tuning, and production support. This role involves designing and maintaining robust ETL pipelines to support digital transformation initiatives for clients in manufacturing, automotive, transportation, and engineering domains.Key Responsibilities : - Design, develop, and maintain ETL workflows using Informatica PowerCenter.- Troubleshoot and optimize ETL jobs for performance and reliability.- Analyze complex data sets and write advanced SQL queries for data validation and transformation.- Collaborate with data architects and business analysts to implement data warehousing solutions.- Apply SDLC methodologies throughout the ETL development lifecycle.- Support production environments by identifying and resolving data and performance issues.- Work with Unix shell scripting for job automation and scheduling.Required Skills : - 35 years of hands-on experience with Informatica PowerCenter.- Proficiency in SQL and familiarity with NoSQL platforms.- Experience in ETL performance tuning and troubleshooting.- Solid understanding of Unix/Linux environments and scripting.- Excellent verbal and written communication skills.Preferred Qualifications : - AWS Certification or experience with cloud-based data integration is a plus.- Exposure to data modeling and data governance practices. (ref: hirist.tech)",
         {
          "posted_at": "4 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "4 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyIC0gU1FML1Bvd2VyIENlbnRlciIsImNvbXBhbnlfbmFtZSI6IlJlbm92aXNpb24gQXV0b21hdGlvbiBTZXJ2aWNlcyBQdnQuTHRkLiIsImFkZHJlc3NfY2l0eSI6IlRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlVqTnczajVrcnJpS21VQjdBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=UjNw3j5krriKmUB7AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2OuwrCQBBFsfUTrKYW3RXBRitRESWFr14mcUxW1pmwM0b_zl8zsbkcbnE4_W-vv9rxXdITLRQIm0sGa2ooSk0JxnA-Zv4g75ZXxPa_9pKDEqaiAmHYipSRBovKrNa596rRlWqdzRXy9MKUy8c_JNdurlphojqi0XU6m3xczeVwcSKWJmhofcuXSdfS4plSEwpSODTmMrs5CAwXisglMo5gx7eAP-LlOHnCAAAA&shmds=v1_AdeF8KiGqykFvEjWsWwQECdNjsOUTEn9RtZl-sAMt92jVTfR6w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=UjNw3j5krriKmUB7AAAAAA%3D%3D",
         null,
         "Informatica ETL Developer - SQL/Power Center",
         "Indeed"
        ],
        [
         [
          {
           "link": "https://jobsnew.analyticsvidhya.com/jobs/etl-developer-hyderabad-2-3-years-of-experience/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Analytics Vidhya"
          }
         ],
         "A Client of Analytics Vidhya",
         "Role Summary:\n\n•ETL Developer is responsible for Design and Development of ETL Jobs which follow standards, best practices and are maintainable, modular and reusable.\n\n•ETL Developer will analyze and review complex object and data models and the metadata repository in order to structure the processes and data for better management and efficient access.\n\n•Working on multiple projects, and delegating work to Junior Analysts to deliver projects on time.\n\n•Training and mentoring Junior Analysts and building their proficiency in the ETL process.Job Duties:\n\n•Preparing mapping document to extract, transform, and load data ensuring compatibility with all tables and requirement specifications.\n\n•Experience in ETL system design and development with Talend / Pentaho PDI is essential.\n\n•Create quality rules in Talend.\n\n•Tune Talend jobs for performance optimization.\n\n•Write relational and multidimensional database queries.\n\n•Functional Knowledge of Talend Adminstration Center, Job Servers & Load balancing setup, and all its administrative fucntions.\n\n•Develop, maintain, and enhance unit test suites to verify the accuracy of ETL processes, dimensional data, OLAP cubes and various forms of BI content including reports, dashboards, and analytical models.\n\n•Exposure in Map Reduce components of Talend / Pentaho PDI.\n\n•Creating and deploying Talend / Pentaho custom components is an add-on advantage.\n\nJob Specification:\n\n•BE, B.Tech / MS Degree in Computer Science, Engineering or a related subject.\n\n•Having an experience of 2 – 3+ years.\n\n•Comprehensive understanding and working knowledge in Data Warehouse loading, tuning, and maintenance.\n\n•Proficiency with Talend or related tools like Pentaho Data Integration / Kettle.\n\n•Working knowledge of relational database theory and dimensional database models.\n\n•Ability to write complex SQL database queries.\n\n•Ability to work independently.",
         {
          "salary": "₹1.5M a year",
          "schedule_type": "Full-time"
         },
         [
          "₹1.5M a year",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyLSBIeWRlcmFiYWQgKDItMysgWWVhcnMgb2YgRXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJBIENsaWVudCBvZiBBbmFseXRpY3MgVmlkaHlhIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJYOGlLR3lLZ0p6MDQ5UkdIQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=X8iKGyKgJz049RGHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WOuw6CUBBEY8snWG3pg4fR2EhFlKjE0phYmQVWuOa6S9gbAz_odwmVzTQnc2a878TL0usFDvQhKw21AZz6klrMsYTZOtgs4U7YKsgT0m7ghrigOQSQSQ46oKIGYTiKVJamce1co7soUrVhpQ6dKcJC3pEw5dJFL8l1jIfW2FJj0dFjvV11YcPVIk5gbwe_G8cSRtsPbYWbKesewfD_mQ9XssgVMvpw5tLgD84obTrJAAAA&shmds=v1_AdeF8KhNH4dOhsM_ohqTFHeygbi7_Rs650WHWh26bXMFYZ5jQQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=X8iKGyKgJz049RGHAAAAAA%3D%3D",
         null,
         "ETL Developer- Hyderabad (2-3+ Years of Experience)",
         "Analytics Vidhya"
        ],
        [
         [
          {
           "link": "https://www.jobnet.com.au/in/en/search-jobs-in-Maharashtra,-India/ETL-DEVELOPER-EFD64B886A6B4BB7C2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "JobNet"
          },
          {
           "link": "https://en-in.whatjobs.com/jobs/data-transformation?id=153796242&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "WhatJobs"
          },
          {
           "link": "https://in.expertini.com/jobs/job/etl-developer-borivali-panzer-technologies-pvt-ltd-2950-9852891/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Expertini"
          },
          {
           "link": "https://www.recruit.net/job/etl-developer-jobs/6CD83997F05151D4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Recruit.net"
          }
         ],
         "Luxoft",
         "Project Description:\n\nOur client is an EU subsidiary of a Global Financial Bank working in multiple markets and asset classes.\n\nDWH/ETL developer will work closely with the Development Lead to design, build interfaces and integrate data from a variety from internal and external data sources into the new Enterprise Data Warehouse environment. The ETL Developer will be responsible for developing ETL primarily utilizing Microsoft & Azure technologies within industry recognized ETL standards, architecture, and best practices.\n\nResponsibilities:\n• Act as a technical expert in the designing, coding, unit testing, supporting, and debugging of data warehouse software components in all aspects of SDLC\n• Apply cloud and ETL engineering skills to solve problems and design approaches\n• Troubleshoot and debug ETL pipelines and creating unit tests for ETL pipelines.\n• Assess query performance and actively contribute to optimizing the code\n• Write technical documentation and specifications\n• Support internal audit by submitting required evidence\n• Create reports and dashboards in the BI portal\n• Work with Development Lead, DWH Architect and QA Engineers to plan, implement and deliver best ETL strategies\n• Work with business analysts to understand requirements to create technical design specifications, gaining a sound understanding of business processes for related applications so that integration processes fulfill the end-user requirements\n• Communicate effectively in a collaborative, complex and high performing team environment as per Agile principles\n\nMandatory Skills Description:\n• Proven work experience as an ETL Developer\n• Advanced knowledge of relational databases and dimensional Data Warehouse modelling concepts\n• Good understanding of physical and logical data modeling\n• Very good understanding of modern SaaS/PaaS data solutions in a cost conscious approach\n• Expert level of knowledge of Microsoft Data stack\n• Experience in developing and deploying data oriented solutions in Cloud (Azure/Synapse Analytics/Fabric)\n• Experience in designing and implementing data transformation and ETL layers using Data Factory, Notebooks\n• Experience with PowerBI for report & dashboard creation. PowerQuery and/or DAX is an advantage.\n• Experience in/understanding of Azure Data Lake Storage\n• Knowledge/use of CI/CD tools and principles, preferably Azure DevOps or Bamboo\n• Strong SQL knowledge, able to create complex SQL queries and good understanding of stored procedures, views, indexes, functions, etc.\n• Good working knowledge of at least one Scripting language. Python is an advantage.\n• Experience with GIT repositories and working with branches. GitHub, Azure DevOps or Bitbucket experience are preferable.\n• Ability to troubleshoot and solve complex technical problems\n• Good understanding of software development best practices\n• Working experience in Agile projects; preferably using JIRA\n• Experience in working in high priority projects preferably greenfield project experience\n• Able to communicate complex information clearly and concisely.\n• Able to work independently and also to collaborate across the organization\n• Highly developed problem-solving skills with minimal supervision\n• Understanding of data governance and enterprise concepts preferably in banking environment\n• Verbal and written communication skills in English are essential.\n\nNice-to-Have Skills Description:\n• Microsoft Fabric\n• Snowflake\n• Background in SSIS/SSAS/SSRS\n• Azure DevTest Labs, ARM templates\n• Azure PurView\n• Banking/finance experience",
         {
          "posted_at": "4 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "4 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiTHV4b2Z0IiwiYWRkcmVzc19jaXR5IjoiTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJlZF9YUGZ2UVNVcTZXdXltQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Maharashtra, India",
         "ETL Developer",
         "https://www.google.com/search?ibp=htl;jobs&q=ETL+Developer&htidocid=ed_XPfvQSUq6WuymAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCMBAAUFz7CeJwg5NoI4KLXRVR6uZeLvFsIjEXcqdk9c_V5S2v-Uya-eHaw57eFDlTgRWc2YIQFueBExyZx0jTzqtm2RkjEttRFDW41vHTcCLL1TzYyp9BPBbKEZWGzXZd25zGxax_Vb4rhAQX_D2K14JLOKVbwC8_vPQqhAAAAA&shmds=v1_AdeF8KjMEQiuWVFBnZYKg-t8P774XOcFwvkoOM2R-qPEBeYQMQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=ETL+Developer&htidocid=ed_XPfvQSUq6WuymAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c225abea5b2c7ccab5c/images/b0a1e06c4ca31a9aca07e30835060ab20487250180fd7e64e147a75b89f26c9d.png",
         "ETL Developer",
         "JobNet"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/data-engineer-hadoop-spark-scala-hive-at-visa-4251105891?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://powertofly.com/jobs/detail/2317944?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "PowerToFly"
          },
          {
           "link": "https://www.kitjob.in/job/155049377/bcy532-data-engineer-hadoop-spark-scala-hive-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "Visa",
         "Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n\nMake an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n\nJob Description\n\nTranslate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\n\nGood to have GenAI Exposure and Agentic AI Knowledge.\n\nWork with business partners directly to seek clarity on requirements.\n\nDefine solutions in terms of components, modules, and algorithms.\n\nDesign, develop, document, and implement new programs and subprograms, as well as enhancements, modifications and corrections to existing software.\n\nCreate technical documentation and procedures for installation and maintenance.\n\nWrite Unit Tests covering known use cases using appropriate tools.\n\nIntegrate test frameworks in the development process.\n\nWork with operations to get the solutions deployed.\n\nTake ownership of production deployment of code.\n\nCome up with Coding and Design best practices.\n\nThrive in a self-motivated, internal-innovation driven environment.\n\nAdapt quickly to new application knowledge and changes.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\nQualifications\n\nBasic Qualifications\n\n-Minimum of 6 months of work experience or a Bachelor's Degree\n\nPreferred Qualifications\n\n-Bachelor degree in Computer Science.\n\n-Minimum of 1 plus years of software development experience in Hadoop using\n\nSpark, Scala, Hive.\n\n-Expertise in Object Oriented Programming Language Java, Python.\n\n-Experience using CI CD Process, version control and bug tracking tools.\n\n-Result-oriented with strong analytical and problem-solving skills.\n\n-Experience with automation of job execution, validation and comparison of data\n\nfiles on Hadoop Environment at the field level.\n\n-Experience in leading a small team and being a team player.\n\n-Strong communication skills with proven ability to present complex ideas and\n\ndocument them in a clear and concise way.\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         {
          "posted_at": "5 days ago",
          "schedule_type": "Full-time"
         },
         [
          "5 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChIYWRvb3AsIFNwYXJrLCBTY2FsYSwgSGl2ZSkiLCJjb21wYW55X25hbWUiOiJWaXNhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImVULTZKTlc2NHVzTUxhdS1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=eT-6JNW64usMLau-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXFsQrCMBAAUFwL_oDTjSq1EcFFJ0Gxugqu5ZoeSTTehVyQ_oW_rC7vVZ9JdThiQTixC0yUYd7iIJJquCXMz18WI9bQhjctYAVX6UEJs_UgDGcRF2m296Uk3RmjGhunBUuwjZWXEaZeRvOQXv906jFTilio22zXY5PYLaf3oAiB4cJDwC9Wqp1ikgAAAA&shmds=v1_AdeF8KhYrHXm7wogwEwbFZ9W4s61HkTq4FBK03n19qF8gZ_PUA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=eT-6JNW64usMLau-AAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f7e4a0d0dc158f67fd721978ef95aab8a775788a042a09089d.jpeg",
         "Data Engineer (Hadoop, Spark, Scala, Hive)",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/data-engineer-spark-python-at-etelligens-technologies-4230192247?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.shine.com/jobs/data-engineer-spark-python/etelligens-technologies/17263944?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://www.kitjob.in/job/149564571/vq345-data-engineer-spark-python-pune/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          },
          {
           "link": "https://in.findjob24h.com/it/big-data-engineer-sparkpython-job26347?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Findjob24h.com"
          }
         ],
         "Etelligens Technologies",
         "Job Description\n\nWe are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n\nKey Responsibilities\n• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n• Perform data cleansing and migration from diverse sources to target systems.\n• Collaborate with stakeholders to understand requirements and convert them into technical\n\nsolutions.\n• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n• Support, troubleshoot, and maintain data pipelines and workflows.\n• Participate in all phases of software development lifecycle including unit testing, integration\n\ntesting, and performance testing.\n• Contribute to the modernization of data platforms and analytics tools on Azure.\n• Ensure data quality and integrity across all pipelines and systems.\n\nRequired Skills & Qualifications\n• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n• 1+ year of experience in Python and Spark scripting.\n• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n• Good understanding of Data Warehousing (DWH) concepts.\n• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n• Hands-on experience in data cleansing, transformation, and migration projects.\n• Ability to work independently and within a team environment.\n• Microsoft Certified : Azure Data Engineer Associate\n• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n\nLocation : Artha SEZ, Greater Noida West\n\n(ref:hirist.tech)",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=Dc6DSNLMiNYCEl95AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHsQrCMBAAUFz7CU43CzaiuOhqEZ0E3cs1HklqvAu5G-p_-MHW5cFrvotmd0JD6DgkJqqwhnvB-nK3j0XhuVcZQAmrjzD_LBIyLY_RrOjBOdXcBjW05FsvbydMg0xulEH_9BqxUslo1G_3m6ktHFbQGeWcArHCg3xkyRISKSSGCz8T_gDku5DQlwAAAA&shmds=v1_AdeF8KhUS3kDcBE04kKw4L-SXFj_kSbNYqcLjnZpQzO2TV5B5A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=Dc6DSNLMiNYCEl95AAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f79c16e836afec51432ee9e157dc4b5b6344f07d3b2165e389.jpeg",
         "Data Engineer - Spark/Python",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://www.iitjobs.com/job/spark-engineer-usa-staffingine-llc-78519?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Iitjobs"
          }
         ],
         "Staffingine LLC",
         "- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred",
         {
          "posted_at": "1 day ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "1 day ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3NsQrCMBAA0L1fIE63uAgmIrjoKCJKt35AucRrkhrvQi5DJ79ddHnr6z7dZihYX3DlkJiowg4e4kAJq48gDDeRkGl9jq0VPVmrmk3Qhi154-VthcnJYmdx-mPUiJVKxkbj4bhfTOGwXQ0Npyn9C-j7CySGOz8TfgHTIbRdgAAAAA&shmds=v1_AdeF8KjUl41s9rD9xNd7RywARJoJQ3Nel1qruGSZXxbjxJAYWA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ha78KI06qq94Uf3oAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f775d6990ffc61dcc6da45082c8a36a2aa06bfab96684f1183.jpeg",
         "Spark Engineer",
         "Iitjobs"
        ],
        [
         [
          {
           "link": "https://www.pyjobs.com/job/staff-data-engineer-spark-python-hadoop-oMy9olMy?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "PyJobs"
          }
         ],
         "Visa",
         "Company Description\n\nVisa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n\nWhen you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n\nJoin Visa: A Network Working for Everyone.\nJob Description\n\nThe Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n\nAre you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n\nAs a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n\nEssential Functions\n• Work with manager and clients to fully understand business requirements and desired business outcomes\n• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n• Execute data engineering projects ranging from small to large either individually or as part of a project team\n• Ensure project delivery within timelines and budget requirements\n• Provide coaching and mentoring to junior team members\n\nThis is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\nQualifications\n\n• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n• Minimum of 4 years of hands-on expertise with Java or Scala\n• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n• Experience working in an Agile and Test Driven Development environment.\n• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n• Experience with SAS as a statistical package is preferred\n• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFwLvoDTjSq1EcFFN1H8mYSCa7m2aRKNdyF3Q30LH1n8hq_4TopDrTgMcERFOJELZG2GeZ0wv0q4f9QzlXDBnjktYAU3bkEs5s4DE5yZXbSzvVdNsjNGJFZOFDV0Vcdvw2RbHs2TW_nXiMdsU0S1zWa7HqtEbjl9BEEIBFfqA_4AG-QyopMAAAA&shmds=v1_AdeF8KjOBn1yJUpUCbIoZfiL9XOirLPsVpNYluZLK1C-IlMUqQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=ZA_PbHJweZ-tmarhAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f7a5cc450c7c7438378f704be88128d8f54c6129c21b3ecd44.jpeg",
         "Staff Data Engineer (Spark, Python, Hadoop)",
         "PyJobs"
        ],
        [
         [
          {
           "link": "https://www.glassdoor.co.in/job-listing/databricks-engineer-spark-pyspark-enkefalos-technologies-llp-JV_KO0,33_KE34,60.htm?jl=1009780925122&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://in.indeed.com/viewjob?jk=b95950f98d5b6678&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://levels.fyi/jobs?jobId=99571824638993094&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Levels.fyi"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/6215885900103024640?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://www.kitjob.in/job/153480629/c-941-data-engineer-etl-sql-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "Enkefalos Technologies LLP",
         "Databricks Engineer – Spark / PySpark\n\nLocation : Remote / Mysore\n\nJoining : Immediate\n\nExperience : 5+ years\n\nResponsibilities :\n\nWill implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n\nRequirements:\n• PySpark (RDDs, DataFrames, performance tuning)\n• Building gold‐layer data models for financial reporting\n• Experience with complex joins, aggregations, GL hierarchies\n• Version handling (Actuals vs Budget), currency conversions\n\nJob Type: Full-time\n\nPay: ₹500,395.35 - ₹1,840,348.25 per year\n\nBenefits:\n• Flexible schedule\n• Paid sick time\n• Provident Fund\n• Work from home\n\nApplication Question(s):\n• Have you worked on ADF/ADLS ?\n• Do you have hands-on experience of Spark / PySpark\n\nExperience:\n• Databricks Engineering: 4 years (Required)\n\nWork Location: Remote",
         {
          "posted_at": "6 days ago",
          "qualifications": "No degree mentioned",
          "salary": "₹500,395–₹1,840,348 a year",
          "schedule_type": "Full-time",
          "work_from_home": "true"
         },
         [
          "6 days ago",
          "₹500,395–₹1,840,348 a year",
          "Work from home",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Anywhere",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=k4Rb92L6YIBechULAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXGsQrCMBAAUFz7CU4HbkIbEVzU0SJKh4Luco1nEhvvQi5D_Rs_VcTl8arPrNofsOCQgx0VWnaBiTLUcEmYRzDQv_-r4SwDKGG2HoThKOIizXe-lKRbY1Rj47RgCbax8jLCNMhknjLoj5t6zJQiFrqtN6upSeyWi5ZHemAUhStZzxLFBVLouh4Cw4nvAb-C4XW0owAAAA&shmds=v1_AdeF8KizOswJH_SyEGgjpgJ7bj5Cwqu4B0GS78yPFj9XWFNoVQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=k4Rb92L6YIBechULAAAAAA%3D%3D",
         null,
         "Databricks Engineer - Spark / PySpark",
         "Glassdoor"
        ],
        [
         [
          {
           "link": "https://www.hirist.tech/j/pi-square-technologies-spark-and-scala-engineer-1486469?ref=kp_prm&jobPos=5&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://in.jooble.org/jdp/-2093773132764585923?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://www.hirist.com/j/pi-square-technologies-spark-scala-engineer-1486469.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/3286396871957807104?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://www.kitjob.in/job/149843579/square-technologies-spark-scala-engineer-tfe-344-india/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          },
          {
           "link": "https://www.recruit.net/job/pi-square-technologies-spark-scala-jobs/3F001AE1CE143060?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Recruit.net"
          }
         ],
         "sandeep raja",
         "Job Summary :\n\nWe are seeking a skilled and motivated Spark & Scala Engineer with 46 years of hands-on experience in building scalable data processing applications. The ideal candidate will have a strong background in big data ecosystems and a solid understanding of distributed computing using Apache Spark with Scala.\n\nRoles and Responsibilities :\n\n- Design, build, and maintain efficient, reusable, and reliable Apache Spark applications.\n\n- Optimize Spark applications for maximum speed and scalability.\n\n- Implement data ingestion and ETL processes.\n\n- Collaborate with data scientists and architects to implement complex big data solutions.\n\n- Debug and resolve issues in Spark applications.\n\n- Stay up to date with the latest trends in big data technologies and Apache Spark.\n\n- Write clean, readable, and maintainable code.\n\n- Participate in code reviews and contribute to team knowledge sharing.\n\nRequired Skills :\n\n- 46 years of experience working with Apache Spark (core, SQL, streaming).\n\n- Strong proficiency in Scala programming.\n\n- Experience in building and optimizing data pipelines and ETL workflows.\n\n- Proficient in working with big data tools and frameworks (e.g., Hive, HDFS, Kafka).",
         {
          "posted_at": "27 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "27 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJQaSBTcXVhcmUgVGVjaG5vbG9naWVzIC0gU3BhcmsgXHUwMDI2IFNjYWxhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoic2FuZGVlcCByYWphIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlFMNDBNZDljZDlqQkxBX3BBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=QL40Md9cd9jBLA_pAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNoQ7CMBCA4WDnMahTCBJWQoIBPQgokuGXW3dpO8pd6ZVkD8PDMsxvPvFX30V1vgdo3x_MBA-yniWKC6SwhTZhfsIaWosRoWEXmCjPcJMelDBbD8JwEXGRVidfStKjMaqxdlqwBFtbeRlh6mUyo_T6T6d-fqWIhbr9YTfVid1mqcgDUYKMI0JguPIQ8AfNb9jFnwAAAA&shmds=v1_AdeF8KiZSTs7t7WCPWgpQK5yqbyREzWZSeEDIFaZ99Xx8ZkX-g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=QL40Md9cd9jBLA_pAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f7f322242bb49fd97df7d7ea982022c5843da4afdce119aefc.jpeg",
         "Pi Square Technologies - Spark & Scala Engineer",
         "Hirist"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/spark-developer-at-infosys-4228309581?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.shine.com/jobs/spark-developer/infosys/17011039?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://in.bebee.com/job/186f1d5c41ccc611ca87f0b144a7e5ae?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://jobgether.com/offer/6836bc7c448ef06f94b4759c-spark-developer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobgether"
          },
          {
           "link": "https://jobindian.in/ko/spark-developer-job100000?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobindian.in"
          },
          {
           "link": "https://indianjob24h.in/es/spark-developer-job100000?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indianjob24h.in"
          }
         ],
         "Infosys",
         "• Primary skills:Technology->Big Data - Data Processing->Spark\n\nA day in the life of an Infoscion\n• As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\n• You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\n• You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\n• You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\n• You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n• Knowledge of more than one technology\n• Basics of Architecture and Design fundamentals\n• Knowledge of Testing tools\n• Knowledge of agile methodologies\n• Understanding of Project life cycle activities on development and maintenance projects\n• Understanding of one or more Estimation methodologies, Knowledge of Quality processes\n• Basics of business domain to understand the business requirements\n• Analytical abilities, Strong Technical Skills, Good communication skills\n• Good understanding of the technology and domain\n• Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\n• Awareness of latest technologies and trends\n• Excellent problem solving, analytical and debugging skills",
         {
          "posted_at": "16 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "16 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJTcGFyayBEZXZlbG9wZXIiLCJjb21wYW55X25hbWUiOiJJbmZvc3lzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkZ2VmRvVC1NQnFvWHRRUmJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=FvVdoT-MBqoXtQRbAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQoCMQwA0P0mZ6dMDoKtCC66CnKufsCR1thWaxKaouffi294w2dYXRXbE070pipKDTZwkQBG2GIGYTiLpErLY-5d7eC9WXXJOvYSXZSXF6Ygs39IsH-TZWykFTtNu_12dsppvRj5LvY1KAwj3wr-AKcB2QF5AAAA&shmds=v1_AdeF8Kh9NNVFIzcDYAUyzgZze6PxeWepYB2Nz99wOAFvlj190A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=FvVdoT-MBqoXtQRbAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f7288e0093cef41baca5f816735e5c318463cf69b421b2cd83.jpeg",
         "Spark Developer",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://citi.wd5.myworkdayjobs.com/en-US/2/job/Data-Engineer---AVP_24785645?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://in.indeed.com/viewjob?jk=481bb9445cf687c8&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Indeed"
          },
          {
           "link": "https://www.hirist.tech/j/celonis-data-engineer-etlsql-10-12-yrs-1479538?ref=rl&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirist"
          },
          {
           "link": "https://jobgether.com/offer/67969bf4c58f91dcb9676b4e-ria---data-engineer-with-q-a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobgether"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/veersa-technologies-data-engineer-informatica-mdm-at-veersa-technologies-4243113652?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://in.jooble.org/jdp/3467413193916249586?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/data-engineer-2-5-yrs-exp-guardian-management-services-JV_KO0,25_KE26,54.htm?jl=1009784233452&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.foundit.in/job/debut-infotech-data-engineer-python-scala-debut-infotech-pvt-ltd-india-34609162?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Foundit.in"
          }
         ],
         "12542 Citicorp Services India Private Limited",
         "The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting",
         {
          "schedule_type": "Full-time"
         },
         [
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=lBQYlRnJjd-RRHF9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWLTQ_BQBBA4-onOM0RqS4NDpzqI0IcJE04yrQd7eja2exu8Nf8OxWXd3nvdT-d7mqDAWFrKjZEDvqZkddNY0MRZBZdE0F6yQYwgvR8anmQHDyhK2oQAzuRSlNvWYdg_UIp73Vc-YCBi7iQhxJDubzVXXL_w9XX6MhqDHRNZuN3bE01nE-S2TSBNbeTOAsZuScX5GFvSkY4OX62PRz5wYFKYPMXX3VpMJW8AAAA&shmds=v1_AdeF8Kis8dqR1eNIZfspzvlHRHi-cHVVT9hIYJVFIboxveOlTA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=lBQYlRnJjd-RRHF9AAAAAA%3D%3D",
         null,
         "Data Engineer (Snowflake, Spark, AWS) - AVP",
         "Workday"
        ],
        [
         [
          {
           "link": "https://powertofly.com/jobs/detail/2317939?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "PowerToFly"
          },
          {
           "link": "https://jobs.weekday.works/visa-sw-engineer-(java-and-bigdata%2Fhadoop%2Fspark)-1yr?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs.weekday.works"
          },
          {
           "link": "https://www.kitjob.in/job/155049294/v777-engineer-java-and-bigdata-hadoop-spark-1yr-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          }
         ],
         "VISA",
         "Job Description\n\nThis position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. They will be an integral part of the Payment Products Development team, focusing on the development of software solutions that leverage data to address business challenges. They will be extensively involved in hands-on activities including proof of concepts, development, and testing. They should demonstrate adaptability and readiness to change tasks according to the team's requirements.\n\nKey Responsibilities\n• Deliver high-quality products and features, taking a holistic view from the initial idea to final delivery. This involves design, development, testing, and accountability for release deployments as well as providing support post-deployment.\n• Understand the product in its entirety and ensure the timely delivery of your assigned module within an agile team setting.\n• Accurately scope efforts, identify risks, and clearly communicate trade-offs with team members and other stakeholders.\n• Coordinate and participate in Continuous Integration activities, testing automation frameworks, and other related tasks, in addition to contributing to core product code.\n• Effectively communicate status updates, issues, and potential risks accurately and promptly.\n• Perform other tasks related to data governance and system infrastructure as required.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\nQualifications\n\nBasic Qualifications\n\n-Bachelor's degree in Computer Science or equivalent field\n\n-Relevant working experience of up to 2 years in the industry\n\n-Proven experience in software development, particularly in data-centric\n\nprojects, demonstrating adherence to standard development best practices\n\n-Strong understanding and practical experience with data structures and\n\nalgorithms, with a passion for tackling complex problems\n\n-Proficiency in Java programming\n\n-Hands-on experience with Big Data technologies such as Hadoop, Spark, and\n\nHive\n\n-Strong knowledge of Unix/Linux operating systems and Shell Scripting\n\n-Proficiency in working with RDBMS and SQL\n\n-Basic knowledge of manual and automated testing\n\n-Familiarity with version control systems, specifically Git\n\n-Awareness of and experience with software design patterns\n\n-Experience working within an Agile framework\n\nPreferred Qualifications\n\n-Proficiency in Scala & Kafka programming is a good to have\n\n-Experience with Airflow for workflow management\n\n-Familiarity with AI concepts and tools, including GitHub Copilot for code\n\ndevelopment\n\n-Exposure to AI/ML development is an added advantage\n\n-Proficiency in working with In-memory Databases like Redis\n\n-Good knowledge of API development is highly advantageous\n\n-Strong verbal and written communication skills, with a proactive and self-\n\nmotivated approach to improving existing processes to enable faster\n\niterations.\n\n-Demonstrated intellectual and analytical rigor, with a keen attention to detail\n\n-Team-oriented, energetic, and collaborative approach to work, coupled with a\n\ndiplomatic and adaptable style\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         {
          "posted_at": "5 days ago",
          "schedule_type": "Full-time"
         },
         [
          "5 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTVyBFbmdpbmVlciAoSmF2YSBhbmQgQmlnZGF0YS9IYWRvb3AvU3BhcmspIDF5ciIsImNvbXBhbnlfbmFtZSI6IlZJU0EiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoidHV1TkJsMDF3amRIc0xnSEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=tuuNBl01wjdHsLgHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFwLvoDTjSrYqOBSJwV_2rWgY7k2RxqtdyEXpL6HDyx-w5d9J9m5vsOJnWeiCPMK3wjIFo7eWUxormhFgqkDxucCNp8IK6ikBSWMXQ_CcBFxA832fUpBC2NUh9xpwuS7vJOXEaZWRvOQVv812mOkMGCiZrtbj3lgt5zeyvoAnqFk6_EHGOnmHZcAAAA&shmds=v1_AdeF8KjVjVJowf_Si6rhmDvQLwvNY8s0nZa1Y6FrC7MYGcN41w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=tuuNBl01wjdHsLgHAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f7f2857876d96d4a19482a79d8d87bbea86705b552fa82bf54.jpeg",
         "SW Engineer (Java and Bigdata/Hadoop/Spark) 1yr",
         "PowerToFly"
        ],
        [
         [
          {
           "link": "https://www.iitjobs.com/job/big-data-lead-lead-data-engineerspark-tech-lead-mclean-va-usa-tanisha-systems-inc-3899?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Iitjobs"
          },
          {
           "link": "https://www.techgig.com/jobs/Tech-Lead-Big-Data/70663711?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "TechGig"
          }
         ],
         "Tanisha Systems  Inc",
         "Job Description:Mandatory:10+ years of experience in solution, design and development of applications using Java 8+/J2EE, Spring, Spring-Boot, Micro Services, RESTful Services and with experience in Big Data and with experience working in heavy data background needed.Develop, program, and maintain applications using the Apache Spark open source frameworkWork with different aspects of the Spark ecosystem, including Spark SQL, Data Frames, Datasets, and streaming.Spark Developer must have strong programming skills in Java, Scala, or PythonFamiliar with big data processing tools and techniquesProven experience as a Spark Developer or a related roleStrong programming skills in Java, Scala, or PythonFamiliarity with big data processing tools and techniquesExperience with the Hadoop ecosystemGood understanding of distributed systemsExperience with streaming data platformsMust have strong experience in Big Data and with experience working in heavy data background needed.Must be strong in Cloud AWS event-based architecture, Kubernetes, ELK (Elasticsearch, Logstash & Kibana)Must have excellent experience in designing and Implementing cloud-based solutions in various AWS Services (: s3, Lambda, Step Function, AMQ, SNS, SQS, CloudWatch Events, etc.)Must be well experienced in design and development of Microservice using Spring-Boot and REST API and with GraphQLMust have solid knowledge and experience in NoSQL (MongoDB)Good knowledge and experience in any Queue based implementations.Strong knowledge/experience in ORM Framework - JPA / HibernateGood knowledge in technical concepts – Security, Transaction, Monitoring, PerformanceShould we well have versed with TDD/ATDDShould have experience on Java, Python and Spark2+ years of experience in designing and Implementing cloud-based solutions in various AWS ServicesStrong experience in DevOps tool chain (Jenkins, Artifactory, Ansible/Chef/Puppet/Spinnaker, Maven/Gradle, Atlassian Tool suite)Very Good knowledge and experience in Non-Functional (Technical) Requirements like Security, Transaction, Performance, etc.Excellent analytical and problem-solving skills Nice to have:· Experience in Experience with OAuth implementation using Ping Identity· Experience in API Management (Apigee) or Service Mesh (Istio)· Good knowledge and experience in Queue/Topic (Active-MQ) based implementations.· Good knowledge and experience in Scheduler and Batch Jobs· Experience with scripting languages using Unix· Preferably certified in AWS",
         {
          "posted_at": "4 hours ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "4 hours ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJCaWcgRGF0YSBMZWFkLyBMZWFkIERhdGEgRW5naW5lZXIvU3BhcmsgVGVjaCBMZWFkIiwiY29tcGFueV9uYW1lIjoiVGFuaXNoYSBTeXN0ZW1zICBJbmMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQmRWaDV5cnl3aTREUFNOT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "India",
         "Spark Engineer",
         "https://www.google.com/search?ibp=htl;jobs&q=Spark+Engineer&htidocid=BdVh5yrywi4DPSNOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWJuwrCQBBFsc0nCMLUglkRbLSTiA_skj5MNsNmNZlZdraIP-R3Gk1zLvec7LPIrifvoMCE8CBszZ_zP7PzTBRNGTC-oCLbzXUDd2lACeNkhOEi4npaHruUgh6MUe1zpwmTt7mVwQhTI6N5SqM_1NphpNBjonq33455YLdeVch-ClC-NdGgADe24Hma1uMXP5iqRqkAAAA&shmds=v1_AdeF8KgvLioLXzdCbGcM30mO5raEHL9pykXDnB4l0r3zd6_bmQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Spark+Engineer&htidocid=BdVh5yrywi4DPSNOAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c25ff596ec88cb9bae8/images/51c80805102136f70b7e87e564a84e4f2b75a0e90adaeea18f77af68fffe6ec4.jpeg",
         "Big Data Lead/ Lead Data Engineer/Spark Tech Lead",
         "Iitjobs"
        ],
        [
         [
          {
           "link": "https://www.novartis.com/careers/career-search/job/details/req-10055119-data-insights-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Novartis"
          },
          {
           "link": "https://novartis.wd3.myworkdayjobs.com/en-US/Novartis_Careers/job/Data-Insights-Analyst_REQ-10055119-1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://in.bebee.com/job/3b5171d6da89c919a74528de8a507f49?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://unstop.com/jobs/data-insights-analyst-acko-general-insurance-1485517?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Unstop"
          },
          {
           "link": "https://apna.co/job/hyderabad/data-insights-analyst-1566740135?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Apna"
          },
          {
           "link": "https://www.techgig.com/jobs/Data-Insights-Analyst/71084590?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "TechGig"
          },
          {
           "link": "https://in.trabajo.org/job-3150-9c03d04ebc9df7a060c6901ca379a236?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          }
         ],
         "IN10 (FCRS = IN010) Novartis Healthcare Private Limited",
         "Responsible for leveraging Google Analytics 4 (GA4) to extract, analyze, and interpret data to support business objectives and decision-making processes. Requires proficient understanding and translating business requirements into actionable insights and recommendations derived from GA4 data.Key Responsibilities:1.Data Extraction & Analysis: Extract and analyze GA4 data to provide meaningful insights into user behavior, website performance, and digital marketing effectiveness.2.Reporting & Dashboarding: Develop custom reports, dashboards, and data visualizations using GA4 data to monitor key metrics and performance indicators.3.Insights and Recommendations: Providing actionable insights and recommendations to optimize website performance, user experience, and marketing campaigns.4.Customization & Configurations: Configure GA4 data streams, events, and conversions to align with business goals and KPIs (key performance indicators). Utilize custom dimensions, metrics, and attribution models to capture and analyze data relevant to unique business needs.5.Pharma knowledge: Understanding of the Pharma industry landscape, including regulatory compliance, patient journey mapping, and healthcare data privacy considerationsMinimum Requirement:·2-4 Years of experience in digital analytics, hands on experience on Google Analytics 4.·Proficiency in Google Analytics and Google Tag Manager.·Strong analytical skills and ability to interpret data.·Understanding of digital marketing concepts and website optimization techniques.·Excellent communication skills to present findings and recommendations effectively·GA4 Certification is strongly preferred",
         {
          "posted_at": "5 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "5 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEluc2lnaHRzIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJJTjEwIChGQ1JTID0gSU4wMTApIE5vdmFydGlzIEhlYWx0aGNhcmUgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImFiMU5uYTFGN1dHbFd4eC1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=ab1Nna1F7WGlWxx-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOwQqCQBCAYbr6CJ3mWFFqQZciKIrKCIl6ABnXxd1Yd2RnEHuo3jG7_LcP_ug7iuYnFITMs62NMBw8ug8LLOBGJbDGoAyQhwtR7fR4a0Ra3iQJs4trFhSrYkVNQl6X1CdvKvmfgg0G3ToUXazWaR-3vp7ts3yZwuR8fL5gB1meLtMp5NRhEMtw1ejEqIHBI9hukHC3jRVdgfXDYGXxBzQioVWwAAAA&shmds=v1_AdeF8Ki5IovOpeRqUkuegClUSkUiOKvtd7O7Wkq4pvGULDQ7Nw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=ab1Nna1F7WGlWxx-AAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550f217f2561801770bc1323d74230147da3ed24178c6aa37305.png",
         "Data Insights Analyst",
         "Novartis"
        ],
        [
         [
          {
           "link": "https://www.wellsfargojobs.com/en/jobs/r-467470/senior-data-management-analyst/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Wells Fargo"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/senior-data-management-analyst-wells-fargo-JV_IC2865319_KO0,30_KE31,42.htm?jl=1009782512404&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://builtin.com/job/senior-data-management-analyst/6335579?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://in.jooble.org/jdp/908886558276547854?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jooble"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/senior-data-management-analyst-at-wells-fargo-4253125411?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://in.bebee.com/job/14a3ffa859552460b9db99b4a442d49c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BeBee"
          },
          {
           "link": "https://jobs.anitab.org/companies/wells-fargo/jobs/51859236-senior-data-management-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "AnitaB.org Job Board"
          },
          {
           "link": "https://www.jobaaj.com/job/wells-fargo-senior-data-management-analyst-hyderabad-telangana-india-3-to-5-years-322854?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobaaj"
          }
         ],
         "Wells Fargo",
         "About this role:\n\nWells Fargo is seeking a Senior Data Management Analyst\n\nIn this role, you will:\n• Lead or participate in moderately complex programs and initiatives for data quality, governance, and metadata activities\n• Design and conduct moderately complex analysis to identify and remediate data quality, data integrity, process, and control gaps\n• Analyze, assess, and test data controls and data systems to ensure quality and risk compliance standards are met and adhere to data governance standards and procedures\n• Identify data quality metrics and execute data quality audits to benchmark the state of data quality\n• Develop recommendations for optimal approaches to resolve data quality issues and implement plans for assessing the quality of new data sources leveraging domain expertise and data, business, or process analysis to inform and support solution design\n• Lead project teams and mentor less experienced staff members\n• Drive planning and coordination on moderately complex remediation efforts acting as central point of contact\n• Consult with clients to assess the current state of data and metadata quality within area of assigned responsibility\n• Participate in cross-functional groups to develop companywide data governance strategies\n• Provide input into communication routines with stakeholders, business partners, and experienced leaders\n\nRequired Qualifications:\n• 4+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\n• Experience in large enterprise data initiatives\n• Contact center business or technology experience\n• Experience implementing, managing, or utilizing big data environments (Hadoop, Big Query, etc.)\n• Experience using standard BI tools (Tableau, Power BI, MicroStrategy, etc.) preferably from big data environments\n\nPosting End Date:\n24 Jun 2025\n• Job posting may come down early due to volume of applicants.\n\nWe Value Equal Opportunity\n\nWells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n\nEmployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n\nCandidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n\nApplicants with Disabilities\n\nTo request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n\nDrug and Alcohol Policy\n\nWells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n\nWells Fargo Recruitment and Hiring Requirements:\n\na. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n\nb. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
         {
          "posted_at": "4 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "4 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBNYW5hZ2VtZW50IEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJXZWxscyBGYXJnbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEFWbXpFcV9ySTNSUG1faUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=xAVmzEq_rI3RPm_iAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOvQrCQAwAYFz7CE6ZS-2J0EUnQfwDJwXHkmvD9eSalEuG-jY-qrp881d8FkVzJ46S4YCGcEPGQCOxwZ4xvdVgBVfxoIS5G0AYTiIh0XI3mE26dU411UENLXZ1J6MTJi-ze4nXP60OmGlKaNRumvVcTxzK8kkpKRwxB4HIcH73lNFjX8GDEnL4NSq4cB_xCz8NyUCjAAAA&shmds=v1_AdeF8Kh4C0j69LKLY7UW2LTRMhZnqjSHPg5xPB9oEh8vmo3emw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=xAVmzEq_rI3RPm_iAAAAAA%3D%3D",
         null,
         "Senior Data Management Analyst",
         "Wells Fargo"
        ],
        [
         [
          {
           "link": "https://www.deshawindia.com/careers/associate-analyst-data-analytics-4672?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "D. E. Shaw India"
          },
          {
           "link": "https://www.simplyhired.co.in/job/Vi8-jUJ3J6G_26K9tGu-RlIWdVx9IVaiKRgGRiYJpIOy241Gn8r41Q?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://www.hirewand.com/apply/job/detail?jid=1694148sid%3D5af414cf6ed33ca5454ecbdb&src=jobpost&cpid=1694&uid=85694&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Hirewand"
          },
          {
           "link": "https://in.trabajo.org/job-3396-c87d6663e26180f8f37f411234282c84?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          },
          {
           "link": "https://jobs.weekday.works/coursevita-tutor---data-analytics?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs.weekday.works"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/8884656763974975488?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          },
          {
           "link": "https://en-in.whatjobs.com/jobs/business-intelligence?id=147270418&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "WhatJobs"
          },
          {
           "link": "https://callcenterjob.co.in/associateanalyst-data-analytics-hyderabad_1067056?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Post Job"
          }
         ],
         "D. E. Shaw India",
         "The Financial Research (FinRes) group provides research and analysis support to the entire investment life cycle of the firm’s global proprietary trading strategies. The group caters to almost all the quantitative and qualitative strategies of the firm and its research support covers multiple asset classes, sectors, and geographies.",
         {
          "posted_at": "14 hours ago",
          "schedule_type": "Full-time"
         },
         [
          "14 hours ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUvQW5hbHlzdCAtIERhdGEgQW5hbHl0aWNzIiwiY29tcGFueV9uYW1lIjoiRC4gRS4gU2hhdyBJbmRpYSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiak9McEJja1dWaXhiWjJ4c0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=jOLpBckWVixbZ2xsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNzQrCMBCE8dpH8LRnqYkIHtRToeLPVe9lk4YkJe6WbsD2kXxLq14G5psPpngvin0lwjZidroiTJNkWEONGeFXc7QygxsbEIeDDcAEZ2af3PIYcu7loLVIUl4yzrKy_NRMzvCoOzbyjUYCDq5P80ez3W1G1ZNf6VrBScE94Auu1EaESHCZWjegwbaEh0tIHgnL__wBfMXaI6wAAAA&shmds=v1_AdeF8KiGYFSm74ttc37WRtV6BAmPdDCIXraWSL4lzvQKbqdoJQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=jOLpBckWVixbZ2xsAAAAAA%3D%3D",
         null,
         "Associate/Analyst - Data Analytics",
         "D. E. Shaw India"
        ],
        [
         [
          {
           "link": "https://jobs.bms.com/careers/job/137468587991-senior-analyst-data-risk-office-hyderabad-ts-in?domain=bms.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "BMS Careers - Bristol Myers Squibb"
          },
          {
           "link": "https://bristolmyerssquibb.wd5.myworkdayjobs.com/BMS/job/Hyderabad---TS---IN/Senior-Analyst--Data-Risk-Office_R1592258?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/senior-analyst-data-risk-office-at-bristol-myers-squibb-4250791425?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/senior-analyst-data-risk-office-bristol-myers-squibb-JV_IC2865319_KO0,31_KE32,52.htm?jl=1009783451547&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://www.simplyhired.co.in/job/LBe73mWfRZmlfdY6A1WI6A2BVaIPe0pBYiqd0yrkBfRdvI_f8gJ8GA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "SimplyHired"
          },
          {
           "link": "https://jobs.sandscapitalventures.com/companies/karuna-therapeutics/jobs/52872727-senior-analyst-data-risk-office?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Sands Capital Job Board"
          },
          {
           "link": "https://getmereferred.com/job-listing/senior-analyst-data-risk-office-bristol-myers-squibb-hyderabad-5-to-10-years-experience-afe03867-553a-46ae-8e65-7e5c0565e5c2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Job Referral"
          },
          {
           "link": "https://www.adzuna.in/details/5258440582?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Adzuna"
          }
         ],
         "Bristol Myers Squibb",
         "Working with Us\nChallenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n\nRoles & Responsibilities\n\nFunctional and Technical\n• Execution and monitoring of data privacy office key activties.\n• Assist in the development and implementation of data privacy policies and procedures to ensure compliance with international, federal, and state regulations, including GDPR, CCPA, and HIPAA.\n• Work closely with legal, IT, HR, and other departments to address data privacy concerns and implement best practices.\n• Assist in the development and delivery of data privacy training and awareness programs across the organization. Keeps up to date with evolution of regulations impacting privacy, ethics, and data.\n• Experienced in configuring and working with various industry leading data risk and privacy tools like OneTrust, TrustArc, Microsoft Purview etc.\n• Demonstrate critical thinking around interpreting business and industry challenges and recommending best practices-based solutions to improve products, processes, systems and reduce risk.\n• Implement organizational IT controls in accordance with applicable regulations and the ability to evaluate and understand the impact of new regulations and requirements.\n\nPeople Management:\n• Responsible for training and mentoring junior staff to meet BMS standards.\n• Preferred experience with working in a multi-cultural, multi-location and diverse environments.\n\nQualifications & Experience\n• B.E./B.Tech. or equivalent in computer science, engineering, life science field\n• Recognized privacy/DLP certifications and experience preferred.\n• At least years of privacy program management, compliance, or strong operations management experience (regulated or healthcare or tech preferred).\n• Knowledge of HIPAA, GDPR, CPRA, PIPL etc., and other privacy regulations is a must.\n• Ability to make decisions that impact own work and other groups/teams and works under minimal supervision.\n• Demonstrates openness to learning and developing. Takes a responsibility for their own and team’s development and growth.\n• Demonstrates an understanding of factors driving team performance and how they contribute to the team's overall success.\n• Excellent English Oral and written communication skills including the ability to deliver clear and articulate presentations.\n• Ability to use PowerPoint, Excel, Word, or other technologies to communicate complex topics to stakeholders, manage personal workload, and track projects and issues.\n\nIf you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n\nUniquely Interesting Work, Life-changing Careers\nWith a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n\nOn-site Protocol\n\nBMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n\nSite-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n\nBMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n\nIf you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",
         {
          "posted_at": "4 days ago",
          "schedule_type": "Full-time"
         },
         [
          "4 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHlzdC0gRGF0YSBSaXNrIE9mZmljZSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJsWm9Rdlc1dnAyWjZyYVlMQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=lZoQvW5vp2Z6raYLAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEzQqCQBAAYLr6CJ3mHKlRRFCnIugHIsjuMruOurXN2M4G-ky9ZPQdvuQ7SlYFsZMAW0Y_aExhjxHh5vQJ17p2liCFsxhQwmBbEIaDSONpvGlj7HSd56o-azRidDaz8sqFyUifP8Tov1JbDNR5jFTOl7M-67iZLHbBaRQPl4GCQvH-OGPAMRyHigIarKZwJ4_cIOMUTlw5_AHauiYbrgAAAA&shmds=v1_AdeF8KiC1TH2oVRja-VY93SlEgKpT1ldb1bxA3z1PjzhBfDP2g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=lZoQvW5vp2Z6raYLAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550f6f29ce77ef26d8d11fbeba5f6e8402d56f4e9d4b9a38c143.png",
         "Senior Analyst- Data Risk Office",
         "BMS Careers - Bristol Myers Squibb"
        ],
        [
         [
          {
           "link": "https://in.linkedin.com/jobs/view/data-analyst-ii-%E2%80%93-product-information-capabilities-digital-technology-at-general-mills-india-4253777713?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://in.trabajo.org/job-3396-15a6a47b27f82dc8a87ff91fa314b933?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs Trabajo.org"
          }
         ],
         "General Mills India",
         "India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.\n\nPosition Title\n\nSoftware Engineer II – Product Information Capability\n\nFunction/Group\n\nDigital & Technology\n\nLocation\n\nMumbai\n\nShift Timing\n\nRegular\n\nRole Reports to\n\nD&T Manager – Product Information Capability\n\nRemote/Hybrid/in-Office\n\nHybrid\n\nAbout General Mills\n\nWe make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and Häagen-Dazs, we’ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\n\nHow we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\n\nus into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\n\nGeneral Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\n\nWith our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital & Technology (D&T) Innovation, Technology & Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy & Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\n\nWe advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\n\nJob Overview\n\nFunction Overview\n\nThe Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The team's expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\n\nThe team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the \"Work with Heart\" philosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the following Link\n\nPurpose of the role\n\nThis is an exciting time to work in General Mills' Supply Chain & ITQ Organization! We are accelerating Digital Transformation of our Product Information Capabilities (PIC) capabilities to provide a competitive advantage to our business. To this end, we are seeking a skilled and motivated STIBO Developer to join our Product Information Capabilities team. As a STIBO Developer, you will play a crucial role in designing, developing, and implementing solutions within our STIBO STEP platform to support our product information management (PIM) and master data management (MDM) processes. You will collaborate with cross-functional teams to understand business requirements, translate them into technical specifications, and deliver high-quality solutions that meet our business needs.\n\nKey Accountabilities\n• Design, develop, and maintain STIBO STEP solutions to support product information management processes.\n• Write and maintain code for business rules to ensure data quality and consistency.\n• Configure outbound and inbound integrations to exchange data with other systems.\n• Configure gateway endpoints for seamless data flow.\n• Develop and maintain data models within STIBO STEP to accurately represent product information.\n• Build web UI screens for data entry, validation, and reporting.\n• Develop solutions based on documented requirements and specifications.\n• Participate in agile project development, including sprint planning, daily stand-ups, and retrospectives.\n• Collaborate with business analysts, data architects, and other developers to ensure solutions align with business needs and technical standards.\n• Troubleshoot and resolve issues related to STIBO STEP implementations.\n• Stay up-to-date with the latest STIBO STEP features and best practices.\n• Create and maintain technical documentation for STIBO STEP solutions.\n\nMinimum Qualifications\n• Education – Full time graduation from an accredited university (Mandatory- Note: This is the minimum education criteria which cannot be altered)\n• Experience with tools and concepts related to MDM and/or Digital Product Publishing, such as data governance, data quality, data integration and data exporting\n• Exposure to Product Information Management Systems (PIM/MDM)\n• Technical expertise into Stibo platform\n• Experience with Data Syndication partners such as 1WorldSync, Syndigo and Salsify.\n• Exposure to GDSN Standards\n• Strong team player and collaborator with core focus on solution making; Effective analytical and technical skills; Ability to work in a cross functional team environment\n\nPreferred Qualifications\n• Product Information Management / Master Data Management\n• STIBO STEP certification\n• Business Analysis skills\n• SQL, Cloud GCP\n• Agile / SCRUM Delivery\n• Familiarity with Service Bus Integration\n• Preferably experience in Consumer Goods industry.",
         {
          "posted_at": "2 days ago",
          "schedule_type": "Full-time"
         },
         [
          "2 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUkg4oCTIFByb2R1Y3QgSW5mb3JtYXRpb24gQ2FwYWJpbGl0aWVzIHwgRGlnaXRhbCBcdTAwMjYgVGVjaG5vbG9neSIsImNvbXBhbnlfbmFtZSI6IkdlbmVyYWwgTWlsbHMgSW5kaWEiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTWplT1ByZWI4UVZPLVpDVUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=MjeOPreb8QVO-ZCUAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWOMYpCQRBEMfUIYtDRBoL_i7DJGi0KoiAYbC49Yzu_l3b6M92CgoF32APtXfYkjmxSVAWvqoa_g2FYoSN8ZpSbOWw28Pf4gX3R4yXWmE9azuisGZbYY2BhZzK4w4oTOwq8wRfFLqtousEUthrACEvsoDJr1SQ0WnTuvX20rZk0ybwWxibqudVMQa_ttwZ7ycE6LNQLOh3m77Nr0-c0Ga8pU6lLOxaxeunICJz_zRMwu8VYwgAAAA&shmds=v1_AdeF8KgX9-NPInj0R6woa0p_BPSRuDiPPFd-gvZ8aIgLjy7J9Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=MjeOPreb8QVO-ZCUAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550fd832cbd45dde8873d2028e59e4230f6145f1024b5313cd32.jpeg",
         "Data Analyst II – Product Information Capabilities | Digital & Technology",
         "LinkedIn"
        ],
        [
         [
          {
           "link": "https://www.wellsfargojobs.com/en/jobs/r-467474/lead-data-management-analyst/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Wells Fargo"
          },
          {
           "link": "https://sanofi.wd3.myworkdayjobs.com/SanofiCareers/job/Hyderabad/Data-Management-Analyst_R2720815?utm_source=Correlation+Ventures+job+board&utm_medium=getro.com&gh_src=Correlation+Ventures+job+board&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://builtin.com/job/lead-data-management-analyst/6458888?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/lead-data-management-analyst-at-wells-fargo-4253126114?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.jobzmall.com/sanofi/job/data-management-analyst-13?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "JobzMall"
          },
          {
           "link": "https://jobs.anitab.org/companies/wells-fargo/jobs/48461478-data-management-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "AnitaB.org Job Board"
          },
          {
           "link": "https://www.jobaaj.com/job/wells-fargo-data-management-analyst-hyderabad-2-to-4-years-662803?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobaaj"
          },
          {
           "link": "https://expoint.co/job/933024495?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Expoint"
          }
         ],
         "Wells Fargo",
         "About this role:\n\nWells Fargo is seeking a Lead Data Management Analyst\n\nIn this role, you will:\n• Organize and lead complex companywide initiatives to ensure that data quality is maintained so that data can effectively support business processes\n• Oversee analysis and reporting in support of regulatory requirements\n• Identify and recommend analysis of data quality or integrity issues\n• Evaluate data quality metrics and data quality audits to benchmark the state of data quality\n• Make decisions in complex and multi-faceted situations requiring solid understanding of data governance standards and procedures\n• Identify new data sources and develop recommendations for assessing the quality of new data\n• Lead project teams and mentor less experienced staff members\n• Recommend remediation of process or control gaps that align to management strategy\n• Serve as relationship manager for a line of business\n• Consult with and provide recommendations to senior management regarding assessments of the current state of data and metadata quality within area of assigned responsibility\n• Represent client in cross-functional groups to develop companywide data governance strategies\n• Strategically collaborate and consult with peers, colleagues, and mid-level to senior managers to coordinate and drive collaboration on solution design and remediation execution\n\nRequired Qualifications:\n• 5+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\n• Experience in Data Management, Business Analysis, Analytics, Project Management.\n\nPosting End Date:\n24 Jun 2025\n• Job posting may come down early due to volume of applicants.\n\nWe Value Equal Opportunity\n\nWells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n\nEmployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n\nCandidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n\nApplicants with Disabilities\n\nTo request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n\nDrug and Alcohol Policy\n\nWells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n\nWells Fargo Recruitment and Hiring Requirements:\n\na. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n\nb. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
         {
          "posted_at": "4 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "4 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgTWFuYWdlbWVudCBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiV2VsbHMgRmFyZ28iLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ik5YbnE1ZlFtc3M5WXNVQ0JBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=NXnq5fQmss9YsUCBAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOvQrCQAwAYFz7CE6ZS-2J4qKTIP6hm-BYcr1wrVyTcslgX8ZnVZdv_orPrFjfCAMc0BDuyBhpIDbYM6ZJDRZwFQ9KmNsOhOEkEhPNd53ZqFvnVFMd1dD6tm5lcMLk5e1e4vVPox1mGhMaNavN8l2PHMvySSkpHDFHgZ7hPAXK6DFU8KCEHH-NCi4cevwCvzbU-KEAAAA&shmds=v1_AdeF8KhS9H58A4V380Akejd_Z20iYBTasKKVtt4Fa7d0Hh4WNg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=NXnq5fQmss9YsUCBAAAAAA%3D%3D",
         null,
         "Lead Data Management Analyst",
         "Wells Fargo"
        ],
        [
         [
          {
           "link": "https://wellfound.com/jobs/3320043-senior-data-analyst-marketing-science?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Wellfound"
          },
          {
           "link": "https://builtin.com/job/senior-marketing-science-analyst/6457491?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/senior-data-analyst-marketing-science-at-crunchyroll-4251557667?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          }
         ],
         "Crunchyroll",
         "About the role\n\nWe are looking for a hands-on Senior Marketing Science Analyst with a passion for understanding data, tracking business trends, and storytelling to join the Center for Data and Insights team. This position will report to the Vice President, Business Intelligence & Analytics, and will collaborate with data engineers, campaign managers, and marketing leaders to investigate campaign trends, build business insights, and recommend proactive measures to enable better business decisions every day. The right person for this position is a service-minded, empathetic problem-solver who will be motivated by the opportunity to build a centralized insights service team from the ground up!\n• Partner with Branch/ Adjust MMPs, direct response, and media buying agencies to define and implement campaign measurement of integrated marketing campaigns, including TV, OOH, Digital, and Social Media across a wide range of business lines (theatrical, streaming, e-commerce, etc.)\n• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n• Evaluating media measurement leveraging MMM (Marketing Mix Modeling), MTA (Multi-Touch Attribution), and Incrementality A/B Testing\n• Lead / Assist in development of testing roadmaps, measurement plans, KPI & KLI reporting based on set marketing objectives linked to key campaigns and always-on digital marketing tactics.\n• Deliver ad hoc analysis for client stakeholders throughout marketing, working with the channel leads, planning, and client service departments.\n• Partner with paid marketing teams in building weekly insight decks for the Marketing leadership team by collaborating effectively across teams, documenting the impact of strategic initiatives on benchmarks, and tracking the progress towards company goals.\n• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n• Connect ideas into cohesive, well-grounded recommendations, using creative, structured, and analytical thinking with the help of effective data visualization\n• Work with offshore and onsite teams and lead the sprint planning/management\n• Maintain a culture of high-quality output and outstanding customer service by effectively communicating at all levels, ensuring that work gets done, and responding effectively to\n\nAbout You\n• 5+ years of experience with data analysis, paid campaign analytics, statistics, experimentation, and optimization.\n• 3+ years of experience in writing complex SQL queries, experience in marketing data automation.\n• 3+ years of experience with data visualization tools like Tableau, Superset, etc.\n• Understanding of install and user-level paid campaign tracking to support multi-touch attribution via UTMs and MMP data for all major marketing channels\n• Proficiency in data analysis, including defining critical metrics, statistical and predictive modeling concepts, descriptive statistics, and experimental design\n• Experience in Marketing analytics tools like Google Analytics, Adjust, Braze, Branch, etc.\n• Experience working with large data sets (Terabytes of data/ billions of records).\n• Deep expertise in measuring marketing performance against lifetime value metrics.\n• Outstanding teamwork skills: You have excellent interpersonal skills. You’re a good listener. You place the success of the company ahead of any particular idea. Proven ability to work across a globally, matrixed organization.\n• BS in Statistics, Computer Science, Information Systems, or a related field\n\nAbout the Team\n\nThe Center for Data and Insights (CDI) is the centralized team of data engineering, BI, analytics, and data science experts, passionate about servicing the organization with timely and certified reports and insights! The mission of the group is to inspire, support, and guide our stakeholders to be data-aware as well as build out the systems of intelligence to discover insights and act on them.\n\nWhy you will love working at Crunchyroll\n\nIn addition to getting to work with fun, passionate and inspired colleagues, you will also enjoy the following benefits and perks:\n• Best-in class medical, dental, and vision private insurance healthcare coverage\n• Access to counseling & mental health sessions 24/7 through our Employee Assistance Program (EAP)\n• Free premium access to Crunchyroll\n• Professional Development\n• Company's Paid Parental Leave\n• up to 26 weeks for birthing parents\n• up to 12 weeks for non-birthing parents\n• Hybrid Work Schedule\n• Paid Time Off\n• Flex Time Off\n• 5 Yasumi Days\n• Half-Day Fridays during the summer\n• Winter Break\n\n#LifeAtCrunchyroll #LI-Hybrid",
         {
          "posted_at": "4 days ago",
          "schedule_type": "Full-time"
         },
         [
          "4 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBBbmFseXN0LCBNYXJrZXRpbmcgU2NpZW5jZSIsImNvbXBhbnlfbmFtZSI6IkNydW5jaHlyb2xsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiI3QU16TGktYmRTM1gybEV4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=7AMzLi-bdS3X2lExAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAAYFz7CE43l9qI4KI4iII_4KR7uaRHEo13JTmhfSMfU_yGr_rOqt2dOEqGIyrCnjFNRRu4YX6RRvZwd5HYESzgKhYKYXYBhOEk4hPNt0F1KBtjSkmtL4oaXevkbYTJymieYsu_rgTMNCRU6lbr5dgO7Ov6kD_swpQlJYgM56mnjBb7Bh6UkD0yNnDhPuIPsgWr7asAAAA&shmds=v1_AdeF8KgdWeAg2CAGkFkBzRphX7RkPGLpS9TVp0CfVLA3JykpGQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=7AMzLi-bdS3X2lExAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550f6f73161419591ea7d7c57bacc38d027cbd58da61246d96d7.jpeg",
         "Senior Data Analyst, Marketing Science",
         "Wellfound"
        ],
        [
         [
          {
           "link": "https://eworker.co/job/principal-data-analyst-14/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "EWorker"
          }
         ],
         "Storable",
         "About the Role:\nWe’re seeking a strategic, data-driven professional to lead our Business Analysis initiatives for our Marketplace product within the Self-Storage vertical. This is a ‘hands on keyboard’ high-impact role that bridges business strategy, analytical rigor, and data visualization —ideal for someone who thrives on translating data into actionable insights that drive performance. As the Principal Data Analyst, you will own our marketplace performance metrics, lead the development of world-class dashboards and reporting frameworks, and partner closely with Product, Marketing, Operations, and Finance teams to inform strategic decisions. You’ll drive both hands-on analysis and organizational data literacy while mentoring a team of analysts and collaborating with engineering on data infrastructure.\nKey Responsibilities:\n\nOwn and continuously optimize key Marketplace KPIs (conversion, revenue yield, inventory, pricing, etc.).\nServe as the steward of metric definitions and business logic—ensuring consistency, accuracy, and alignment across stakeholders.\nHave a deep understanding of how to run a BI environment. Proactive, insightful, curious.\nBuild and scale world class self-service dashboards and Tableau reports for internal and executive stakeholders.\nLead cross-functional analysis projects to uncover trends, diagnose problems, and identify growth and efficiency opportunities.\nTranslate complex data into clear, actionable insights and concise narratives for business and executive audiences.\nDrive rigorous SQL-based analysis from raw datasets, ensuring high-quality, reproducible outputs.\nGuide and collaborate with junior analysts in other groups and collaborate with engineering to shape data pipelines and definitions.\nCollaborate with cross functional partners and teams (Product,Marketing, Finance, Business Insights) to test hypotheses, evaluate feature/campaign performance, and measure outcomes.\nIdentify data gaps or integrity risks and work with stakeholders to improve reliability and accuracy.\nChampion a culture of curiosity, experimentation, and evidence-based decision-making.\nProactively keep other teams in the loop on new data capabilities unlocked and changes being made.\n\nRequirements:\n\n5-7 years of experience in business analysis, data analytics, or related roles, ideally with at least 2+ years in a leadership capacity.\nAdvanced proficiency in SQL with experience working with complex data models and the ability to query complex relational datasets.\nOther data engineering experience is a significant plus to facilitate sourcing/formating of data.\nDeep understanding of performance metrics and business levers in a supply/demand or marketplace environment.\nExperience leading analytics efforts in partnership with cross-functional teams including Product, Marketing, Finance, and Engineering.\nProven track record of developing intuitive, scalable dashboards using Tableau (or similar BI platforms).\nStrong business acumen with the ability to connect the dots between business objectives and analytical frameworks.\nExceptional communication skills with the ability to distill technical findings for non-technical audiences.\nCapable of influencing and informing executive stakeholders with clear, concise insights.\nDemonstrated experience owning KPIs, building data products, and drive insights into action in a fast paced environment.\nAbility to navigate ambiguity, manage and prioritize competing needs, and work cross-functionally.\n\nPreferred Qualifications:\n\nExperience in the storage, real estate, or marketplace industries strongly preferred\nFamiliarity with modern data stack tools such as Snowflake, dbt, or similar Experience analyzing marketplace dynamics or supply/demand business models\nExposure to experimentation frameworks, A/B testing, or uplift modeling\nPrior exposure to high-growth SaaS or Marketplace operations\nData engineering capabilities—especially in sourcing, transforming, or centralizing disparate datasets.\n\nAbout Us:\nAt Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\nWe leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.\nImportant Notice: Protect Yourself from Fraudulent Activities Targeting Job Seekers\nWe’ve been made aware of fraudulent activities where an individual or group is pretending to represent Storable, attempting to deceive job seekers by using our company’s good name and stellar reputation.\nTo protect yourself, please consider the following guidelines:\n– Official Communication: All genuine communication from Storable will come from official email addresses, ending in “@storable.com.” Be cautious of any communication that doesn’t match this criteria.– No Unsolicited Offers: We do not extend job offers without a formal interview process. If you receive an unsolicited job offer claiming to be from Storable or any of its representatives, it’s a red flag.– Verification: If you’re uncertain about the legitimacy of any job offer or communication claiming to be from Storable, please directly contact our HR department directly at POps@storable.com for verification.\nYour security and trust are paramount to us. If you suspect you’ve been contacted by someone falsely claiming to be from Storable or using our company’s name for any dubious purpose, please immediately report the incident to POps@storable.com\nWe’re committed to ensuring a transparent and secure hiring process.\nThank you for your vigilance and interest in joining our team.",
         {
          "posted_at": "18 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "18 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiU3RvcmFibGUiLCJhZGRyZXNzX2NpdHkiOiJTZXJpbGluZ2FtcGFsbGUgKE0pLCBIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InljaTdhWW0wWnR2MEMzbkZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "Serilingampalle (M), Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=yci7aYm0Ztv0C3nFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNvQrCMBRGce0jON1RpW1EcFEXQfAHBKHu5aa9pJH03pBkaF_JpzQu33DOga_4LorqFSx31qODCyaEM6ObY4IKHqIhEoZuAGG4ihhHy-OQko8HpWJ0tYkJk-3qTkYlTFom9REd_9PGAQN5h4na3X471Z7N5tQkCagdgWVoKFhn2eCYvzNaPdcl3OaecoF9CW9ymC1jCXfuLf4Ag_mA060AAAA&shmds=v1_AdeF8KjCBpWl289vZv2SnP2YCKXhew82EyOgaZRf9mt6vq4tDw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=yci7aYm0Ztv0C3nFAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550f8f526436192f16ed0a38f7cdb2e7bb6d317729673fd2f965.jpeg",
         "Principal Data Analyst",
         "EWorker"
        ],
        [
         [
          {
           "link": "https://powertofly.com/jobs/detail/2319977?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "PowerToFly"
          },
          {
           "link": "https://www.kitjob.in/job/156094017/ilc024-data-analyst-1-bengaluru/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Kit Job"
          },
          {
           "link": "https://in.findjob24h.com/de/data-analyst-1-job228081?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobs In India. New Jobs, New Recruitment And Fastest 2025 - Findjob24h.com"
          }
         ],
         "UnitedHealth Group",
         "At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.\n\nPrimary Responsibilities:\n• Validate data with administrative source systems (source of truth)\n• Analyze complex datasets\n• Generate actionable insights and recommendations based on data analysis\n• Database Management:\n• Develop and maintain data models, data dictionaries, and other documentation\n• Troubleshoot and resolve database-related issues\n• Data Extraction and Transformation:\n• Support capital project that will, in part, import and transform data from various sources for broker bonus calculations\n• Ensure data integrity and quality through rigorous validation and testing\n• Data Visualization and Reporting:\n• Create visually appealing and informative dashboards and reports\n• Present findings and insights to both technical and non-technical stakeholders in a clear and concise manner\n• Continuous Learning and Improvement:\n• Stay up to date with the latest data analysis techniques and tools\n• Identify opportunities to improve data analysis processes and methodologies\n• Actively participate in knowledge sharing and mentoring within the team\n• Comply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\n• Undergraduate degree or equivalent experience\n• 4+ years Experience as SAS Data Analyst\n• Experience with relational database management systems (e.g., MySQL, Oracle, SQL Server)\n• Experience with statistical analysis\n• Familiarity with data visualization tools (e.g., Tableau, Power BI)\n• Proven excellent problem-solving and critical thinking skills\n• Proven solid communication and presentation skills to effectively convey complex data analysis findings to both technical and non-technical stakeholders\n• Proven ability to work independently and collaboratively in a fast-paced, deadline-driven environment\n• Proven detail-oriented with a focus on accuracy and data integrity\n\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone - of every race, gender, sexuality, age, location and income - deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.\n\n#NTRQ",
         {
          "posted_at": "3 days ago",
          "schedule_type": "Full-time"
         },
         [
          "3 days ago",
          "Full-time"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IlVuaXRlZEhlYWx0aCBHcm91cCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJhdHFPMEswckdadGwwUy1xQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=atqO0K0rGZtl0S-qAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7Cbrc4iKYqOCikyBU3Z3LpT2alHgXcifU0T8X3_Ca76JZX9EQLoz5owZ72MJDAihh7SMIQysyZlqeo1nRk_eq2Y1qaKl3vby8MAWZ_SRB_3UasVLJaNQdjrvZFR43qycno-FGmC1CW-VdIDHceUj4A48vSB6EAAAA&shmds=v1_AdeF8KgRRYJHoDYbXc7fjK1BSe6PTzGUlRHkX-gsGlpdfX6DEA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=atqO0K0rGZtl0S-qAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550f061c5dcfce02185763416e4255437348fd02f62c016e5bce.jpeg",
         "Data Analyst 1",
         "PowerToFly"
        ],
        [
         [
          {
           "link": "https://builtin.com/job/data-analyst-competitive-benchmarking-reporting/4372058?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Built In"
          },
          {
           "link": "https://www.shine.com/jobs/data-reporting-analyst/hitachi-careers/17179165?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Shine"
          },
          {
           "link": "https://reputation.wd1.myworkdayjobs.com/en-US/External/job/Data-Analyst---Competitive-Benchmarking---Reporting_JR101313?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Workday"
          },
          {
           "link": "https://in.linkedin.com/jobs/view/data-analyst-%E2%80%93-competitive-benchmarking-reporting-at-reputation-4175824635?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "LinkedIn"
          },
          {
           "link": "https://www.glassdoor.co.in/job-listing/data-analyst-%E2%80%93-competitive-benchmarking-and-reporting-reputation-JV_IC2865319_KO0,53_KE54,64.htm?jl=1009664407483&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Glassdoor"
          },
          {
           "link": "https://in.jobrapido.com/jobpreview/1314205064239251456?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
           "title": "Jobrapido.com"
          }
         ],
         "Reputation",
         "Why Work at Reputation?\n• Reputation reached over $100m in Annual Recurring Revenue (ARR) in 2022 and continues to grow worldwide.\n• We've raised over $200 million in funding from A-list venture capital firms such as Bessemer Venture Partner and Kleiner Perkins, including $150 million in equity financing from Marlin Equity Partners in January 2022.\n• Reputation is trusted by more than 250 partners, including Google, Facebook, Salesforce, J.D. Power, Amazon and Web.com.\n• Our industry leading platform has been recognized by Forrester and Gartner as a vendor of choice in Voice of the Customer, Customer Feedback Management, and Social Suites research reports.\n• The platform is used by 10+ major automotive OEMs and 16,000 auto dealerships, more than 250 healthcare systems, and over 100 leading property management firms.\n• Our executive management team is committed to building a performance-based culture where excellence is rewarded and careers are developed.\n• Who thrives at Reputation? Managers who embody a player-coach mentality. Employees who value teamwork and cross-functional collaboration. People who emphasize perseverance and hustle over quick wins and luck.\n• Our Mission: We exist to forge relationships between companies and communities.\n\nWe are seeking a Data Analyst with expertise in data stewardship and analysis to manage our competitive benchmark data tenants and produce customer-facing reports for marketing and sales. The ideal candidate is proficient in BigQuery (SQL), detail-oriented, and capable of working independently on complex data projects.\n\nResponsibilities:\n• Data Stewardship & Governance: Ensure data accuracy, integrity, and accessibility by managing competitive benchmark data tenants, implementing verification processes, and maintaining governance best practices.\n• Data Pipeline Management: Develop and maintain data ingestion pipelines, including API integrations and web scraping, ensuring timely updates and scalability.\n• Data Validation & Quality Control: Oversee large-scale data input and validation, balancing hands-on data manipulation with automation strategies. Collaborate with data owners to resolve quality issues.\n• Advanced Analytics & Reporting: Conduct in-depth data analysis to uncover trends, patterns, and insights that drive business decisions. Scope, design, and execute analysis projects with minimal supervision.\n• Industry & Market Insights: Develop data-driven industry reports for marketing and sales, leveraging analytics to provide insights on market trends and competitive landscapes.\n• Cross-Functional Collaboration: Work closely with product managers, engineers, marketing, and leadership to resolve data issues, provide training, and ensure alignment on data best practices.\n• Tool & Process Optimization: Design scalable internal tools and reporting solutions to enhance data accessibility, automation, and usability across teams.\n• Stakeholder Communication: Translate complex data insights into actionable recommendations for key stakeholders, including non-technical audiences.\n\nQualifications:\n• 5+ years of proven experience in data analysis, data stewardship, and managing large-scale datasets, with a strong focus on ensuring data accuracy, integrity, and accessibility.\n• Must be proficient with SQL and NoSQL (BigQuery), as well as have the ability to thrive in an environment that often requires patience and creativity to query, extract and validate data in which there are high levels of uncertainty.\n• Strong prior professional experience managing databases and using applicable tools is required.\n• Experience with and knowledge of ETL processes and data migration.\n• Understanding of and prior experience with General Data Protection Regulation.\n• Demonstrated experience and proficiency in using Python in real-world, business applications are substantial advantages to candidates.\n• Highly articulate with outstanding verbal and written communication skills; you are able to explain data-driven insights and analyses intuitively to clients, executives, and technical and non-technical coworkers in both large and small groups.\n• You should be comfortable taking feedback from everyone – Product Managers, Engineers, customers, the CEO, etc. – to help understand business objectives and develop the most powerful analyses and tools possible.\n• Proven ability to operate in a fast-paced, data-driven environment.\n\nWhen you join Reputation, you can expect:\n• Flexible working arrangements.\n• Career growth with paid training tuition opportunities.\n• Active Employee Resource Groups (ERGs) to engage with.\n• An equitable work environment.\n\nOur employees say it best:\n\nAccording to Glassdoor, 94% of our employees approve of our CEO, Joe Burton.\n\nOur employees highlight our:\n• Ample Opportunities- “There are many opportunities to learn and grow. Many open roles are replaced with internal promotions.”\n• Positive Culture- \"Great opportunity and exceptional culture.\" “You will never have a better culture anywhere else. Period.”\n• Training and Tools- “All managers truly want you to succeed, and you are given great tools and training to be successful in your role.”\n• Balance- “Great work life balance and awesome team environment!”\n\nDiversity Programs & Initiatives:\n\nOur Reputation Nation spans around the world. This global perspective allows us to intentionally unlock the magic that comes from diversity of experience to contribute to our success.\n\nAt Reputation, we believe in:\n• Diversity: Embracing a culture that values uniqueness.\n• Inclusion: Inviting diverse groups to take part in company life.\n• Belonging: Helping each individual feel accepted for who they are.\n\n\"At Reputation, we see diversity and inclusion as the foundation for an equitable workplace. Our goal is to empower all of our employees, regardless of their background, to make an impact in their work each and every day.\" - Joe Burton, CEO, Reputation\n\nAdditionally, we offer a variety of benefits and perks, such as:\n• Health Insurance & Wellness Benefits: Group Health Insurance: Medical Insurance with floater policy of up to 10,00,000 for employee + spouse + 2 dependent children + 2 parents / parent-in-laws\n• Maternity Benefits: Medical insurance up to 75,000 INR, 26 weeks of leave for birth, adoption or surrogacy\n• Life Insurance: Insurance at 3x annual cost to the company (Term Insurance, GPA)\n• Accident/Disability Insurance: Insured at 3x base salary for permanent total disability, permanent partial disability and temporary total disability (GPA)\n• OPD: of 7500 per annum per employee\n\nLeaves\n• 10 Company observed holidays a year (Refer to the Holiday Calendar for the Year)\n• 12 Casual/Sick leaves (Pro-rata calculated)\n• 02 Earned Leaves per Month (Pro-rata calculated)\n• 04 Employee Recharge days (aka company holiday/office closed)\n• Maternity & Paternity (6 months)\n• Bereavement Leave (10 Days)\n\nCar Lease:\nReputation offers Car Lease Program that allows employees to lease a car with no upfront cost or down payment. They benefit from a fixed monthly lease rental and 20-30% tax savings.\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\nTo learn more about how we handle the personal data of applicants, visit our Candidate Privacy Notice.\n\nApplicants only - No 3rd party agency candidates.",
         {
          "posted_at": "6 days ago",
          "qualifications": "No degree mentioned",
          "schedule_type": "Full-time"
         },
         [
          "6 days ago",
          "Full-time",
          "No degree mentioned"
         ],
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3Qg4oCTIENvbXBldGl0aXZlIEJlbmNobWFya2luZyBcdTAwMjYgUmVwb3J0aW5nIiwiY29tcGFueV9uYW1lIjoiUmVwdXRhdGlvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiT0NPR3UtU1dxWjh4X0gzS0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "Hyderabad, Telangana, India",
         "Data Analyst",
         "https://www.google.com/search?ibp=htl;jobs&q=Data+Analyst&htidocid=OCOGu-SWqZ8x_H3KAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMOwrCQBBAsc0RrKYSlJiIYKOVH_CDldjLJBk2q5uZZXeU2HkH7-KBPImmebzmveTTS44bVIQlo3tGhe_rDWtpPKlV-yBYEZd1g-Fm2cAATuQlaOdjOEgBkTCUNQjDVsQ46i9qVR_neR6jy0xUVFtmpTS5MBXS5lcpYodLrDGQd6h0mc4mbebZjIb__b1L_j_LsHtWFLDAKoUzOWSDjCnsubL4A5xdZbm5AAAA&shmds=v1_AdeF8KgqJaK4z_DrbsmmMwvzImCqZmsYLB7D-WOnGPBr39JPlQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Analyst&htidocid=OCOGu-SWqZ8x_H3KAAAAAA%3D%3D",
         "https://serpapi.com/searches/68579c26d3fb8566c6fdfdd7/images/fada94f4e909550fc40b2f11b139c5c1588093d189c8c58e29d87754dd6f59e2.jpeg",
         "Data Analyst – Competitive Benchmarking & Reporting",
         "Built In"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "apply_options",
         "type": "{\"containsNull\":true,\"elementType\":{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"},\"type\":\"array\"}"
        },
        {
         "metadata": "{}",
         "name": "company_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "detected_extensions",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "extensions",
         "type": "{\"containsNull\":true,\"elementType\":\"string\",\"type\":\"array\"}"
        },
        {
         "metadata": "{}",
         "name": "job_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "search_role",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "share_link",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "thumbnail",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "via",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"✅Reading from Bronze Layer -------> Silver Layer\")\n",
    "df=spark.read.table(\"lakehouse.bronze.jobs_raw\")\n",
    "\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03dd22b3-54a6-4cb6-bb4b-fd994de55596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- ### Cleaning and Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24201ab9-1eb7-4f6c-920e-c612c6b08b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>company_name</th><th>location</th><th>description</th><th>job_id</th><th>posted_at</th><th>search_role</th></tr></thead><tbody><tr><td>Lead Consultant - Technical Lead - Fullstack Data Engineer</td><td>AstraZeneca</td><td>Chennai, Tamil Nadu, India</td><td>Job Title: Lead Consultant - Technical Lead - Data, Analytics & AI\n",
       "Career Level: E\n",
       "\n",
       "Introduction to role: Are you ready to redefine an industry and change lives? Join our Operations IT organization, where your work will have a direct impact on patients by redefining our ability to develop life-changing medicines. We empower the business to perform at its peak, combining innovative science with leading digital technology platforms and data. As part of the Data Analytics and AI (DA&AI) group, you'll deliver innovative solutions that drive the transformation of medicine development. As the Technical Lead, you'll oversee the technical delivery of products and projects aligned with business objectives, tapping into your expertise in Data and Cloud Engineering, Software Engineering, AI, and more. Collaborate with stakeholders to ensure seamless delivery and maximum business impact!\n",
       "\n",
       "Accountabilities:\n",
       "• Bridge business needs with technical solutions by leading IT application design and implementation.\n",
       "• Collaborate with stakeholders to define and deliver requirements, translating them into detailed specifications.\n",
       "• Own the technical vision of products and compile detailed technical designs, refining user Epics/stories.\n",
       "• Optimize performance, mitigate risks, and ensure alignment to timelines and resource allocations.\n",
       "• Advise on industry trends and standard methodologies to enhance performance and business outcomes.\n",
       "• Provide technical direction and guidance to IT teams and business units.\n",
       "• Contribute to Data & Software Engineering standards and best practices.\n",
       "• Research new technologies to boost system performance and scalability.\n",
       "• Lead and mentor technical teams (Data Engineering, Cloud Engineering, Software Engineering) and work with AI/GenAI leads to foster collaboration and innovation.\n",
       "• Ensure platform stability, scalability, and simplicity while adhering to regulatory requirements and data security standards.\n",
       "• Foster continuous improvement and innovation.\n",
       "• Supervise technical leadership throughout the software development lifecycle and provide hands-on support in development, technical delivery, review, and testing while collaborating with global teams.\n",
       "• Apply central Enterprise Data Platforms and guide DevOps, DataOps, and MLOps teams to ensure standard methodologies.\n",
       "• Ensure data solutions align with FAIR principles and support end-to-end data science and machine learning.\n",
       "\n",
       "Essential Skills/Experience:\n",
       "• Minimum 10 years of experience in the design, development, and delivery of software and data engineering solutions.\n",
       "• Extensive technical expertise in Data Engineering, Software Engineering, and Cloud Engineering.\n",
       "• Strong foundational knowledge of AI Engineering principles and practices.\n",
       "• Deep understanding of DevOps, MLOps, and DataOps methodologies with practical implementation experience.\n",
       "• Demonstrated success in product development and/or product management, delivering complex solutions end-to-end.\n",
       "• Demonstrable ability to provide technical thought leadership across Data, Analytics, and AI domains.\n",
       "• Exceptional communication, customer management, and multi-functional collaboration skills.\n",
       "• Robust analytical and problem-solving abilities with a collaborative, team-oriented approach.\n",
       "• Hands-on experience driving innovation throughout the full product development lifecycle.\n",
       "• Solid understanding of Data Mesh and Data Product concepts and architectures.\n",
       "• Proficiency in Agile methodologies and facilitating iterative, multi-functional team delivery.\n",
       "• Hands-on experience designing, implementing, and optimizing data pipelines using leading ETL tools.\n",
       "• Skilled in architecting, deploying, and managing scalable, secure AWS cloud environments.\n",
       "• Proficient in workflow orchestration tools such as Apache Airflow.\n",
       "• Practical experience implementing DataOps practices with tools like DataOps.Live.\n",
       "• Strong expertise in data storage and analytics platforms such as Snowflake.\n",
       "• Ability to deliver actionable insights through business intelligence tools, including Power BI.\n",
       "• Extensive full-stack development experience, including backend proficiency with Node.js and Python and frontend expertise with ReactJS or NextJS.\n",
       "• Experience designing and deploying Generative AI solutions.\n",
       "• Hands-on implementation of AI/ML models using platforms such as Amazon SageMaker.\n",
       "• Advanced programming skills, especially in Python.\n",
       "• Solid knowledge of both SQL and NoSQL database technologies.\n",
       "• Familiarity with agile ways of working and iterative development environments.\n",
       "• Experience working in large, multinational organizations or pharmaceutical environments is highly desirable.\n",
       "• Demonstrated leadership and mentoring skills, with a demonstrable ability to develop high-performing technical teams.\n",
       "\n",
       "Desirable Skills/Experience:\n",
       "• Bachelor's or master's degree in health sciences, Life Sciences, Data Management, IT, or a related field.\n",
       "• Experience in the pharmaceutical industry or a similar multinational environment.\n",
       "• AWS Cloud or relevant data/software engineering certifications.\n",
       "• Awareness of use case specific GenAI tools available in the market and their application in day-to-day work scenarios.\n",
       "• Possess working knowledge of basic prompting techniques and continuously improve these skills.\n",
       "• Stay up to date with developments in AI and GenAI, applying new insights to work-related situations.\n",
       "\n",
       "When we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That's why we work, on average, a minimum of three days per week from the office. But that doesn't mean we're not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\n",
       "\n",
       "At AstraZeneca, we are at a crucial stage of our journey to become a digital and data-led enterprise. Our commitment to innovation empowers us to make the impossible possible by building partnerships and ecosystems that drive scale and speed for exponential growth. With investment backing us all the way, we are focused on disrupting the industry while making a meaningful impact through our work. Here you'll find countless opportunities to learn, grow, and contribute to developing life-changing medicines.\n",
       "\n",
       "Ready to make a difference? Apply now to join our team!\n",
       "\n",
       "Date Posted\n",
       "30-Jun-2025\n",
       "\n",
       "Closing Date\n",
       "\n",
       "AstraZeneca embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills. We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements.</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIENvbnN1bHRhbnQgLSBUZWNobmljYWwgTGVhZCAtIEZ1bGxzdGFjayBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiQXN0cmFaZW5lY2EiLCJhZGRyZXNzX2NpdHkiOiJDaGVubmFpLCBUYW1pbCBOYWR1LCBJbmRpYSIsImh0aWRvY2lkIjoiRGVXa2pFZDgxNXcwSUpJY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>3 days ago</td><td>Data Engineer</td></tr><tr><td>AWS Data Engineer</td><td>Cognizant</td><td>Hyderabad, Telangana, India (+1 other)</td><td>Job Summary:\n",
       "\n",
       "Experience : 4 - 8 years\n",
       "\n",
       "Location : Bangalore\n",
       "\n",
       "The Data Engineer will contribute to building state-of-the-art data Lakehouse platforms in AWS, leveraging Python and Spark. You will be part of a dynamic team, building innovative and scalable data solutions in a supportive and hybrid work environment. You will design, implement, and optimize workflows using Python and Spark, contributing to our robust data Lakehouse architecture on AWS. Success in this role requires previous experience of building data products using AWS services, familiarity with Python and Spark, problem-solving skills, and the ability to collaborate effectively within an agile team.\n",
       "\n",
       "Must Have Tech Skills:\n",
       "\n",
       "· Demonstrable previous experience as a data engineer.\n",
       "• Technical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\n",
       "\n",
       "· Proficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n",
       "\n",
       "Nice To Have Tech Skills:\n",
       "\n",
       "· Familiar with data services in a Lakehouse architecture.\n",
       "\n",
       "· Familiar with technical design practices, allowing for the creation of scalable, reliable data products that meet both technical and business requirements\n",
       "\n",
       "· A master’s degree or relevant certifications (e.g., AWS Certified Solutions Architect, Certified Data Analytics) is advantageous\n",
       "\n",
       "Key Accountabilities:\n",
       "• Writes high quality code, ensuring solutions meet business requirements and technical standards.\n",
       "• Works with architects, Product Owners, and Development leads to decompose solutions into Epics, assisting the design and planning of these components.\n",
       "• Creates clear, comprehensive technical documentation that supports knowledge sharing and compliance. Experience in decomposing solutions into components (Epics, stories) to streamline development.\n",
       "• Actively contributes to technical discussions, supporting a culture of continuous learning and innovation.\n",
       "\n",
       "Key Skills:\n",
       "• Proficient in Python and familiar with a variety of development technologies.\n",
       "• Previous experience of implementing data pipelines, including use of ETL tools to streamline data ingestion, transformation, and loading.\n",
       "• Solid understanding of AWS services and cloud solutions, particularly as they pertain to data engineering practices. Familiar with AWS solutions including IAM, Step Functions, Glue, Lambda, RDS, SQS, API Gateway, Athena.\n",
       "• Proficient in quality assurance practices, including code reviews, automated testing, and best practices for data validation.\n",
       "• Experienced in Agile development, including sprint planning, reviews, and retrospectives\n",
       "\n",
       "Educational Background:\n",
       "• Bachelor’s degree in computer science, Software Engineering, or related essential.\n",
       "\n",
       "Bonus Skills:\n",
       "• Financial Services expertise preferred, working with Equity and Fixed Income asset classes and a working knowledge of Indices.\n",
       "• Familiar with implementing and optimizing CI/CD pipelines. Understands the processes that enable rapid, reliable releases, minimizing manual effort and supporting agile development cycles.</td><td>eyJqb2JfdGl0bGUiOiJBV1MgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkNvZ25pemFudCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiWjYyVVNZZkhtUHk0dHdwNkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 days ago</td><td>Data Engineer</td></tr><tr><td>Engineer III Consultant-Data Engineering</td><td>Verizon</td><td>Hyderabad, Telangana, India (+2 others)</td><td>When you join Verizon\n",
       "\n",
       "You want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n",
       "\n",
       "What you’ll be doing…\n",
       "\n",
       "We are looking for data engineers who can work with world class team members to help drive telecom business to its full potential. We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n",
       "\n",
       "As a data engineer you will be collaborating with business product owners, coaches, industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n",
       "\n",
       "As a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n",
       "• Understanding the business requirements and converting them to technical design.\n",
       "• Working on Data Ingestion, Preparation and Transformation.\n",
       "• Developing data streaming applications.\n",
       "• Debugging the production failures and identifying the solution.\n",
       "• Working on ETL/ELT development.\n",
       "• Understanding devops process and contributing for devops pipelines\n",
       "\n",
       "What we’re looking for...\n",
       "\n",
       "You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n",
       "\n",
       "You’ll need to have…\n",
       "• Bachelor’s degree or four or more years of work experience.\n",
       "• Four or more years of relevant work experience.\n",
       "• Experience with Data Warehouse concepts and Data Management life cycle.\n",
       "• Experience in Big Data technologies - GCP/Hadoop/Spark/Composer/DataFlow/Bigquery.\n",
       "• Experience in complex SQL.\n",
       "• Experience working on Streaming ETL pipelines\n",
       "• Expertise in Java\n",
       "• Experience with MemoryStore / Redis / Spanner\n",
       "• Experience in troubleshooting the data issues.\n",
       "• Experience with data pipeline and workflow management & Governance tools.\n",
       "• Knowledge of Information Systems and their applications to data management processes.\n",
       "\n",
       "Even better if you have one or more of the following…\n",
       "• Three or more years of relevant experience.\n",
       "• Any relevant Certification on ETL/ELT developer.\n",
       "• Certification in GCP-Data Engineer.\n",
       "• Accuracy and attention to detail.\n",
       "• Good problem solving, analytical, and research capabilities.\n",
       "• Good verbal and written communication.\n",
       "• Experience presenting to and influence stakeholders.\n",
       "• Experience in driving a small team of 2 or more members for technical delivery\n",
       "\n",
       "#AI&D\n",
       "\n",
       "Where you’ll be working\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "Scheduled Weekly Hours\n",
       "40\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Verizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.</td><td>eyJqb2JfdGl0bGUiOiJFbmdpbmVlciBJSUkgQ29uc3VsdGFudC1EYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiVmVyaXpvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEhtc2E1cThWc3RlSGhKaUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Data Engineer</td></tr><tr><td>Data Engineer (Python, Pyspark, and Azure Databricks) || (4 - 6 Years)</td><td>Emids</td><td>Bengaluru, Karnataka, India</td><td>Hi All,\n",
       "\n",
       "Greetings for the day!!\n",
       "\n",
       "We are currently hiring for Data Engineer (Python, Pyspark, and Azure Databricks) for Emids(MNC) at Bangalore location.\n",
       "\n",
       "Role: Data Engineer\n",
       "\n",
       "Exp: 5 to 8 Years\n",
       "\n",
       "Location: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\n",
       "\n",
       "NP: Immediate to 15 Days (Try to find only immediate joiners)\n",
       "\n",
       "Note: Candidate Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.\n",
       "\n",
       "Role Overview:\n",
       "\n",
       "We are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Lead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\n",
       "• Architect scalable data streaming and processing solutions to support healthcare data workflows.\n",
       "• Develop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\n",
       "• Ensure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\n",
       "• Collaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\n",
       "• Troubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\n",
       "• Mentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\n",
       "• Stay updated with the latest cloud technologies, big data frameworks, and industry trends.\n",
       "\n",
       "Required Skills & Qualifications:\n",
       "• 4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\n",
       "• Expertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\n",
       "• Experience with Azure Databricks (or willingness to learn and adopt it quickly).\n",
       "• Hands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\n",
       "• Proficiency in SQL, NoSQL databases, and data modeling for big data processing.\n",
       "• Knowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\n",
       "• Experience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\n",
       "• Strong analytical skills, problem-solving mindset, and ability to lead complex data projects.\n",
       "• Excellent communication and stakeholder management skills.\n",
       "\n",
       "Note: This is not a contract position, this will be a permanent position with Emids.\n",
       "\n",
       "Interested candidates Can Share Your Updated Profile with details for below Email.\n",
       "\n",
       "NAME:\n",
       "\n",
       "CCTC:\n",
       "\n",
       "ECTC:\n",
       "\n",
       "Notice Period:\n",
       "\n",
       "Offers in Hand :\n",
       "\n",
       "Email ID: Ravi.chekka@emids.com</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChQeXRob24sIFB5c3BhcmssIGFuZCBBenVyZSBEYXRhYnJpY2tzKSB8fCAoNCAtIDYgWWVhcnMpIiwiY29tcGFueV9uYW1lIjoiRW1pZHMiLCJhZGRyZXNzX2NpdHkiOiJCZW5nYWx1cnUsIEthcm5hdGFrYSwgSW5kaWEiLCJodGlkb2NpZCI6Im9TMEEyREhLeU9wd0lVdnVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>8 hours ago</td><td>Data Engineer</td></tr><tr><td>Senior Data Engineer</td><td>Mastercard</td><td>Pune, Maharashtra, India</td><td>Job Title:\n",
       "\n",
       "Senior Data Engineer\n",
       "\n",
       "Overview:\n",
       "\n",
       "Position Overview:\n",
       "\n",
       "The Senior Data Engineer, MyMPA will be part of GBSC’s Automation & Engineering Team, responsible for implementing enhancements and periodic refreshes of an enterprise-wide data platform.\n",
       "\n",
       "This role will also work closely with the VP of Analytics & Metrics and Director of FP&A and gather requirements for changes and enhancements to the application and contribute to the technology platform's evolution as it grows to support the rapidly expanding Mastercard business.\n",
       "\n",
       "The ideal candidate will have hands-on development skills combined with an ability to analyze and understand end user requirements that are critical success factors within this role. This role requires the skills and desire to work as an individual contributor as well as, collaborate cross functionally with various business constituents.\n",
       "\n",
       "1. Have you ever worked on an enterprise-wide reporting solution that relied heavily on your own knowledge and abilities to build and maintain the solution?\n",
       "2. Are you constantly hungry to learn? Do you have the “growth mindset” as opposed to the “fixed mindset”?\n",
       "3. Do you love working with people, helping them, and turning their requirements into something that can make a difference?\n",
       "\n",
       "Role:\n",
       "\n",
       "• Skilled at writing performant and reliable SQL queries to source data from Centralized data repositories such as a Data Warehouse.\n",
       "• Proficient at designing and developing Relational and Multi-dimensional Databases to host the sourced data and work with Tableau and Power BI developers within and outside the team to support reporting and dashboarding needs of the organization.\n",
       "• Develop ETL workflows and macros in Alteryx to source data for projects and build high-level and detailed data validations to ensure accuracy of the sourced data\n",
       "• Contribute to initiatives aimed at automating the data extraction, application of data quality checks to ensure datasets are released on time and with 100% accuracy.\n",
       "• Develop technical components to ensure department’s compliance with audit requirements such as SOX and other statutory/audit requirements as applicable from time to time.\n",
       "• Organize and lead discussions with customers to brainstorm on data quality issues and contribute to discussions to devise business rules to address data quality issues\n",
       "• Use MS-Excel and MS-PPT to capture findings and present to customers in an easy-to-understand and impactful manner\n",
       "• Liaison with the internal groups in MasterCard Operations and Technology to ensure our solutions remain in compliance with MasterCard technical standards. Navigate O&T requirements around change management and new development.\n",
       "\n",
       "All About You:\n",
       "\n",
       "• Strong understanding of Windows and Linux server.\n",
       "• Good understanding of SQL Server or Oracle DB.\n",
       "• Solid understanding of Essbase technology – understand how this technology works, for both BSO\n",
       "and ASO cubes.\n",
       "• Develop BSO and ASO cubes with a strong eye for performance.\n",
       "• Strong commitment to quality and error testing code you develop. Strong ability to step in and analyze the code of others on the team.\n",
       "• Be able to work within an Agile environment that is highly responsive to the business. Our team is part of the Finance organization – you must be comfortable with working as part of the business with a strong “roll up your sleeves” mentality.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6Ik1hc3RlcmNhcmQiLCJhZGRyZXNzX2NpdHkiOiJQdW5lLCBNYWhhcmFzaHRyYSwgSW5kaWEiLCJodGlkb2NpZCI6IlV1SE5zREk1eXZ2ckFfdVBBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>Data Engineer</td></tr><tr><td>Architect - Data Engineer</td><td>PepsiCo</td><td>Hyderabad, Telangana, India</td><td>Overview\n",
       "\n",
       "Provide the job title you would like to be displayed on the job posting:\n",
       "\n",
       "Data Platform Engineer – Transformation & Modernization\n",
       "\n",
       "Job Overview: A Data Platform Engineer to be a key player in our transformation and modernization programs, leading the migration of applications from legacy systems to Azure-based architectures. This role involves designing, implementing, and optimizing scalable, cloud-native data solutions using Databricks, Azure DevOps (ADO), and Agile development methodologies.\n",
       "\n",
       "As an active contributor to code development, you will help drive automation, operational excellence, and data quality across our platforms. You will collaborate with data science and product teams to create solutions that enhance our data-driven decision-making capabilities.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Responsibilities: • Lead the migration and modernization of data platforms, moving applications and pipelines to Azure-based solutions.\n",
       "• Actively contribute to code development in projects and services.\n",
       "• Manage and scale data pipelines from internal and external data sources to support new product launches and ensure high data quality.\n",
       "• Develop automation and monitoring frameworks to capture key metrics and operational KPIs for pipeline performance.\n",
       "• Implement best practices around systems integration, security, performance, and data management.\n",
       "• Collaborate with internal teams, including data science and product teams, to drive solutioning and proof-of-concept (PoC) discussions.\n",
       "• Develop and optimize procedures to transition data into production.\n",
       "• Define and manage SLAs for data products and operational processes.\n",
       "• Prototype and build scalable solutions for data engineering and analytics.\n",
       "• Research and apply state-of-the-art methodologies in data and Platform engineering.\n",
       "• Create and maintain technical documentation for knowledge sharing.\n",
       "• Develop reusable packages and libraries to enhance development efficiency.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Qualifications: • Bachelor’s degree in Computer Science, MIS, Business Management, or related field\n",
       "• 10 + years’ experience in Information Technology\n",
       "• 4 + years of Azure, AWS and Cloud technologies\n",
       "• Experience in data platform engineering, with a focus on cloud transformation and modernization.\n",
       "• Strong knowledge of Azure services, including Databricks, Azure Data Factory, Synapse Analytics, and Azure DevOps (ADO).\n",
       "• Proficiency in SQL, Python, and Spark for data engineering tasks.\n",
       "• Hands-on experience building and scaling data pipelines in cloud environments.\n",
       "• Experience with CI/CD pipeline management in Azure DevOps (ADO).\n",
       "• Understanding of data governance, security, and compliance best practices.\n",
       "• Experience working in an Agile development environment.\n",
       "• Prior experience in migrating applications from legacy platforms to the cloud.\n",
       "• Knowledge of Terraform or Infrastructure-as-Code (IaC) for cloud resource management.\n",
       "• Familiarity with Kafka, Event Hubs, or other real-time data streaming solutions.\n",
       "• Experience with lagacy RDBMS (Oracl, DB2, Teradata)\n",
       "• Background in supporting data science models in production.\n",
       "\n",
       "Does the person hired for this job need to be based in a PepsiCo office, or can they be remote?: Employee must be based in a Pepsico office\n",
       "\n",
       "Primary Work Location: Hyderabad HUB-IND\n",
       "\n",
       "Is this role approved for relocation?: No\n",
       "\n",
       "Would you like to initially post this job internally-only or both internally and externally?: Post both internally and externally</td><td>eyJqb2JfdGl0bGUiOiJBcmNoaXRlY3QgLSBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiUGVwc2lDbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNzVXaWFLWW1Xa1V2Vk5VY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Data Engineer</td></tr><tr><td>Senior Analytics Data Engineer</td><td>Okta, Inc.</td><td>Bengaluru, Karnataka, India</td><td>Get to know OktaOkta is The World’s Identity Company. We free everyone to safely use any technology, anywhere, on any device or app. Our flexible and neutral products, Okta Platform and Auth0 Platform, provide secure access, authentication, and automation, placing identity at the core of business security and growth.At Okta, we celebrate a variety of perspectives and experiences. We are not looking for someone who checks every single box - we’re looking for lifelong learners and people who can make us better with their unique experiences. Join our team! We’re building a world where Identity belongs to you.Senior Analytics Engineer\n",
       "We are looking for an experienced Analytics Engineer to join Okta’s enterprise data team. This analyst will have strong background in SaaS subscription and product analytics, a passion for providing customer usage insights to internal stakeholders, and experience organizing complex data into consumable data assets. In this.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHl0aWNzIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJPa3RhLCBJbmMuIiwiYWRkcmVzc19jaXR5IjoiQmVuZ2FsdXJ1LCBLYXJuYXRha2EsIEluZGlhIiwiaHRpZG9jaWQiOiJhZHlINU5lRGFEQWwyUW1aQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>Data Engineer</td></tr><tr><td>Lead Data Engineer - Data Engineering</td><td>Cencora</td><td>India</td><td>Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\n",
       "\n",
       "Job Details\n",
       "\n",
       "PRIMARY DUTIES AND RESPONSIBILITIES:\n",
       "• Leads design, development, and automation of scalable Analytic data engineering products leveraging Machine Learning, cloud infrastructure\n",
       "• Leads the design, build and operationalization of monitoring and tracking of data quality and data flow dynamics\n",
       "• Optimizes existing data processes and implements best-in-class data transformation capabilities\n",
       "• Leverages advanced statistical and computational methodologies to deliver insights and identify strategic opportunities\n",
       "• Implements Data Ingestion Framework and other re-usable components leveraged for delivering data pipeline and supporting ad-hoc data ingestions\n",
       "• Assists with development and storage of analytics-ready data for development of analytic deliverables\n",
       "• Recommends data products to solve business problems meeting multiple stakeholder requirements\n",
       "• Drives project planning processes, delegates non-complex tasks to junior team members\n",
       "• Mentors other team members and assists them with priority setting and issue resolution\n",
       "• Maintains data and information environment that provides consistent and accurate reporting of the client’s data\n",
       "• Leverages Machine Learning to enhance the developed solution\n",
       "• Collaborates with Business and Client representatives to target strategic analytics opportunities and define deliverable scope\n",
       "• Facilitates the resolution of issues regarding projects being worked on to include proper documentation of recommendations for those issues\n",
       "• Manages multiple initiatives in parallel partnering with other Analytics team members and conducts review meetings with stakeholders to drive prioritization effort\n",
       "• Analyzes model errors and design strategies to overcome them\n",
       "• Implements best practices for data engineering to ensure quality delivery of enterprise solutions\n",
       "• Brings expert knowledge of data visualization tools and techniques to drive business analytics and semantic data access requirements.\n",
       "• Work closely with business users, vendors, and delivery teams to understand the business requirements that drive the analysis and design of business analytics and reporting solutions.\n",
       "• Conceptualize, design, and develop data visualization solutions that synthesize data concepts into clear communications for key business stakeholders.\n",
       "• Drives business stakeholder adoption of insights-driven decision making and/or business process innovation.\n",
       "• Leads knowledge transfer around using data visualizations to business stakeholders.\n",
       "• Assist in developing best practices for data presentation and sharing across the organization.\n",
       "• Ensures data visualization standards are maintained and implemented.\n",
       "• Demonstrate an ability to reduce data to the bare minimum of what is needed to optimally communicate a message.\n",
       "• Provides technical leadership, coaching and mentoring to team members and business users.\n",
       "• Participates in POC projects and provides business analytics solutions recommendations.\n",
       "• Evaluates new visualization tools and performs research on best practices.\n",
       "• Contributes to strategic planning meetings and provides guidance and expertise on system options, risk, cost vs. benefits, and impacts on business processes and goals.\n",
       "• Has expertise in multiple technical environments and possesses business knowledge that spans multiple business areas.\n",
       "• Responsible for BI Tool administration & security functions as designated\n",
       "\n",
       ".\n",
       "\n",
       "EDUCATIONAL QUALIFICATIONS:\n",
       "\n",
       "Bachelor’s Degree in Statistics, Computer Science, Information Technology or any other related discipline or equivalent related experience.\n",
       "\n",
       "Preferred Certifications:\n",
       "• Advanced Data Analytics Certifications\n",
       "• AI and ML Certifications\n",
       "• SAS Statistical Business Analyst Professional Certification\n",
       "\n",
       "WORK EXPERIENCE:\n",
       "6+ years of directly-related or relevant experience, preferably in healthcare data analytics or data engineering.\n",
       "\n",
       "Working Hours:\n",
       "\n",
       "7PM IST to 2AM IST; Hybrid Working Model\n",
       "\n",
       "SKILLS & KNOWLEDGE:\n",
       "\n",
       "Behavioral Skills:\n",
       "• Conflict Resolution\n",
       "• Creativity & Innovation\n",
       "• Decision Making\n",
       "• Planning\n",
       "• Presentation Skills\n",
       "• Risk-taking\n",
       "\n",
       "Technical Skills:\n",
       "• Advanced Data Visualization Techniques\n",
       "• Advanced Statistical Analysis\n",
       "• Big Data Analysis Tools and Techniques\n",
       "• Data Governance\n",
       "• Data Management\n",
       "• Data Modelling\n",
       "• Data Quality Assurance\n",
       "• Machine Learning and AI Fundamentals\n",
       "• Programming languages like SQL, R, Python\n",
       "\n",
       "Tools Knowledge:\n",
       "• Business Intelligence Software like Tableau, Power BI, Alteryx, QlikSense\n",
       "• Data Visualization Tools\n",
       "• Microsoft Office Suite\n",
       "• Statistical Analytics tools (SAS, SPSS3)\n",
       "\n",
       "What Cencora offers\n",
       "\n",
       "​Benefit offerings outside the US may vary by country and will be aligned to local market practice. The eligibility and effective date may differ for some benefits and for team members covered under collective bargaining agreements.\n",
       "\n",
       "Full time\n",
       "\n",
       "Affiliated Companies\n",
       "Affiliated Companies: CENCORA INDIA TECHNOLOGY SERVICES PRIVATE LIMITED\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Cencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\n",
       "\n",
       "The company’s continued success depends on the full and effective utilization of qualified individuals. Therefore, harassment is prohibited and all matters related to recruiting, training, compensation, benefits, promotions and transfers comply with equal opportunity principles and are non-discriminatory.\n",
       "\n",
       "Cencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com. We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgLSBEYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiQ2VuY29yYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJHdzRzcW9FbWlPdEVnTDAwQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>3 days ago</td><td>Data Engineer</td></tr><tr><td>Azure Data Engineer – Azure Databricks</td><td>Aiprus Software Private Limited</td><td>Bengaluru, Karnataka, India</td><td>Job Title: Azure Data Engineer – Azure Databricks\n",
       "\n",
       "Location: Bangalore, India\n",
       "\n",
       "Experience: 5 to 10 Years\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "As a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Design and develop scalable data pipelines using Azure Databricks and PySpark.\n",
       "• Transform raw data into actionable insights through advanced data engineering techniques.\n",
       "• Build, deploy, and maintain machine learning models using MLlib, TensorFlow, and MLflow.\n",
       "• Optimize data integration workflows from Azure Blob Storage, Data Lake, and SQL/NoSQL sources.\n",
       "• Execute large-scale data processing using Spark Pools, fine-tuning configurations for performance and cost-efficiency.\n",
       "• Collaborate with data scientists, analysts, and business stakeholders to deliver robust data solutions.\n",
       "• Maintain and enhance Databricks notebooks and Delta Lake architectures.\n",
       "\n",
       "Required Skills & Qualifications:\n",
       "• Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field.\n",
       "• 7+ years of experience in data engineering, with at least 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n",
       "• Strong proficiency in:\n",
       "• Python, PySpark, Pandas, NumPy, SciPy\n",
       "• Spark SQL, DataFrames, RDDs\n",
       "• Delta Lake, Databricks Notebooks, MLflow\n",
       "• Hands-on experience with:\n",
       "• Azure Data Lake, Blob Storage, Synapse Analytics\n",
       "• Excellent problem-solving and communication skills.\n",
       "• Ability to work independently and in a collaborative team environment.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with CI/CD pipelines for data workflows.\n",
       "• Familiarity with data governance and security best practices in Azure.\n",
       "• Knowledge of real-time data processing and streaming technologies.</td><td>eyJqb2JfdGl0bGUiOiJBenVyZSBEYXRhIEVuZ2luZWVyIOKAkyBBenVyZSBEYXRhYnJpY2tzIiwiY29tcGFueV9uYW1lIjoiQWlwcnVzIFNvZnR3YXJlIFByaXZhdGUgTGltaXRlZCIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiek5tRUxOMG5Iam5zazl6TkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 hours ago</td><td>Data Engineer</td></tr><tr><td>Principle Software Engineer for Data Platform - 31866</td><td>Splunk</td><td>Bengaluru, Karnataka, India</td><td>Splunk, a Cisco company, is building a safer and more resilient digital world with an end-to-end full stack platform made for a hybrid, multi-cloud world. Leading enterprises use our unified security and observability platform to keep their digital systems secure and reliable. Our customers love our technology, but it's our caring employees that make Splunk stand out as an amazing career destination. No matter where in the world or what level of the organization, we approach our work with kindness. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Come help organizations be their best, while you reach new heights with a team that has your back.Role SummarySplunk AppDynamics, a leader in observability solutions for both cloud and on-premises environments, empowers customers with end-to-end visibility across their entire application ecosystems, cloud, on-premises, or hybrid. Our team is dedicated to developing, maintaining, and supporting SaaS and on-premises offerings within the observability product suite.We're looking for a Senior Software Engineer to build upon our application and data platform as we continue to innovate on application observability. We move fast and iterate quickly. We are passionate about solving customers’ problems. We have ambitious goals to build best-of-its-kind products. You will help the team win in a fast-growing market. If you are passionate about innovation and embrace the challenge of working on highly scalable systems that handle large volumes of data, this position is for you.Meet the Team (Optional)Data platform is the heart of the AppDynamics architecture. We process more than 100 billion data packets per day, more than 5 peta byte of data flows through our platform. We are in 8 regions having 40 clusters of 7000 nodes. We ingest, process and store all the data that power the dashboards that our customers rely on for observing their infrastructure and their applications.What you'll get to do\n",
       "• Design and build highly scalable solutions\n",
       "• Work with a team of exceptionally capable and dedicated peers, all the way from engineering to product management and customer support\n",
       "• Work in an open environment, work together to get things done and adapt to the team's changing needs\n",
       "• Leverage technologies including Kafka, ElasticSearch, Docker, and Kubernetes across different cloud environments like AWS and Azure\n",
       "• lead critical initiatives for the organisation\n",
       "Must-have Qualifications\n",
       "• 15+ years of full-stack developer experience in designing and developing highly scalable, distributed applications, products, and services.\n",
       "• Expertise in Java programming language.\n",
       "• Strong proficiency in data structures, algorithms, threads, concurrent programming\n",
       "• Extensive knowledge of SQL and at least one relational database engine: MySQL. Hands on experience in RDS or NoSQL (Dynamo, MongoDB) is a big plus\n",
       "• Experience of building applications using microservices architecture with expertise in Dockers and Kubernetes.\n",
       "• Strong communication skills, both verbal and written. Ability to multi-task and adapt quickly to changing requirements, scope, and priorities.\n",
       "• Optimize data pipelines, storage systems, and query engines for performance and efficiency.\n",
       "• Work closely with product teams, data engineers, software developers, and product managers to deliver on business goals.\n",
       "• Mentor team members in architecture principles, coding best practices, and system design.\n",
       "• Ensure robust monitoring, logging, and alerting systems for proactive issue detection and resolution.\n",
       "• Support CI/CD processes and automate testing for data systems\n",
       "• Stay abreast of emerging trends in big data, machine learning, and distributed systems to recommend innovative solutions.\n",
       "Nice-to-have QualificationsWe’ve taken special care to separate the must-have qualifications from the nice-to-haves. “Nice-to-have” means just that: Nice. To. Have. So, don’t worry if you can’t check off every box. We’re not hiring a list of bullet points–we’re interested in the whole you.\n",
       "• Added advantage of having an experience in working on Cloud Observability Space.\n",
       "• experience of other languages like python, etc\n",
       "• experience of front-end technologies\n",
       "Splunk is an Equal Opportunity EmployerSplunk, a Cisco company, is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.\n",
       "\n",
       "Note:</td><td>eyJqb2JfdGl0bGUiOiJQcmluY2lwbGUgU29mdHdhcmUgRW5naW5lZXIgZm9yIERhdGEgUGxhdGZvcm0gLSAzMTg2NiIsImNvbXBhbnlfbmFtZSI6IlNwbHVuayIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiY2k4emNxSnNNanNSemNYUEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>9 days ago</td><td>Data Engineer</td></tr><tr><td>Software Developer- Python</td><td>BNP Paribas India Solutions</td><td>India</td><td>About BNP Paribas India Solutions:\n",
       "\n",
       "Established in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n",
       "\n",
       "About BNP Paribas Group:\n",
       "\n",
       "BNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n",
       "\n",
       "Commitment to Diversity and Inclusion\n",
       "\n",
       "At BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n",
       "\n",
       "About Business line/Function:\n",
       "\n",
       "The Intermediate Holding Company (“IHC”) program structured at the U.S. level across poles of activities of BNP Paribas provides guidance, supports the analysis, impact assessment and drives adjustments of the U.S. platform’s operating model due to the drastic changes introduced by the Enhanced Prudential Standards (“EPS”) for Foreign Banking Organizations (“FBOs”) finalized by the Federal Reserve in February 2014, implementing Section 165 of U.S. Dodd-Frank Act.\n",
       "\n",
       "The IT Transversal Team is part of the Information Technology Group which works simultaneously on a wide range of projects arising from business, strategic initiatives, and regulatory changes and reengineering of existing applications to improve functionality and efficiency.\n",
       "\n",
       "Job Title:\n",
       "\n",
       "Python Developer\n",
       "\n",
       "Date:\n",
       "\n",
       "June-25\n",
       "\n",
       "Department:\n",
       "\n",
       "ITG- Fresh\n",
       "\n",
       "Location:\n",
       "\n",
       "Chennai, Mumbai\n",
       "\n",
       "Business Line / Function:\n",
       "\n",
       "Finance Dedicated Solutions\n",
       "\n",
       "Reports to:\n",
       "\n",
       "(Direct)\n",
       "\n",
       "Grade:\n",
       "\n",
       "(if applicable)\n",
       "\n",
       "(Functional)\n",
       "\n",
       "Number of Direct Reports:\n",
       "\n",
       "NA\n",
       "\n",
       "Directorship / Registration:\n",
       "\n",
       "NA\n",
       "Position Purpose\n",
       "\n",
       "The Python Developer will play a critical role in building and maintaining financial applications and tools that support data processing, analysis, and reporting within a fast-paced financial services environment. This position involves developing scalable and secure systems. The developer will collaborate with business analysts, finance users/or finance BA to translate complex business requirements into efficient, high-quality software solutions.\n",
       "\n",
       "A strong understanding of financial concepts, data integrity, and regulatory compliance is essential. The detailed responsibilities are mentioned below.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Direct Responsibilities\n",
       "\n",
       "- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n",
       "\n",
       "- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n",
       "\n",
       "- Expertise in PySpark for large-scale data processing and loading into databases.\n",
       "\n",
       "- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n",
       "\n",
       "- Strong communication skills to effectively collaborate with team members and stakeholders.\n",
       "\n",
       "- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n",
       "Technical & Behavioral Competencies\n",
       "\n",
       "- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n",
       "\n",
       "- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n",
       "\n",
       "- Expertise in PySpark for large-scale data processing and loading into databases.\n",
       "\n",
       "- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n",
       "\n",
       "- Strong communication skills to effectively collaborate with team members and stakeholders.\n",
       "\n",
       "- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n",
       "\n",
       "- Good analytical, problem solving, & communication skills\n",
       "\n",
       "- Engage in technical discussions and to help in improving the system, process etc\n",
       "\n",
       "Nice to Have\n",
       "\n",
       "- Familiarity with Plotly and Matplotlib for data visualization of large datasets.\n",
       "\n",
       "- Skilled in API programming, handling JSON, CSV, and other unstructured data from various systems.\n",
       "\n",
       "- Familiarity with JavaScript, CSS, and HTML.\n",
       "\n",
       "- Experience with cloud architecture applications such as Dataiku or Databricks; competency with ETL tools.\n",
       "\n",
       "- Knowledge of regulatory frameworks, RISK, CCAR, and GDPR.\n",
       "Specific Qualifications (if required)\n",
       "\n",
       "Skills Referential\n",
       "\n",
       "Behavioural Skills: (Please select up to 4 skills)\n",
       "\n",
       "Ability to collaborate / Teamwork\n",
       "\n",
       "Critical thinking\n",
       "\n",
       "Ability to deliver / Results driven\n",
       "\n",
       "Communication skills - oral & written\n",
       "\n",
       "Transversal Skills: (Please select up to 5 skills)\n",
       "\n",
       "Analytical Ability\n",
       "\n",
       "Ability to develop and adapt a process\n",
       "\n",
       "Ability to understand, explain and support change\n",
       "\n",
       "Ability to develop others & improve their skills\n",
       "\n",
       "Choose an item.\n",
       "\n",
       "Education Level:\n",
       "\n",
       "Bachelor Degree or equivalent\n",
       "\n",
       "Experience Level\n",
       "\n",
       "At least 5 years\n",
       "\n",
       "Other/Specific Qualifications (if required)</td><td>eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBEZXZlbG9wZXItIFB5dGhvbiIsImNvbXBhbnlfbmFtZSI6IkJOUCBQYXJpYmFzIEluZGlhIFNvbHV0aW9ucyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ6VTdCX1R2emRzczk0M0ZoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>7 days ago</td><td>Python Developer</td></tr><tr><td>Freelance Python Developer</td><td>Teqlawn</td><td>Anywhere</td><td>We’re looking for an experienced Python Developer (4–7 years) to build scalable web and application solutions, with opportunities to integrate AI capabilities. Experience in AI integration is a must.\n",
       "\n",
       "Responsibilities:\n",
       "• Develop scalable web and application solutions using Python, with integration of AI/ML components\n",
       "• Collaborate with clients to understand project goals and technical requirements\n",
       "• Write clean, maintainable, and well-documented code\n",
       "• Troubleshoot, debug, and optimize applications for performance and reliability\n",
       "• Ensure timely and efficient delivery of milestones and final deliverables\n",
       "• Participate in code reviews and contribute to maintaining coding standards and best practices\n",
       "• Work with relevant frameworks and libraries such as Django, Flask, FastAPI, NumPy, pandas, and scikit-learn\n",
       "\n",
       "Note: Please share the link to your portfolio along with your application.\n",
       "\n",
       "Job Types: Full-time, Contractual / Temporary, Freelance\n",
       "Contract length: 2 months\n",
       "\n",
       "Pay: ₹50,000.00 - ₹80,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Work from home\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "• Monday to Friday\n",
       "\n",
       "Experience:\n",
       "• Python Development: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlRlcWxhd24iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiY20xcUU4aHhjbFQ3dnVyT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>1 day ago</td><td>Python Developer</td></tr><tr><td>Python Developer — Full-Time | 1-2 years Exp | In-Office | Bangalore</td><td>SERP Hawk</td><td>India</td><td>\uD83D\uDE80 We’re Hiring: Python Developer\n",
       "\n",
       "SERP Hawk is looking for a talented Python Developer to join our dynamic team in Bangalore!\n",
       "\n",
       "\uD83C\uDF1F About Us\n",
       "\n",
       "SERP Hawk is a leading digital marketing and technology solutions company based in Bangalore. We specialize in building robust backend systems, scalable applications, and AI-powered solutions for clients across various industries.\n",
       "\n",
       "\uD83D\uDCCD Location:B-1, Bannerghatta Slip Road, KEB Colony, New Gurappana Palya, 1st Stage, BTM Layout 1, Bengaluru, Karnataka 560029\n",
       "\n",
       "\uD83C\uDF10 Website: www.serphawk.com\n",
       "\n",
       "\uD83D\uDCBC What You’ll Do\n",
       "• Design and develop scalable backend architectures.\n",
       "• Write clean, efficient Python code.\n",
       "• Integrate APIs and databases.\n",
       "• Implement CI/CD pipelines and automated tests.\n",
       "• Ensure high performance, security, and reliability.\n",
       "\n",
       "✅ What We’re Looking For\n",
       "\n",
       "✔️ 1–2 years of experience in Python development.\n",
       "\n",
       "✔️ Proficiency in frameworks like Django, Flask, or FastAPI.\n",
       "\n",
       "✔️ Strong understanding of APIs and databases.\n",
       "\n",
       "✔️ Experience with CI/CD tools and best practices.\n",
       "\n",
       "✔️ Excellent problem-solving skills and a collaborative mindset.\n",
       "\n",
       "\uD83D\uDCA1 Nice to Have\n",
       "\n",
       "⭐ Experience with AI/chatbots.\n",
       "\n",
       "⭐ Knowledge of cloud services and containerization.\n",
       "\n",
       "\uD83D\uDCB0 Salary\n",
       "• ₹20,000 – ₹25,000 per month (based on skills and experience).\n",
       "\n",
       "\uD83D\uDCCC Additional Details\n",
       "\n",
       "\uD83D\uDCBC This is a full-time, in-office role in Bangalore.\n",
       "\n",
       "\uD83C\uDFE2 Candidates must report to the office daily.\n",
       "\n",
       "\uD83C\uDF10 Should be flexible to work and attend meetings or calls as per client time zones.\n",
       "\n",
       "✨ Apply now and grow with us!</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAlCBGdWxsLVRpbWUgfCAxLTIgeWVhcnMgRXhwIHwgSW4tT2ZmaWNlIHwgQmFuZ2Fsb3JlIiwiY29tcGFueV9uYW1lIjoiU0VSUCBIYXdrIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjNBMWtiUmowZFhsRVZsWnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 hours ago</td><td>Python Developer</td></tr><tr><td>SQL + Python</td><td>Wissen Technology</td><td>India</td><td>Wissen Technology is Hiring for SQL With Python\n",
       "\n",
       "About Wissen Technology:\n",
       "\n",
       "Wissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges.\n",
       "\n",
       "Role Overview:\n",
       "\n",
       "We are looking for a skilled and detail-oriented candidate with a strong foundation in SQL, Python, and data processing techniques. The ideal candidate is passionate about transforming raw data into meaningful insights and has hands-on experience across the data pipeline—from data wrangling to visualization.\n",
       "\n",
       "Experience: 3-7 Years\n",
       "\n",
       "Location: Bengaluru\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with SQL (e.g., joins, subqueries, CTEs, window functions).\n",
       "• Proficiency in Python for data manipulation (e.g., pandas, NumPy).\n",
       "• Experience working with relational databases like MySQL, PostgreSQL, SQL Server, or Oracle.\n",
       "• Hands-on experience in data wrangling, cleaning, and feature engineering.\n",
       "• Understanding of ETL processes and tools.\n",
       "• Familiarity with version control systems like Git.\n",
       "• Knowledge of data visualization techniques and tools.\n",
       "• Strong problem-solving and analytical skills.\n",
       "\n",
       "The Wissen Group was founded in the year 2000. Wissen Technology, a part of Wissen Group, was established in the year 2015. Wissen Technology is a specialized technology company that delivers high-end consulting for organizations in the Banking & Finance, Telecom, and Healthcare domains. We help clients build world class products.\n",
       "\n",
       "We offer an array of services including Core Business Application Development, Artificial Intelligence & Machine Learning, Big Data & Analytics, Visualization & Business Intelligence, Robotic Process Automation, Cloud Adoption, Mobility, Digital Adoption, Agile & DevOps, Quality Assurance & Test Automation.\n",
       "\n",
       "Over the years, Wissen Group has successfully delivered $1 billion worth of projects for more than 20 of the Fortune 500 companies. Wissen Technology provides exceptional value in mission critical projects for its clients, through thought leadership, ownership, and assured on-time deliveries that are always ‘first time right’.\n",
       "\n",
       "The technology and thought leadership that the company commands in the industry is the direct result of the kind of people Wissen has been able to attract. Wissen is committed to providing them with the best possible opportunities and careers, which extends to providing the best possible experience and value to our clients.\n",
       "\n",
       "We have been certified as a Great Place to Work® company for two consecutive years (2020-2022) and voted as the Top 20 AI/ML vendor by CIO Insider. Great Place to Work® Certification is recognized world over by employees and employers alike and is considered the ‘Gold Standard’. Wissen Technology has created a Great Place to Work by excelling in all dimensions - High-Trust, High-Performance Culture, Credibility, Respect, Fairness, Pride and Camaraderie.\n",
       "\n",
       "Website: www.wissen.com\n",
       "\n",
       "LinkedIn: https://www.linkedin.com/company/wissen-technology\n",
       "\n",
       "Wissen Leadership: https://www.wissen.com/company/leadership-team/\n",
       "\n",
       "Wissen Live: https://www.linkedin.com/company/wissen-technology/posts/feedView=All\n",
       "\n",
       "Wissen Thought Leadership: https://www.wissen.com/articles/\n",
       "\n",
       "Employee Speak:\n",
       "\n",
       "https://www.ambitionbox.com/overview/wissen-technology-overview\n",
       "\n",
       "https://www.glassdoor.com/Reviews/Wissen-Infotech-Reviews-E287365.htm\n",
       "\n",
       "Great Place to Work:\n",
       "\n",
       "https://www.wissen.com/blog/wissen-is-a-great-place-to-work-says-the-great-place-to-work-institute-india/\n",
       "\n",
       "https://www.linkedin.com/posts/wissen-infotech_wissen-leadership-wissenites-activity-6935459546131763200-xF2k\n",
       "\n",
       "About Wissen Interview Process:\n",
       "\n",
       "https://www.wissen.com/blog/we-work-on-highly-complex-technology-projects-here-is-how-it-changes-whom-we-hire/\n",
       "\n",
       "Latest in Wissen in CIO Insider:\n",
       "\n",
       "https://www.cioinsiderindia.com/vendor/wissen-technology-setting-new-benchmarks-in-technology-consulting-cid-1064.html</td><td>eyJqb2JfdGl0bGUiOiJTUUwgKyBQeXRob24iLCJjb21wYW55X25hbWUiOiJXaXNzZW4gVGVjaG5vbG9neSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJITW5vZUwxWlF1QWVma0hlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>7 hours ago</td><td>Python Developer</td></tr><tr><td>Python Developer Full time</td><td>Variance Technologies Private Limited</td><td>Anywhere</td><td>Job Opportunity: Python Developer at Variance Technologies Private Limited!\n",
       "\n",
       "Role: Python Developer\n",
       "\n",
       "Duration: 1 Months\n",
       "\n",
       "Location: Hybrid / Remote\n",
       "\n",
       "Responsibilities:\n",
       "\n",
       "Collaborate with our development team to develop and maintain Python-based applications tailored for finance-related tasks\n",
       "\n",
       "Implement object-oriented programming principles to ensure the scalability and maintainability of codebase\n",
       "\n",
       "Gain hands-on experience with Pandas for data manipulation and analysis within finance contexts\n",
       "\n",
       "Support integration of REST and WebSocket APIs for seamless communication with financial data providers and platforms\n",
       "\n",
       "Assist in automating financial modeling and analysis workflows using Python, including Excel automation for enhanced efficiency\n",
       "\n",
       "Requirements:\n",
       "\n",
       "Currently pursuing or recently completed a degree in Computer Science, Finance, or a related field\n",
       "\n",
       "Basic proficiency in Python programming language, with a strong willingness to learn and grow\n",
       "\n",
       "Exceptional attention to detail and proactive attitude towards problem-solving\n",
       "\n",
       "Genuine interest in the intersection of finance and technology\n",
       "\n",
       "Bonus Skills:\n",
       "\n",
       "Familiarity with fundamental financial concepts and markets\n",
       "\n",
       "Exposure to Python libraries such as Pandas, NumPy, or SciPy\n",
       "\n",
       "Demonstrated interest in financial data analysis and visualization techniques\n",
       "\n",
       "Basic understanding of REST and WebSocket APIs\n",
       "\n",
       "Perks:\n",
       "\n",
       "Hands-on experience working on real-world projects at the forefront of finance and technology\n",
       "\n",
       "Mentorship and guidance from seasoned professionals in the field\n",
       "\n",
       "Networking opportunities with industry experts to expand your professional connections\n",
       "\n",
       "Flexible scheduling to accommodate academic commitments\n",
       "\n",
       "Potential for transition to a full-time position based on exceptional performance and availability\n",
       "\n",
       "Ready to kickstart your career with a prestigious 1-month internship at Variance Technologies Private Limited? Apply now by sending your resume and a tailored cover letter expressing your interest and qualifications to careers@variancefintech.com. Join our team and embark on a journey towards shaping the future of finance through technology!\n",
       "\n",
       "Variance Technologies Private Limited is committed to fostering a diverse and inclusive workplace where all individuals are empowered to thrive. We welcome applicants from all backgrounds and identities to apply.\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: From ₹35,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Work from home\n",
       "\n",
       "Schedule:\n",
       "• Monday to Friday\n",
       "\n",
       "Education:\n",
       "• Bachelor's (Preferred)\n",
       "\n",
       "Experience:\n",
       "• Python: 1 year (Preferred)\n",
       "• total work: 1 year (Preferred)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIEZ1bGwgdGltZSIsImNvbXBhbnlfbmFtZSI6IlZhcmlhbmNlIFRlY2hub2xvZ2llcyBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiaXZCN3JUQTN5YnkxRzV2MUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>5 days ago</td><td>Python Developer</td></tr><tr><td>Python Developer – AI & LLM Integrations</td><td>Discover WebTech Private Limited</td><td>India</td><td>We are hiring a skilled Python Developer with a minimum of 3 years of experience, who has hands-on expertise in developing AI modules, building intelligent agents using LLMs, and working with cutting-edge frameworks such as LangChain, LangGraph, and LangSmith.\n",
       "\n",
       "The ideal candidate should be capable of designing, developing, and deploying backend services as well as intelligent AI-driven tools and systems.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design and implement intelligent agents using LLM-based frameworks like LangChain, LangGraph, and LangSmith.\n",
       "• Build backend systems using Python (Django, FastAPI, or Flask).\n",
       "• Develop and integrate APIs, third-party tools, and cloud services.\n",
       "• Create AI modules that interact with knowledge bases, APIs, and perform multi-step reasoning.\n",
       "• Implement prompt engineering, memory chains, and agent behavior logic.\n",
       "• Collaborate with cross-functional teams to deliver robust AI features.\n",
       "• Optimize code for scalability, performance, and reliability.\n",
       "\n",
       "Required Skills and Qualifications\n",
       "• 3+ years of hands-on experience with Python.\n",
       "• Proficiency in LangChain, LangGraph, or LangSmith.\n",
       "• Strong experience in working with LLMs like OpenAI, Cohere, or Anthropic.\n",
       "• Deep understanding of prompt engineering and agent orchestration.\n",
       "• Experience with APIs, JSON, and external integrations.\n",
       "• Knowledge of data storage systems (PostgreSQL, MongoDB).\n",
       "• Familiarity with Docker, Git, and CI/CD tools.\n",
       "• Excellent problem-solving and debugging skills.\n",
       "\n",
       "Preferred Qualifications\n",
       "• Knowledge of vector databases (e.g., Pinecone, Weaviate, FAISS).\n",
       "• Experience with microservices, asynchronous programming, and message queues (Celery, RabbitMQ).\n",
       "• Familiarity with frontend technologies (React.js or Vue.js) is a plus.\n",
       "• Exposure to cloud platforms such as AWS, GCP, or Azure.\n",
       "\n",
       "Job Types: Full-time, Permanent\n",
       "\n",
       "Pay: ₹30,000.00 - ₹70,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Health insurance\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "\n",
       "Work Location: In person</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBBSSBcdTAwMjYgTExNIEludGVncmF0aW9ucyIsImNvbXBhbnlfbmFtZSI6IkRpc2NvdmVyIFdlYlRlY2ggUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Im1JUlJqRWJSV0pCSkdQR0VBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>3 days ago</td><td>Python Developer</td></tr><tr><td>Full Stack Developer (Python / React JS)</td><td>Hitachi Careers</td><td>India</td><td>Our Company\n",
       "\n",
       "We're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n",
       "\n",
       "Our group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n",
       "\n",
       "Imagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n",
       "\n",
       "The team\n",
       "\n",
       "We are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "\n",
       "The role: Full Stack Developer/Specialist\n",
       "\n",
       "Responsibilities:\n",
       "• Design, develop, and maintain applications.\n",
       "• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n",
       "• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n",
       "• Optimise applications for performance, scalability, and user experience.\n",
       "• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n",
       "• Provide training and support to end-users and IT staff on functionalities and best practices.\n",
       "• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n",
       "• Participate in project planning, execution, and post-implementation support.\n",
       "• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n",
       "• What you'll bring\n",
       "Qualifications:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n",
       "• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n",
       "• Expertise in development and customisation.\n",
       "• Proficiency in Python and React JS\n",
       "• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n",
       "• Experience with RESTful APIs and web services.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Strong communication and collaboration skills.\n",
       "• Ability to work independently and as part of a team in a fast-paced environment.\n",
       "• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n",
       "• Prior experience in building AI applications is a plus.\n",
       "• Prior experience with automation tools like UIPath is a plus.\n",
       "• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\n",
       "Preferred Skills\n",
       "• Certification in\n",
       "• Experience with other low-code/no-code platforms.\n",
       "• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n",
       "• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n",
       "• Experience with Agile development methodologies.\n",
       "About us\n",
       "\n",
       "We're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n",
       "\n",
       "#LI-MS3\n",
       "\n",
       "Championing diversity, equity, and inclusion\n",
       "\n",
       "Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n",
       "\n",
       "How we look after you\n",
       "\n",
       "We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n",
       "\n",
       "We're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.</td><td>eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>14 days ago</td><td>Python Developer</td></tr><tr><td>Python Back-End Developer</td><td>Goodyear</td><td>India</td><td>Location: IN - Hyderabad Telangana\n",
       "\n",
       "Goodyear Talent Acquisition Representative: M Bhavya Sree\n",
       "\n",
       "Sponsorship Available: No\n",
       "\n",
       "Relocation Assistance Available: No\n",
       "\n",
       "Duties and Responsibilities:\n",
       "\n",
       "• Develop and support Data Driven applications\n",
       "\n",
       "• Help design and develop back-end services and APIs for data-driven applications and simulations.\n",
       "\n",
       "• Work with our technical partners to collaborate on system requirements and data integration needs for our new applications.\n",
       "\n",
       "• Support the deployment and scaling of new back-end technologies and cloud-native architectures within the organization.\n",
       "\n",
       "• Work closely with our data scientists to support model deployment into production environments.\n",
       "\n",
       "• Develop and maintain server-side components for digital tools and products using Python or other modern back-end technologies and frameworks. Build scalable, secure, and efficient services that support a seamless experience across multiple platforms.\n",
       "\n",
       "• Design, implement, and maintain robust database systems (SQL and NoSQL), ensuring high availability and performance for critical applications.\n",
       "\n",
       "• Contribute to DevOps practices including CI/CD pipelines, infrastructure as code, containerization (Docker), and orchestration (Kubernetes).\n",
       "\n",
       "• Learn about the tire industry and tire manufacturing processes from subject matter experts.\n",
       "\n",
       "• Be a part of cross-functional teams working together to deliver impactful results.\n",
       "\n",
       "Skills Required:\n",
       "\n",
       "• Significant experience in server-side development using Python\n",
       "\n",
       "• Strong understanding of RESTful API design, microservices architecture, and service-oriented design\n",
       "\n",
       "• Experience with relational and non-relational databases such as PostgreSQL, MySQL, MongoDB, or DynamoDB\n",
       "\n",
       "• Application of software design skills and methodologies (algorithms, data structures, design patterns, software architecture and testing)\n",
       "\n",
       "• Hands-on experience working with cloud platforms such as AWS, Microsoft Azure, or Google Cloud Platform\n",
       "\n",
       "• Good teamwork skills - ability to work in a team environment and deliver results on time.\n",
       "\n",
       "• Strong communication skills - capable of conveying information concisely to diverse audiences.\n",
       "\n",
       "• Exposure to DevOps practices including CI/CD pipelines (e.g., GitHub Actions, Jenkins), containerization (e.g., Docker), and orchestration tools (e.g., Kubernetes)\n",
       "\n",
       "• Familiarity with front-end technologies like React, HTML, CSS, and JavaScript for API integration purposes\n",
       "\n",
       "Goodyear is an Equal Employment Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to that individual's race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender identity, age, physical or mental disability, ethnicity, citizenship, or any other characteristic protected by law.\n",
       "\n",
       "Goodyear is one of the world’s largest tire companies. It employs about 68,000 people and manufactures its products in 53 facilities in 20 countries around the world. Its two Innovation Centers in Akron, Ohio and Colmar-Berg, Luxembourg strive to develop state-of-the-art products and services that set the technology and performance standard for the industry. For more information about Goodyear and its products, go to www.goodyear.com/corporate\n",
       "\n",
       "#Li-Hybrid</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gQmFjay1FbmQgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiR29vZHllYXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoid0VrdmdmMVREWEhlTFhuQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Python Developer</td></tr><tr><td>Generative AI & Backend Developer(python)</td><td>Intellypod</td><td>Anywhere</td><td>Job Description (JD) For Gen Ai with Python:\n",
       "\n",
       "We're Hiring: GenAI & Backend Developer (Python)\n",
       "\n",
       "Work Location: Remote (Work From Home)\n",
       "\n",
       "Experience: 2+ Years\n",
       "\n",
       "Immediate Joiners Preferred\n",
       "\n",
       "Company: IntellyPod\n",
       "\n",
       "Apply at: hrd@intellypod.com | hr@intellypod.com\n",
       "\n",
       "About the Role:\n",
       "\n",
       "IntellyPod is looking for a passionate Backend Developer with hands-on experience in GenAI, LLMs, and Python. If you’re excited about building scalable APIs, integrating AI models, and working with the latest in backend and GenAI technologies — we’d love to connect!\n",
       "\n",
       "Key Responsibilities:\n",
       "\n",
       "· Develop and maintain Python-based backend services.\n",
       "\n",
       "· Design and implement RESTful APIs.\n",
       "\n",
       "· Integrate GenAI/LLM solutions into applications.\n",
       "\n",
       "· Manage and optimize SQL/NoSQL databases.\n",
       "\n",
       "· Collaborate with cross-functional tech teams.\n",
       "\n",
       "Must-Have Skills:\n",
       "\n",
       "· 2+ years of experience in backend development (Python).\n",
       "\n",
       "· Experience with GenAI and Large Language Models (e.g., GPT, LLaMA).\n",
       "\n",
       "· Strong knowledge of REST APIs and database design.\n",
       "\n",
       "· Familiarity with Git and backend architecture best practices.\n",
       "\n",
       "Need to Have:\n",
       "\n",
       "· Experience with AWS/GCP/Azure.\n",
       "\n",
       "· Docker, Kubernetes, or CI/CD exposure.\n",
       "\n",
       "· Familiarity with vector databases (e.g., Pinecone, FAISS).\n",
       "\n",
       "· Prompt engineering or LLM fine-tuning knowledge.\n",
       "\n",
       "Why Join Us?\n",
       "\n",
       "· 100% Remote – Flexible work setup\n",
       "\n",
       "· Work on next-gen AI products\n",
       "\n",
       "· Fast-growing, collaborative tech team\n",
       "\n",
       "· Opportunity to innovate with emerging AI tools\n",
       "\n",
       "Ready to build the future with us? Send your resume to: hrd@intellypod.com | hr@intellypod.com #GenAI #BackendDeveloper #PythonJobs #LLM #RemoteJob #HiringNow #IntellyPod\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: Up to ₹70,000.00 per month\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Application Question(s):\n",
       "• Are an immediate joiner -\n",
       "\n",
       "Are on notice period if yes [Then how many days]\n",
       "• Write YES or NO\n",
       "\n",
       "1) Need to ask have you worked on LLM based project -\n",
       "\n",
       "2) Have you worked on chatbot types apps -\n",
       "\n",
       "3) Have you strong knowleged of OOps and Python basic -\n",
       "\n",
       "4) Have you knowledge of Rest APi development -\n",
       "\n",
       "Experience:\n",
       "• 5G: 3 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJHZW5lcmF0aXZlIEFJIFx1MDAyNiBCYWNrZW5kIERldmVsb3BlcihweXRob24pIiwiY29tcGFueV9uYW1lIjoiSW50ZWxseXBvZCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiI4ekp1Sk9ZUFlTbWtOcFRuQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>5 hours ago</td><td>Python Developer</td></tr><tr><td>Junior Python Developer</td><td>Dehazelabs</td><td>Anywhere</td><td>Location: Onsite, Kokapet, Hyderabad, Telangana.\n",
       "\n",
       "Job Type: Full-Time\n",
       "\n",
       "About Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n",
       "\n",
       "Responsibilities:\n",
       "• Develop and maintain Python applications and services\n",
       "• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n",
       "• Write clean, efficient, and well-documented code\n",
       "• Troubleshoot, debug, and optimize applications for performance and scalability\n",
       "• Participate in code reviews and maintain coding standards\n",
       "• Stay up-to-date with the latest industry trends and technologies\n",
       "\n",
       "Requirements:\n",
       "• Ability to code in Python and SQL\n",
       "• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n",
       "• Knowledge of version control systems (e.g., Git)\n",
       "• Excellent problem-solving skills and attention to detail\n",
       "• Strong communication and teamwork abilities.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n",
       "• Knowledge of RESTful APIs and microservices architecture.</td><td>eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Python Developer</td></tr><tr><td>Etl Developer</td><td>Vivid Resourcing</td><td>Bilaspur, Chhattisgarh, India</td><td>Job Title:\n",
       "Data Engineer / ETL Developer\n",
       "\n",
       "Location:\n",
       "US, remote from India\n",
       "\n",
       "Department:\n",
       "IT / Data & Analytics\n",
       "\n",
       "Reports To:\n",
       "Head of Data & Analytics\n",
       "\n",
       "Employment Type:\n",
       "Contract, 12 months\n",
       "\n",
       "Pay:\n",
       "$28-35 per hour payrolled or self-employed\n",
       "\n",
       "About the Role\n",
       "\n",
       "We are seeking a Data Engineer / ETL Developer to help design, build, and maintain data pipelines that support our internal reporting and analytics needs. This role will focus on integrating data from systems such as ERP, MES , and production databases into a centralized data platform to enable reliable and timely insights across the business. You will work alongside BI developers, IT staff, and business users to ensure the smooth flow of data and contribute to the foundation of a modern business intelligence environment.\n",
       "\n",
       "Key Responsibilities\n",
       "\n",
       "ETL & Data Pipeline Development\n",
       "• Develop and maintain ETL/ELT pipelines to move data from source systems (ERP, MES, SQL databases, flat files) into our centralized platform.\n",
       "• Use tools such as Azure Data Factory , SSIS , or similar to orchestrate and automate data workflows.\n",
       "\n",
       "Data Modeling & Integration\n",
       "• Assist in designing and building data models and data marts optimized for Power BI dashboards.\n",
       "• Support the creation and maintenance of dataflows and datasets in Power BI Service.\n",
       "\n",
       "Data Quality & Documentation\n",
       "• Implement data validation, transformation, and cleansing logic to ensure high-quality, reliable data.\n",
       "• Document data processes, business rules, and data mappings to support knowledge sharing and governance.\n",
       "\n",
       "Cross-Functional Collaboration\n",
       "• Work with internal teams (production, quality, operations, finance) to understand data requirements and reporting needs.\n",
       "• Collaborate with BI developers to ensure the data pipeline supports efficient and user-friendly reporting.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\n",
       "• 2+ years of experience in data engineering or ETL development roles.\n",
       "• Solid experience with SQL and ETL tools (e.G., Azure Data Factory , SSIS , Informatica , etc.).\n",
       "• Familiarity with Power BI , including working with datasets, dataflows, or basic DAX.\n",
       "• Understanding of data integration, transformation, and warehousing concepts.\n",
       "• Experience working with or integrating manufacturing systems (ERP, MES) is a strong plus.\n",
       "\n",
       "Preferred Skills\n",
       "• Experience with cloud data platforms (especially Microsoft Azure ).\n",
       "• Exposure to Python or scripting for automation.\n",
       "• Familiarity with data governance and documentation practices.\n",
       "• Experience with manufacturing environments or industrial data is beneficial.\n",
       "\n",
       "Soft Skills\n",
       "• Strong attention to detail and a logical, structured approach to problem-solving.\n",
       "• Willingness to learn and grow in a fast-paced environment.\n",
       "• Good communication and collaboration skills across technical and non-technical teams.\n",
       "• Proactive and solutions-oriented mindset.</td><td>eyJqb2JfdGl0bGUiOiJFdGwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiVml2aWQgUmVzb3VyY2luZyIsImFkZHJlc3NfY2l0eSI6IkJpbGFzcHVyLCBDaGhhdHRpc2dhcmgsIEluZGlhIiwiaHRpZG9jaWQiOiJueDl3cW1oMXpfcnBSX2dFQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>14 hours ago</td><td>ETL Developer</td></tr><tr><td>Senior ETL and Backend Developer (Salesforce)</td><td>S&P Global</td><td>Hyderabad, Telangana, India (+1 other)</td><td>About the Role:\n",
       "\n",
       "Grade Level (for internal use):\n",
       "10\n",
       "\n",
       "Title: Senior ETL and Backend Developer (Salesforce)\n",
       "\n",
       "Job Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n",
       "\n",
       "The Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n",
       "\n",
       "The team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n",
       "\n",
       "The Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n",
       "\n",
       "Responsibilities:\n",
       "• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n",
       "• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n",
       "• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n",
       "• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n",
       "• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n",
       "• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\n",
       "Monitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n",
       "• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\n",
       "Qualifications:\n",
       "\n",
       "Basic Qualifications:\n",
       "• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n",
       "• A minimum of 8+ years of experience in software engineering & Architecture.\n",
       "• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n",
       "• A minimum of 3+ years of Salesforce development, administration/Integration.\n",
       "• Proficiency in Informatica PowerCenter and other ETL tools.\n",
       "• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n",
       "• Experience with Salesforce integration and administration.\n",
       "• Proficiency in backend development languages (e.g., Java, Python, C#).\n",
       "• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Ability to work independently and as part of a team.\n",
       "• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n",
       "• Experience with other ETL tools and data integration platforms.\n",
       "• Informatica Certified Professional\n",
       "Salesforce Certified Administrator or Developer\n",
       "• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n",
       "• Excellent problem solving, analytical and technical troubleshooting skills.\n",
       "• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n",
       "• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n",
       "\n",
       "About S&P Global Commodity Insights\n",
       "At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n",
       "\n",
       "We’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n",
       "\n",
       "S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n",
       "\n",
       "For more information, visit http://www.spglobal.com/commodity-insights.\n",
       "\n",
       "What’s In It For You?\n",
       "\n",
       "Our Purpose:\n",
       "\n",
       "Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n",
       "\n",
       "Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n",
       "\n",
       "Our People:\n",
       "\n",
       "We're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n",
       "\n",
       "From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n",
       "\n",
       "Our Values:\n",
       "\n",
       "Integrity, Discovery, Partnership\n",
       "\n",
       "At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n",
       "\n",
       "Benefits:\n",
       "\n",
       "We take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n",
       "\n",
       "Our benefits include:\n",
       "• Health & Wellness: Health care coverage designed for the mind and body.\n",
       "• Flexible Downtime: Generous time off helps keep you energized for your time on.\n",
       "• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n",
       "• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n",
       "• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n",
       "• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n",
       "\n",
       "For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n",
       "\n",
       "Global Hiring and Opportunity at S&P Global:\n",
       "\n",
       "At S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "Equal Opportunity Employer\n",
       "\n",
       "S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n",
       "\n",
       "If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n",
       "\n",
       "US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n",
       "\n",
       "Job ID: 316835\n",
       "Posted On: 2025-06-03\n",
       "Location: Hyderabad, Telangana, India</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjJlUWpKemNOclM4MXR6QzhBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>30 days ago</td><td>ETL Developer</td></tr><tr><td>Senior Etl Developer</td><td>Vivid Resourcing</td><td>Bilaspur, Chhattisgarh, India</td><td>Job Title:\n",
       "Senior Data Engineer / ETL Developer\n",
       "\n",
       "Location:\n",
       "US, from India\n",
       "\n",
       "Department:\n",
       "IT / Data & Analytics\n",
       "\n",
       "Reports To:\n",
       "Director of Data & Analytics\n",
       "\n",
       "Employment Type:\n",
       "Contract, 12 months\n",
       "\n",
       "Pay:\n",
       "$30-38 per hour payrolled or self-employed\n",
       "\n",
       "About the Role\n",
       "\n",
       "We are seeking a skilled and motivated Senior Data Engineer / ETL Developer to lead the design and implementation of end-to-end data integration solutions powering this new platform. You will work closely with IT, business stakeholders, and BI developers to build scalable data pipelines, robust data models, and a high-performance reporting environment centered around Power BI.\n",
       "\n",
       "This role is a key contributor in shaping our data infrastructure and delivering timely, accurate insights across the organization—from the shop floor to the boardroom.\n",
       "\n",
       "Key Responsibilities\n",
       "\n",
       "Data Engineering & Integration\n",
       "• Design, build, and maintain scalable ETL/ELT pipelines to extract, transform, and load data from diverse manufacturing systems (ERP, MES, IoT sensors, SQL databases, flat files, APIs).\n",
       "• Develop and manage data workflows using tools such as Azure Data Factory, SQL Server Integration Services (SSIS), or other modern data orchestration platforms.\n",
       "\n",
       "Platform & Architecture Support\n",
       "• Contribute to the design and architecture of the new internal analytics platform, ensuring flexibility, scalability, and cost-efficiency.\n",
       "• Design and optimize data lake and data warehouse solutions using cloud-native tools (preferably Microsoft Azure).\n",
       "\n",
       "Power BI Enablement\n",
       "• Build and optimize semantic data models, dataflows, and datasets for use in Power BI.\n",
       "• Collaborate with Power BI developers and business analysts to ensure data models meet reporting requirements, including DAX performance tuning and efficient data structure design.\n",
       "\n",
       "Data Governance & Quality\n",
       "• Implement data validation, cleansing, and monitoring processes to ensure data quality, reliability, and accuracy.\n",
       "• Contribute to data governance policies, including documentation, lineage tracking, and security controls.\n",
       "\n",
       "Collaboration & Mentorship\n",
       "• Work closely with cross-functional teams (IT, production, quality, finance, supply chain) to gather data requirements and deliver meaningful solutions.\n",
       "• Provide technical leadership and mentor junior developers or analysts in best practices for data engineering and analytics.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor’s or Master’s degree in Computer Science, Information Systems, Engineering, or a related field.\n",
       "• 5+ years of experience in data engineering, ETL development, or database architecture roles.\n",
       "• Proven experience with ETL tools such as Azure Data Factory, SSIS, or Informatica.\n",
       "• Advanced SQL skills, including performance tuning, stored procedures, and query optimization.\n",
       "• Strong experience with Power BI, including DAX, data modeling, and dataset optimization.\n",
       "• Experience with data warehousing (e.G., Azure Synapse, SQL Server, Snowflake) and cloud platforms (preferably Azure).\n",
       "• Understanding of manufacturing systems and data (e.G., ERP, MES, shop floor data, SCADA, historians).\n",
       "\n",
       "Preferred Skills\n",
       "• Experience with Python or other scripting languages for automation and data manipulation.\n",
       "• Familiarity with time-series data and integration from IoT or edge devices.\n",
       "• Knowledge of DevOps practices, CI/CD for data pipelines, and version control using Git.\n",
       "• Exposure to data governance frameworks and tools like Purview, Alation, or Collibra.\n",
       "• Power BI Service administration experience and integration with Power Platform (e.G., Power Automate, Power Apps)\n",
       "\n",
       "Key Competencies\n",
       "• Strong analytical and problem-solving skills with attention to detail.\n",
       "• Excellent communication skills, with the ability to bridge technical and business domains.\n",
       "• Self-starter with the ability to lead initiatives and work independently or collaboratively.\n",
       "• A passion for continuous improvement and innovation in a manufacturing setting.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRXRsIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlZpdmlkIFJlc291cmNpbmciLCJhZGRyZXNzX2NpdHkiOiJCaWxhc3B1ciwgQ2hoYXR0aXNnYXJoLCBJbmRpYSIsImh0aWRvY2lkIjoiMnQxYXo2bnUzRU1abUlLLUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>14 hours ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer – IBM DataStage</td><td>Tata Consultancy Services</td><td>Hyderabad, Telangana, India</td><td>Job Title: ETL Developer – IBM DataStage\n",
       "\n",
       "Experience: 5 to 10 years\n",
       "\n",
       "Location: Hyderabad, Chennai, Mumbai, Bangalore, Ahmedabad, Indore\n",
       "\n",
       "Employment Type: Full-time\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "We are seeking a skilled ETL Developer with hands-on experience in IBM DataStage to join our dynamic data engineering team. The ideal candidate will have a strong background in ETL development, data warehousing concepts, and performance optimization. This role involves designing and implementing robust ETL solutions, collaborating with cross-functional teams, and ensuring high data quality and integrity.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Design, develop, and implement ETL processes using IBM DataStage.\n",
       "• Work across the full project lifecycle including low-level design, development, testing, and deployment.\n",
       "• Develop parallel jobs using various DataStage stages such as Copy, Join, Merge, Lookup, Funnel, Filter, Sort, Remove Duplicates, Aggregator, Change Capture, and Transformer.\n",
       "• Integrate data from heterogeneous sources including Oracle, SQL Server, and flat files.\n",
       "• Optimize and troubleshoot existing ETL processes for performance improvements.\n",
       "• Collaborate with data analysts and architects to understand business requirements and translate them into technical solutions.\n",
       "• Ensure data quality and integrity across multiple data sources.\n",
       "• Create and maintain technical documentation for ETL processes.\n",
       "• Participate in code reviews and adhere to ETL best practices.\n",
       "• Work in Agile environments and use tools like JIRA for tracking tasks and issues.\n",
       "• Demonstrate strong problem-solving skills and the ability to troubleshoot technical issues effectively.\n",
       "• Understand and support operational requirements as part of business delivery.\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with IBM DataStage for ETL development and migration.\n",
       "• Solid understanding of database and data warehousing concepts.\n",
       "• Proficiency in SQL and UNIX.\n",
       "• Experience working with large datasets and complex data transformations.\n",
       "• Familiarity with Agile methodologies and tools like JIRA.\n",
       "• Excellent communication and collaboration skills.</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIOKAkyBJQk0gRGF0YVN0YWdlIiwiY29tcGFueV9uYW1lIjoiVGF0YSBDb25zdWx0YW5jeSBTZXJ2aWNlcyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoibkdQWmp0dFNpdlVKUWVEYkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>7 days ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>Insight Global</td><td>Hyderabad, Telangana, India</td><td>We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\n",
       "Responsibilities:\n",
       "Lead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\n",
       "Collaborate with developers to ensure a smooth transition and integration of the upgraded system.\n",
       "Enhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\n",
       "Provide support and guidance to the ETL development lead.\n",
       "Engage with end-users to ensure their needs are met during and after the upgrade process.\n",
       "Utilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\n",
       "Requirements:\n",
       "Minimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\n",
       "Proven experience with SQL Server upgrades, particularly from 2012 to 2022.\n",
       "Strong SQL Server skills, including building tables, stored procedures, views, and functions.\n",
       "Experience with .NET development is highly desirable to be leveraged for other projects\n",
       "Excellent problem-solving skills and attention to detail.\n",
       "Ability to work effectively in a team environment and communicate with end-users.\n",
       "Preferred Qualifications:\n",
       "Experience with SQL Server 2016.\n",
       "Experience with Oracle\n",
       "Familiarity with the latest features and enhancements in SQL Server 2022.\n",
       "Strong understanding of database architecture and migration strategies.\n",
       "\n",
       "We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ikw2UEZlVW5YOU90VExsNnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>sonataOne</td><td>Hyderabad, Telangana, India</td><td>Real-time data Ingestion, Streaming data, Kafka, AWS Cloud streaming tools, ETL, Semi-structured data formats like JSON, XML\n",
       "\n",
       "Tools: Talend, Kafka, AWS Event Bridge, Lamda and and Strong SQL & Python</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoic29uYXRhT25lIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>EPAM - ETL Developer - SSIS/SSRS</td><td>EPAM Systems India Private Limited</td><td>Hyderabad, Telangana, India</td><td>Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n",
       "\n",
       "Experience : 5+ Years\n",
       "\n",
       "Location : Hyderabad\n",
       "\n",
       "Job Description :\n",
       "\n",
       "We are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n",
       "\n",
       "You will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n",
       "\n",
       "Key Responsibilities :\n",
       "\n",
       "- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n",
       "\n",
       "- Develop, deploy, and maintain SSIS packages for ETL processes.\n",
       "\n",
       "- Work on data warehouse design, development, and maintenance.\n",
       "\n",
       "- Ensure high performance and reliability of data integration workflows.\n",
       "\n",
       "- Troubleshoot SQL queries and ETL issues; optimize database performance.\n",
       "\n",
       "- Collaborate with cross-functional teams to define data solutions.\n",
       "\n",
       "- Work with complex data structures including JSON and XML.\n",
       "\n",
       "- Understand and implement different data models (star, snowflake, etc.).</td><td>eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IkVQQU0gU3lzdGVtcyBJbmRpYSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InZ6RDdVNWpsc2hzYS03eW5BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>ETL Developer</td></tr><tr><td>ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location</td><td>Fiserv</td><td>India</td><td>Calling all innovators – find your future at Fiserv.\n",
       "\n",
       "We’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n",
       "\n",
       "Job Title\n",
       "\n",
       "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n",
       "\n",
       "Work Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n",
       "• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n",
       "• Participates in client kickoff activities including requirements and data gathering sessions.\n",
       "• Ability to analyze and migrate incoming external data into Fiserv solutions.\n",
       "• Strong working knowledge of ETL processes and best practices.\n",
       "• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n",
       "• Documents business processes and identifies opportunities for process redesign.\n",
       "• Reports system defects and identifies opportunities for system enhancements.\n",
       "• Performs as a technical consulting resource for new clients during the implementation process.\n",
       "• Supports several project managers during the pre and post go-live activities\n",
       "• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n",
       "• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n",
       "• Provides technical and analytical guidance to the project team.\n",
       "• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n",
       "• The position requires working in shifts, 2 PM to 11 PM IST\n",
       "\n",
       "What you will need to have:\n",
       "• Minimum of 5 – 10 years of experience in software development.\n",
       "• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n",
       "• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n",
       "• Self-starter who can work effectively, both independently and in a team environment.\n",
       "• Strong analytical, organizational, and problem-solving skills.\n",
       "• Excellent verbal and written communication skills.\n",
       "• Ability to maintain a professional attitude and demeanor in high pressure situations.\n",
       "• Ability to multi-task and manage multiple projects simultaneously.\n",
       "• Weekend/evening availability and support (10% - 25%).\n",
       "\n",
       "What would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n",
       "\n",
       "Thank you for considering employment with Fiserv. Please:\n",
       "• Apply using your legal name\n",
       "• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n",
       "\n",
       "What you should know about us:\n",
       "\n",
       "Fiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n",
       "\n",
       "Our commitment to Diversity and Inclusion:\n",
       "\n",
       "We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n",
       "\n",
       "Warning about fake job posts:\n",
       "\n",
       "Please be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n",
       "\n",
       "Any communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n",
       "\n",
       "If you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>ETL Developer/Senior Consultant Specialist</td><td>HSBC</td><td>India</td><td>Some careers shine brighter than others.If you're looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.HSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.We are currently seeking an experienced professional to join our team in the role of Senior Consultant Specialist.In this role, you will:- Communicating effectively with senior stakeholders.- Work with a team of technologists, ensuring prioritization of tasks and supporting removal of blockers.- Working on burning the product backlog and oversee product demo's- Understanding complex business requirements and new technologies and understand how to influence teams to adopt new practice and embrace automation.- Managing of risks and issues\n",
       "\n",
       "Requirements\n",
       "• name : HSBC\n",
       "• location : India, IN</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyL1NlbmlvciBDb25zdWx0YW50IFNwZWNpYWxpc3QiLCJjb21wYW55X25hbWUiOiJIU0JDIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRIYmhtVUVOWmF3OGh3Q0lBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>ETL Developer</td></tr><tr><td>Senior Informatica Developer</td><td>EverestDX Inc</td><td>Hyderabad, Telangana, India</td><td>About the Company:\n",
       "\n",
       "Everest DX – We are a Digital Platform Services company, headquartered in Stamford. Our Platform/Solution includes Orchestration, Intelligent operations with BOTs’, AI-powered analytics for Enterprise IT. Our vision is to enable Digital Transformation for enterprises to deliver seamless customer experience, business efficiency and actionable insights through an integrated set of futuristic digital technologies.\n",
       "\n",
       "Digital Transformation Services - Specialized in Design, Build, Develop, Integrate, and Manage cloud solutions and modernize Data centers, build a Cloud-native application and migrate existing applications into secure, multi-cloud environments to support digital transformation. Our Digital\n",
       "\n",
       "Platform Services enable organizations to reduce IT resource requirements and improve productivity, in addition to lowering costs and speeding digital transformation.\n",
       "\n",
       "Digital Platform - Cloud Intelligent Management (CiM) - An Autonomous Hybrid Cloud Management Platform that works across multi-cloud environments. helps enterprise Digital Transformation get most out of the cloud strategy while reducing Cost, Risk and Speed.\n",
       "\n",
       "To know more please visit: http://www.everestdx.com\n",
       "\n",
       "Responsibilities:\n",
       "• Candidate should hands-on experience on ETL and SQL.\n",
       "• Design, develop, and optimize ETL workflows using Informatica PowerCenter.\n",
       "• Implement cloud-based ETL solutions using Informatica IDMC and IICS.\n",
       "• Should have expertise on all transformations in Power Center and IDMC/IICS.\n",
       "• Should have experience or knowledge on the PC to IICS migration using CDI PC tool or some other tool.\n",
       "• Lead data migration projects, transitioning data from on-premise to cloud environments.\n",
       "• Write complex SQL queries and perform data validation and transformation.\n",
       "• Conduct detailed data analysis to ensure accuracy and integrity of migrated data.\n",
       "• Troubleshoot and optimize ETL processes for performance and error handling.\n",
       "• Collaborate with cross-functional teams to gather requirements and design solutions.\n",
       "• Create and maintain documentation for ETL processes and system configurations.\n",
       "• Implement industry best practices for data integration and performance tuning.\n",
       "\n",
       "Required Skills:\n",
       "• Hands-on experience with Informatica Power Center, IDMC and IICS.\n",
       "• Strong expertise in writing complex SQL queries and database management.\n",
       "• Experience in data migration projects (on-premise to cloud).\n",
       "• Strong data analysis skills for large datasets and ensuring accuracy.\n",
       "• Solid understanding of ETL design & development concepts.\n",
       "• Familiarity with cloud platforms (AWS, Azure).\n",
       "• Experience with version control tools (e.g., Git) and deployment processes.\n",
       "\n",
       "Preferred Skills:\n",
       "• Experience with data lakes, data warehousing, or big data platforms.\n",
       "• Familiarity with Agile methodologies.\n",
       "• Knowledge of other ETL tools</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgSW5mb3JtYXRpY2EgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiRXZlcmVzdERYIEluYyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNDdnQ200aEh5aHpSWlhDOEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>2 days ago</td><td>ETL Developer</td></tr><tr><td>Spark Engineer</td><td>Staffingine LLC</td><td>India</td><td>- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>Spark Engineer</td></tr><tr><td>Data Engineer - Spark/Python</td><td>Etelligens Technologies</td><td>India</td><td>Job Description\n",
       "\n",
       "We are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n",
       "• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n",
       "• Perform data cleansing and migration from diverse sources to target systems.\n",
       "• Collaborate with stakeholders to understand requirements and convert them into technical\n",
       "\n",
       "solutions.\n",
       "• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n",
       "• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n",
       "• Support, troubleshoot, and maintain data pipelines and workflows.\n",
       "• Participate in all phases of software development lifecycle including unit testing, integration\n",
       "\n",
       "testing, and performance testing.\n",
       "• Contribute to the modernization of data platforms and analytics tools on Azure.\n",
       "• Ensure data quality and integrity across all pipelines and systems.\n",
       "\n",
       "Required Skills & Qualifications\n",
       "• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n",
       "• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n",
       "• 1+ year of experience in Python and Spark scripting.\n",
       "• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n",
       "• Good understanding of Data Warehousing (DWH) concepts.\n",
       "• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n",
       "• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n",
       "• Hands-on experience in data cleansing, transformation, and migration projects.\n",
       "• Ability to work independently and within a team environment.\n",
       "• Microsoft Certified : Azure Data Engineer Associate\n",
       "• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n",
       "\n",
       "Location : Artha SEZ, Greater Noida West\n",
       "\n",
       "(ref:hirist.tech)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Staff Data Engineer (Spark, Python, Hadoop)</td><td>Visa</td><td>India</td><td>Company Description\n",
       "\n",
       "Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n",
       "\n",
       "When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n",
       "\n",
       "Join Visa: A Network Working for Everyone.\n",
       "Job Description\n",
       "\n",
       "The Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n",
       "\n",
       "Are you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n",
       "\n",
       "As a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n",
       "\n",
       "Essential Functions\n",
       "• Work with manager and clients to fully understand business requirements and desired business outcomes\n",
       "• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n",
       "• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n",
       "• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n",
       "• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n",
       "• Execute data engineering projects ranging from small to large either individually or as part of a project team\n",
       "• Ensure project delivery within timelines and budget requirements\n",
       "• Provide coaching and mentoring to junior team members\n",
       "\n",
       "This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\n",
       "Qualifications\n",
       "\n",
       "• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n",
       "• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n",
       "• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n",
       "• Minimum of 4 years of hands-on expertise with Java or Scala\n",
       "• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n",
       "• Experience working in an Agile and Test Driven Development environment.\n",
       "• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n",
       "• Experience with SAS as a statistical package is preferred\n",
       "• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Databricks Engineer - Spark / PySpark</td><td>Enkefalos Technologies LLP</td><td>Anywhere</td><td>Databricks Engineer – Spark / PySpark\n",
       "\n",
       "Location : Remote / Mysore\n",
       "\n",
       "Joining : Immediate\n",
       "\n",
       "Experience : 5+ years\n",
       "\n",
       "Responsibilities :\n",
       "\n",
       "Will implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n",
       "\n",
       "Requirements:\n",
       "• PySpark (RDDs, DataFrames, performance tuning)\n",
       "• Building gold‐layer data models for financial reporting\n",
       "• Experience with complex joins, aggregations, GL hierarchies\n",
       "• Version handling (Actuals vs Budget), currency conversions\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500,395.35 - ₹1,840,348.25 per year\n",
       "\n",
       "Benefits:\n",
       "• Flexible schedule\n",
       "• Paid sick time\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Application Question(s):\n",
       "• Have you worked on ADF/ADLS ?\n",
       "• Do you have hands-on experience of Spark / PySpark\n",
       "\n",
       "Experience:\n",
       "• Databricks Engineering: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>15 days ago</td><td>Spark Engineer</td></tr><tr><td>Senior PySpark Data Engineer (Big Data, Cloud Data Solutions, & Python)</td><td>Synechron</td><td>India</td><td>Job Summary\n",
       "\n",
       "Synechron is seeking a skilled PySpark Data Engineer to design, develop, and optimize data processing solutions leveraging modern big data technologies. In this role, you will lead efforts to build scalable data pipelines, support data integration initiatives, and work closely with cross-functional teams to enable data-driven decision-making. Your expertise will contribute to enhancing business insights and operational efficiency, positioning Synechron as a pioneer in adopting emerging data technologies.\n",
       "\n",
       "Software Requirements\n",
       "\n",
       "Required Software Skills:\n",
       "• PySpark (Apache Spark with Python) – experience in developing data pipelines\n",
       "• Apache Spark ecosystem knowledge\n",
       "• Python programming (versions 3.7 or higher)\n",
       "• SQL and relational database management systems (e.g., PostgreSQL, MySQL)\n",
       "• Cloud platforms (preferably AWS or Azure)\n",
       "• Version control: GIT\n",
       "• Data workflow orchestration tools like Apache Airflow\n",
       "• Data management tools: SQL Developer or equivalent\n",
       "\n",
       "Preferred Software Skills:\n",
       "• Experience with Hadoop ecosystem components\n",
       "• Knowledge of containerization (Docker, Kubernetes)\n",
       "• Familiarity with data lake and data warehouse solutions (e.g., AWS S3, Redshift, Snowflake)\n",
       "• Monitoring and logging tools (e.g., Prometheus, Grafana)\n",
       "\n",
       "Overall Responsibilities\n",
       "• Lead the design and implementation of large-scale data processing solutions using PySpark and related technologies\n",
       "• Collaborate with data scientists, analysts, and business teams to understand data requirements and deliver scalable pipelines\n",
       "• Mentor junior team members on best practices in data engineering and emerging technologies\n",
       "• Evaluate new tools and methodologies to optimize data workflows and improve data quality\n",
       "• Ensure data solutions are robust, scalable, and aligned with organizational data governance policies\n",
       "• Stay informed on industry trends and technological advancements in big data and analytics\n",
       "• Support production environment stability and performance tuning of data pipelines\n",
       "• Drive innovative approaches to extract value from large and complex datasets\n",
       "\n",
       "Technical Skills (By Category)\n",
       "\n",
       "Programming Languages:\n",
       "• Required: Python (PySpark experience minimum 2 years)\n",
       "• Preferred: Scala (for Spark), SQL, Bash scripting\n",
       "\n",
       "Databases/Data Management:\n",
       "• Relational databases (PostgreSQL, MySQL)\n",
       "• Distributed storage solutions (HDFS, cloud object storage like S3 or Azure Blob Storage)\n",
       "• Data warehousing platforms (Snowflake, Redshift – preferred)\n",
       "\n",
       "Cloud Technologies:\n",
       "• Required: Experience deploying and managing data solutions on AWS or Azure\n",
       "• Preferred: Knowledge of cloud-native services like EMR, Data Factory, or Azure Data Lake\n",
       "\n",
       "Frameworks and Libraries:\n",
       "• Apache Spark (PySpark)\n",
       "• Airflow or similar orchestration tools\n",
       "• Data processing frameworks (Kafka, Spark Streaming – preferred)\n",
       "\n",
       "Development Tools and Methodologies:\n",
       "• Version control with GIT\n",
       "• Agile management tools: Jira, Confluence\n",
       "• Continuous integration/deployment pipelines (Jenkins, GitLab CI)\n",
       "\n",
       "Security Protocols:\n",
       "• Understanding of data security, access controls, and GDPR compliance in cloud environments\n",
       "\n",
       "Experience Requirements\n",
       "• Minimum of 5+ years in data engineering, with hands-on PySpark experience\n",
       "• Proven track record of developing, deploying, and maintaining scalable data pipelines\n",
       "• Experience working with data lakes, data warehouses, and cloud data services\n",
       "• Demonstrated leadership in projects involving big data technologies\n",
       "• Experience mentoring junior team members and collaborating across teams\n",
       "• Prior experience in financial, healthcare, or retail sectors is beneficial but not mandatory\n",
       "\n",
       "Day-to-Day Activities\n",
       "• Develop, optimize, and deploy big data pipelines using PySpark and related tools\n",
       "• Collaborate with data analysts, data scientists, and business teams to define data requirements\n",
       "• Conduct code reviews, troubleshoot pipeline issues, and optimize performance\n",
       "• Mentor junior team members on best practices and emerging technologies\n",
       "• Design solutions for data ingestion, transformation, and storage\n",
       "• Evaluate new tools and frameworks for continuous improvement\n",
       "• Maintain documentation, monitor system health, and ensure security compliance\n",
       "• Participate in sprint planning, daily stand-ups, and project retrospectives to align priorities\n",
       "\n",
       "Qualifications\n",
       "• Bachelor’s or Master’s degree in Computer Science, Information Technology, or related discipline\n",
       "• Relevant industry certifications (e.g., AWS Data Analytics, GCP Professional Data Engineer) preferred\n",
       "• Proven experience working with PySpark and big data ecosystems\n",
       "• Strong understanding of software development lifecycle and data governance standards\n",
       "• Commitment to continuous learning and professional development in data engineering technologies\n",
       "\n",
       "Professional Competencies\n",
       "• Analytical mindset and problem-solving acumen for complex data challenges\n",
       "• Effective leadership and team management skills\n",
       "• Excellent communication skills tailored to technical and non-technical audiences\n",
       "• Adaptability in fast-evolving technological landscapes\n",
       "• Strong organizational skills to prioritize tasks and manage multiple projects\n",
       "• Innovation-driven with a passion for leveraging emerging data technologies\n",
       "\n",
       "S YNECHRON’S DIVERSITY & INCLUSION STATEMENT\n",
       "\n",
       "Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.\n",
       "\n",
       "All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.\n",
       "\n",
       "Candidate Application Notice</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgUHlTcGFyayBEYXRhIEVuZ2luZWVyIChCaWcgRGF0YSwgQ2xvdWQgRGF0YSBTb2x1dGlvbnMsIFx1MDAyNiBQeXRob24pIiwiY29tcGFueV9uYW1lIjoiU3luZWNocm9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjJYOEpqZUhEQmxiRndHQklBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>1 day ago</td><td>Spark Engineer</td></tr><tr><td>Senior Data Engineer (Delta Lake, Spark & Unity Catalog)</td><td>306 - GoTo Technologies India Private Limited</td><td>India</td><td>Job Description Where you’ll work: India (Remote) Engineering at GoTo We’re the trailblazers of remote work technology. We build powerful, flexible work software that empowers everyone to live their best life, at work and beyond. And blaze even more trails along the way. There’s ample room for growth – so you can blaze your own trail here too. When you join a GoTo product team, you’ll take on a key role in this process and see your work be used by millions of users worldwide. Your Day to Day As a Senior Data Engineer, you would be: Design, develop, and maintain robust, scalable, and efficient ETL/ELT data pipelines to process structured and unstructured data from diverse sources. Architect and implement cloud-native data solutions using AWS services including S3, EMR, Lambda, and EKS. Build and optimize large-scale data processing workflows using Apache Spark and Databricks, with hands-on experience implementing and managing Delta Lake tables for high-volume batch and streaming use cases. Manage and govern data using Unity Catalog, maintaining strong data lineage, access controls, and metadata management. Develop and maintain data models (relational and dimensional) to support analytics, reporting, and machine learning use cases. Schedule, monitor, and orchestrate workflows using Apache Airflow or similar orchestration tools. Implement data quality checks, logging, monitoring, and alerting to ensure reliability and visibility of data pipelines. Collaborate with analysts, data scientists, and business stakeholders to deliver high-quality, trusted data for downstream applications and enable self-service BI tooling. Ensure adherence to best practices in data governance, security, and compliance. Mentor junior engineers and contribute to engineering standards, including CI/CD, automated testing, and documentation What We’re Looking For As an Senior Data Engineer, your background will look like: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. 5+ years of relevant experience in data engineering or software development, with a proven track record designing and maintaining production-grade data pipelines. Proficiency in Python programming and SQL for data transformation and analytics. Strong hands-on experience with Apache Spark and Databricks, including direct exposure to Delta Lake for data lake management, ACID transactions, schema enforcement and evolution, and time travel functionalities. Experience managing and organizing data access with Unity Catalog. In-depth experience with AWS services—specifically S3, EMR, Lambda, and EKS—and an excellent understanding of cloud-based data architecture and security best practices. Strong data modeling skills (dimensional, normalized) and understanding of data warehousing and lakehouse paradigms. Hands-on experience orchestrating workflows using Apache Airflow or similar orchestration frameworks. Familiarity with BI and data visualization tools (e.g., Tableau, Power BI). Experience establishing data quality processes, monitoring, and observability for pipelines. Exceptional communication, collaboration, and problem-solving skills, with the ability to thrive in an agile, multicultural team environment. At GoTo, authenticity and inclusive culture are key to our thriving workplace, where diverse perspectives drive innovation and growth. Our team of GoGetters is passionate about learning, exploring, and working together to achieve success while staying committed to delivering exceptional experiences for our customers. We take pride in supporting our employees with comprehensive benefits, wellness programs, and global opportunities for professional and personal development. By maintaining an inclusive environment, we empower our teams to do their best work, make a meaningful impact, and grow their career. Learn more. At GoTo, we’re bold enough to imagine a world of work without limits—where curiosity and AI-driven innovation fuel our constant growth. As the leader in cloud communications and IT, we solve real-world challenges through practical, cutting-edge solutions and an unwavering customer-first mindset. Our culture is rooted in inclusion, ownership, and transparency, fueling an environment where every voice contributes to both personal and collective achievement. Here, collaboration sparks bold ideas, and authenticity is celebrated—empowering you to adapt, evolve, and make a real impact. Join GoTo, and help shape the future of work while accelerating your own growth alongside exceptional people who are redefining what’s possible.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAoRGVsdGEgTGFrZSwgU3BhcmsgXHUwMDI2IFVuaXR5IENhdGFsb2cpIiwiY29tcGFueV9uYW1lIjoiMzA2IC0gR29UbyBUZWNobm9sb2dpZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlJxSFNWX2s0aVZkRWhBNGRBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>10 days ago</td><td>Spark Engineer</td></tr><tr><td>Cloud Data Engineer- Spark & Databricks</td><td>Brighttier</td><td>Anywhere</td><td>This a Full Remote job, the offer is available from: India\n",
       "\n",
       "Job Title: Cloud Engineer – Spark/Databricks Specialist\n",
       "Location: Remote\n",
       "Job Type: Contract\n",
       "Industry: IT/Cloud Engineering\n",
       "Job Summary:\n",
       "We are looking for a highly skilled Cloud Engineer with a specialization in Apache Spark and Databricks to join our dynamic team. The ideal candidate will have extensive experience working with cloud platforms such as AWS, Azure, and GCP, and a deep understanding of data engineering, ETL processes, and cloud-native tools. Your primary responsibility will be to design, develop, and maintain scalable data pipelines using Spark and Databricks, while optimizing performance and ensuring data integrity across diverse environments.\n",
       "Key Responsibilities:\n",
       "Design and Development:\n",
       "• Architect, develop, and maintain scalable ETL pipelines using Databricks, Apache Spark (Scala, Python), and other cloud-native tools such as AWS Glue, Azure Data Factory, and GCP Dataflow.\n",
       "• Design and build data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n",
       "• Implement efficient data ingestion, transformation, and processing workflows with Spark and Databricks.\n",
       "• Optimize the performance of ETL processes for faster data processing and lower costs.\n",
       "• Develop and manage data pipelines using other ETL tools such as Informatica, SAP Data Intelligence, and others as needed.\n",
       "Data Integration and Management:\n",
       "• Integrate structured and unstructured data sources (relational databases, APIs, ERP systems) into the cloud data infrastructure.\n",
       "• Ensure data quality, validation, and integrity through rigorous testing.\n",
       "• Perform data extraction and integration from SAP or ERP systems, ensuring seamless data flow.\n",
       "Performance Optimization:\n",
       "• Monitor, troubleshoot, and enhance the performance of Spark/Databricks pipelines.\n",
       "• Implement best practices for data governance, security, and compliance across data workflows.\n",
       "Collaboration and Communication:\n",
       "• Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders, to define data requirements and deliver scalable solutions.\n",
       "• Provide technical guidance and recommendations on cloud data engineering processes and tools.\n",
       "Documentation and Maintenance:\n",
       "• Document data engineering solutions, ETL pipelines, and workflows.\n",
       "• Maintain and support existing data pipelines, ensuring they operate effectively and align with business goals.\n",
       "Qualifications:\n",
       "Education:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n",
       "Experience:\n",
       "• 7+ years of experience in cloud data engineering or similar roles.\n",
       "• Expertise in Apache Spark and Databricks for data processing.\n",
       "• Proven experience with cloud platforms like AWS, Azure, and GCP.\n",
       "• Experience with cloud-native ETL tools such as AWS Glue, Azure Data Factory, Kafka, GCP Dataflow, etc.\n",
       "• Hands-on experience with data platforms like Redshift, Snowflake, Azure Synapse, and BigQuery.\n",
       "• Experience in extracting data from SAP or ERP systems is preferred.\n",
       "• Strong programming skills in Python, Scala, or Java.\n",
       "• Proficient in SQL and query optimization techniques.\n",
       "Skills:\n",
       "• In-depth knowledge of Spark/Scala for high-performance data processing.\n",
       "• Strong understanding of data modeling, ETL/ELT processes, and data warehousing concepts.\n",
       "• Familiarity with data governance, security, and compliance best practices.\n",
       "• Excellent problem-solving, communication, and collaboration skills.\n",
       "Preferred Qualifications:\n",
       "• Certifications in cloud platforms (e.g., AWS Certified Data Analytics, Google Professional Data Engineer, Azure Data Engineer Associate).\n",
       "• Experience with CI/CD pipelines and DevOps practices for data engineering.\n",
       "• Exposure to Apache Hadoop, Kafka, or other data frameworks is a plus.\n",
       "\n",
       "This offer from \"Brighttier\" has been enriched by Jobgether.com and got a 74% flex score.</td><td>eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyLSBTcGFyayBcdTAwMjYgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJyaWdodHRpZXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTzdzcjNWUGpVa2l1R0VpZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Data Engineer (Snowflake, Spark, AWS) - AVP</td><td>12542 Citicorp Services India Private Limited</td><td>India</td><td>The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Data Engineer-Python,PySpark,SQL ,Spark Architecture,Azure Databricks</td><td>Siemens Healthineers</td><td>India</td><td>jobid\n",
       "• 460574\n",
       "\n",
       "jobfamily\n",
       "• Research & Development\n",
       "\n",
       "company\n",
       "• Siemens Healthcare Private Limited\n",
       "\n",
       "organization\n",
       "• Siemens Healthineers\n",
       "\n",
       "jobType\n",
       "• Full-time\n",
       "\n",
       "experienceLevel\n",
       "• Experienced Professional\n",
       "\n",
       "contractType\n",
       "• Permanent\n",
       "\n",
       "As a Data Engineer , you are required to:\n",
       "\n",
       "Design, build, and maintain data pipelines that efficiently process and transport data from various sources to storage systems or processing environments while ensuring data integrity, consistency, and accuracy across the entire data pipeline.\n",
       "\n",
       "Integrate data from different systems, often involving data cleaning, transformation (ETL), and validation. Design the structure of databases and data storage systems, including the design of schemas, tables, and relationships between datasets to enable efficient querying. Work closely with data scientists, analysts, and other stakeholders to understand their data needs and ensure that the data is structured in a way that makes it accessible and usable.\n",
       "\n",
       "Stay up-to-date with the latest trends and technologies in the data engineering space, such as new data storage solutions, processing frameworks, and cloud technologies. Evaluate and implement new tools to improve data engineering processes.\n",
       "\n",
       "Qualification : Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Science, Engineering is desirable.\n",
       "\n",
       "Experience level : At least 3 - 5 years hands-on experience in Data Engineering\n",
       "\n",
       "Desired Knowledge & Experience :\n",
       "• Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\n",
       "• Knowing Spark internals: Catalyst/Tungsten/Photon\n",
       "• Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\n",
       "• IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\n",
       "• Test: pytest, Great Expectations\n",
       "• CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\n",
       "• Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\n",
       "• Languages: Python/Functional Programming (FP)\n",
       "• SQL : TSQL/Spark SQL/HiveQL\n",
       "• Storage : Data Lake and Big Data Storage Design\n",
       "\n",
       "additionally it is helpful to know basics of:\n",
       "• Data Pipelines : ADF/Synapse Pipelines/Oozie/Airflow\n",
       "• Languages: Scala, Java\n",
       "• NoSQL : Cosmos, Mongo, Cassandra\n",
       "• Cubes : SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\n",
       "• SQL Server : TSQL, Stored Procedures\n",
       "• Hadoop : HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\n",
       "• Data Catalog : Azure Purview, Apache Atlas, Informatica\n",
       "\n",
       "Required Soft skills & Other Capabilities :\n",
       "\n",
       "Great attention to detail and good analytical abilities.\n",
       "\n",
       "Good planning and organizational skills\n",
       "\n",
       "Collaborative approach to sharing ideas and finding solutions\n",
       "\n",
       "Ability to work independently and also in a global team environment.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVB5dGhvbixQeVNwYXJrLFNRTCAsU3BhcmsgQXJjaGl0ZWN0dXJlLEF6dXJlIERhdGFicmlja3MiLCJjb21wYW55X25hbWUiOiJTaWVtZW5zIEhlYWx0aGluZWVycyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJXVkxrZFF1QU4tSGFXQjIzQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>8 days ago</td><td>Spark Engineer</td></tr><tr><td>Cloud Data Engineer (Spark/Databricks)</td><td>Antal Job Board</td><td>Nagpur, Maharashtra, India</td><td>Vacancy No\n",
       "VN1228\n",
       "\n",
       "Business Unit\n",
       "EMEA\n",
       "\n",
       "Job Location\n",
       "India\n",
       "\n",
       "Employment Type\n",
       "Full Time\n",
       "\n",
       "Job Details and Responsibilities\n",
       "We are seeking an experienced Cloud Data Engineer with a strong background in AWS, Azure, and GCP. The ideal candidate will have extensive experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, and other ETL tools like Informatica, SAP Data Intelligence, etc. You will be responsible for designing, implementing, and maintaining robust data pipelines and building scalable data lakes. Experience with various data platforms like Redshift, Snowflake, Databricks, Synapse, Snowflake and others is essential. Familiarity with data extraction from SAP or ERP systems is a plus.\n",
       "\n",
       "Key Responsibilities:\n",
       "\n",
       "Design and Development:\n",
       "• Design, develop, and maintain scalable ETL pipelines using cloud-native tools (AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.).\n",
       "• Architect and implement data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n",
       "• Develop and optimize data ingestion, transformation, and loading processes using Databricks, Snowflake, Redshift, BigQuery and Azure Synapse.\n",
       "• Implement ETL processes using tools like Informatica, SAP Data Intelligence, and others.\n",
       "• Develop and optimize data processing jobs using Spark Scala.\n",
       "Data Integration and Management:\n",
       "• Integrate various data sources, including relational databases, APIs, unstructured data, and ERP systems into the data lake.\n",
       "• Ensure data quality and integrity through rigorous testing and validation.\n",
       "• Perform data extraction from SAP or ERP systems when necessary.\n",
       "Performance Optimization:\n",
       "• Monitor and optimize the performance of data pipelines and ETL processes.\n",
       "• Implement best practices for data management, including data governance, security, and compliance.\n",
       "Collaboration and Communication:\n",
       "• Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\n",
       "• Collaborate with cross-functional teams to design and implement data solutions that meet business needs.\n",
       "Documentation and Maintenance:\n",
       "• Document technical solutions, processes, and workflows.\n",
       "• Maintain and troubleshoot existing ETL pipelines and data integrations.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Education:\n",
       "\n",
       "Bachelor’s degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n",
       "\n",
       "Experience:\n",
       "• 7+ years of experience as a Data Engineer or in a similar role.\n",
       "• Proven experience with cloud platforms: AWS, Azure, and GCP.\n",
       "• Hands-on experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.\n",
       "• Experience with other ETL tools like Informatica, SAP Data Intelligence, etc.\n",
       "• Experience in building and managing data lakes and data warehouses.\n",
       "• Proficiency with data platforms like Redshift, Snowflake, BigQuery, Databricks, and Azure Synapse.\n",
       "• Experience with data extraction from SAP or ERP systems is a plus.\n",
       "• Strong experience with Spark and Scala for data processing.\n",
       "\n",
       "Skills:\n",
       "• Strong programming skills in Python, Java, or Scala.\n",
       "• Proficient in SQL and query optimization techniques.\n",
       "• Familiarity with data modeling, ETL/ELT processes, and data warehousing concepts.\n",
       "• Knowledge of data governance, security, and compliance best practices.\n",
       "• Excellent problem-solving and analytical skills.\n",
       "• Strong communication and collaboration skills.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with other data tools and technologies such as Apache Spark, or Hadoop.\n",
       "• Certifications in cloud platforms (AWS Certified Data Analytics – Specialty, Google Professional Data Engineer, Microsoft Certified: Azure Data Engineer Associate).\n",
       "• Experience with CI/CD pipelines and DevOps practices for data engineering\n",
       "• Selected applicant will be subject to a background investigation, which will be conducted and the results of which will be used in compliance with applicable law.\n",
       "\n",
       "What we offer in return:\n",
       "• Remote Working: Lemongrass always has been and always will offer 100% remote work\n",
       "• Flexibility: Work where and when you like most of the time\n",
       "• Training: A subscription to A Cloud Guru and generous budget for taking certifications and other resources you’ll find helpful\n",
       "• State of the art tech: An opportunity to learn and run the latest industry standard tools\n",
       "• Team: Colleagues who will challenge you giving the chance to learn from them and them from you\n",
       "\n",
       "Lemongrass Consulting is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate on the basis of race, religion, color, national origin, religious creed, gender, sexual orientation, gender identity, gender expression, age, genetic information, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics\n",
       "\n",
       "About Lemongrass\n",
       "Lemongrass (lemongrasscloud.com) is a global leader in SAP consulting, focused on helping organizations transform their business processes through innovative solutions and technologies. With a strong commitment to customer success, Lemongrass partners with companies to drive their digital transformation journeys, enabling them to unlock the full potential of their SAP investments.\n",
       "\n",
       "We do this with our continuous innovation, automation, migration and operation, delivered on the world's most comprehensive cloud platforms – AWS, Azure and GCP and SAP Cloud ERP. We have been working with AWS and SAP since 2010 and we are a Premier Amazon Partner Network (APN) Consulting Partner. We are also a Microsoft Gold Partner, a Google Cloud Partner and an SAP Certified Silver Partner.\n",
       "\n",
       "Our team is what makes Lemongrass exceptional and why we have the excellent reputation in the market that we enjoy today. At Lemongrass, you will work with the smartest and most motivated people in the business. We take pride in our culture of innovation and collaboration that drives us to deliver exceptional benefits to our clients every day.</td><td>eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyIChTcGFyay9EYXRhYnJpY2tzKSIsImNvbXBhbnlfbmFtZSI6IkFudGFsIEpvYiBCb2FyZCIsImFkZHJlc3NfY2l0eSI6Ik5hZ3B1ciwgTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJCQXpseTdfU0lsQjByTmF2QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>28 days ago</td><td>Spark Engineer</td></tr><tr><td>Data Analyst III</td><td>Bristol Myers Squibb</td><td>Hyderabad, Telangana, India</td><td>Working with Us\n",
       "Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n",
       "\n",
       "Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n",
       "\n",
       "The US Commercial Analytics, Market Access Data Analyst will play a crucial role in supporting data-driven decision-making processes, generating insights, and providing strategic guidance to optimize our Market Access & Pricing activities in the US. This position requires a deep expertise in secondary analytics and statistical analytic techniques, along with a firm grasp of the US healthcare landscape, especially market access and pricing dynamics. The Business Analyst III will typically be responsible for leading support for multiple therapeutic areas in Market Access and will collaborate with cross-functional teams to enhance understanding of business performance and drive growth.\n",
       "\n",
       "Roles and Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Perform statistical analyses, data mining and predictive modelling to uncover trends and drivers of performance\n",
       "• Collaborate with stakeholders (US based Market Access Business Insights & Analytics team and US Market Access team members) to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects including payer, PBM and patient level subnational & claims data analyses, formulary and policy level analysis, and ad-hoc analytics based on secondary data to support US Market Access strategy\n",
       "• Prepare reports, dashboards and presentations to communicate findings, insights and recommendations to stakeholders\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up-to-date with industry trends (US access and health care landscape), best practices, and emerging technologies\n",
       "• Partner with US based Market Access BIA/Commercial team members (for a given project scope)\n",
       "• Provide overall project oversight and training/mentorship to junior analysts and team members as required\n",
       "• Supports culture of fact-based decision making through application of best-in-class, innovative, and appropriate methodologies to address key business questions\n",
       "\n",
       "Skills & Competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions, around US pricing and reimbursement strategies\n",
       "• Proficiency in statistical analysis techniques and applications of those to US commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and strategic thinking, with the ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome Candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• 5-8 years prior Pharmaceutical industry experience with 1-2 years within US Market Access & Pricing Analytics\n",
       "• Experience working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims, medical policy and formulary data), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with reimbursement and regulatory requirements and compliance in the US biopharma industry\n",
       "• Certification or training in relevant analytics or business intelligence tools is a plus\n",
       "\n",
       "If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n",
       "\n",
       "Uniquely Interesting Work, Life-changing Careers\n",
       "With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n",
       "\n",
       "On-site Protocol\n",
       "\n",
       "BMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n",
       "\n",
       "Site-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n",
       "\n",
       "BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n",
       "\n",
       "BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n",
       "\n",
       "BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n",
       "\n",
       "If you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n",
       "\n",
       "Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUlJIiwiY29tcGFueV9uYW1lIjoiQnJpc3RvbCBNeWVycyBTcXVpYmIiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZHU3JGRGo4OTcybHdnQWpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>3 days ago</td><td>Data Analyst</td></tr><tr><td>Data Science Analyst</td><td>IN10 (FCRS = IN010) Novartis Healthcare Private Limited</td><td>India</td><td>About the Role:Within Insights and Commercial Solutions, below are the key service groups:  Marketing Effectiveness (Business Analytics, Forecasting, Competitive Intelligence)  Field Force Operations  Commercial Consulting  Brand Insights  Patient access services (Pricing and market access, etc.)  Primary Market Research Provide analytics support to Novartis internal customers (CPOs & Regional marketing and sales teams) on various projects • Support and facilitate data enabled decision making for Novartis internal customers using data analysis and data science techniques/methods on internal and external (3rd party) data to solve business problems • Support I&CS business in building capabilities by involving in various initiatives like knowledge sharing, on-boarding and training support, support in all business-related tasks/activities, building process documentation and knowledge repositories.Analyst, Data ScienceLocation – Hyderabad #LI Hybrid Key Responsibilities:Delivering projects and managing internal customer expectations across multi-channel marketing analytics, Portfolio Analytics, Targeting and Segmentation, Predictive Analytics, Resource Allocation and Optimization, Other ad-hoc requests.Create and deliver customer requirements as agreed SLAs (timeliness, accuracy, quality, etc.) and drive excellent customer satisfaction. Deliver advanced analytical and statistical solutions for various projects related to promotion evaluation, multi-channel marketing (MCM) campaign design, return on investment (ROI) analysis, resource allocation, segmentation, targeting, and other ad-hoc business questionsSupport exploratory research to identify new areas of application of advanced analytics/data science in providing enhanced decision-making support. Deliver services through a structured project management approach with appropriate documentation and communication throughout the delivery of servicesAutomation of project codes and development of front-end delivery solutions. Support in creation and maintenance of standard operating procedures (SOPs), quality checklists that will enable excellent quality outputs within the function.Support in developing and maintaining knowledge repositories that captures qualitative and quantitative reports of brands, disease areas, macroeconomic trends of Novartis operating markets, etc.Comply with all internal functional operating procedures like time tracking, KPI tracking and reporting, and other internal systems and processes. Comply to all Novartis operating procedures as per legal/IT/HR requirementsEssential Requirements:Technical Skills: Statistical Modeling Experience (Covering but not limited to, Regression, Classification, Supervised and Unsupervised Learning Methods, Optimization techniques, Predictive Models, Deep learning Models, e.g. Neural Networks, Natural Language Processing, Text Mining etc.)Programming languages/tools – R, SAS, Python, SQLVisualization tools – Qlikview, Qliksense, TableauWorking knowledge of MS-Office (MS Excel, VBA, PowerPoint, Access)Pharmaceutical industry domain/datasets knowledge – desirableEducation:Graduation/Post Graduation in Business Administration, Mathematics, Economics, Statistics, Engineering or Quantitative streamsExperience: Experience (Minimum 2years) in analytics in marketing analytics, experience in pharma industry is preferableShould have strong quantitative and systems backgroundStrong analytical thinking with problem solving approachShould have good ability to understand new data sources in short timeframe and embed them into standard analysesA good understanding of pharmaceutical domain and data would be ideal. Should have worked in an international company with exposure to working in a cross-cultural environmentStrong and proactive business results focus, and proven ability to provide insights that increase productivityWhy Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook. https://www.novartis.com/careers/benefits-rewards Commitment to Diversity and Inclusion: Novartis is committed to building an outstanding, inclusive work environment and diverse teams' representative of the patients and communities we serve. Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVuY2UgQW5hbHlzdCIsImNvbXBhbnlfbmFtZSI6IklOMTAgKEZDUlMgPSBJTjAxMCkgTm92YXJ0aXMgSGVhbHRoY2FyZSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiejA4Y3NEMUZUdmdvZExfWkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>7 days ago</td><td>Data Analyst</td></tr><tr><td>Data Sr.Modeler/Data Analyst( Immediate Joiner)</td><td>The Talent Quest</td><td>Hyderabad, Telangana, India</td><td>Vacancy with a company focused on digital transformation, specializing in intelligent automation, digitalization, data science & analytics, and mobile enablement. They help businesses improve cost efficiency, productivity, and agility by reducing turnaround time and errors. The company provides services and solutions including operations digital transformation consulting, next-gen shared services setup consulting, cognitive RPA deployment, and AI-enabled CX enhancement. Founded in 2020 ;with HQ in Gurugram, India; the Company is now operating from Noida, Mumbai, Hyderabad, and Bengaluru as well.\n",
       "\n",
       "Job Role:We are seeking a highly skilled and detail-oriented Data Modeler to join our Data\n",
       "\n",
       "Management team. The ideal candidate will be responsible for designing and\n",
       "\n",
       "implementing logical and physical data models to support enterprise data\n",
       "\n",
       "initiatives. This role requires close collaboration with business stakeholders, data\n",
       "\n",
       "architects, and engineers to ensure data is structured and accessible for analytics,\n",
       "\n",
       "reporting, and operational needs.\n",
       "\n",
       "The successful candidate will:\n",
       "\n",
       "Provides technical expertise in needs identification, data modelling, data\n",
       "\n",
       "movement and transformation mapping (source to target), automation and testing\n",
       "\n",
       "strategies, translating business needs into technical solutions with adherence to\n",
       "\n",
       "established data guidelines and approaches from a business unit or project\n",
       "\n",
       "perspective.\n",
       "\n",
       "7-10 Years industry implementation experience with one or more data\n",
       "\n",
       "modelling tools such as Erwin, ERStudio, PowerDesigner etc.\n",
       "\n",
       " Minimum of 8 years of data architecture, data modelling or similar\n",
       "\n",
       "experience\n",
       "\n",
       " 5-7 years of management experience required\n",
       "\n",
       " 5-7 years consulting experience preferred\n",
       "\n",
       " Experience working with dimensionally modelled data\n",
       "\n",
       " Bachelor’s degree or equivalent experience, Master’s Degree Preferred\n",
       "\n",
       " Understanding of cloud (Azure, AWS, GCP, Snowflake preferred) and on\n",
       "\n",
       "premises architectures\n",
       "\n",
       "Job Types: Full-time, Permanent\n",
       "\n",
       "Pay: Up to ₹3,000,000.00 per year\n",
       "\n",
       "Benefits:\n",
       "• Cell phone reimbursement\n",
       "• Internet reimbursement\n",
       "• Life insurance\n",
       "• Paid sick time\n",
       "• Paid time off\n",
       "• Work from home\n",
       "\n",
       "Work Location: In person</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIFNyLk1vZGVsZXIvRGF0YSBBbmFseXN0KCBJbW1lZGlhdGUgSm9pbmVyKSIsImNvbXBhbnlfbmFtZSI6IlRoZSBUYWxlbnQgUXVlc3QiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ino0YkdrY3RHOUdZdmFTRVVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>2 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst - INTL - Mexico or India</td><td>Insight Global</td><td>Hyderabad, Telangana, India</td><td>Project Background:\n",
       "Mosaic is our financial planning and reporting transformation program, bringing an aligned business process and toolset to all key business units enabling us to get consistent and standardized financial metrics across geographies. Started in 2019 the program is already rolled out to Europe and most of North America and continues across the world.\n",
       "There are three key components to the program:\n",
       "1. A standardized planning tool IBM Cognos TM1\n",
       "2. A global data warehouse based on Azure Cloud know as SPOT (single point of truth). The SPOT team is made up of functional data analysts & data engineers\n",
       "3. An extensive suite of dashboards known as Cockpit using Tableau. The Cockpit team are generally finance analysts with Tableau skills\n",
       "Role Background:\n",
       "We are looking for an additional functional/data analyst to join the existing team of functional/data analysts working on the SPOT data warehouse.\n",
       "The role of the functional/data analyst is to be the bridge between the Cockpit team, Data Engineering team, and Cognos TM1 team. The analyst understands the business data requirements and translates these into data definition and data transformation documents that are then build by the data engineers. The analyst can also be involved in testing, data validation and any sustain activity related to the flow and accuracy of the data in SPOT & Cockpit.\n",
       "Typically, the functional data analyst receives requirements for new and/or amended data pipelines from the Cockpit team or works with the Cognos TM1 team on new data sets that need to flow to SPOT & Cockpit.\n",
       "The current functional data analysts come from a range of backgrounds from data engineers to business analysts. What unites the team is a passion for data, attention to detail, ability to navigate and understand complex data flows, and a customer-centric mindset. This is a high profile and at times high pressure transformation program and everyone on the team must be responsive to our stakeholders needs.\n",
       "Key Accountabilities:\n",
       "This role will primarily work with the North America Cockpit team primarily based in Mexico City, but as part of the team they will be expected to build a good knowledge of all data pipelines and work on any area as needed. The key elements of the role are:\n",
       "Develop and maintain SPOT solution design & data architecture:\n",
       "o Ensure SPOT solution design & data model is up to date with latest business requirements\n",
       "o Contribute to the delivery of current technical roadmap that can support the predicted growth in data volumes\n",
       "Translate and communicate business requirements across all IT delivery teams and/or partners:\n",
       "o Collaborate closely with SPOT Cockpit teams to ensure business requirements received from Global & Regional Planning teams are well-understood and translated into the SPOT solution design (functional & data model)\n",
       "Act as an owner in the diagnosis and resolution of any data issues raised with the team, partnering with other teams as needed.\n",
       "\n",
       "We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBJTlRMIC0gTWV4aWNvIG9yIEluZGlhIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Il96ZU5oSUE0Ujg3ak1KZHNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>6 days ago</td><td>Data Analyst</td></tr><tr><td>IN Specialist 3- Data Analyst - TRAS -Assurance– Bangalore</td><td>PwC</td><td>India</td><td>Line of ServiceAssuranceIndustry/SectorNot ApplicableSpecialismAssuranceManagement LevelSpecialistJob Description & SummaryAt PwC, our people in audit and assurance focus on providing independent and objective assessments of financial statements, internal controls, and other assurable information enhancing the credibility and reliability of this information with a variety of stakeholders. They evaluate compliance with regulations including assessing governance and risk management processes and related controls.In digital assurance at PwC, you will focus on providing assurance services over clients' digital environment, including processes and controls, cyber security measures, data and AI systems, and their associated governance, to help organisations and their stakeholders build trust in their technology while complying with relevant regulations.*Why PWCAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm’s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations. \"Job Description & SummaryABOUT TECHNOLOGY RISK ASSURANCE SERVICES:The Technology Risk Assurance Services practice in the firm, provides both audit and non-audit services related to controls around the financial reporting process, including financial business process and IT management controls.SPA provides:· Financial and operation applications/business process controls reviews· Database security controls reviews· IT general controls reviews· Infrastructure security reviews· Third party assurance and opinion services· Sarbanes-Oxley readiness, process improvement and sustainability services· Due diligence on systems and controls· Pre- and post-implementation systems reviews· Project assurance services· Data services (e.g., CAATs, data quality reviews)· Computer security reviewsResponsibilities (essential functions and responsibilities of the job):The tasks, roles and responsibilities concerned with the job will include the following:Executing various audit engagements as a support to field in-charge andApplying The Firm’s methodology while executing various job rolesApply current knowledge of IT trends and systems to identify security and risk management issues, and other opportunities for improvementAssist in the planning and execution of business process control reviews, information systems audits, and other Assurance offerings across a variety of industriesA good understanding of clients Business Risks, Audit Risks, IT Risks and the mitigating controls in addressing these risksGain strong comprehension of client operations, processes, and business objectives and utilize that knowledge on engagements.Evaluate and test business processes and controls and identify areas of risks.Perform general computer and application controls reviews including ERPs like SAP, Oracle Application, JD Edwards, MFG Pro, etc.Comply with requirements of ICAI Auditing Standards like AAS 29Data Analysis using CAATs tool like Excel, ACL, SQL etc*Mandatory skill sets - Data Analyst*Preferred Skill Sets – Business Analyst*Years of experience required – 0-1 Years*Education Qualification – Post Graduation in Data ScienceEducation (if blank, degree and/or field of study not specified)Degrees/Field of Study required: Postgraduate (Diploma)Degrees/Field of Study preferred:Certifications (if blank, certifications not specified)Required SkillsData AnalyticsOptional SkillsAccepting Feedback, Accepting Feedback, Active Listening, Artificial Intelligence (AI) Platform, Auditing Methodologies, Cloud Engineering, Communication, Compliance and Governance, Compliance and Standards, Compliance Auditing, Corporate Governance, Cybersecurity Governance, Cybersecurity Risk Management, Data Quality, Data Quality Assessment, Data Quality Assurance Testing, Data Validation, Emotional Regulation, Empathy, Governance Framework, Inclusion, Information Assurance, Information Security Governance, Intellectual Curiosity, Internal Controls {+ 11 more}Desired Languages (If blank, desired languages not specified)Travel RequirementsNot SpecifiedAvailable for Work Visa Sponsorship?NoGovernment Clearance Required?NoJob Posting End Date</td><td>eyJqb2JfdGl0bGUiOiJJTiBTcGVjaWFsaXN0IDMtIERhdGEgQW5hbHlzdCAtIFRSQVMgLUFzc3VyYW5jZeKAkyBCYW5nYWxvcmUiLCJjb21wYW55X25hbWUiOiJQd0MiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiZTlNdVBjTnNJWHlmb3ZrSkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>1 day ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst 1</td><td>Bristol Myers Squibb</td><td>Hyderabad, Telangana, India</td><td>The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n",
       "\n",
       "Roles & Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up to date with industry trends, best practices, and emerging technologies\n",
       "\n",
       "Skills and competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n",
       "• commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• Proven experience (1-3 years) in a similar business analyst role\n",
       "• Prior Pharmaceutical industry and/or healthcare consulting experience required\n",
       "• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with regulatory requirements and compliance in the US biopharma industry\n",
       "\n",
       "The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n",
       "\n",
       "Roles & Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up to date with industry trends, best practices, and emerging technologies\n",
       "\n",
       "Skills and competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n",
       "• commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• Proven experience (1-3 years) in a similar business analyst role\n",
       "• Prior Pharmaceutical industry and/or healthcare consulting experience required\n",
       "• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with regulatory requirements and compliance in the US biopharma industry</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ1ZUlVZURnaVJaU1ludkdlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>16 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst Finance (VBA, Power Query, Power BI, Python) 4+ years of experience)</td><td>Dupont</td><td>Hyderabad, Telangana, India</td><td>At DuPont, our purpose is to empower the world with essential innovations to thrive. We work on things that matter. Whether it’s providing clean water to more than a billion people on the planet, producing materials that are essential in everyday technology devices from smartphones to electric vehicles, or protecting workers around the world. Discover the many reasons the world’s most talented people are choosing to work at DuPont. Why Join Us | DuPont Careers\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "The Data Analyst/Developer will play a crucial role in developing automated data solutions, creating comprehensive dashboards, and utilizing advanced Excel functionalities. This position will require collaboration with various teams to translate data needs into actionable intelligence.\n",
       "\n",
       "Key Areas of Expertise and Responsibilities:\n",
       "\n",
       "1. Visual Basic for Applications (VBA)\n",
       "• Responsibilities:\n",
       "• Develop and maintain complex VBA applications to automate repetitive tasks.\n",
       "• Incorporate SAP Scripting within VBA to optimize business processes.\n",
       "• Troubleshoot and debug existing VBA code to ensure smooth functionality.\n",
       "• Criteria:\n",
       "• Advanced proficiency in VBA programming.\n",
       "• Demonstrated experience with SAP interfaces and scripting.\n",
       "• Ability to write modular, efficient, and maintainable code.\n",
       "• Knowledge of Excel object model and its functionalities.\n",
       "\n",
       "2. Power Query\n",
       "• Responsibilities:\n",
       "• Utilize Power Query for data extraction, transformation, and loading (ETL) processes.\n",
       "• Develop and maintain data models in Excel to streamline data preparation.\n",
       "• Create and optimize Power Query scripts for efficient data processing.\n",
       "• Criteria:\n",
       "• Intermediate experience with Power Query including M language for data transformation.\n",
       "• Familiarity with connecting Power Query to various data sources (e.g., databases, APIs).\n",
       "• Ability to perform data cleansing and manipulation through Power Query.\n",
       "\n",
       "3. Power BI\n",
       "• Responsibilities:\n",
       "• Create interactive, user-friendly dashboards and reports using Power BI.\n",
       "• Collaborate with stakeholders to gather reporting requirements and deliver insights.\n",
       "• Optimize Power BI reports for performance and usability.\n",
       "• Criteria:\n",
       "• Intermediate knowledge of Power BI Desktop and Power BI Service.\n",
       "• Ability to create DAX measures and calculated columns for enhanced analytics.\n",
       "• Familiarity with data visualization best practices and techniques.\n",
       "\n",
       "4. Python\n",
       "• Responsibilities:\n",
       "• Develop Python scripts to automate data manipulation and Excel-related tasks.\n",
       "• Utilize libraries such as Pandas and NumPy for data analysis and automation.\n",
       "• Collaborate with the data team to integrate Python solutions with existing tools.\n",
       "• Criteria:\n",
       "• Intermediate proficiency in Python, especially in data manipulation and automation.\n",
       "• Experience with libraries like Pandas, NumPy, and openpyxl for Excel automation.\n",
       "• Understanding of APIs and ability to retrieve data programmatically.\n",
       "\n",
       "Qualifications:\n",
       "• Bachelor’s degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n",
       "• A minimum of 4 of experience in a data-related role, focusing on the technologies mentioned.\n",
       "• Strong analytical and problem-solving skills with attention to detail.\n",
       "• Excellent communication skills and the ability to work collaboratively with diverse teams.\n",
       "\n",
       "Preferred Skills:\n",
       "• Experience with SQL and relational databases for data querying and data management.\n",
       "• Familiarity with other business intelligence tools (like Tableau or QlikView) is a plus.\n",
       "• Knowledge of machine learning principles is an advantage.\n",
       "• Understanding of data warehousing concepts and methodologies.\n",
       "\n",
       "Join our Talent Community to stay connected with us!\n",
       "\n",
       "On May 22, 2024, we announced a plan to separate our Electronics and Water businesses in a tax-free manner to its shareholders. On January 15, 2025, we announced that we are targeting November 1, 2025, for the completion of the intended separation of the Electronics business (the “Intended Electronics Separation”)*. We also announced that we would retain the Water business. We are committed to ensuring a smooth and successful separation process for the Future Electronics business. We look forward to welcoming new talent interested in contributing to the continued success and growth of our evolving organization.\n",
       "\n",
       "(1)The separation transactions are subject to satisfaction of customary conditions, including final approval by DuPont's Board of Directors, receipt of tax opinion from counsel, the filing and effectiveness of Form 10 registration statements with the U.S. Securities and Exchange Commission, applicable regulatory approvals, and satisfactory completion of financing.  For further discussion of risks, uncertainties and assumptions that could impact the achievement, expected timing and intended benefits of the separation transactions, see DuPont’s announcement.\n",
       "\n",
       "DuPont is an equal opportunity employer. Qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability or any other protected class. If you need a reasonable accommodation to search or apply for a position, please visit our Accessibility Page for Contact Information.\n",
       "\n",
       "DuPont offers a comprehensive pay and benefits package. To learn more visit the Compensation and Benefits page.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgRmluYW5jZSAoVkJBLCBQb3dlciBRdWVyeSwgUG93ZXIgQkksIFB5dGhvbikgNCsgeWVhcnMgb2YgZXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJEdXBvbnQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImNGeEdTMkhJQXpJRW5kb2ZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>2 days ago</td><td>Data Analyst</td></tr><tr><td>Sustainability Data Analyst</td><td>Carrier</td><td>Hyderabad, Telangana, India</td><td>Role: Sustainability Data Analyst\n",
       "\n",
       "Location: Hyderabad, India\n",
       "\n",
       "Full/ Part-time: Full time\n",
       "\n",
       "Build a career with confidence\n",
       "\n",
       "Carrier Global Corporation, global leader in intelligent climate and energy solutions is committed to creating solutions that matter for people and our planet for generations to come. From the beginning, we've led in inventing new technologies and entirely new industries. Today, we continue to lead because we have a world-class, diverse workforce that puts the customer at the center of everything we do.\n",
       "\n",
       "About the role\n",
       "\n",
       "We are seeking a results-driven Sustainability Data Analyst to join Carrier’s product sustainability analytics team. The ideal candidate will have a strong background in sustainability practices and data analytics, with a focus on analyzing environmental impact and identifying opportunities for improvement. This role involves conducting comprehensive sustainability/energy audits, analyzing data, and developing strategies to enhance sustainability performance.\n",
       "\n",
       "Key responsibilities:\n",
       "• Perform detailed sustainability audits and analyze data to identify trends and areas for improvement.\n",
       "• Develop and implement strategies to enhance sustainability and reduce environmental impact.\n",
       "• Monitor and evaluate the performance of sustainability initiatives using data analytics.\n",
       "• Collaborate with cross-functional teams to optimize sustainability practices.\n",
       "• Prepare reports and presentations on sustainability metrics and audit findings.\n",
       "• Stay updated on industry trends and best practices in sustainability and data analytics.\n",
       "\n",
       "Minimum Requirements:\n",
       "\n",
       "Education: Bachelor’s or Master’s degree in Mechanical, Energy Engineering, Environmental Science, Sustainability, Data Analytics, or a related field.\n",
       "\n",
       "Experience: atleast 6 years in sustainability, data analytics, or BI development roles to be able to drive himself towards business goals independently.\n",
       "\n",
       "Key Skills:\n",
       "• Strong analytical skills, attention to detail and ability to think from first principles.\n",
       "• Excellent communication and teamwork abilities.\n",
       "• Proficiency in data analytics software and tools. Knowledge of python, SQL, Power-BI, would be added advantage.\n",
       "• Knowledge of relevant regulations and standards in sustainability.\n",
       "• Familiarity with data visualization tools and techniques.\n",
       "• Willingness to be flexible, learn new tools, techniques and deliver.\n",
       "\n",
       "Benefits\n",
       "\n",
       "We are committed to offering competitive benefits programs for all of our employees, and enhancing our programs when necessary.\n",
       "• Enjoy your best years with our retirement savings plan\n",
       "• Have peace of mind and body with our health insurance\n",
       "• Make yourself a priority with flexible schedules, parental leave and our holiday purchase scheme\n",
       "• Drive forward your career through professional development opportunities\n",
       "• Achieve your personal goals with our Employee Assistance Programme.\n",
       "\n",
       "Our commitment to you\n",
       "\n",
       "Our greatest assets are the expertise, creativity and passion of our employees. We strive to provide a great place to work that attracts, develops and retains the best talent, promotes employee engagement, fosters teamwork and ultimately drives innovation for the benefit of our customers. We strive to create an environment where you feel that you belong, with diversity and inclusion as the engine to growth and innovation. We develop and deploy best-in-class programs and practices, providing enriching career opportunities, listening to employee feedback and always challenging ourselves to do better. This is The Carrier Way.\n",
       "\n",
       "Join us and make a difference.\n",
       "\n",
       "Apply Now!\n",
       "\n",
       "Carrier is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.\n",
       "\n",
       "Job Applicant's Privacy Notice:\n",
       "\n",
       "Click on this link to read the Job Applicant's Privacy Notice</td><td>eyJqb2JfdGl0bGUiOiJTdXN0YWluYWJpbGl0eSBEYXRhIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJDYXJyaWVyIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJPMkpEMHBVcG0xc3dxYnc1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>9 days ago</td><td>Data Analyst</td></tr><tr><td>Sr. Data Analyst</td><td>iCIMS Talent Acquisition</td><td>Rai Durg, Telangana, India</td><td>Job Overview\n",
       "\n",
       "The Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.\n",
       "\n",
       "About Us\n",
       "\n",
       "When you join iCIMS, you join the team helping global companies transform business and the world through the power of talent. Our customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile. As the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent. We’re passionate about helping companies build a diverse, winning workforce and about building our home team. We're dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\n",
       "\n",
       "Responsibilities\n",
       "• Perform various data analysis functions to analyze data from a variety of sources including external labor market data and research and internal data sets from our platforms \n",
       "• Incorporate information from a variety of systems to produce comprehensive and compelling narratives for thought-leadership initiatives and customer engagements\n",
       "• Demonstrate critical thinking - identify the story in context using multiple data sets, and present results. A strong proficiency in data storytelling will be critical to success in this role.\n",
       "• Understand principles of quality data visualization and apply them in Tableau to create and maintain custom dashboards for consumption by other employees\n",
       "• Find and investigate data quality issues, root causes and recommend remedies to be implemented by the data scientists and engineers  \n",
       "• Liaise with teams around our business to understand their problems, determine how our team can help, then use our database to produce the content they need\n",
       "• Identify data mapping and enrichment requirements. Familiarity with SQL, especially the logic behind different types of data joins and writing efficient queries, will be necessary\n",
       "• Consistently ensure that business is always conducted with integrity and that behavior aligns with iCIMS policies, procedures, and core competencies\n",
       "\n",
       "Additional Job Responsibilities: \n",
       "• Produce and adapt data visualizations in response to business requests for internal and external use\n",
       "• Shows good judgement in prioritizing their own commitments and those of the larger team, while demonstrating initiative and appropriate urgency when needed\n",
       "• Mentor junior team members in best practices for analytics, data visualization, and data storytelling. Exemplify these standards and guide teammates in following them.\n",
       "• Think creatively to produce unique, actionable insights from complex datasets, which can deliver value to our business and to our customers.\n",
       "\n",
       "Qualifications\n",
       "• 5-10 years professional experience working in an analytics capacity\n",
       "• Excellent communication skills, especially with regards to data storytelling – finding insights from complex datasets and sharing those findings with key stakeholders\n",
       "• Strong data analytics and visualization skills\n",
       "• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n",
       "• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n",
       "• Advanced knowledge of Excel (Pivot tables, VLOOKUPs, IF statements)\n",
       "• Familiarity with data guardrails to ensure compliance with applicable data governance regulations and privacy laws (i.e., GDPR)\n",
       "\n",
       "Preferred\n",
       "• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n",
       "• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n",
       "\n",
       "EEO Statement\n",
       "\n",
       "iCIMS is a place where everyone belongs. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at iCIMS.\n",
       "\n",
       "We are proud to be an equal opportunity and affirmative action employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at careers@icims.com.\n",
       "\n",
       "Compensation and Benefits\n",
       "\n",
       "Competitive health and wellness benefits include medical insurance (employee and dependent family members), personal accident and group term life insurance, bonding and parental leave, lifestyle spending account reimbursements, wellness services offerings, sick and casual/emergency days, paid holidays, tuition reimbursement, retirals (PF - employer contribution) and gratuity. Benefits and eligibility may vary by location, role, and tenure. Learn more here: https://careers.icims.com/benefits</td><td>eyJqb2JfdGl0bGUiOiJTci4gRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiaUNJTVMgVGFsZW50IEFjcXVpc2l0aW9uIiwiYWRkcmVzc19jaXR5IjoiUmFpIER1cmcsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImZMUlRBQjlESVFWdFdOeGpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>21 days ago</td><td>Data Analyst</td></tr><tr><td>Master Data Analyst</td><td>C32511 Alfa Laval India Private Limited</td><td>India</td><td>Purpose of the Job: To provide direct support to the Sales and Order Handling teams by managing customer master data and administering the Order Management System. This role ensures data integrity, system readiness, and compliance with internal controls during order booking and execution processes. Key Responsibilities: Customer Master Data Management Manage the full lifecycle of customer codes, including creation, amendment, and deactivation. Conduct background checks using the Global World Check system and coordinate with the Export Control Super User for compliance validation. Ensure customer data is configured correctly in the system for seamless order booking and execution. Maintain data accuracy and cleanliness in line with Alfa Laval’s global data standards. System Administration Act as the system administrator for the Customer Order Management system. Maintain system master data and troubleshoot operational issues. Ensure the system is up-to-date and aligned with business requirements. Order Booking Compliance Perform basic compliance checks during order booking. Verify completeness and correctness of mandatory information. Ensure all required approvals are in place as per the authorization matrix. Process Activation Initiate internal processes required for order execution. Collaborate with cross-functional teams to ensure timely and accurate order processing. Key Competencies: Strong attention to detail and data accuracy Knowledge of compliance and export control processes Proficiency in ERP and order management systems Analytical and problem-solving skills Effective communication and collaboration across teams Qualifications & Experience: Bachelor’s degree in Business Administration, Information Systems, or related field 3+ years of experience in data management or order processing roles Familiarity with data governance and compliance frameworks Experience with customer master data and ERP systems (e.g., SAP) \"We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\" Every day, we get opportunities to make a positive impact – on our colleagues, partners, customers and society. Together, we’re pioneering the solutions of the future and unlocking the full potential of precious resources. Trusted to act on initiative, we challenge conventional thinking to develop world-leading technologies that inspire progress in vital areas, including energy, food, water and shipping. As we push forward, the innovative, open spirit that fuels our 140-year-old start-up culture and rapid growth also drives our personal growth. So, as we shape a more resourceful, less wasteful world, we build our careers too. Meet our dedicated people who always go the extra mile Alfa Laval Career Site Or sign up for Job Alerts to stay in touch Sign up for Job Alerts</td><td>eyJqb2JfdGl0bGUiOiJNYXN0ZXIgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiQzMyNTExIEFsZmEgTGF2YWwgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImhJMnc5MHYxS1E3QXVQTkNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>6 days ago</td><td>Data Analyst</td></tr><tr><td>Engr II-Data Engineering</td><td>Verizon</td><td>India</td><td>When you join Verizon\n",
       "\n",
       "You want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n",
       "\n",
       "What You’ll Be Doing...\n",
       "\n",
       "As a Data Engineer with ETL/ELT expertise for our growing data platform and analytics teams, you will understand and enable the required data sets from different sources. This includes both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n",
       "• Understanding the business requirements.\n",
       "• Transforming technical design.\n",
       "• Working on data ingestion, preparation and transformation.\n",
       "• Developing the scripts for data sourcing and parsing.\n",
       "• Developing data streaming applications.\n",
       "• Debugging the production failures and identifying the solution.\n",
       "• Working on ETL/ELT development.\n",
       "\n",
       "What We’re Looking For...\n",
       "\n",
       "You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems.\n",
       "\n",
       "You'll Need To Have\n",
       "• Bachelor’s degree or one or more years of relevant experience required, demonstrated through work experience and/or military experience.\n",
       "• Experience with Data Warehouse concepts and Data Management life cycle.\n",
       "\n",
       "Even better if you have one or more of the following:\n",
       "• Any related Certification on ETL/ELT developer.\n",
       "• Accuracy and attention to detail.\n",
       "• Good problem solving, analytical, and research capabilities.\n",
       "• Good verbal and written communication.\n",
       "• Experience presenting to and influencing partners.\n",
       "\n",
       "Why Verizon?\n",
       "\n",
       "Verizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.\n",
       "• We are a ‘pay for performance’ company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.\n",
       "• Your benefits are market competitive and delivered by some of the best providers.\n",
       "• You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.\n",
       "• We offer generous paid time off benefits to help you manage your work life balance and opportunities for flexible working arrangements*.\n",
       "• Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.\n",
       "• You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.\n",
       "\n",
       "Your benefits package will vary depending on the country in which you work.\n",
       "• subject to business approval\n",
       "\n",
       "If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.\n",
       "\n",
       "Where you’ll be working\n",
       "\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "Scheduled Weekly Hours\n",
       "\n",
       "40\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Verizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.</td><td>eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkpaY3lnNzJpTUJGcHpfUTJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>1 day ago</td><td>Data Engineer</td></tr><tr><td>Associate Analyst - Data Engineer</td><td>PepsiCo</td><td>Hyderabad, Telangana, India</td><td>Overview\n",
       "\n",
       "PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT.  The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics, and new product development.  PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\n",
       "\n",
       "What PepsiCo Data Management and Operations does:\n",
       "\n",
       "Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\n",
       "\n",
       "Responsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\n",
       "\n",
       "Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\n",
       "\n",
       "Increase awareness about available data and democratize access to it across the company.\n",
       "\n",
       " \n",
       "\n",
       "               As a data engineer, you will be the key technical expert building PepsiCo's data products to drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help developing very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n",
       "\n",
       "Responsibilities\n",
       "• Act as a subject matter expert across different digital projects.\n",
       "• Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\n",
       "• Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\n",
       "• Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\n",
       "• Responsible for implementing best practices around systems integration, security, performance, and data management.\n",
       "• Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\n",
       "• Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\n",
       "• Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\n",
       "• Develop and optimize procedures to “productionalize” data science models.\n",
       "• Define and manage SLA’s for data products and processes running in production.\n",
       "• Support large-scale experimentation done by data scientists.\n",
       "• Prototype new approaches and build solutions at scale.\n",
       "• Research in state-of-the-art methodologies.\n",
       "• Create documentation for learnings and knowledge transfer.\n",
       "• Create and audit reusable packages or libraries.\n",
       "\n",
       "Qualifications\n",
       "• 4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n",
       "• 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n",
       "• 3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n",
       "• 2+ years in cloud data engineering experience in Azure.\n",
       "• Fluent with Azure cloud services. Azure Certification is a plus.\n",
       "• Experience in Azure Log Analytics\n",
       "• Experience with integration of multi cloud services with on-premises technologies.\n",
       "• Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\n",
       "• Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\n",
       "• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n",
       "• Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\n",
       "• Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\n",
       "• Experience with Statistical/ML techniques is a plus.\n",
       "• Experience with building solutions in the retail or in the supply chain space is a plus.\n",
       "• Experience with version control systems like Github and deployment & CI tools.\n",
       "• Working knowledge of agile development, including DevOps and DataOps concepts.\n",
       "• B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.\n",
       "\n",
       " Skills, Abilities, Knowledge:\n",
       "• Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\n",
       "• Strong change manager. Comfortable with change, especially that which arises through company growth.\n",
       "• Ability to understand and translate business requirements into data and technical requirements.\n",
       "• High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\n",
       "• Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\n",
       "• Strong organizational and interpersonal skills; comfortable managing trade-offs.</td><td>eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUgQW5hbHlzdCAtIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJQZXBzaUNvIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJkdUI4NkhSU1JwRWNPdlBCQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>1 month ago</td><td>Data Engineer</td></tr><tr><td>Senior Big Data Engineer</td><td>Qualcomm</td><td>Hyderabad, Telangana, India</td><td>Company:\n",
       "Qualcomm India Private Limited\n",
       "\n",
       "Job Area:\n",
       "Engineering Group, Engineering Group > Software Engineering\n",
       "\n",
       "General Summary:\n",
       "\n",
       "As a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n",
       "\n",
       "Minimum Qualifications:\n",
       "• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n",
       "OR\n",
       "Master's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n",
       "OR\n",
       "PhD in Engineering, Information Systems, Computer Science, or related field.\n",
       "\n",
       "• 2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n",
       "\n",
       "General Summary:\n",
       "\n",
       "Preferred Qualifications\n",
       "• 3+ years of experience as a Data Engineer or in a similar role\n",
       "• Experience with data modeling, data warehousing, and building ETL pipelines\n",
       "• Solid working experience with Python, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n",
       "• Experience with Big Data tools, platforms and architecture with solid working experience with SQL\n",
       "• Experience working in a very large data warehousing environment, Distributed System.\n",
       "• Solid understanding on various data exchange formats and complexities\n",
       "• Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n",
       "• Strong data visualization skills\n",
       "• Basic understanding of Machine Learning; Prior experience in ML Engineering a plus\n",
       "• Ability to manage on-premises data and make it inter-operate with AWS based pipelines\n",
       "• Ability to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n",
       "\n",
       "Education\n",
       "• Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n",
       "• Preferred Qualifications: Masters in CS/ECE with a Data Science / ML Specialization\n",
       "\n",
       "Minimum Qualifications:\n",
       "• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n",
       "\n",
       "OR\n",
       "\n",
       "Master's degree in Engineering, Information Systems, Computer Science, or related field\n",
       "\n",
       "OR\n",
       "\n",
       "PhD in Engineering, Information Systems, Computer Science, or related field.\n",
       "• 3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n",
       "\n",
       "Develops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n",
       "\n",
       "Principal Duties and Responsibilities:\n",
       "• Completes assigned coding tasks to specifications on time without significant errors or bugs.\n",
       "• Adapts to changes and setbacks in order to manage pressure and meet deadlines.\n",
       "• Collaborates with others inside project team to accomplish project objectives.\n",
       "• Communicates with project lead to provide status and information about impending obstacles.\n",
       "• Quickly resolves complex software issues and bugs.\n",
       "• Gathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n",
       "• Seeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n",
       "• Participates in technical conversations with tech leads/managers.\n",
       "• Anticipates and communicates issues with project team to maintain open communication.\n",
       "• Makes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n",
       "• Prioritizes project deadlines and deliverables with minimal supervision.\n",
       "• Resolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n",
       "• Writes readable code for large features or significant bug fixes to support collaboration with other engineers.\n",
       "• Determines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n",
       "• Unit tests own code to verify the stability and functionality of a feature.\n",
       "\n",
       "Applicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n",
       "\n",
       "Qualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n",
       "\n",
       "To all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n",
       "\n",
       "If you would like more information about this role, please contact Qualcomm Careers.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQmlnIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJRdWFsY29tbSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieVRVTHdrQjB2Ujk5bmJQUUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 days ago</td><td>Data Engineer</td></tr><tr><td>Manager Data Engineer - AWS Databricks</td><td>Blend360</td><td>Hyderabad, Telangana, India</td><td>Company Description\n",
       "\n",
       "Blend is a premier AI services provider, committed to co-creating meaningful impact for its clients through the power of data science, AI, technology, and people. With a mission to fuel bold visions, Blend tackles significant challenges by seamlessly aligning human expertise with artificial intelligence. The company is dedicated to unlocking value and fostering innovation for its clients by harnessing world-class people and data-driven strategy. We believe that the power of people and AI can have a meaningful impact on your world, creating more fulfilling work and projects for our people and clients. For more information, visit www.blend360.com\n",
       "\n",
       "Job Description\n",
       "\n",
       "We are seeking a seasoned Data Engineering Manager with 8+ years of experience to lead and grow our data engineering capabilities. This role demands strong hands-on expertise in Python, SQL, Spark, and advanced proficiency in AWS and Databricks. As a technical leader, you will be responsible for architecting and optimizing scalable data solutions that enable analytics, data science, and business intelligence across the organization.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Lead the design, development, and optimization of scalable and secure data pipelines using AWS services such as Glue, S3, Lambda, EMR, and Databricks Notebooks, Jobs, and Workflows.\n",
       "• Oversee the development and maintenance of data lakes on AWS Databricks, ensuring performance and scalability.\n",
       "• Build and manage robust ETL/ELT workflows using Python and SQL, handling both structured and semi-structured data.\n",
       "• Implement distributed data processing solutions using Apache Spark/PySpark for large-scale data transformation.\n",
       "• Collaborate with cross-functional teams including data scientists, analysts, and product managers to ensure data is accurate, accessible, and well-structured.\n",
       "• Enforce best practices for data quality, governance, security, and compliance across the entire data ecosystem.\n",
       "• Monitor system performance, troubleshoot issues, and drive continuous improvements in data infrastructure.\n",
       "• Conduct code reviews, define coding standards, and promote engineering excellence across the team.\n",
       "• Mentor and guide junior data engineers, fostering a culture of technical growth and innovation.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Requirements\n",
       "• 8+ years of experience in data engineering with proven leadership in managing data projects and teams.\n",
       "• Expertise in Python, SQL, Spark (PySpark), and experience with AWS and Databricks in production environments.\n",
       "• Strong understanding of modern data architecture, distributed systems, and cloud-native solutions.\n",
       "• Excellent problem-solving, communication, and collaboration skills.\n",
       "• Prior experience mentoring team members and contributing to strategic technical decisions is highly desirable.</td><td>eyJqb2JfdGl0bGUiOiJNYW5hZ2VyIERhdGEgRW5naW5lZXIgLSBBV1MgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJsZW5kMzYwIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJxelhlVGQtWWc2ekE5U3R4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>3 days ago</td><td>Data Engineer</td></tr><tr><td>Data Engineer INTL India - EOR 6fb570f8</td><td>Insight Global</td><td>Hyderabad, Telangana, India</td><td>- In this role, you will be building data pipeline solutions by designing, adopting, and applying big data strategies and architectures. You must have knowledge of scalable system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration, data transformation, data modeling, and data cleansing.\n",
       "- The Sr. Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for business stakeholders.\n",
       "- Design and develop a logical data model for a large multi-tenant application with intent to facilitate consistency and efficiency.\n",
       "- Collaborate with analysts and internal clients to understand the goals of the overall system data architecture, and further develop on business needs/issues.\n",
       "- Test/troubleshoot problems and conduct root cause analysis.\n",
       "- Own communications with technical contacts at software vendors to escalate software issues, recommend enhancements, and find sustainable workarounds to issues.\n",
       "- Work in tandem with analysts and other stakeholders to develop and execute necessary processes and controls around the flow of data to meet data governance standards.\n",
       "- Verify accuracy of table changes and data transformation processes\n",
       "- Deliver fully tested code prior to prod-deployment when appropriate.\n",
       "- Recommend and implement enhancements that address system performance requirements, streamline processes and improve data integrity.\n",
       "- Create sound technical documentation and train peer developers on this documentation as development completes.\n",
       "- Additional duties as assigned to ensure company success.\n",
       "The compensation for this role varies depending on depth and length of experience, and can range from 27-36 LAKH.\n",
       "\n",
       "We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElOVEwgSW5kaWEgLSBFT1IgNmZiNTcwZjgiLCJjb21wYW55X25hbWUiOiJJbnNpZ2h0IEdsb2JhbCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiZUtoaVo5cWozWC1wLXlBVEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 days ago</td><td>Data Engineer</td></tr><tr><td>R&D Data Engineer</td><td>Sanofi</td><td>Hyderabad, Telangana, India</td><td>Position Title: R&D Data Engineer\n",
       "\n",
       "About the Job\n",
       "\n",
       "At Sanofi, we’re committed to providing the next-gen healthcare that patients and customers need. It’s about harnessing data insights and leveraging AI responsibly to search deeper and solve sooner than ever before. Join our R&D Data & AI Products and Platforms Team as an R&D Data Engineer\n",
       "\n",
       "and you can help make it happen.\n",
       "\n",
       "What you will be doing:\n",
       "\n",
       "Sanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives.\n",
       "\n",
       "The R&D Data & AI Products and Platforms Team is a key team within R&D Digital, focused on developing and delivering Data and AI products for R&D use cases. This team plays a critical role in pursuing broader democratization of data across R&D and providing the foundation to scale AI/ML, advanced analytics, and operational analytics capabilities.\n",
       "\n",
       "As an R&D Data Engineer, you will join this dynamic team committed to driving strategic and operational digital priorities and initiatives in R&D. You will work as a part of a Data & AI Product Delivery Pod, lead by a Product Owner, in an agile environment to deliver Data & AI Products. As a part of this team, you will be responsible for the design and development of data pipelines and workflows to ingest, curate, process, and store large volumes of complex structured and unstructured data. You will have the ability to work on multiple data products serving multiple areas of the business.\n",
       "\n",
       "Our vision for digital, data analytics and AI\n",
       "\n",
       "Join us on our journey in enabling Sanofi’s Digital Transformation through becoming an AI first organization. This means:\n",
       "• AI Factory - Versatile Teams Operating in Cross Functional Pods: Utilizing digital and data resources to develop AI products, bringing data management, AI and product development skills to products, programs and projects to create an agile, fulfilling and meaningful work environment.\n",
       "• Leading Edge Tech Stack: Experience build products that will be deployed globally on a leading-edge tech stack.\n",
       "• World Class Mentorship and Training: Working with renown leaders and academics in machine learning to further develop your skillsets.\n",
       "\n",
       "We are an innovative global healthcare company with one purpose: to chase the miracles of science to improve people’s lives. We’re also a company where you can flourish and grow your career, with countless opportunities to explore, make connections with people, and stretch the limits of what you thought was possible. Ready to get started?\n",
       "\n",
       "Main Responsibilities:\n",
       "\n",
       "Data Product Engineering:\n",
       "• Provide input into the engineering feasibility of developing specific R&D Data/AI Products\n",
       "• Provide input to Data/AI Product Owner and Scrum Master to support with planning, capacity, and resource estimates\n",
       "• Design, build, and maintain scalable and reusable ETL / ELT pipelines to ingest, transform, clean, and load data from sources into central platforms / repositories\n",
       "• Structure and provision data to support modeling and data discovery, including filtering, tagging, joining, parsing and normalizing data\n",
       "• Collaborate with Data/AI Product Owner and Scrum Master to share Progress on engineering activities and inform of any delays, issues, bugs, or risks with proposed remediation plans\n",
       "• Design, develop, and deploy APIs, data feeds, or specific features required by product design and user stories\n",
       "• Optimize data workflows to drive high performance and reliability of implemented data products\n",
       "• Oversee and support junior engineer with Data/AI Product testing requirements and execution\n",
       "\n",
       "Innovation & Team Collaboration:\n",
       "• Stay current on industry trends, emerging technologies, and best practices in data product engineering\n",
       "• Contribute to a team culture of of innovation, collaboration, and continuous learning within the product team\n",
       "\n",
       "About You:\n",
       "\n",
       "Key Functional Requirements & Qualifications:\n",
       "• Bachelor’s degree in software engineering or related field, or equivalent work experience\n",
       "• 3-5 years of experience in data product engineering, software engineering, or other related field\n",
       "• Understanding of R&D business and data environment preferred\n",
       "• Excellent communication and collaboration skills\n",
       "• Working knowledge and comfort working with Agile methodologies\n",
       "\n",
       "Key Technical Requirements & Qualifications:\n",
       "• Proficiency with data analytics and statistical software (incl. SQL, Python, Java, Excel, AWS, Snowflake, Informatica)\n",
       "• Deep understanding and proven track record of developing data pipelines and workflows\n",
       "\n",
       "Why Choose Us?\n",
       "• Bring the miracles of science to life alongside a supportive, future-focused team\n",
       "• Discover endless opportunities to grow your talent and drive your career, whether it’s through a promotion or lateral move, at home or internationally\n",
       "• Enjoy a thoughtful, well-crafted rewards package that recognizes your contribution and amplifies your impact\n",
       "• Take good care of yourself and your family, with a wide range of health and wellbeing benefits including high-quality healthcare, prevention and wellness programs\n",
       "\n",
       "Pursue Progress. Discover Extraordinary.\n",
       "\n",
       "Progress doesn’t happen without people – people from different backgrounds, in different locations, doing different roles, all united by one thing: a desire to make miracles happen. You can be one of those people. Chasing change, embracing new ideas and exploring all the opportunities we have to offer. Let’s pursue progress. And let’s discover Extraordinary together.\n",
       "\n",
       "At Sanofi, we provide equal opportunities to all regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or gender identity.\n",
       "\n",
       "Watch our ALL IN video and check out our Diversity Equity and Inclusion actions at sanofi.com!</td><td>eyJqb2JfdGl0bGUiOiJSXHUwMDI2RCBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiU2Fub2ZpIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJWUzRuREMyZ2hRajk3SVhDQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>Data Engineer</td></tr><tr><td>Data Engineer-Senior II</td><td>Federal Express Corporation AMEA</td><td>Hyderabad, Telangana, India</td><td>Responsible for supporting the development and maintenance of business intelligence and analytics systems, implementing data warehouse architecture, enabling self-service data exploration, designing data models, implementing data security strategies, optimizing production processes, providing user training and support, and overseeing project management activities as a Data Engineer to facilitate data-driven decision-making and generate insights for business stakeholders.\n",
       "\n",
       "1. Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making.\n",
       "2. Implement of business intelligence and analytics systems, ensuring alignment with business requirements.\n",
       "3. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets.\n",
       "4. Enable self-service data exploration capabilities for users to analyze and visualize data independently.\n",
       "5. Develop reporting and analysis applications to generate insights from data for business stakeholders.\n",
       "6. Design and implement data models to organize and structure data for analytical purposes.\n",
       "7. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information.\n",
       "8. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability.\n",
       "9. Assist in training and support to users on business intelligence tools and applications.\n",
       "10. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions.\n",
       "\n",
       "Education: Bachelors’ degree or equivalent in Computer Science, MIS, or similar discipline.\n",
       "Accreditation: Specific business accreditation for Business Intelligence.\n",
       "\n",
       "Experience: Relevant work experience in data engineering based on the following number of years:\n",
       "Associate: Prior experience not required\n",
       "Standard I: Two (2) years\n",
       "Standard II: Three (3) years\n",
       "Senior I: Four (4) years\n",
       "Senior II: Five (5) years\n",
       "\n",
       "Knowledge, Skills and Abilities\n",
       "• Fluency in English\n",
       "• Analytical Skills\n",
       "• Accuracy & Attention to Detail\n",
       "• Numerical Skills\n",
       "• Planning & Organizing Skills\n",
       "• Presentation Skills\n",
       "\n",
       "Preferred Qualifications:\n",
       "\n",
       "Pay Transparency:\n",
       "\n",
       "Pay:\n",
       "\n",
       "Additional Details:\n",
       "\n",
       "FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n",
       "\n",
       "All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\n",
       "Our Company\n",
       "\n",
       "FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World’s Most Admired Companies by \"Fortune\" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\n",
       "Our Philosophy\n",
       "\n",
       "The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\n",
       "Our Culture\n",
       "\n",
       "Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970’s. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today’s global marketplace.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVNlbmlvciBJSSIsImNvbXBhbnlfbmFtZSI6IkZlZGVyYWwgRXhwcmVzcyBDb3Jwb3JhdGlvbiBBTUVBIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJhZDU5RmU0Vl94QzdTVGFqQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>1 day ago</td><td>Data Engineer</td></tr><tr><td>Lead Data Engineer(Snowflake,PowerBi)</td><td>Thomson Reuters</td><td>Hyderabad, Telangana, India (+1 other)</td><td>Want to be part of the Data & Analytics organization, whose strategic goal is to create a world-class Data & Analytics company by building, embedding, and maturing a data-driven culture across Thomson Reuters.\n",
       "\n",
       "About The Role\n",
       "We are looking for a highly motivated individual with strong organizational and technical skills for the position of Lead Data Engineer/ Data Engineering Manager (Snowflake). You will play a critical role working on cutting edge of Data Engineering and analytics, leveraging predictive models, machine learning and generative AI to drive business insights and facilitating informed decision-making and help Thomson Reuters rapidly scale data-driven initiatives.\n",
       "\n",
       "Effectively communicate across various levels, including Executives, and functions within the global organization.\n",
       "Demonstrate strong leadership skills with ability to drive projects/tasks to delivering value\n",
       "Engage with stakeholders, business analysts and project team to understand the data requirements.\n",
       "Design analytical frameworks to provide insights into a business problem.\n",
       "Explore and visualize multiple data sets to understand data available and prepare data for problem solving.\n",
       "Design database models (if a data mart or operational data store is required to aggregate data for modeling).\n",
       "\n",
       "About You\n",
       "You're a fit for the Lead Data Engineer/ Data Engineering Manager (Snowflake), if your background includes:\n",
       "Qualifications: B-Tech/M-Tech/MCA or equivalent\n",
       "Experience: 7-9 years of corporate experience\n",
       "Location: Bangalore, India\n",
       "Hands-on experience in developing data models for large scale data warehouse/data Lake – Snowflake, BW\n",
       "Map the data journey from operational system sources through any transformations in transit to its delivery into enterprise repositories (Warehouse, Data Lake, Master Data, etc.)\n",
       "Enabling on the overall master and reference data strategy, including the procedures to ensure the consistency and quality of Finance reference data.\n",
       "Experience across ETL, SQL and other emerging data technologies with experience in integrations of a cloud-based analytics environment\n",
       "Build and refine end-to-end data workflows to offer actionable insights\n",
       "Fair understanding of Data Strategy, Data Governance Process\n",
       "Knowledge in BI analytics and visualization tools: Power BI, Tableau\n",
       "\n",
       "#LI-NR1\n",
       "\n",
       "What’s in it For You?\n",
       "• Hybrid Work Model: We’ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\n",
       "• Flexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\n",
       "• Career Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow’s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\n",
       "• Industry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\n",
       "• Culture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\n",
       "• Social Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\n",
       "• Making a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\n",
       "\n",
       "About Us\n",
       "\n",
       "Thomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\n",
       "\n",
       "We are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\n",
       "\n",
       "As a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\n",
       "\n",
       "We also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here.\n",
       "\n",
       "Learn more on how to protect yourself from fraudulent job postings here.\n",
       "\n",
       "More information about Thomson Reuters can be found on thomsonreuters.com.</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIoU25vd2ZsYWtlLFBvd2VyQmkpIiwiY29tcGFueV9uYW1lIjoiVGhvbXNvbiBSZXV0ZXJzIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJzSFl5TXBoOXlWbDl4R0pBQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>13 days ago</td><td>Data Engineer</td></tr><tr><td>Engr II-Data Engineering</td><td>Verizon</td><td>Hyderabad, Telangana, India (+1 other)</td><td>When you join Verizon\n",
       "\n",
       "You want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n",
       "\n",
       "What you’ll be doing…\n",
       "\n",
       "We are looking for data engineers who can work with world class team members to help drive telecom business to its full potential . We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n",
       "\n",
       "As a data engineer you will be collaborating with business product owners , coaches , industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n",
       "\n",
       "As a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n",
       "\n",
       "Understanding the business requirements and the technical design.\n",
       "\n",
       "Working on Data Ingestion, Preparation and Transformation.\n",
       "\n",
       "Developing data streaming applications.\n",
       "\n",
       "Debugging the production failures and identifying the solution.\n",
       "\n",
       "Working on ETL/ELT development.\n",
       "\n",
       "Where you'll be working:\n",
       "\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "What we’re looking for...\n",
       "\n",
       "You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n",
       "\n",
       "You’ll need to have:\n",
       "\n",
       "Bachelor’s degree or one or more years of work experience.\n",
       "\n",
       "Experience with Data Warehouse concepts and Data Management life cycle.\n",
       "\n",
       "Experience in any DBMS\n",
       "\n",
       "Experience in Shell scripting, Spark, Scala.\n",
       "\n",
       "Knowledge in GCP/BigQuery.\n",
       "\n",
       "Even better if you have:\n",
       "\n",
       "Two or more years of relevant experience.\n",
       "\n",
       "Any relevant Certification on ETL/ELT developer.\n",
       "\n",
       "Certification in GCP-Data Engineer.\n",
       "\n",
       "Accuracy and attention to detail.\n",
       "\n",
       "Good problem solving, analytical, and research capabilities.\n",
       "\n",
       "Good verbal and written communication.\n",
       "\n",
       "Experience presenting to and influence stakeholders.\n",
       "\n",
       "#AI&D\n",
       "\n",
       "Where you’ll be working\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "Scheduled Weekly Hours\n",
       "40\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Verizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.</td><td>eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJJZ05hWTZ2NWxkNUFrcWRyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>Data Engineer</td></tr><tr><td>Senior Data Engineer - Data Integration</td><td>EPAM Systems</td><td>Hyderabad, Telangana, India</td><td>EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.\n",
       "\n",
       "Our company is looking for an experienced Senior Data Engineer to join our team.\n",
       "\n",
       "As a Senior Data Engineer, you will be working on a project that focuses on data integration and ETL for cloud-based platforms. You will be responsible for designing and implementing complex data solutions, ensuring that the data is accurate, reliable, and easily accessible.\n",
       "\n",
       "RESPONSIBILITIES\n",
       "• Design and implement complex data solutions for cloud-based platforms\n",
       "• Develop ETL processes using SQL, Python, and other relevant technologies\n",
       "• Ensure that data is accurate, reliable, and easily accessible for all stakeholders\n",
       "• Collaborate with cross-functional teams to understand data integration needs and requirements\n",
       "• Develop and maintain documentation, including technical specifications, data flow diagrams, and data mappings\n",
       "• Monitor and optimize data integration processes for performance and efficiency, ensuring data accuracy and integrity\n",
       "\n",
       "REQUIREMENTS\n",
       "• Bachelor's degree in Computer Science, Electrical Engineering, or a related field\n",
       "• 5-8 years of experience in data engineering\n",
       "• Experience with cloud-native or Spark-based ETL tools such as AWS Glue, Azure Data Factory, or GCP Dataflow\n",
       "• Strong knowledge of SQL for data querying and manipulation\n",
       "• Experience with Snowflake for data warehousing\n",
       "• Experience with cloud platforms such as AWS, GCP, or Azure for data storage and processing\n",
       "• Excellent problem-solving skills and attention to detail\n",
       "• Good verbal and written communication skills in English at a B2 level\n",
       "\n",
       "NICE TO HAVE\n",
       "• Experience with ETL using Python\n",
       "\n",
       "WE OFFER\n",
       "• Opportunity to work on technical challenges that may impact across geographies\n",
       "• Vast opportunities for self-development: online university, knowledge sharing opportunities globally, learning opportunities through external certifications\n",
       "• Opportunity to share your ideas on international platforms\n",
       "• Sponsored Tech Talks & Hackathons\n",
       "• Unlimited access to LinkedIn learning solutions\n",
       "• Possibility to relocate to any EPAM office for short and long-term projects\n",
       "• Focused individual development\n",
       "• Benefit package\n",
       "• Health benefits\n",
       "• Retirement benefits\n",
       "• Paid time off\n",
       "• Flexible benefits\n",
       "• Forums to explore beyond work passion (CSR, photography, painting, sports, etc.)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAtIERhdGEgSW50ZWdyYXRpb24iLCJjb21wYW55X25hbWUiOiJFUEFNIFN5c3RlbXMiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZnZkJvSVM5OFhDSUw3NWNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>Data Engineer</td></tr><tr><td>Python Developer – Telegram Bot Integration & Excel Automation</td><td>SANGA & ASSOCIATES - EQUIDOTE</td><td>Anywhere</td><td>Job Title:\n",
       "\n",
       "Python Developer – Telegram Bot Integration & Excel Automation\n",
       "\n",
       "Job Description:\n",
       "\n",
       "We are seeking a skilled and detail-oriented Python Developer to help automate the process of sending structured trade updates from an Excel file to a Telegram channel using a Telegram bot.\n",
       "\n",
       "This is a freelance / part-time project with the potential for ongoing work based on performance.\n",
       "\n",
       "Responsibilities:\n",
       "• Read data from an Excel file that is regularly updated using Python.\n",
       "• Format and send messages to a Telegram channel or group using the Telegram Bot API.\n",
       "• Implement conditions to filter or trigger messages based on specific columns (e.g., P&L thresholds).\n",
       "• Ensure the messages are well-formatted and synchronized.\n",
       "• (Optional) Schedule the script to run at regular intervals (e.g., every 5 mins or on update).\n",
       "• Ensure error handling, avoid duplicate messages, and maintain clean logs.\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with Python scripting\n",
       "• Proficiency in using pandas for Excel/CSV handling\n",
       "• Working knowledge of the Telegram Bot API\n",
       "• Experience with HTTP requests (requests library)\n",
       "• Ability to format dynamic messages (Markdown/HTML for Telegram)\n",
       "• (Optional but a plus) Familiarity with scheduling tools like schedule, cron jobs, or apscheduler\n",
       "\n",
       "Nice to Have:\n",
       "• Understanding of stock market data or options trading (for better context)\n",
       "• Experience integrating with trading APIs or using TradingView alerts\n",
       "• Basic knowledge of Excel automation or VBA\n",
       "\n",
       "Project Details:\n",
       "• Project Type: One-time setup, with possible ongoing maintenance\n",
       "• Location: Remote (India preferred)\n",
       "• Start Date: Immediate\n",
       "\n",
       "How to Apply:\n",
       "\n",
       "Please apply with:\n",
       "• A short summary of your experience with Python + Telegram Bots\n",
       "• A link to any relevant projects or GitHub repos\n",
       "• Your expected rate and estimated time to complete the task\n",
       "\n",
       "Job Type: Freelance\n",
       "\n",
       "Benefits:\n",
       "• Health insurance\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "\n",
       "Supplemental Pay:\n",
       "• Performance bonus\n",
       "• Yearly bonus\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBUZWxlZ3JhbSBCb3QgSW50ZWdyYXRpb24gXHUwMDI2IEV4Y2VsIEF1dG9tYXRpb24iLCJjb21wYW55X25hbWUiOiJTQU5HQSBcdTAwMjYgQVNTT0NJQVRFUyAtIEVRVUlET1RFIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNBNXdGSEplRUV3c2lQVWJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>1 day ago</td><td>Python Developer</td></tr><tr><td>Python Developer - Remote</td><td>Xpress Health</td><td>Anywhere</td><td>Job Title: Python Developer\n",
       "Location: Remote\n",
       "Salary: Up to ₹12 LPA (based on experience and skillset)\n",
       "Experience: 3–6 years (preferred)\n",
       "Employment Type: Full-time\n",
       "\n",
       "About Xpress Health\n",
       "\n",
       "Xpress Health is a healthtech platform transforming clinical workforce management by connecting healthcare professionals to facilities in real time. With operations across Ireland and rapid expansion underway, we’re solving critical staffing challenges in the healthcare sector using advanced technology and automation.\n",
       "\n",
       "Role Overview\n",
       "\n",
       "We are looking for a Python Developer with solid experience in backend development and building scalable, secure systems. You will work closely with cross-functional teams to build backend services, automate operations, and power data-driven features in our real-time healthcare staffing platform. If you're passionate about solving real-world problems through code, this role is for you.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and maintain backend services and APIs using Python (preferably with frameworks like Django or Flask).\n",
       "• Build scalable systems for real-time scheduling, user management, and analytics.\n",
       "• Integrate third-party APIs and internal services securely and efficiently.\n",
       "• Work with databases (SQL and NoSQL) to design efficient schemas and queries.\n",
       "• Optimize performance and ensure system reliability under scale.\n",
       "• Collaborate with frontend, product, and QA teams to deliver complete features.\n",
       "• Write clean, maintainable, and well-documented code.\n",
       "• Participate in code reviews, system design discussions, and architecture planning.\n",
       "\n",
       "Requirements\n",
       "• 3–6 years of professional experience with Python backend development.\n",
       "• Strong knowledge of Django, Flask, or other web frameworks.\n",
       "• Proficient in working with relational databases (MySQL/PostgreSQL) and REST APIs.\n",
       "• Experience with Git, CI/CD pipelines, and working in an Agile environment.\n",
       "• Strong debugging, testing, and problem-solving skills.\n",
       "• Good communication and ability to collaborate with remote teams.\n",
       "\n",
       "Preferred Qualifications\n",
       "• Experience in healthcare, staffing, or enterprise SaaS platforms.\n",
       "• Familiarity with containerization tools like Docker and orchestration platforms (Kubernetes).\n",
       "• Exposure to cloud platforms like AWS, GCP, or Azure.\n",
       "• Knowledge of async programming and task queues (e.g., Celery, Redis).\n",
       "• Experience working with frontend teams using React/Vue (a plus).\n",
       "\n",
       "What We Offer\n",
       "• Competitive salary up to ₹12 LPA, depending on experience.\n",
       "• A mission-driven environment working on meaningful, real-world problems.\n",
       "• Opportunity to shape a rapidly scaling healthtech product.\n",
       "• Flexible work culture with remote options and learning opportunities.\n",
       "• Collaborative, cross-functional team with international exposure.\n",
       "\n",
       "Be part of a product-first team building real-time tech that supports critical healthcare systems. Join Xpress Health as a Python Developer !!!\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: Up to ₹1,200,000.00 per year\n",
       "\n",
       "Benefits:\n",
       "• Paid time off\n",
       "• Work from home\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Evening shift\n",
       "• Fixed shift\n",
       "• Monday to Friday\n",
       "• UK shift\n",
       "\n",
       "Application Question(s):\n",
       "• What is your current and expected CTC?\n",
       "• Are you currently working? If yes, what is your notice period?\n",
       "\n",
       "Experience:\n",
       "• Python : 5 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIC0gUmVtb3RlIiwiY29tcGFueV9uYW1lIjoiWHByZXNzIEhlYWx0aCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJGWjc0LW4wM2NjU2xGWVVOQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>2 days ago</td><td>Python Developer</td></tr><tr><td>Full Stack Developer (Python / React JS)</td><td>Hitachi Careers</td><td>India</td><td>Our Company\n",
       "\n",
       "We're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n",
       "\n",
       "Our group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n",
       "\n",
       "Imagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n",
       "\n",
       "The team\n",
       "\n",
       "We are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "\n",
       "The role: Full Stack Developer/Specialist\n",
       "\n",
       "Responsibilities:\n",
       "• Design, develop, and maintain applications.\n",
       "• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n",
       "• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n",
       "• Optimise applications for performance, scalability, and user experience.\n",
       "• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n",
       "• Provide training and support to end-users and IT staff on functionalities and best practices.\n",
       "• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n",
       "• Participate in project planning, execution, and post-implementation support.\n",
       "• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n",
       "• What you'll bring\n",
       "Qualifications:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n",
       "• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n",
       "• Expertise in development and customisation.\n",
       "• Proficiency in Python and React JS\n",
       "• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n",
       "• Experience with RESTful APIs and web services.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Strong communication and collaboration skills.\n",
       "• Ability to work independently and as part of a team in a fast-paced environment.\n",
       "• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n",
       "• Prior experience in building AI applications is a plus.\n",
       "• Prior experience with automation tools like UIPath is a plus.\n",
       "• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\n",
       "Preferred Skills\n",
       "• Certification in\n",
       "• Experience with other low-code/no-code platforms.\n",
       "• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n",
       "• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n",
       "• Experience with Agile development methodologies.\n",
       "About us\n",
       "\n",
       "We're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n",
       "\n",
       "#LI-MS3\n",
       "\n",
       "Championing diversity, equity, and inclusion\n",
       "\n",
       "Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n",
       "\n",
       "How we look after you\n",
       "\n",
       "We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n",
       "\n",
       "We're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.</td><td>eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>Python Developer</td></tr><tr><td>Python Developer Role</td><td>Pitangent Analytics and Technology Solutions Pvt. Ltd.</td><td>India</td><td>Overview\n",
       "\n",
       "Pi tangent Group of Companies is an ISO:9001: 2015 Certified, CMMIL-3, Award winning Software Development Company in Eastern India. It caters to areas like AI/ML to Web development to SAAS engineering. The Group is actively seeking out talented, enthusiastic Python Developers as a value addition to the growing team at Kolkata.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design and develop robust backend applications using Python.\n",
       "• Collaborate with front-end developers to integrate user-facing elements with server-side logic.\n",
       "• Implement RESTful APIs for seamless communication between server and client.\n",
       "• Write reusable, testable, and efficient code following best practices.\n",
       "• Manage and optimize multiple databases and data storage solutions.\n",
       "• Perform unit and integration testing to ensure software reliability.\n",
       "• Participate in code reviews and maintain version control in Git.\n",
       "• Gather and analyze user requirements to provide optimal solutions\n",
       "• Contribute to project documentation and specifications.\n",
       "• Collaborate with QA engineers to troubleshoot and resolve issues.\n",
       "• Maintain quality assurance processes to ensure best practices are enforced.\n",
       "• Engage in agile development practices, participating in sprints and meetings.\n",
       "• Mentor junior developers and provide guidance as needed.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor's degree in computer science or related field.\n",
       "• 1-2 yrs of experience in Python development.\n",
       "• Strong understanding of Django or Flask web frameworks.\n",
       "• Proficient with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB).\n",
       "• Experience with version control systems, preferably Git.\n",
       "• Solid understanding of RESTful API design principles.\n",
       "• Familiarity with front-end technologies (HTML, CSS, JavaScript).\n",
       "• Experience with containerization tools such as Docker.\n",
       "• Strong communication and teamwork abilities.\n",
       "• Familiarity with cloud services (AWS, Azure) is a plus.\n",
       "• Understanding of security principles and best practices.\n",
       "• Experience with Agile/Scrum methodologies.\n",
       "• Proven ability to manage multiple tasks and meet deadlines.\n",
       "\n",
       "Skills: agile,communication,nosql,scrum,aws lambda,docker,sql,azure,html,javascript,mongodb,css,django,aws,git,postgresql,restful apis,python,flask</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIFJvbGUiLCJjb21wYW55X25hbWUiOiJQaXRhbmdlbnQgQW5hbHl0aWNzIGFuZCBUZWNobm9sb2d5IFNvbHV0aW9ucyBQdnQuIEx0ZC4iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiLUNHQ2RNdUVNQ2V4X3F2MkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>2 days ago</td><td>Python Developer</td></tr><tr><td>Python and Groovy Framework Developer</td><td>Aptita</td><td>India</td><td>Urgent Hiring!!!\n",
       "\n",
       "Role : Python and Groovy Framework Developer\n",
       "\n",
       "Mandatory Skills: Python, Appium, Groovy, Git\n",
       "\n",
       "Experience: 3 to 8 Years\n",
       "\n",
       "Location: Bengaluru\n",
       "\n",
       "Contract - 1Year\n",
       "\n",
       "Job Description:\n",
       "\n",
       "Qualifications\n",
       "\n",
       " Bachelor’s or master’s degree in Computer Science, Software Engineering, or a\n",
       "\n",
       "related field\n",
       "\n",
       " 6 to 8 years of relevant experience in quality assurance, with a proven track record in\n",
       "\n",
       "WebKit or browser engine testing, including team leadership responsibilities.\n",
       "\n",
       " Proficiency in CI/CD and test automation frameworks such as Appium, and scripting\n",
       "\n",
       "languages like Python, or Shell.\n",
       "\n",
       "Job Overview\n",
       "\n",
       "We are seeking a dynamic and experienced Lead Python & Groovy Framework developer\n",
       "\n",
       "to join our team You will be part of a fast-paced, Agile development team and work on a\n",
       "\n",
       "variety of projects, from building new tools and solutions to improving existing ones.\n",
       "\n",
       "In this role, you will have the chance to grow your skills and take your career to the next\n",
       "\n",
       "level. We offer a supportive, challenging, and exciting work environment, with\n",
       "\n",
       "opportunities for professional development, training, and advancement.\n",
       "\n",
       "If you are a Python & Groovy Framework Developer Engineer with a passion for\n",
       "\n",
       "technology and a drive to continuously improve processes, we want to hear from you!\n",
       "\n",
       "If you are passionate about browser engine technologies, performance optimization, and\n",
       "\n",
       "leadership, we encourage you to apply!\n",
       "\n",
       "Primary Skills:\n",
       "\n",
       " Strong experience in Python Framework development, with the ability to automate\n",
       "\n",
       "and optimize processes using Jenkins Pipeline script\n",
       "\n",
       " Good knowledge in Groovy scripting\n",
       "\n",
       " Expertise in CI/CD tools such as Jenkins, CircleCI, or GitLab\n",
       "\n",
       " Good understanding of Appium.\n",
       "\n",
       "Strong Problem solving and debugging skills.\n",
       "\n",
       " Excellent communication and collaboration skills, both with technical and non-\n",
       "\n",
       "technical stakeholders\n",
       "\n",
       " Version Control: Familiarity with version control systems such as Git for reviewing\n",
       "\n",
       "changes and ensuring test coverage.\n",
       "\n",
       " Communication: Strong communication and collaboration skills for working with\n",
       "\n",
       "cross-functional teams.\n",
       "\n",
       " Agile Methodologies: Experience with Agile Scrum methodologies\n",
       "\n",
       "Notice Period: Immediate- 30 Days\n",
       "\n",
       "Email to : sharmila.m@aptita.com\n",
       "\n",
       "·</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gYW5kIEdyb292eSBGcmFtZXdvcmsgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQXB0aXRhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Imx1ckt5NFVRSi0wb3JkMi1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>2 days ago</td><td>Python Developer</td></tr><tr><td>AI Python Developer</td><td>Allianz Insurance</td><td>India</td><td>We are seeking a skilled AI Developer proficient in Python to support various AI use cases within our dynamic team. The ideal candidate will possess a strong understanding of Python programming and a basic familiarity with PyTest. This role will also provide opportunities to work with cloud computing technologies, particularly Azure, and develop APIs.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Utilize advanced Python skills to support and enhance AI use cases, ensuring the efficient and effective implementation of AI solutions.\n",
       "• Develop and execute comprehensive test scripts using PyTest to ensure the reliability and accuracy of AI models and applications.\n",
       "• Collaborate with cross-functional teams, including data scientists, engineers, and product managers, to integrate AI capabilities into existing systems and workflows.\n",
       "• Analyze and optimize AI algorithms for performance and scalability, ensuring they meet business requirements.\n",
       "• Stay updated with the latest advancements in AI and machine learning technologies and apply them to improve existing solutions.\n",
       "\n",
       "Requirements:\n",
       "\n",
       "Must-Have\n",
       "• Python: Advanced proficiency with extensive experience in writing efficient, maintainable, and scalable code. Demonstrated ability to solve complex problems using Python.\n",
       "• PyTest: Basic experience in writing and executing unit tests, with a fundamental understanding of test-driven development practices.\n",
       "\n",
       "Good-to-Have\n",
       "• Cloud Computing: Familiarity with cloud platforms and services, with hands-on experience in deploying and managing AI applications in a cloud environment.\n",
       "• Azure: Experience with Microsoft Azure, particularly in using its AI and machine learning services.\n",
       "• APIs: Knowledge of designing, developing, and consuming APIs, particularly RESTful APIs, for integrating AI solutions with other systems and applications\n",
       "\n",
       "About Allianz Technology\n",
       "\n",
       "Allianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 13,000 employees located in 22 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry. We oversee the full digitalization spectrum – from one of the industry’s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n",
       "\n",
       "D&I statement\n",
       "\n",
       "Allianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, race or ethnicity, age, nationality, religion, disability, or philosophy of life. Join us. Let´s care for tomorrow.</td><td>eyJqb2JfdGl0bGUiOiJBSSBQeXRob24gRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQWxsaWFueiBJbnN1cmFuY2UiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQkg5Z0tqUFM2TjBGeExOb0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>24 days ago</td><td>Python Developer</td></tr><tr><td>Developer- Angular, Python & Azure</td><td>The Value Maximizer</td><td>India</td><td>About the Role :\n",
       "\n",
       "As a Senior Developer, you will play a critical role in designing, implementing, and optimizing IoT applications, Salesforce development, and web solutions. This role is ideal for experienced developers eager to lead projects, mentor junior team members, and shape technical strategies across diverse platforms and technologies. You will be an important part of our international project teams, will work closely with team members from the headquarters and if qualified can lead international software development projects.\n",
       "\n",
       "Key Responsibilities :\n",
       "• Central role and expert for development of IoT frontend applications with Angular, Capacitor, and PrimeNG\n",
       "• Design and implement IoT backend systems using C#, .Net 8.0, and the Azure platform\n",
       "• Management and optimization of CI/CD pipelines using Azure DevOps and Bicep\n",
       "• Drive advanced Salesforce development efforts, including customizations, integrations, and automation\n",
       "• Manage and enhance the SharePoint Online intranet platform\n",
       "• Architect and implement Power Platform solutions tailored to business needs\n",
       "• Develop and maintain complex web applications using Django (Python) and PHP\n",
       "• Supervise website development and optimization with Typo3, PHP, and SolR, collaborating closely with the marketing department\n",
       "• Monitor and ensure the performance, compliance, and SEO optimization of global websites and portals\n",
       "\n",
       "Key Requirements :\n",
       "• 6-8 years of experience\n",
       "• Strong expertise in Angular, Python, and C#\n",
       "• Advanced experience with CI/CD pipelines and version control systems (e.g., Azure DevOps or Git)\n",
       "• Proficiency in web technologies such as PHP, MySQL, and modern frameworks\n",
       "• In-depth knowledge of Salesforce, Azure cloud solutions, and SharePoint Online\n",
       "• Proven ability to lead technical projects, mentor team members, and collaborate across departments\n",
       "• Excellent problem-solving skills, attention to detail, and a proactive approach to innovation\n",
       "• Proficiency in German is an advantage but not a requirement. Candidates must, however, have a minimum of B2-level English proficiency to communicate effectively within the international teams and our headquarters</td><td>eyJqb2JfdGl0bGUiOiJEZXZlbG9wZXItIEFuZ3VsYXIsIFB5dGhvbiBcdTAwMjYgQXp1cmUiLCJjb21wYW55X25hbWUiOiJUaGUgVmFsdWUgTWF4aW1pemVyIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRteXhMclAxTG5oaWFJZEVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>Python Developer</td></tr><tr><td>Freelance Python Developer</td><td>GBIM Technologies Pvt.Ltd.</td><td>Anywhere</td><td>We’re Hiring – Freelance Python Developer (Experienced)\n",
       "We are currently looking for a freelance Python developer who brings hands-on experience and can help us build and debug efficient backend systems, automate processes, and solve critical issues with speed and precision.\n",
       "Key Expertise Required:\n",
       "\n",
       "Python (Backend Development)\n",
       "\n",
       "Web Scraping & Data Extraction\n",
       "\n",
       "Web Automation\n",
       "\n",
       "Flask | Pandas | ETL\n",
       "\n",
       "AWS (Basic to Intermediate)\n",
       "\n",
       "Google / Meta / LinkedIn / Third-Party API Integration\n",
       "\n",
       "Problem-solving mindset – quick in identifying & fixing bugs/errors\n",
       "\n",
       "If you are a solution-driven individual with a solid track record in handling Python-based backend projects and automation tasks, we’d love to connect with you!\n",
       "Please DM or share your portfolio/work samples at [hr@gbim.com / sarabjeet.mann@gbim.in ].\n",
       "#PythonDeveloper #FreelanceOpportunity #WebScraping #Automation #BackendDevelopment #Flask #AWS #APIIntegration #DataExtraction #HiringNow #LinkedInJobs\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500.00 - ₹10,000.00 per hour\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "• Monday to Friday\n",
       "\n",
       "Work Location: Remote\n",
       "\n",
       "Speak with the employer\n",
       "+91-XXXXXXXXXX</td><td>eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkdCSU0gVGVjaG5vbG9naWVzIFB2dC5MdGQuIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNtMXFFOGh4Y2xUN3Z1ck9BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>3 days ago</td><td>Python Developer</td></tr><tr><td>Junior Python Developer</td><td>Dehazelabs</td><td>Anywhere</td><td>Location: Onsite, Kokapet, Hyderabad, Telangana.\n",
       "\n",
       "Job Type: Full-Time\n",
       "\n",
       "About Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n",
       "\n",
       "Responsibilities:\n",
       "• Develop and maintain Python applications and services\n",
       "• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n",
       "• Write clean, efficient, and well-documented code\n",
       "• Troubleshoot, debug, and optimize applications for performance and scalability\n",
       "• Participate in code reviews and maintain coding standards\n",
       "• Stay up-to-date with the latest industry trends and technologies\n",
       "\n",
       "Requirements:\n",
       "• Ability to code in Python and SQL\n",
       "• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n",
       "• Knowledge of version control systems (e.g., Git)\n",
       "• Excellent problem-solving skills and attention to detail\n",
       "• Strong communication and teamwork abilities.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n",
       "• Knowledge of RESTful APIs and microservices architecture.</td><td>eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Python Developer</td></tr><tr><td>DET-Senior GIG Python Developer-GDSNF02</td><td>EY</td><td>India</td><td>At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. Senior Python Developer – Client – Merck, H and M, TIAA, MINT Job Description Bachelor’s or master’s degree with 3+ years of strong Python development experience Design, develop, and maintain high-performance web applications using Python and related frameworks. Strong understanding of Python OOPs, Data type, Data Structure and algorithm , Exception handling, Decarotor , Generator, Iterator, Automation. Strong understanding of Python Libraries (Pandas, TensorFlow, Numpy, SciPy) Experience in Cloud Azure / AWS Develop, optimize, and manage complex APIs (RESTful or GraphQL). Collaborate with cross-functional teams to define, design, and ship new features. Troubleshoot and resolve advanced technical issues in development and production environments. Conduct technical evaluations of new tools and frameworks, recommending their adoption when appropriate. Stay ahead of emerging trends in Python development, ensuring the team remains at the forefront of innovation. Advanced proficiency in Python and frameworks like Django,Flask, or FastAPI. Good understanding of Database Postgres / MySQL & ORM Library i.e. SQL Alchemy/ any ORM libraries Understanding of Code Repository tools i.e. GIT , SVN Strong understanding of DevOps principles(Docker, Kubernetes and microservices) EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.</td><td>eyJqb2JfdGl0bGUiOiJERVQtU2VuaW9yIEdJRyBQeXRob24gRGV2ZWxvcGVyLUdEU05GMDIiLCJjb21wYW55X25hbWUiOiJFWSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ4Mk5pUFZHVVJ3cjczaVE1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>18 hours ago</td><td>Python Developer</td></tr><tr><td>Informatica ETL Developer: Agile Dev Team Member IV</td><td>Capgemini</td><td>Hyderabad, Telangana, India</td><td>The ideal candidate will be responsible for designing, developing, and deploying scalable ETL processes using Informatica PowerCenter to support our data warehousing and analytics initiatives. You will collaborate with business and technical stakeholders to ensure high data quality, availability, and performance. Key Responsibilities:Design, develop, and maintain ETL workflows and mappings using Informatica PowerCenter or Informatica Intelligent Cloud Services (IICS).Extract, transform, and load data from various source systems (e.g., SQL Server, Oracle, flat files, cloud APIs) into data warehouses or operational data stores.Optimize ETL performance, conduct tuning, and ensure error handling and logging.Collaborate with data architects and analysts to understand data requirements and deliver high-quality data solutions.Work with QA teams to support data validation and testing efforts.Support data integration, migration, and transformation initiatives.Document ETL processes, data flows, and job schedules.Monitor daily ETL jobs and resolve production issues in a timely manner.RequirementsBachelor’s degree in Computer Science, Information Systems, or a related field (or equivalent work experience).3+ years of experience with Informatica PowerCenter or Informatica IICS.Strong SQL skills and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).Solid understanding of data warehousing concepts and dimensional modeling.Experience in performance tuning and troubleshooting ETL processes.Hands-on experience with job scheduling tools (e.g., Autosys, Control-M, Tidal).Familiarity with version control systems and DevOps practices.Preferred Qualifications:Experience with cloud data platforms (e.g., Snowflake, AWS Redshift, Azure Synapse).Exposure to data governance and data quality tools.Knowledge of scripting languages (e.g., Shell, Python).Experience working in Agile/Scrum environments.Familiarity with BI tools (e.g., Tableau, Power BI) is a plus. BenefitsThis position comes with competitive compensation and benefits package: Competitive salary and performance-based bonuses Comprehensive benefits package Home Office model Career development and training opportunities Flexible work arrangements (remote and/or office-based) Dynamic and inclusive work culture within a globally known group Private Health Insurance Pension Plan Paid Time Off Training & Development *Note: Benefits differ based on employee level</td><td>eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyOiBBZ2lsZSBEZXYgVGVhbSBNZW1iZXIgSVYiLCJjb21wYW55X25hbWUiOiJDYXBnZW1pbmkiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InFZVlFqNTFEM2VzZ0lZSExBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 days ago</td><td>ETL Developer</td></tr><tr><td>Senior ETL and Backend Developer (Salesforce)</td><td>S&P Global</td><td>Hyderabad, Telangana, India (+1 other)</td><td>About the Role:\n",
       "\n",
       "Grade Level (for internal use):\n",
       "10\n",
       "\n",
       "Title: Senior ETL and Backend Developer (Salesforce)\n",
       "\n",
       "Job Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n",
       "\n",
       "The Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n",
       "\n",
       "The team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n",
       "\n",
       "The Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n",
       "\n",
       "Responsibilities:\n",
       "• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n",
       "• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n",
       "• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n",
       "• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n",
       "• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n",
       "• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\n",
       "Monitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n",
       "• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\n",
       "Qualifications:\n",
       "\n",
       "Basic Qualifications:\n",
       "• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n",
       "• A minimum of 8+ years of experience in software engineering & Architecture.\n",
       "• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n",
       "• A minimum of 3+ years of Salesforce development, administration/Integration.\n",
       "• Proficiency in Informatica PowerCenter and other ETL tools.\n",
       "• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n",
       "• Experience with Salesforce integration and administration.\n",
       "• Proficiency in backend development languages (e.g., Java, Python, C#).\n",
       "• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Ability to work independently and as part of a team.\n",
       "• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n",
       "• Experience with other ETL tools and data integration platforms.\n",
       "• Informatica Certified Professional\n",
       "Salesforce Certified Administrator or Developer\n",
       "• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n",
       "• Excellent problem solving, analytical and technical troubleshooting skills.\n",
       "• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n",
       "• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n",
       "\n",
       "About S&P Global Commodity Insights\n",
       "At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n",
       "\n",
       "We’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n",
       "\n",
       "S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n",
       "\n",
       "For more information, visit http://www.spglobal.com/commodity-insights.\n",
       "\n",
       "What’s In It For You?\n",
       "\n",
       "Our Purpose:\n",
       "\n",
       "Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n",
       "\n",
       "Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n",
       "\n",
       "Our People:\n",
       "\n",
       "We're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n",
       "\n",
       "From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n",
       "\n",
       "Our Values:\n",
       "\n",
       "Integrity, Discovery, Partnership\n",
       "\n",
       "At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n",
       "\n",
       "Benefits:\n",
       "\n",
       "We take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n",
       "\n",
       "Our benefits include:\n",
       "• Health & Wellness: Health care coverage designed for the mind and body.\n",
       "• Flexible Downtime: Generous time off helps keep you energized for your time on.\n",
       "• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n",
       "• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n",
       "• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n",
       "• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n",
       "\n",
       "For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n",
       "\n",
       "Global Hiring and Opportunity at S&P Global:\n",
       "\n",
       "At S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "Equal Opportunity Employer\n",
       "\n",
       "S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n",
       "\n",
       "If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n",
       "\n",
       "US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n",
       "\n",
       "Job ID: 316835\n",
       "Posted On: 2025-06-03\n",
       "Location: Hyderabad, Telangana, India</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjVGNUtpak5fRjROQXhDcERBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>19 days ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>Zensar Technologies</td><td>Madhavaram, Telangana, India</td><td>Job Description\n",
       "\n",
       "Primary Skill Set\n",
       "• ETL Informatica\n",
       "• SQL\n",
       "• Unix\n",
       "• Realtime Data Integration (CDC) using Power Exchange (Can be optional if candidate good in other skills)\n",
       "\n",
       "Good to Have\n",
       "\n",
       "Experience on working with Mainframe Databases/files\n",
       "\n",
       "ETL Batch Scheduling tools like TWS/Tidal\n",
       "\n",
       "Roles & Responsibilities\n",
       "\n",
       "Informatica PowerCenter, Unix scripting, SQL/PLSQL\n",
       "\n",
       "Knowledge of Informatica Power Exchange is preferred\n",
       "\n",
       "Experience With Mainframe Sources/targets Is Preferred\n",
       "• Bachelor’s degree in Computer Science or similar field or equivalent work experience.\n",
       "• 5-8 years of development experience with Informatica Power Center on Data Integration projects.\n",
       "• Strong analytic, problem-solving and organizational skills.\n",
       "• Excellent SQL knowledge and ability to write the complex queries keeping performance aspect in mind.\n",
       "• Experience with analysis of business requirements, designing and writing technical specifications to design.\n",
       "• Hands-on experience to process mainframe files using Informatica Power Exchange.\n",
       "• Hands-on experience with UNIX shell scripting.\n",
       "• Participate in testing and issue resolution to validate functionality and performance.\n",
       "• Hands-on experience on any job scheduling tool, TWS is preferred.\n",
       "• Good written and verbal communication skills.\n",
       "\n",
       "Location\n",
       "\n",
       "1 st Preference: Noida\n",
       "\n",
       "2 nd Preference: Hyderabad\n",
       "\n",
       "3 rd Preference: Gurgaon</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiWmVuc2FyIFRlY2hub2xvZ2llcyIsImFkZHJlc3NfY2l0eSI6Ik1hZGhhdmFyYW0sIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InJTQzlGVjVoRHhXZ1h3QURBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>11 days ago</td><td>ETL Developer</td></tr><tr><td>Insight Global</td><td>Insight Global</td><td>Hyderabad, Telangana, India</td><td>We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\n",
       "Responsibilities:\n",
       "Lead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\n",
       "Collaborate with developers to ensure a smooth transition and integration of the upgraded system.\n",
       "Enhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\n",
       "Provide support and guidance to the ETL development lead.\n",
       "Engage with end-users to ensure their needs are met during and after the upgrade process.\n",
       "Utilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\n",
       "Requirements:\n",
       "Minimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\n",
       "Proven experience with SQL Server upgrades, particularly from 2012 to 2022.\n",
       "Strong SQL Server skills, including building tables, stored procedures, views, and functions.\n",
       "Experience with .NET development is highly desirable to be leveraged for other projects\n",
       "Excellent problem-solving skills and attention to detail.\n",
       "Ability to work effectively in a team environment and communicate with end-users.\n",
       "Preferred Qualifications:\n",
       "Experience with SQL Server 2016.\n",
       "Experience with Oracle\n",
       "Familiarity with the latest features and enhancements in SQL Server 2022.\n",
       "Strong understanding of database architecture and migration strategies.\n",
       "\n",
       "We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>eyJqb2JfdGl0bGUiOiJJbnNpZ2h0IEdsb2JhbCIsImNvbXBhbnlfbmFtZSI6Ikluc2lnaHQgR2xvYmFsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location</td><td>Fiserv</td><td>India</td><td>Calling all innovators – find your future at Fiserv.\n",
       "\n",
       "We’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n",
       "\n",
       "Job Title\n",
       "\n",
       "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n",
       "\n",
       "Work Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n",
       "• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n",
       "• Participates in client kickoff activities including requirements and data gathering sessions.\n",
       "• Ability to analyze and migrate incoming external data into Fiserv solutions.\n",
       "• Strong working knowledge of ETL processes and best practices.\n",
       "• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n",
       "• Documents business processes and identifies opportunities for process redesign.\n",
       "• Reports system defects and identifies opportunities for system enhancements.\n",
       "• Performs as a technical consulting resource for new clients during the implementation process.\n",
       "• Supports several project managers during the pre and post go-live activities\n",
       "• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n",
       "• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n",
       "• Provides technical and analytical guidance to the project team.\n",
       "• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n",
       "• The position requires working in shifts, 2 PM to 11 PM IST\n",
       "\n",
       "What you will need to have:\n",
       "• Minimum of 5 – 10 years of experience in software development.\n",
       "• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n",
       "• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n",
       "• Self-starter who can work effectively, both independently and in a team environment.\n",
       "• Strong analytical, organizational, and problem-solving skills.\n",
       "• Excellent verbal and written communication skills.\n",
       "• Ability to maintain a professional attitude and demeanor in high pressure situations.\n",
       "• Ability to multi-task and manage multiple projects simultaneously.\n",
       "• Weekend/evening availability and support (10% - 25%).\n",
       "\n",
       "What would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n",
       "\n",
       "Thank you for considering employment with Fiserv. Please:\n",
       "• Apply using your legal name\n",
       "• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n",
       "\n",
       "What you should know about us:\n",
       "\n",
       "Fiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n",
       "\n",
       "Our commitment to Diversity and Inclusion:\n",
       "\n",
       "We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n",
       "\n",
       "Warning about fake job posts:\n",
       "\n",
       "Please be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n",
       "\n",
       "Any communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n",
       "\n",
       "If you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>EPAM - ETL Developer - SSIS/SSRS</td><td>Swathi V</td><td>Hyderabad, Telangana, India</td><td>Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n",
       "\n",
       "Experience : 5+ Years\n",
       "\n",
       "Location : Hyderabad\n",
       "\n",
       "Job Description :\n",
       "\n",
       "We are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n",
       "\n",
       "You will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n",
       "\n",
       "Key Responsibilities :\n",
       "\n",
       "- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n",
       "\n",
       "- Develop, deploy, and maintain SSIS packages for ETL processes.\n",
       "\n",
       "- Work on data warehouse design, development, and maintenance.\n",
       "\n",
       "- Ensure high performance and reliability of data integration workflows.\n",
       "\n",
       "- Troubleshoot SQL queries and ETL issues; optimize database performance.\n",
       "\n",
       "- Collaborate with cross-functional teams to define data solutions.\n",
       "\n",
       "- Work with complex data structures including JSON and XML.\n",
       "\n",
       "- Understand and implement different data models (star, snowflake, etc.).</td><td>eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IlN3YXRoaSBWIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ2ekQ3VTVqbHNoc2EtN3luQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>Data ETL Developer / BI Engineer</td><td>American Express Global Business Travel</td><td>India</td><td>ETL Developer\n",
       "\n",
       "Amex GBT is a place where colleagues find inspiration in travel as a force for good and – through their work – can make an impact on our industry. We’re here to help our colleagues achieve success and offer an inclusive and collaborative culture where your voice is valued.\n",
       "\n",
       "We are looking for an experienced Data ETL Developer / BI Engineer who loves solving complex problems across a full spectrum of data & technologies. You will lead the building effort of GBT's new BI platform and manage the legacy platform to seamlessly support our business function around data and analytics. You will create dashboards, databases, and other platforms that allow for the efficient collection and evaluation of BI data.\n",
       "\n",
       "What You’ll Do on a Typical Day:\n",
       "• Design, implement, and maintain systems that collect and analyze business intelligence data.\n",
       "• Design and architect an analytical data store or cluster for the enterprise and implement data pipelines that extract, transform, and load data into an information product that helps the organization reach strategic goals.\n",
       "• Create physical and logical data models to store and share data that can be easily consumed for different BI needs.\n",
       "• Develop Tableau dashboards and features.\n",
       "• Create scalable and high-performance data load and management process to make data available near real-time to support on-demand analytics and insights.\n",
       "• Translate complex technical and functional requirements into detailed designs.\n",
       "• Investigate and analyze alternative solutions to data storing, processing, etc., to ensure the most streamlined approaches are implemented.\n",
       "• Serve as a mentor to junior staff by conducting technical training sessions and reviewing project outputs\n",
       "• Design & develop, and maintain a data model implementing ETL processes.\n",
       "• Manage and maintain the database, warehouse, & cluster with other dependent infrastructure.\n",
       "• Work closely with data, products, and another team to implement data analytic solutions.\n",
       "• Support production application and Incident management.\n",
       "• Help define data governance policies and support data versioning processes\n",
       "• Maintain security and data privacy by working closely with the Data Protection Officer internally.\n",
       "• Analyze a vast number of data stores and uncover insights\n",
       "\n",
       "What We’re Looking For:\n",
       "• Degree in computer sciences or engineering\n",
       "• Overall, 3-5 years of experience in data & data warehouse, ETL, and data modeling.\n",
       "• 2+ years of experience working and managing large data stores, complex data pipelines, and BI solutions.\n",
       "• Strong experience in SQL and writing complex queries.\n",
       "• Hands-on experience with Tableau development.\n",
       "• Hands-on working experience on Redshift, data modeling, data warehouse, ETL tool, Python, and Shell scripting.\n",
       "• Understanding of data warehousing and data modeling techniques\n",
       "• Strong data engineering skills on the AWS Cloud Platform are essential.\n",
       "• Knowledge of Linux, SQL, and any scripting language\n",
       "• Good interpersonal skills and a positive attitude\n",
       "• Experience in travel data would be a plus.\n",
       "\n",
       "Location\n",
       "Gurgaon, India\n",
       "\n",
       "The #TeamGBT Experience\n",
       "\n",
       "Work and life: Find your happy medium at Amex GBT.\n",
       "• Flexible benefits are tailored to each country and start the day you do. These include health and welfare insurance plans, retirement programs, parental leave, adoption assistance, and wellbeing resources to support you and your immediate family.\n",
       "• Travel perks: get a choice of deals each week from major travel providers on everything from flights to hotels to cruises and car rentals.\n",
       "• Develop the skills you want when the time is right for you, with access to over 20,000 courses on our learning platform, leadership courses, and new job openings available to internal candidates first.\n",
       "• We strive to champion Inclusion in every aspect of our business at Amex GBT. You can connect with colleagues through our global INclusion Groups, centered around common identities or initiatives, to discuss challenges, obstacles, achievements, and drive company awareness and action.\n",
       "• And much more!\n",
       "\n",
       "All applicants will receive equal consideration for employment without regard to age, sex, gender (and characteristics related to sex and gender), pregnancy (and related medical conditions), race, color, citizenship, religion, disability, or any other class or characteristic protected by law.\n",
       "\n",
       "Click Here for Additional Disclosures in Accordance with the LA County Fair Chance Ordinance.\n",
       "\n",
       "Furthermore, we are committed to providing reasonable accommodation to qualified individuals with disabilities. Please let your recruiter know if you need an accommodation at any point during the hiring process. For details regarding how we protect your data, please consult the Amex GBT Recruitment Privacy Statement.\n",
       "\n",
       "What if I don’t meet every requirement? If you’re passionate about our mission and believe you’d be a phenomenal addition to our team, don’t worry about “checking every box;\" please apply anyway. You may be exactly the person we’re looking for!\n",
       "Experience Level\n",
       "Mid Level\n",
       "\n",
       "More about this Data ETL Developer / BI Engineer job\n",
       "\n",
       "American Express Global Business Travel is aggressively hiring for the job profile of Data ETL Developer / BI Engineer at undefined in null locality. Kindly go through the FAQs below to get all answers related to the given job.\n",
       "\n",
       "1. How much salary can I expect as a Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n",
       "\n",
       "Ans. You can expect a minimum salary of 0 INR. The salary offered will depend on your skills, experience and performance in the interview.\n",
       "\n",
       "2. What is the eligibility criteria to apply for Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n",
       "\n",
       "Ans. The candidate should have completed undefined degree and people who have 3 to 5 years are eligible to apply for this job. You can apply for more jobs in undefined to get hired quickly.\n",
       "\n",
       "3. Is there any specific skill required for this job?\n",
       "\n",
       "Ans. The candidate should have undefined skills and sound communication skills for this job.\n",
       "\n",
       "4. Who can apply for this job?\n",
       "\n",
       "Ans. Both Male and Female candidates can apply for this job.\n",
       "\n",
       "5. Is it a work from home job?\n",
       "\n",
       "Ans. No, it’s not a work from home job and can’t be done online. You can explore and apply for other work from home jobs in undefined at apna.\n",
       "\n",
       "6. Are there any charges or deposits required while applying for the role or while joining?\n",
       "\n",
       "Ans. No work-related deposit needs to be made during your employment with the company.\n",
       "\n",
       "7. How can I apply for this job?\n",
       "\n",
       "Ans. Go to the apna app and apply for this job. Click on the apply button and call HR directly to schedule your interview.\n",
       "\n",
       "8. What is the last date to apply?\n",
       "\n",
       "Ans. The last date to apply for this job is .\n",
       "\n",
       "For more details, download apna app and find Full Time jobs in undefined. Through apna, you can find jobs in 74 cities across India. Join NOW!</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVUTCBEZXZlbG9wZXIgLyBCSSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkFtZXJpY2FuIEV4cHJlc3MgR2xvYmFsIEJ1c2luZXNzIFRyYXZlbCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJnY3VjckFjcm5KVmRmdjRxQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>6 days ago</td><td>ETL Developer</td></tr><tr><td>Informatica ETL Developer - SQL/Power Center</td><td>Renovision Automation Services Pvt.Ltd.</td><td>Telangana, India</td><td>Experience - 6+ yearsWork Mode - HybridJob Summary : We are seeking a skilled Informatica ETL Developer with 5+ years of experience in ETL and Business Intelligence projects. The ideal candidate will have a strong background in Informatica PowerCenter, a solid understanding of data warehousing concepts, and hands-on experience in SQL, performance tuning, and production support. This role involves designing and maintaining robust ETL pipelines to support digital transformation initiatives for clients in manufacturing, automotive, transportation, and engineering domains.Key Responsibilities : - Design, develop, and maintain ETL workflows using Informatica PowerCenter.- Troubleshoot and optimize ETL jobs for performance and reliability.- Analyze complex data sets and write advanced SQL queries for data validation and transformation.- Collaborate with data architects and business analysts to implement data warehousing solutions.- Apply SDLC methodologies throughout the ETL development lifecycle.- Support production environments by identifying and resolving data and performance issues.- Work with Unix shell scripting for job automation and scheduling.Required Skills : - 35 years of hands-on experience with Informatica PowerCenter.- Proficiency in SQL and familiarity with NoSQL platforms.- Experience in ETL performance tuning and troubleshooting.- Solid understanding of Unix/Linux environments and scripting.- Excellent verbal and written communication skills.Preferred Qualifications : - AWS Certification or experience with cloud-based data integration is a plus.- Exposure to data modeling and data governance practices. (ref: hirist.tech)</td><td>eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyIC0gU1FML1Bvd2VyIENlbnRlciIsImNvbXBhbnlfbmFtZSI6IlJlbm92aXNpb24gQXV0b21hdGlvbiBTZXJ2aWNlcyBQdnQuTHRkLiIsImFkZHJlc3NfY2l0eSI6IlRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlVqTnczajVrcnJpS21VQjdBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer- Hyderabad (2-3+ Years of Experience)</td><td>A Client of Analytics Vidhya</td><td>Hyderabad, Telangana, India</td><td>Role Summary:\n",
       "\n",
       "•ETL Developer is responsible for Design and Development of ETL Jobs which follow standards, best practices and are maintainable, modular and reusable.\n",
       "\n",
       "•ETL Developer will analyze and review complex object and data models and the metadata repository in order to structure the processes and data for better management and efficient access.\n",
       "\n",
       "•Working on multiple projects, and delegating work to Junior Analysts to deliver projects on time.\n",
       "\n",
       "•Training and mentoring Junior Analysts and building their proficiency in the ETL process.Job Duties:\n",
       "\n",
       "•Preparing mapping document to extract, transform, and load data ensuring compatibility with all tables and requirement specifications.\n",
       "\n",
       "•Experience in ETL system design and development with Talend / Pentaho PDI is essential.\n",
       "\n",
       "•Create quality rules in Talend.\n",
       "\n",
       "•Tune Talend jobs for performance optimization.\n",
       "\n",
       "•Write relational and multidimensional database queries.\n",
       "\n",
       "•Functional Knowledge of Talend Adminstration Center, Job Servers & Load balancing setup, and all its administrative fucntions.\n",
       "\n",
       "•Develop, maintain, and enhance unit test suites to verify the accuracy of ETL processes, dimensional data, OLAP cubes and various forms of BI content including reports, dashboards, and analytical models.\n",
       "\n",
       "•Exposure in Map Reduce components of Talend / Pentaho PDI.\n",
       "\n",
       "•Creating and deploying Talend / Pentaho custom components is an add-on advantage.\n",
       "\n",
       "Job Specification:\n",
       "\n",
       "•BE, B.Tech / MS Degree in Computer Science, Engineering or a related subject.\n",
       "\n",
       "•Having an experience of 2 – 3+ years.\n",
       "\n",
       "•Comprehensive understanding and working knowledge in Data Warehouse loading, tuning, and maintenance.\n",
       "\n",
       "•Proficiency with Talend or related tools like Pentaho Data Integration / Kettle.\n",
       "\n",
       "•Working knowledge of relational database theory and dimensional database models.\n",
       "\n",
       "•Ability to write complex SQL database queries.\n",
       "\n",
       "•Ability to work independently.</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyLSBIeWRlcmFiYWQgKDItMysgWWVhcnMgb2YgRXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJBIENsaWVudCBvZiBBbmFseXRpY3MgVmlkaHlhIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJYOGlLR3lLZ0p6MDQ5UkdIQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>Luxoft</td><td>Maharashtra, India</td><td>Project Description:\n",
       "\n",
       "Our client is an EU subsidiary of a Global Financial Bank working in multiple markets and asset classes.\n",
       "\n",
       "DWH/ETL developer will work closely with the Development Lead to design, build interfaces and integrate data from a variety from internal and external data sources into the new Enterprise Data Warehouse environment. The ETL Developer will be responsible for developing ETL primarily utilizing Microsoft & Azure technologies within industry recognized ETL standards, architecture, and best practices.\n",
       "\n",
       "Responsibilities:\n",
       "• Act as a technical expert in the designing, coding, unit testing, supporting, and debugging of data warehouse software components in all aspects of SDLC\n",
       "• Apply cloud and ETL engineering skills to solve problems and design approaches\n",
       "• Troubleshoot and debug ETL pipelines and creating unit tests for ETL pipelines.\n",
       "• Assess query performance and actively contribute to optimizing the code\n",
       "• Write technical documentation and specifications\n",
       "• Support internal audit by submitting required evidence\n",
       "• Create reports and dashboards in the BI portal\n",
       "• Work with Development Lead, DWH Architect and QA Engineers to plan, implement and deliver best ETL strategies\n",
       "• Work with business analysts to understand requirements to create technical design specifications, gaining a sound understanding of business processes for related applications so that integration processes fulfill the end-user requirements\n",
       "• Communicate effectively in a collaborative, complex and high performing team environment as per Agile principles\n",
       "\n",
       "Mandatory Skills Description:\n",
       "• Proven work experience as an ETL Developer\n",
       "• Advanced knowledge of relational databases and dimensional Data Warehouse modelling concepts\n",
       "• Good understanding of physical and logical data modeling\n",
       "• Very good understanding of modern SaaS/PaaS data solutions in a cost conscious approach\n",
       "• Expert level of knowledge of Microsoft Data stack\n",
       "• Experience in developing and deploying data oriented solutions in Cloud (Azure/Synapse Analytics/Fabric)\n",
       "• Experience in designing and implementing data transformation and ETL layers using Data Factory, Notebooks\n",
       "• Experience with PowerBI for report & dashboard creation. PowerQuery and/or DAX is an advantage.\n",
       "• Experience in/understanding of Azure Data Lake Storage\n",
       "• Knowledge/use of CI/CD tools and principles, preferably Azure DevOps or Bamboo\n",
       "• Strong SQL knowledge, able to create complex SQL queries and good understanding of stored procedures, views, indexes, functions, etc.\n",
       "• Good working knowledge of at least one Scripting language. Python is an advantage.\n",
       "• Experience with GIT repositories and working with branches. GitHub, Azure DevOps or Bitbucket experience are preferable.\n",
       "• Ability to troubleshoot and solve complex technical problems\n",
       "• Good understanding of software development best practices\n",
       "• Working experience in Agile projects; preferably using JIRA\n",
       "• Experience in working in high priority projects preferably greenfield project experience\n",
       "• Able to communicate complex information clearly and concisely.\n",
       "• Able to work independently and also to collaborate across the organization\n",
       "• Highly developed problem-solving skills with minimal supervision\n",
       "• Understanding of data governance and enterprise concepts preferably in banking environment\n",
       "• Verbal and written communication skills in English are essential.\n",
       "\n",
       "Nice-to-Have Skills Description:\n",
       "• Microsoft Fabric\n",
       "• Snowflake\n",
       "• Background in SSIS/SSAS/SSRS\n",
       "• Azure DevTest Labs, ARM templates\n",
       "• Azure PurView\n",
       "• Banking/finance experience</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiTHV4b2Z0IiwiYWRkcmVzc19jaXR5IjoiTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJlZF9YUGZ2UVNVcTZXdXltQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>ETL Developer</td></tr><tr><td>Data Engineer (Hadoop, Spark, Scala, Hive)</td><td>Visa</td><td>India</td><td>Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n",
       "\n",
       "Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n",
       "\n",
       "Job Description\n",
       "\n",
       "Translate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\n",
       "\n",
       "Good to have GenAI Exposure and Agentic AI Knowledge.\n",
       "\n",
       "Work with business partners directly to seek clarity on requirements.\n",
       "\n",
       "Define solutions in terms of components, modules, and algorithms.\n",
       "\n",
       "Design, develop, document, and implement new programs and subprograms, as well as enhancements, modifications and corrections to existing software.\n",
       "\n",
       "Create technical documentation and procedures for installation and maintenance.\n",
       "\n",
       "Write Unit Tests covering known use cases using appropriate tools.\n",
       "\n",
       "Integrate test frameworks in the development process.\n",
       "\n",
       "Work with operations to get the solutions deployed.\n",
       "\n",
       "Take ownership of production deployment of code.\n",
       "\n",
       "Come up with Coding and Design best practices.\n",
       "\n",
       "Thrive in a self-motivated, internal-innovation driven environment.\n",
       "\n",
       "Adapt quickly to new application knowledge and changes.\n",
       "\n",
       "This is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Basic Qualifications\n",
       "\n",
       "-Minimum of 6 months of work experience or a Bachelor's Degree\n",
       "\n",
       "Preferred Qualifications\n",
       "\n",
       "-Bachelor degree in Computer Science.\n",
       "\n",
       "-Minimum of 1 plus years of software development experience in Hadoop using\n",
       "\n",
       "Spark, Scala, Hive.\n",
       "\n",
       "-Expertise in Object Oriented Programming Language Java, Python.\n",
       "\n",
       "-Experience using CI CD Process, version control and bug tracking tools.\n",
       "\n",
       "-Result-oriented with strong analytical and problem-solving skills.\n",
       "\n",
       "-Experience with automation of job execution, validation and comparison of data\n",
       "\n",
       "files on Hadoop Environment at the field level.\n",
       "\n",
       "-Experience in leading a small team and being a team player.\n",
       "\n",
       "-Strong communication skills with proven ability to present complex ideas and\n",
       "\n",
       "document them in a clear and concise way.\n",
       "\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChIYWRvb3AsIFNwYXJrLCBTY2FsYSwgSGl2ZSkiLCJjb21wYW55X25hbWUiOiJWaXNhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImVULTZKTlc2NHVzTUxhdS1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 days ago</td><td>Spark Engineer</td></tr><tr><td>Data Engineer - Spark/Python</td><td>Etelligens Technologies</td><td>India</td><td>Job Description\n",
       "\n",
       "We are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n",
       "• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n",
       "• Perform data cleansing and migration from diverse sources to target systems.\n",
       "• Collaborate with stakeholders to understand requirements and convert them into technical\n",
       "\n",
       "solutions.\n",
       "• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n",
       "• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n",
       "• Support, troubleshoot, and maintain data pipelines and workflows.\n",
       "• Participate in all phases of software development lifecycle including unit testing, integration\n",
       "\n",
       "testing, and performance testing.\n",
       "• Contribute to the modernization of data platforms and analytics tools on Azure.\n",
       "• Ensure data quality and integrity across all pipelines and systems.\n",
       "\n",
       "Required Skills & Qualifications\n",
       "• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n",
       "• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n",
       "• 1+ year of experience in Python and Spark scripting.\n",
       "• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n",
       "• Good understanding of Data Warehousing (DWH) concepts.\n",
       "• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n",
       "• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n",
       "• Hands-on experience in data cleansing, transformation, and migration projects.\n",
       "• Ability to work independently and within a team environment.\n",
       "• Microsoft Certified : Azure Data Engineer Associate\n",
       "• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n",
       "\n",
       "Location : Artha SEZ, Greater Noida West\n",
       "\n",
       "(ref:hirist.tech)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Spark Engineer</td><td>Staffingine LLC</td><td>India</td><td>- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>1 day ago</td><td>Spark Engineer</td></tr><tr><td>Staff Data Engineer (Spark, Python, Hadoop)</td><td>Visa</td><td>India</td><td>Company Description\n",
       "\n",
       "Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n",
       "\n",
       "When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n",
       "\n",
       "Join Visa: A Network Working for Everyone.\n",
       "Job Description\n",
       "\n",
       "The Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n",
       "\n",
       "Are you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n",
       "\n",
       "As a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n",
       "\n",
       "Essential Functions\n",
       "• Work with manager and clients to fully understand business requirements and desired business outcomes\n",
       "• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n",
       "• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n",
       "• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n",
       "• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n",
       "• Execute data engineering projects ranging from small to large either individually or as part of a project team\n",
       "• Ensure project delivery within timelines and budget requirements\n",
       "• Provide coaching and mentoring to junior team members\n",
       "\n",
       "This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\n",
       "Qualifications\n",
       "\n",
       "• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n",
       "• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n",
       "• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n",
       "• Minimum of 4 years of hands-on expertise with Java or Scala\n",
       "• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n",
       "• Experience working in an Agile and Test Driven Development environment.\n",
       "• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n",
       "• Experience with SAS as a statistical package is preferred\n",
       "• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Databricks Engineer - Spark / PySpark</td><td>Enkefalos Technologies LLP</td><td>Anywhere</td><td>Databricks Engineer – Spark / PySpark\n",
       "\n",
       "Location : Remote / Mysore\n",
       "\n",
       "Joining : Immediate\n",
       "\n",
       "Experience : 5+ years\n",
       "\n",
       "Responsibilities :\n",
       "\n",
       "Will implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n",
       "\n",
       "Requirements:\n",
       "• PySpark (RDDs, DataFrames, performance tuning)\n",
       "• Building gold‐layer data models for financial reporting\n",
       "• Experience with complex joins, aggregations, GL hierarchies\n",
       "• Version handling (Actuals vs Budget), currency conversions\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500,395.35 - ₹1,840,348.25 per year\n",
       "\n",
       "Benefits:\n",
       "• Flexible schedule\n",
       "• Paid sick time\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Application Question(s):\n",
       "• Have you worked on ADF/ADLS ?\n",
       "• Do you have hands-on experience of Spark / PySpark\n",
       "\n",
       "Experience:\n",
       "• Databricks Engineering: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 days ago</td><td>Spark Engineer</td></tr><tr><td>Pi Square Technologies - Spark & Scala Engineer</td><td>sandeep raja</td><td>India</td><td>Job Summary :\n",
       "\n",
       "We are seeking a skilled and motivated Spark & Scala Engineer with 46 years of hands-on experience in building scalable data processing applications. The ideal candidate will have a strong background in big data ecosystems and a solid understanding of distributed computing using Apache Spark with Scala.\n",
       "\n",
       "Roles and Responsibilities :\n",
       "\n",
       "- Design, build, and maintain efficient, reusable, and reliable Apache Spark applications.\n",
       "\n",
       "- Optimize Spark applications for maximum speed and scalability.\n",
       "\n",
       "- Implement data ingestion and ETL processes.\n",
       "\n",
       "- Collaborate with data scientists and architects to implement complex big data solutions.\n",
       "\n",
       "- Debug and resolve issues in Spark applications.\n",
       "\n",
       "- Stay up to date with the latest trends in big data technologies and Apache Spark.\n",
       "\n",
       "- Write clean, readable, and maintainable code.\n",
       "\n",
       "- Participate in code reviews and contribute to team knowledge sharing.\n",
       "\n",
       "Required Skills :\n",
       "\n",
       "- 46 years of experience working with Apache Spark (core, SQL, streaming).\n",
       "\n",
       "- Strong proficiency in Scala programming.\n",
       "\n",
       "- Experience in building and optimizing data pipelines and ETL workflows.\n",
       "\n",
       "- Proficient in working with big data tools and frameworks (e.g., Hive, HDFS, Kafka).</td><td>eyJqb2JfdGl0bGUiOiJQaSBTcXVhcmUgVGVjaG5vbG9naWVzIC0gU3BhcmsgXHUwMDI2IFNjYWxhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoic2FuZGVlcCByYWphIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlFMNDBNZDljZDlqQkxBX3BBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>27 days ago</td><td>Spark Engineer</td></tr><tr><td>Spark Developer</td><td>Infosys</td><td>India</td><td>• Primary skills:Technology->Big Data - Data Processing->Spark\n",
       "\n",
       "A day in the life of an Infoscion\n",
       "• As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\n",
       "• You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\n",
       "• You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\n",
       "• You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\n",
       "• You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n",
       "• Knowledge of more than one technology\n",
       "• Basics of Architecture and Design fundamentals\n",
       "• Knowledge of Testing tools\n",
       "• Knowledge of agile methodologies\n",
       "• Understanding of Project life cycle activities on development and maintenance projects\n",
       "• Understanding of one or more Estimation methodologies, Knowledge of Quality processes\n",
       "• Basics of business domain to understand the business requirements\n",
       "• Analytical abilities, Strong Technical Skills, Good communication skills\n",
       "• Good understanding of the technology and domain\n",
       "• Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\n",
       "• Awareness of latest technologies and trends\n",
       "• Excellent problem solving, analytical and debugging skills</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBEZXZlbG9wZXIiLCJjb21wYW55X25hbWUiOiJJbmZvc3lzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkZ2VmRvVC1NQnFvWHRRUmJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>16 days ago</td><td>Spark Engineer</td></tr><tr><td>Data Engineer (Snowflake, Spark, AWS) - AVP</td><td>12542 Citicorp Services India Private Limited</td><td>India</td><td>The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>Spark Engineer</td></tr><tr><td>SW Engineer (Java and Bigdata/Hadoop/Spark) 1yr</td><td>VISA</td><td>India</td><td>Job Description\n",
       "\n",
       "This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. They will be an integral part of the Payment Products Development team, focusing on the development of software solutions that leverage data to address business challenges. They will be extensively involved in hands-on activities including proof of concepts, development, and testing. They should demonstrate adaptability and readiness to change tasks according to the team's requirements.\n",
       "\n",
       "Key Responsibilities\n",
       "• Deliver high-quality products and features, taking a holistic view from the initial idea to final delivery. This involves design, development, testing, and accountability for release deployments as well as providing support post-deployment.\n",
       "• Understand the product in its entirety and ensure the timely delivery of your assigned module within an agile team setting.\n",
       "• Accurately scope efforts, identify risks, and clearly communicate trade-offs with team members and other stakeholders.\n",
       "• Coordinate and participate in Continuous Integration activities, testing automation frameworks, and other related tasks, in addition to contributing to core product code.\n",
       "• Effectively communicate status updates, issues, and potential risks accurately and promptly.\n",
       "• Perform other tasks related to data governance and system infrastructure as required.\n",
       "\n",
       "This is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Basic Qualifications\n",
       "\n",
       "-Bachelor's degree in Computer Science or equivalent field\n",
       "\n",
       "-Relevant working experience of up to 2 years in the industry\n",
       "\n",
       "-Proven experience in software development, particularly in data-centric\n",
       "\n",
       "projects, demonstrating adherence to standard development best practices\n",
       "\n",
       "-Strong understanding and practical experience with data structures and\n",
       "\n",
       "algorithms, with a passion for tackling complex problems\n",
       "\n",
       "-Proficiency in Java programming\n",
       "\n",
       "-Hands-on experience with Big Data technologies such as Hadoop, Spark, and\n",
       "\n",
       "Hive\n",
       "\n",
       "-Strong knowledge of Unix/Linux operating systems and Shell Scripting\n",
       "\n",
       "-Proficiency in working with RDBMS and SQL\n",
       "\n",
       "-Basic knowledge of manual and automated testing\n",
       "\n",
       "-Familiarity with version control systems, specifically Git\n",
       "\n",
       "-Awareness of and experience with software design patterns\n",
       "\n",
       "-Experience working within an Agile framework\n",
       "\n",
       "Preferred Qualifications\n",
       "\n",
       "-Proficiency in Scala & Kafka programming is a good to have\n",
       "\n",
       "-Experience with Airflow for workflow management\n",
       "\n",
       "-Familiarity with AI concepts and tools, including GitHub Copilot for code\n",
       "\n",
       "development\n",
       "\n",
       "-Exposure to AI/ML development is an added advantage\n",
       "\n",
       "-Proficiency in working with In-memory Databases like Redis\n",
       "\n",
       "-Good knowledge of API development is highly advantageous\n",
       "\n",
       "-Strong verbal and written communication skills, with a proactive and self-\n",
       "\n",
       "motivated approach to improving existing processes to enable faster\n",
       "\n",
       "iterations.\n",
       "\n",
       "-Demonstrated intellectual and analytical rigor, with a keen attention to detail\n",
       "\n",
       "-Team-oriented, energetic, and collaborative approach to work, coupled with a\n",
       "\n",
       "diplomatic and adaptable style\n",
       "\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>eyJqb2JfdGl0bGUiOiJTVyBFbmdpbmVlciAoSmF2YSBhbmQgQmlnZGF0YS9IYWRvb3AvU3BhcmspIDF5ciIsImNvbXBhbnlfbmFtZSI6IlZJU0EiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoidHV1TkJsMDF3amRIc0xnSEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>5 days ago</td><td>Spark Engineer</td></tr><tr><td>Big Data Lead/ Lead Data Engineer/Spark Tech Lead</td><td>Tanisha Systems  Inc</td><td>India</td><td>Job Description:Mandatory:10+ years of experience in solution, design and development of applications using Java 8+/J2EE, Spring, Spring-Boot, Micro Services, RESTful Services and with experience in Big Data and with experience working in heavy data background needed.Develop, program, and maintain applications using the Apache Spark open source frameworkWork with different aspects of the Spark ecosystem, including Spark SQL, Data Frames, Datasets, and streaming.Spark Developer must have strong programming skills in Java, Scala, or PythonFamiliar with big data processing tools and techniquesProven experience as a Spark Developer or a related roleStrong programming skills in Java, Scala, or PythonFamiliarity with big data processing tools and techniquesExperience with the Hadoop ecosystemGood understanding of distributed systemsExperience with streaming data platformsMust have strong experience in Big Data and with experience working in heavy data background needed.Must be strong in Cloud AWS event-based architecture, Kubernetes, ELK (Elasticsearch, Logstash & Kibana)Must have excellent experience in designing and Implementing cloud-based solutions in various AWS Services (: s3, Lambda, Step Function, AMQ, SNS, SQS, CloudWatch Events, etc.)Must be well experienced in design and development of Microservice using Spring-Boot and REST API and with GraphQLMust have solid knowledge and experience in NoSQL (MongoDB)Good knowledge and experience in any Queue based implementations.Strong knowledge/experience in ORM Framework - JPA / HibernateGood knowledge in technical concepts – Security, Transaction, Monitoring, PerformanceShould we well have versed with TDD/ATDDShould have experience on Java, Python and Spark2+ years of experience in designing and Implementing cloud-based solutions in various AWS ServicesStrong experience in DevOps tool chain (Jenkins, Artifactory, Ansible/Chef/Puppet/Spinnaker, Maven/Gradle, Atlassian Tool suite)Very Good knowledge and experience in Non-Functional (Technical) Requirements like Security, Transaction, Performance, etc.Excellent analytical and problem-solving skills Nice to have:· Experience in Experience with OAuth implementation using Ping Identity· Experience in API Management (Apigee) or Service Mesh (Istio)· Good knowledge and experience in Queue/Topic (Active-MQ) based implementations.· Good knowledge and experience in Scheduler and Batch Jobs· Experience with scripting languages using Unix· Preferably certified in AWS</td><td>eyJqb2JfdGl0bGUiOiJCaWcgRGF0YSBMZWFkLyBMZWFkIERhdGEgRW5naW5lZXIvU3BhcmsgVGVjaCBMZWFkIiwiY29tcGFueV9uYW1lIjoiVGFuaXNoYSBTeXN0ZW1zICBJbmMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQmRWaDV5cnl3aTREUFNOT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 hours ago</td><td>Spark Engineer</td></tr><tr><td>Data Insights Analyst</td><td>IN10 (FCRS = IN010) Novartis Healthcare Private Limited</td><td>India</td><td>Responsible for leveraging Google Analytics 4 (GA4) to extract, analyze, and interpret data to support business objectives and decision-making processes. Requires proficient understanding and translating business requirements into actionable insights and recommendations derived from GA4 data.Key Responsibilities:1.Data Extraction & Analysis: Extract and analyze GA4 data to provide meaningful insights into user behavior, website performance, and digital marketing effectiveness.2.Reporting & Dashboarding: Develop custom reports, dashboards, and data visualizations using GA4 data to monitor key metrics and performance indicators.3.Insights and Recommendations: Providing actionable insights and recommendations to optimize website performance, user experience, and marketing campaigns.4.Customization & Configurations: Configure GA4 data streams, events, and conversions to align with business goals and KPIs (key performance indicators). Utilize custom dimensions, metrics, and attribution models to capture and analyze data relevant to unique business needs.5.Pharma knowledge: Understanding of the Pharma industry landscape, including regulatory compliance, patient journey mapping, and healthcare data privacy considerationsMinimum Requirement:·2-4 Years of experience in digital analytics, hands on experience on Google Analytics 4.·Proficiency in Google Analytics and Google Tag Manager.·Strong analytical skills and ability to interpret data.·Understanding of digital marketing concepts and website optimization techniques.·Excellent communication skills to present findings and recommendations effectively·GA4 Certification is strongly preferred</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEluc2lnaHRzIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJJTjEwIChGQ1JTID0gSU4wMTApIE5vdmFydGlzIEhlYWx0aGNhcmUgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImFiMU5uYTFGN1dHbFd4eC1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 days ago</td><td>Data Analyst</td></tr><tr><td>Senior Data Management Analyst</td><td>Wells Fargo</td><td>Hyderabad, Telangana, India</td><td>About this role:\n",
       "\n",
       "Wells Fargo is seeking a Senior Data Management Analyst\n",
       "\n",
       "In this role, you will:\n",
       "• Lead or participate in moderately complex programs and initiatives for data quality, governance, and metadata activities\n",
       "• Design and conduct moderately complex analysis to identify and remediate data quality, data integrity, process, and control gaps\n",
       "• Analyze, assess, and test data controls and data systems to ensure quality and risk compliance standards are met and adhere to data governance standards and procedures\n",
       "• Identify data quality metrics and execute data quality audits to benchmark the state of data quality\n",
       "• Develop recommendations for optimal approaches to resolve data quality issues and implement plans for assessing the quality of new data sources leveraging domain expertise and data, business, or process analysis to inform and support solution design\n",
       "• Lead project teams and mentor less experienced staff members\n",
       "• Drive planning and coordination on moderately complex remediation efforts acting as central point of contact\n",
       "• Consult with clients to assess the current state of data and metadata quality within area of assigned responsibility\n",
       "• Participate in cross-functional groups to develop companywide data governance strategies\n",
       "• Provide input into communication routines with stakeholders, business partners, and experienced leaders\n",
       "\n",
       "Required Qualifications:\n",
       "• 4+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n",
       "\n",
       "Desired Qualifications:\n",
       "• Experience in large enterprise data initiatives\n",
       "• Contact center business or technology experience\n",
       "• Experience implementing, managing, or utilizing big data environments (Hadoop, Big Query, etc.)\n",
       "• Experience using standard BI tools (Tableau, Power BI, MicroStrategy, etc.) preferably from big data environments\n",
       "\n",
       "Posting End Date:\n",
       "24 Jun 2025\n",
       "• Job posting may come down early due to volume of applicants.\n",
       "\n",
       "We Value Equal Opportunity\n",
       "\n",
       "Wells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n",
       "\n",
       "Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n",
       "\n",
       "Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n",
       "\n",
       "Applicants with Disabilities\n",
       "\n",
       "To request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n",
       "\n",
       "Drug and Alcohol Policy\n",
       "\n",
       "Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n",
       "\n",
       "Wells Fargo Recruitment and Hiring Requirements:\n",
       "\n",
       "a. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n",
       "\n",
       "b. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBNYW5hZ2VtZW50IEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJXZWxscyBGYXJnbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEFWbXpFcV9ySTNSUG1faUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Associate/Analyst - Data Analytics</td><td>D. E. Shaw India</td><td>Hyderabad, Telangana, India</td><td>The Financial Research (FinRes) group provides research and analysis support to the entire investment life cycle of the firm’s global proprietary trading strategies. The group caters to almost all the quantitative and qualitative strategies of the firm and its research support covers multiple asset classes, sectors, and geographies.</td><td>eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUvQW5hbHlzdCAtIERhdGEgQW5hbHl0aWNzIiwiY29tcGFueV9uYW1lIjoiRC4gRS4gU2hhdyBJbmRpYSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiak9McEJja1dWaXhiWjJ4c0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>14 hours ago</td><td>Data Analyst</td></tr><tr><td>Senior Analyst- Data Risk Office</td><td>Bristol Myers Squibb</td><td>Hyderabad, Telangana, India</td><td>Working with Us\n",
       "Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n",
       "\n",
       "Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n",
       "\n",
       "Roles & Responsibilities\n",
       "\n",
       "Functional and Technical\n",
       "• Execution and monitoring of data privacy office key activties.\n",
       "• Assist in the development and implementation of data privacy policies and procedures to ensure compliance with international, federal, and state regulations, including GDPR, CCPA, and HIPAA.\n",
       "• Work closely with legal, IT, HR, and other departments to address data privacy concerns and implement best practices.\n",
       "• Assist in the development and delivery of data privacy training and awareness programs across the organization. Keeps up to date with evolution of regulations impacting privacy, ethics, and data.\n",
       "• Experienced in configuring and working with various industry leading data risk and privacy tools like OneTrust, TrustArc, Microsoft Purview etc.\n",
       "• Demonstrate critical thinking around interpreting business and industry challenges and recommending best practices-based solutions to improve products, processes, systems and reduce risk.\n",
       "• Implement organizational IT controls in accordance with applicable regulations and the ability to evaluate and understand the impact of new regulations and requirements.\n",
       "\n",
       "People Management:\n",
       "• Responsible for training and mentoring junior staff to meet BMS standards.\n",
       "• Preferred experience with working in a multi-cultural, multi-location and diverse environments.\n",
       "\n",
       "Qualifications & Experience\n",
       "• B.E./B.Tech. or equivalent in computer science, engineering, life science field\n",
       "• Recognized privacy/DLP certifications and experience preferred.\n",
       "• At least years of privacy program management, compliance, or strong operations management experience (regulated or healthcare or tech preferred).\n",
       "• Knowledge of HIPAA, GDPR, CPRA, PIPL etc., and other privacy regulations is a must.\n",
       "• Ability to make decisions that impact own work and other groups/teams and works under minimal supervision.\n",
       "• Demonstrates openness to learning and developing. Takes a responsibility for their own and team’s development and growth.\n",
       "• Demonstrates an understanding of factors driving team performance and how they contribute to the team's overall success.\n",
       "• Excellent English Oral and written communication skills including the ability to deliver clear and articulate presentations.\n",
       "• Ability to use PowerPoint, Excel, Word, or other technologies to communicate complex topics to stakeholders, manage personal workload, and track projects and issues.\n",
       "\n",
       "If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n",
       "\n",
       "Uniquely Interesting Work, Life-changing Careers\n",
       "With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n",
       "\n",
       "On-site Protocol\n",
       "\n",
       "BMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n",
       "\n",
       "Site-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n",
       "\n",
       "BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n",
       "\n",
       "BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n",
       "\n",
       "BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n",
       "\n",
       "If you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n",
       "\n",
       "Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHlzdC0gRGF0YSBSaXNrIE9mZmljZSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJsWm9Rdlc1dnAyWjZyYVlMQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst II – Product Information Capabilities | Digital & Technology</td><td>General Mills India</td><td>India</td><td>India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.\n",
       "\n",
       "Position Title\n",
       "\n",
       "Software Engineer II – Product Information Capability\n",
       "\n",
       "Function/Group\n",
       "\n",
       "Digital & Technology\n",
       "\n",
       "Location\n",
       "\n",
       "Mumbai\n",
       "\n",
       "Shift Timing\n",
       "\n",
       "Regular\n",
       "\n",
       "Role Reports to\n",
       "\n",
       "D&T Manager – Product Information Capability\n",
       "\n",
       "Remote/Hybrid/in-Office\n",
       "\n",
       "Hybrid\n",
       "\n",
       "About General Mills\n",
       "\n",
       "We make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and Häagen-Dazs, we’ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\n",
       "\n",
       "How we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\n",
       "\n",
       "us into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\n",
       "\n",
       "General Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\n",
       "\n",
       "With our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital & Technology (D&T) Innovation, Technology & Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy & Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\n",
       "\n",
       "We advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\n",
       "\n",
       "Job Overview\n",
       "\n",
       "Function Overview\n",
       "\n",
       "The Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The team's expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\n",
       "\n",
       "The team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the \"Work with Heart\" philosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the following Link\n",
       "\n",
       "Purpose of the role\n",
       "\n",
       "This is an exciting time to work in General Mills' Supply Chain & ITQ Organization! We are accelerating Digital Transformation of our Product Information Capabilities (PIC) capabilities to provide a competitive advantage to our business. To this end, we are seeking a skilled and motivated STIBO Developer to join our Product Information Capabilities team. As a STIBO Developer, you will play a crucial role in designing, developing, and implementing solutions within our STIBO STEP platform to support our product information management (PIM) and master data management (MDM) processes. You will collaborate with cross-functional teams to understand business requirements, translate them into technical specifications, and deliver high-quality solutions that meet our business needs.\n",
       "\n",
       "Key Accountabilities\n",
       "• Design, develop, and maintain STIBO STEP solutions to support product information management processes.\n",
       "• Write and maintain code for business rules to ensure data quality and consistency.\n",
       "• Configure outbound and inbound integrations to exchange data with other systems.\n",
       "• Configure gateway endpoints for seamless data flow.\n",
       "• Develop and maintain data models within STIBO STEP to accurately represent product information.\n",
       "• Build web UI screens for data entry, validation, and reporting.\n",
       "• Develop solutions based on documented requirements and specifications.\n",
       "• Participate in agile project development, including sprint planning, daily stand-ups, and retrospectives.\n",
       "• Collaborate with business analysts, data architects, and other developers to ensure solutions align with business needs and technical standards.\n",
       "• Troubleshoot and resolve issues related to STIBO STEP implementations.\n",
       "• Stay up-to-date with the latest STIBO STEP features and best practices.\n",
       "• Create and maintain technical documentation for STIBO STEP solutions.\n",
       "\n",
       "Minimum Qualifications\n",
       "• Education – Full time graduation from an accredited university (Mandatory- Note: This is the minimum education criteria which cannot be altered)\n",
       "• Experience with tools and concepts related to MDM and/or Digital Product Publishing, such as data governance, data quality, data integration and data exporting\n",
       "• Exposure to Product Information Management Systems (PIM/MDM)\n",
       "• Technical expertise into Stibo platform\n",
       "• Experience with Data Syndication partners such as 1WorldSync, Syndigo and Salsify.\n",
       "• Exposure to GDSN Standards\n",
       "• Strong team player and collaborator with core focus on solution making; Effective analytical and technical skills; Ability to work in a cross functional team environment\n",
       "\n",
       "Preferred Qualifications\n",
       "• Product Information Management / Master Data Management\n",
       "• STIBO STEP certification\n",
       "• Business Analysis skills\n",
       "• SQL, Cloud GCP\n",
       "• Agile / SCRUM Delivery\n",
       "• Familiarity with Service Bus Integration\n",
       "• Preferably experience in Consumer Goods industry.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUkg4oCTIFByb2R1Y3QgSW5mb3JtYXRpb24gQ2FwYWJpbGl0aWVzIHwgRGlnaXRhbCBcdTAwMjYgVGVjaG5vbG9neSIsImNvbXBhbnlfbmFtZSI6IkdlbmVyYWwgTWlsbHMgSW5kaWEiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTWplT1ByZWI4UVZPLVpDVUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>2 days ago</td><td>Data Analyst</td></tr><tr><td>Lead Data Management Analyst</td><td>Wells Fargo</td><td>Hyderabad, Telangana, India</td><td>About this role:\n",
       "\n",
       "Wells Fargo is seeking a Lead Data Management Analyst\n",
       "\n",
       "In this role, you will:\n",
       "• Organize and lead complex companywide initiatives to ensure that data quality is maintained so that data can effectively support business processes\n",
       "• Oversee analysis and reporting in support of regulatory requirements\n",
       "• Identify and recommend analysis of data quality or integrity issues\n",
       "• Evaluate data quality metrics and data quality audits to benchmark the state of data quality\n",
       "• Make decisions in complex and multi-faceted situations requiring solid understanding of data governance standards and procedures\n",
       "• Identify new data sources and develop recommendations for assessing the quality of new data\n",
       "• Lead project teams and mentor less experienced staff members\n",
       "• Recommend remediation of process or control gaps that align to management strategy\n",
       "• Serve as relationship manager for a line of business\n",
       "• Consult with and provide recommendations to senior management regarding assessments of the current state of data and metadata quality within area of assigned responsibility\n",
       "• Represent client in cross-functional groups to develop companywide data governance strategies\n",
       "• Strategically collaborate and consult with peers, colleagues, and mid-level to senior managers to coordinate and drive collaboration on solution design and remediation execution\n",
       "\n",
       "Required Qualifications:\n",
       "• 5+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n",
       "\n",
       "Desired Qualifications:\n",
       "• Experience in Data Management, Business Analysis, Analytics, Project Management.\n",
       "\n",
       "Posting End Date:\n",
       "24 Jun 2025\n",
       "• Job posting may come down early due to volume of applicants.\n",
       "\n",
       "We Value Equal Opportunity\n",
       "\n",
       "Wells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n",
       "\n",
       "Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n",
       "\n",
       "Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n",
       "\n",
       "Applicants with Disabilities\n",
       "\n",
       "To request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n",
       "\n",
       "Drug and Alcohol Policy\n",
       "\n",
       "Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n",
       "\n",
       "Wells Fargo Recruitment and Hiring Requirements:\n",
       "\n",
       "a. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n",
       "\n",
       "b. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgTWFuYWdlbWVudCBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiV2VsbHMgRmFyZ28iLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ik5YbnE1ZlFtc3M5WXNVQ0JBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Senior Data Analyst, Marketing Science</td><td>Crunchyroll</td><td>Hyderabad, Telangana, India</td><td>About the role\n",
       "\n",
       "We are looking for a hands-on Senior Marketing Science Analyst with a passion for understanding data, tracking business trends, and storytelling to join the Center for Data and Insights team. This position will report to the Vice President, Business Intelligence & Analytics, and will collaborate with data engineers, campaign managers, and marketing leaders to investigate campaign trends, build business insights, and recommend proactive measures to enable better business decisions every day. The right person for this position is a service-minded, empathetic problem-solver who will be motivated by the opportunity to build a centralized insights service team from the ground up!\n",
       "• Partner with Branch/ Adjust MMPs, direct response, and media buying agencies to define and implement campaign measurement of integrated marketing campaigns, including TV, OOH, Digital, and Social Media across a wide range of business lines (theatrical, streaming, e-commerce, etc.)\n",
       "• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n",
       "• Evaluating media measurement leveraging MMM (Marketing Mix Modeling), MTA (Multi-Touch Attribution), and Incrementality A/B Testing\n",
       "• Lead / Assist in development of testing roadmaps, measurement plans, KPI & KLI reporting based on set marketing objectives linked to key campaigns and always-on digital marketing tactics.\n",
       "• Deliver ad hoc analysis for client stakeholders throughout marketing, working with the channel leads, planning, and client service departments.\n",
       "• Partner with paid marketing teams in building weekly insight decks for the Marketing leadership team by collaborating effectively across teams, documenting the impact of strategic initiatives on benchmarks, and tracking the progress towards company goals.\n",
       "• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n",
       "• Connect ideas into cohesive, well-grounded recommendations, using creative, structured, and analytical thinking with the help of effective data visualization\n",
       "• Work with offshore and onsite teams and lead the sprint planning/management\n",
       "• Maintain a culture of high-quality output and outstanding customer service by effectively communicating at all levels, ensuring that work gets done, and responding effectively to\n",
       "\n",
       "About You\n",
       "• 5+ years of experience with data analysis, paid campaign analytics, statistics, experimentation, and optimization.\n",
       "• 3+ years of experience in writing complex SQL queries, experience in marketing data automation.\n",
       "• 3+ years of experience with data visualization tools like Tableau, Superset, etc.\n",
       "• Understanding of install and user-level paid campaign tracking to support multi-touch attribution via UTMs and MMP data for all major marketing channels\n",
       "• Proficiency in data analysis, including defining critical metrics, statistical and predictive modeling concepts, descriptive statistics, and experimental design\n",
       "• Experience in Marketing analytics tools like Google Analytics, Adjust, Braze, Branch, etc.\n",
       "• Experience working with large data sets (Terabytes of data/ billions of records).\n",
       "• Deep expertise in measuring marketing performance against lifetime value metrics.\n",
       "• Outstanding teamwork skills: You have excellent interpersonal skills. You’re a good listener. You place the success of the company ahead of any particular idea. Proven ability to work across a globally, matrixed organization.\n",
       "• BS in Statistics, Computer Science, Information Systems, or a related field\n",
       "\n",
       "About the Team\n",
       "\n",
       "The Center for Data and Insights (CDI) is the centralized team of data engineering, BI, analytics, and data science experts, passionate about servicing the organization with timely and certified reports and insights! The mission of the group is to inspire, support, and guide our stakeholders to be data-aware as well as build out the systems of intelligence to discover insights and act on them.\n",
       "\n",
       "Why you will love working at Crunchyroll\n",
       "\n",
       "In addition to getting to work with fun, passionate and inspired colleagues, you will also enjoy the following benefits and perks:\n",
       "• Best-in class medical, dental, and vision private insurance healthcare coverage\n",
       "• Access to counseling & mental health sessions 24/7 through our Employee Assistance Program (EAP)\n",
       "• Free premium access to Crunchyroll\n",
       "• Professional Development\n",
       "• Company's Paid Parental Leave\n",
       "• up to 26 weeks for birthing parents\n",
       "• up to 12 weeks for non-birthing parents\n",
       "• Hybrid Work Schedule\n",
       "• Paid Time Off\n",
       "• Flex Time Off\n",
       "• 5 Yasumi Days\n",
       "• Half-Day Fridays during the summer\n",
       "• Winter Break\n",
       "\n",
       "#LifeAtCrunchyroll #LI-Hybrid</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBBbmFseXN0LCBNYXJrZXRpbmcgU2NpZW5jZSIsImNvbXBhbnlfbmFtZSI6IkNydW5jaHlyb2xsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiI3QU16TGktYmRTM1gybEV4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Principal Data Analyst</td><td>Storable</td><td>Serilingampalle (M), Hyderabad, Telangana, India</td><td>About the Role:\n",
       "We’re seeking a strategic, data-driven professional to lead our Business Analysis initiatives for our Marketplace product within the Self-Storage vertical. This is a ‘hands on keyboard’ high-impact role that bridges business strategy, analytical rigor, and data visualization —ideal for someone who thrives on translating data into actionable insights that drive performance. As the Principal Data Analyst, you will own our marketplace performance metrics, lead the development of world-class dashboards and reporting frameworks, and partner closely with Product, Marketing, Operations, and Finance teams to inform strategic decisions. You’ll drive both hands-on analysis and organizational data literacy while mentoring a team of analysts and collaborating with engineering on data infrastructure.\n",
       "Key Responsibilities:\n",
       "\n",
       "Own and continuously optimize key Marketplace KPIs (conversion, revenue yield, inventory, pricing, etc.).\n",
       "Serve as the steward of metric definitions and business logic—ensuring consistency, accuracy, and alignment across stakeholders.\n",
       "Have a deep understanding of how to run a BI environment. Proactive, insightful, curious.\n",
       "Build and scale world class self-service dashboards and Tableau reports for internal and executive stakeholders.\n",
       "Lead cross-functional analysis projects to uncover trends, diagnose problems, and identify growth and efficiency opportunities.\n",
       "Translate complex data into clear, actionable insights and concise narratives for business and executive audiences.\n",
       "Drive rigorous SQL-based analysis from raw datasets, ensuring high-quality, reproducible outputs.\n",
       "Guide and collaborate with junior analysts in other groups and collaborate with engineering to shape data pipelines and definitions.\n",
       "Collaborate with cross functional partners and teams (Product,Marketing, Finance, Business Insights) to test hypotheses, evaluate feature/campaign performance, and measure outcomes.\n",
       "Identify data gaps or integrity risks and work with stakeholders to improve reliability and accuracy.\n",
       "Champion a culture of curiosity, experimentation, and evidence-based decision-making.\n",
       "Proactively keep other teams in the loop on new data capabilities unlocked and changes being made.\n",
       "\n",
       "Requirements:\n",
       "\n",
       "5-7 years of experience in business analysis, data analytics, or related roles, ideally with at least 2+ years in a leadership capacity.\n",
       "Advanced proficiency in SQL with experience working with complex data models and the ability to query complex relational datasets.\n",
       "Other data engineering experience is a significant plus to facilitate sourcing/formating of data.\n",
       "Deep understanding of performance metrics and business levers in a supply/demand or marketplace environment.\n",
       "Experience leading analytics efforts in partnership with cross-functional teams including Product, Marketing, Finance, and Engineering.\n",
       "Proven track record of developing intuitive, scalable dashboards using Tableau (or similar BI platforms).\n",
       "Strong business acumen with the ability to connect the dots between business objectives and analytical frameworks.\n",
       "Exceptional communication skills with the ability to distill technical findings for non-technical audiences.\n",
       "Capable of influencing and informing executive stakeholders with clear, concise insights.\n",
       "Demonstrated experience owning KPIs, building data products, and drive insights into action in a fast paced environment.\n",
       "Ability to navigate ambiguity, manage and prioritize competing needs, and work cross-functionally.\n",
       "\n",
       "Preferred Qualifications:\n",
       "\n",
       "Experience in the storage, real estate, or marketplace industries strongly preferred\n",
       "Familiarity with modern data stack tools such as Snowflake, dbt, or similar Experience analyzing marketplace dynamics or supply/demand business models\n",
       "Exposure to experimentation frameworks, A/B testing, or uplift modeling\n",
       "Prior exposure to high-growth SaaS or Marketplace operations\n",
       "Data engineering capabilities—especially in sourcing, transforming, or centralizing disparate datasets.\n",
       "\n",
       "About Us:\n",
       "At Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\n",
       "We leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.\n",
       "Important Notice: Protect Yourself from Fraudulent Activities Targeting Job Seekers\n",
       "We’ve been made aware of fraudulent activities where an individual or group is pretending to represent Storable, attempting to deceive job seekers by using our company’s good name and stellar reputation.\n",
       "To protect yourself, please consider the following guidelines:\n",
       "– Official Communication: All genuine communication from Storable will come from official email addresses, ending in “@storable.com.” Be cautious of any communication that doesn’t match this criteria.– No Unsolicited Offers: We do not extend job offers without a formal interview process. If you receive an unsolicited job offer claiming to be from Storable or any of its representatives, it’s a red flag.– Verification: If you’re uncertain about the legitimacy of any job offer or communication claiming to be from Storable, please directly contact our HR department directly at POps@storable.com for verification.\n",
       "Your security and trust are paramount to us. If you suspect you’ve been contacted by someone falsely claiming to be from Storable or using our company’s name for any dubious purpose, please immediately report the incident to POps@storable.com\n",
       "We’re committed to ensuring a transparent and secure hiring process.\n",
       "Thank you for your vigilance and interest in joining our team.</td><td>eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiU3RvcmFibGUiLCJhZGRyZXNzX2NpdHkiOiJTZXJpbGluZ2FtcGFsbGUgKE0pLCBIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InljaTdhWW0wWnR2MEMzbkZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>18 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst 1</td><td>UnitedHealth Group</td><td>India</td><td>At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.\n",
       "\n",
       "Primary Responsibilities:\n",
       "• Validate data with administrative source systems (source of truth)\n",
       "• Analyze complex datasets\n",
       "• Generate actionable insights and recommendations based on data analysis\n",
       "• Database Management:\n",
       "• Develop and maintain data models, data dictionaries, and other documentation\n",
       "• Troubleshoot and resolve database-related issues\n",
       "• Data Extraction and Transformation:\n",
       "• Support capital project that will, in part, import and transform data from various sources for broker bonus calculations\n",
       "• Ensure data integrity and quality through rigorous validation and testing\n",
       "• Data Visualization and Reporting:\n",
       "• Create visually appealing and informative dashboards and reports\n",
       "• Present findings and insights to both technical and non-technical stakeholders in a clear and concise manner\n",
       "• Continuous Learning and Improvement:\n",
       "• Stay up to date with the latest data analysis techniques and tools\n",
       "• Identify opportunities to improve data analysis processes and methodologies\n",
       "• Actively participate in knowledge sharing and mentoring within the team\n",
       "• Comply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n",
       "\n",
       "Required Qualifications:\n",
       "• Undergraduate degree or equivalent experience\n",
       "• 4+ years Experience as SAS Data Analyst\n",
       "• Experience with relational database management systems (e.g., MySQL, Oracle, SQL Server)\n",
       "• Experience with statistical analysis\n",
       "• Familiarity with data visualization tools (e.g., Tableau, Power BI)\n",
       "• Proven excellent problem-solving and critical thinking skills\n",
       "• Proven solid communication and presentation skills to effectively convey complex data analysis findings to both technical and non-technical stakeholders\n",
       "• Proven ability to work independently and collaboratively in a fast-paced, deadline-driven environment\n",
       "• Proven detail-oriented with a focus on accuracy and data integrity\n",
       "\n",
       "At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone - of every race, gender, sexuality, age, location and income - deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.\n",
       "\n",
       "#NTRQ</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IlVuaXRlZEhlYWx0aCBHcm91cCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJhdHFPMEswckdadGwwUy1xQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>3 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst – Competitive Benchmarking & Reporting</td><td>Reputation</td><td>Hyderabad, Telangana, India</td><td>Why Work at Reputation?\n",
       "• Reputation reached over $100m in Annual Recurring Revenue (ARR) in 2022 and continues to grow worldwide.\n",
       "• We've raised over $200 million in funding from A-list venture capital firms such as Bessemer Venture Partner and Kleiner Perkins, including $150 million in equity financing from Marlin Equity Partners in January 2022.\n",
       "• Reputation is trusted by more than 250 partners, including Google, Facebook, Salesforce, J.D. Power, Amazon and Web.com.\n",
       "• Our industry leading platform has been recognized by Forrester and Gartner as a vendor of choice in Voice of the Customer, Customer Feedback Management, and Social Suites research reports.\n",
       "• The platform is used by 10+ major automotive OEMs and 16,000 auto dealerships, more than 250 healthcare systems, and over 100 leading property management firms.\n",
       "• Our executive management team is committed to building a performance-based culture where excellence is rewarded and careers are developed.\n",
       "• Who thrives at Reputation? Managers who embody a player-coach mentality. Employees who value teamwork and cross-functional collaboration. People who emphasize perseverance and hustle over quick wins and luck.\n",
       "• Our Mission: We exist to forge relationships between companies and communities.\n",
       "\n",
       "We are seeking a Data Analyst with expertise in data stewardship and analysis to manage our competitive benchmark data tenants and produce customer-facing reports for marketing and sales. The ideal candidate is proficient in BigQuery (SQL), detail-oriented, and capable of working independently on complex data projects.\n",
       "\n",
       "Responsibilities:\n",
       "• Data Stewardship & Governance: Ensure data accuracy, integrity, and accessibility by managing competitive benchmark data tenants, implementing verification processes, and maintaining governance best practices.\n",
       "• Data Pipeline Management: Develop and maintain data ingestion pipelines, including API integrations and web scraping, ensuring timely updates and scalability.\n",
       "• Data Validation & Quality Control: Oversee large-scale data input and validation, balancing hands-on data manipulation with automation strategies. Collaborate with data owners to resolve quality issues.\n",
       "• Advanced Analytics & Reporting: Conduct in-depth data analysis to uncover trends, patterns, and insights that drive business decisions. Scope, design, and execute analysis projects with minimal supervision.\n",
       "• Industry & Market Insights: Develop data-driven industry reports for marketing and sales, leveraging analytics to provide insights on market trends and competitive landscapes.\n",
       "• Cross-Functional Collaboration: Work closely with product managers, engineers, marketing, and leadership to resolve data issues, provide training, and ensure alignment on data best practices.\n",
       "• Tool & Process Optimization: Design scalable internal tools and reporting solutions to enhance data accessibility, automation, and usability across teams.\n",
       "• Stakeholder Communication: Translate complex data insights into actionable recommendations for key stakeholders, including non-technical audiences.\n",
       "\n",
       "Qualifications:\n",
       "• 5+ years of proven experience in data analysis, data stewardship, and managing large-scale datasets, with a strong focus on ensuring data accuracy, integrity, and accessibility.\n",
       "• Must be proficient with SQL and NoSQL (BigQuery), as well as have the ability to thrive in an environment that often requires patience and creativity to query, extract and validate data in which there are high levels of uncertainty.\n",
       "• Strong prior professional experience managing databases and using applicable tools is required.\n",
       "• Experience with and knowledge of ETL processes and data migration.\n",
       "• Understanding of and prior experience with General Data Protection Regulation.\n",
       "• Demonstrated experience and proficiency in using Python in real-world, business applications are substantial advantages to candidates.\n",
       "• Highly articulate with outstanding verbal and written communication skills; you are able to explain data-driven insights and analyses intuitively to clients, executives, and technical and non-technical coworkers in both large and small groups.\n",
       "• You should be comfortable taking feedback from everyone – Product Managers, Engineers, customers, the CEO, etc. – to help understand business objectives and develop the most powerful analyses and tools possible.\n",
       "• Proven ability to operate in a fast-paced, data-driven environment.\n",
       "\n",
       "When you join Reputation, you can expect:\n",
       "• Flexible working arrangements.\n",
       "• Career growth with paid training tuition opportunities.\n",
       "• Active Employee Resource Groups (ERGs) to engage with.\n",
       "• An equitable work environment.\n",
       "\n",
       "Our employees say it best:\n",
       "\n",
       "According to Glassdoor, 94% of our employees approve of our CEO, Joe Burton.\n",
       "\n",
       "Our employees highlight our:\n",
       "• Ample Opportunities- “There are many opportunities to learn and grow. Many open roles are replaced with internal promotions.”\n",
       "• Positive Culture- \"Great opportunity and exceptional culture.\" “You will never have a better culture anywhere else. Period.”\n",
       "• Training and Tools- “All managers truly want you to succeed, and you are given great tools and training to be successful in your role.”\n",
       "• Balance- “Great work life balance and awesome team environment!”\n",
       "\n",
       "Diversity Programs & Initiatives:\n",
       "\n",
       "Our Reputation Nation spans around the world. This global perspective allows us to intentionally unlock the magic that comes from diversity of experience to contribute to our success.\n",
       "\n",
       "At Reputation, we believe in:\n",
       "• Diversity: Embracing a culture that values uniqueness.\n",
       "• Inclusion: Inviting diverse groups to take part in company life.\n",
       "• Belonging: Helping each individual feel accepted for who they are.\n",
       "\n",
       "\"At Reputation, we see diversity and inclusion as the foundation for an equitable workplace. Our goal is to empower all of our employees, regardless of their background, to make an impact in their work each and every day.\" - Joe Burton, CEO, Reputation\n",
       "\n",
       "Additionally, we offer a variety of benefits and perks, such as:\n",
       "• Health Insurance & Wellness Benefits: Group Health Insurance: Medical Insurance with floater policy of up to 10,00,000 for employee + spouse + 2 dependent children + 2 parents / parent-in-laws\n",
       "• Maternity Benefits: Medical insurance up to 75,000 INR, 26 weeks of leave for birth, adoption or surrogacy\n",
       "• Life Insurance: Insurance at 3x annual cost to the company (Term Insurance, GPA)\n",
       "• Accident/Disability Insurance: Insured at 3x base salary for permanent total disability, permanent partial disability and temporary total disability (GPA)\n",
       "• OPD: of 7500 per annum per employee\n",
       "\n",
       "Leaves\n",
       "• 10 Company observed holidays a year (Refer to the Holiday Calendar for the Year)\n",
       "• 12 Casual/Sick leaves (Pro-rata calculated)\n",
       "• 02 Earned Leaves per Month (Pro-rata calculated)\n",
       "• 04 Employee Recharge days (aka company holiday/office closed)\n",
       "• Maternity & Paternity (6 months)\n",
       "• Bereavement Leave (10 Days)\n",
       "\n",
       "Car Lease:\n",
       "Reputation offers Car Lease Program that allows employees to lease a car with no upfront cost or down payment. They benefit from a fixed monthly lease rental and 20-30% tax savings.\n",
       "\n",
       "We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n",
       "\n",
       "To learn more about how we handle the personal data of applicants, visit our Candidate Privacy Notice.\n",
       "\n",
       "Applicants only - No 3rd party agency candidates.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3Qg4oCTIENvbXBldGl0aXZlIEJlbmNobWFya2luZyBcdTAwMjYgUmVwb3J0aW5nIiwiY29tcGFueV9uYW1lIjoiUmVwdXRhdGlvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiT0NPR3UtU1dxWjh4X0gzS0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 days ago</td><td>Data Analyst</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Lead Consultant - Technical Lead - Fullstack Data Engineer",
         "AstraZeneca",
         "Chennai, Tamil Nadu, India",
         "Job Title: Lead Consultant - Technical Lead - Data, Analytics & AI\nCareer Level: E\n\nIntroduction to role: Are you ready to redefine an industry and change lives? Join our Operations IT organization, where your work will have a direct impact on patients by redefining our ability to develop life-changing medicines. We empower the business to perform at its peak, combining innovative science with leading digital technology platforms and data. As part of the Data Analytics and AI (DA&AI) group, you'll deliver innovative solutions that drive the transformation of medicine development. As the Technical Lead, you'll oversee the technical delivery of products and projects aligned with business objectives, tapping into your expertise in Data and Cloud Engineering, Software Engineering, AI, and more. Collaborate with stakeholders to ensure seamless delivery and maximum business impact!\n\nAccountabilities:\n• Bridge business needs with technical solutions by leading IT application design and implementation.\n• Collaborate with stakeholders to define and deliver requirements, translating them into detailed specifications.\n• Own the technical vision of products and compile detailed technical designs, refining user Epics/stories.\n• Optimize performance, mitigate risks, and ensure alignment to timelines and resource allocations.\n• Advise on industry trends and standard methodologies to enhance performance and business outcomes.\n• Provide technical direction and guidance to IT teams and business units.\n• Contribute to Data & Software Engineering standards and best practices.\n• Research new technologies to boost system performance and scalability.\n• Lead and mentor technical teams (Data Engineering, Cloud Engineering, Software Engineering) and work with AI/GenAI leads to foster collaboration and innovation.\n• Ensure platform stability, scalability, and simplicity while adhering to regulatory requirements and data security standards.\n• Foster continuous improvement and innovation.\n• Supervise technical leadership throughout the software development lifecycle and provide hands-on support in development, technical delivery, review, and testing while collaborating with global teams.\n• Apply central Enterprise Data Platforms and guide DevOps, DataOps, and MLOps teams to ensure standard methodologies.\n• Ensure data solutions align with FAIR principles and support end-to-end data science and machine learning.\n\nEssential Skills/Experience:\n• Minimum 10 years of experience in the design, development, and delivery of software and data engineering solutions.\n• Extensive technical expertise in Data Engineering, Software Engineering, and Cloud Engineering.\n• Strong foundational knowledge of AI Engineering principles and practices.\n• Deep understanding of DevOps, MLOps, and DataOps methodologies with practical implementation experience.\n• Demonstrated success in product development and/or product management, delivering complex solutions end-to-end.\n• Demonstrable ability to provide technical thought leadership across Data, Analytics, and AI domains.\n• Exceptional communication, customer management, and multi-functional collaboration skills.\n• Robust analytical and problem-solving abilities with a collaborative, team-oriented approach.\n• Hands-on experience driving innovation throughout the full product development lifecycle.\n• Solid understanding of Data Mesh and Data Product concepts and architectures.\n• Proficiency in Agile methodologies and facilitating iterative, multi-functional team delivery.\n• Hands-on experience designing, implementing, and optimizing data pipelines using leading ETL tools.\n• Skilled in architecting, deploying, and managing scalable, secure AWS cloud environments.\n• Proficient in workflow orchestration tools such as Apache Airflow.\n• Practical experience implementing DataOps practices with tools like DataOps.Live.\n• Strong expertise in data storage and analytics platforms such as Snowflake.\n• Ability to deliver actionable insights through business intelligence tools, including Power BI.\n• Extensive full-stack development experience, including backend proficiency with Node.js and Python and frontend expertise with ReactJS or NextJS.\n• Experience designing and deploying Generative AI solutions.\n• Hands-on implementation of AI/ML models using platforms such as Amazon SageMaker.\n• Advanced programming skills, especially in Python.\n• Solid knowledge of both SQL and NoSQL database technologies.\n• Familiarity with agile ways of working and iterative development environments.\n• Experience working in large, multinational organizations or pharmaceutical environments is highly desirable.\n• Demonstrated leadership and mentoring skills, with a demonstrable ability to develop high-performing technical teams.\n\nDesirable Skills/Experience:\n• Bachelor's or master's degree in health sciences, Life Sciences, Data Management, IT, or a related field.\n• Experience in the pharmaceutical industry or a similar multinational environment.\n• AWS Cloud or relevant data/software engineering certifications.\n• Awareness of use case specific GenAI tools available in the market and their application in day-to-day work scenarios.\n• Possess working knowledge of basic prompting techniques and continuously improve these skills.\n• Stay up to date with developments in AI and GenAI, applying new insights to work-related situations.\n\nWhen we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That's why we work, on average, a minimum of three days per week from the office. But that doesn't mean we're not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\n\nAt AstraZeneca, we are at a crucial stage of our journey to become a digital and data-led enterprise. Our commitment to innovation empowers us to make the impossible possible by building partnerships and ecosystems that drive scale and speed for exponential growth. With investment backing us all the way, we are focused on disrupting the industry while making a meaningful impact through our work. Here you'll find countless opportunities to learn, grow, and contribute to developing life-changing medicines.\n\nReady to make a difference? Apply now to join our team!\n\nDate Posted\n30-Jun-2025\n\nClosing Date\n\nAstraZeneca embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills. We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements.",
         "eyJqb2JfdGl0bGUiOiJMZWFkIENvbnN1bHRhbnQgLSBUZWNobmljYWwgTGVhZCAtIEZ1bGxzdGFjayBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiQXN0cmFaZW5lY2EiLCJhZGRyZXNzX2NpdHkiOiJDaGVubmFpLCBUYW1pbCBOYWR1LCBJbmRpYSIsImh0aWRvY2lkIjoiRGVXa2pFZDgxNXcwSUpJY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "3 days ago",
         "Data Engineer"
        ],
        [
         "AWS Data Engineer",
         "Cognizant",
         "Hyderabad, Telangana, India (+1 other)",
         "Job Summary:\n\nExperience : 4 - 8 years\n\nLocation : Bangalore\n\nThe Data Engineer will contribute to building state-of-the-art data Lakehouse platforms in AWS, leveraging Python and Spark. You will be part of a dynamic team, building innovative and scalable data solutions in a supportive and hybrid work environment. You will design, implement, and optimize workflows using Python and Spark, contributing to our robust data Lakehouse architecture on AWS. Success in this role requires previous experience of building data products using AWS services, familiarity with Python and Spark, problem-solving skills, and the ability to collaborate effectively within an agile team.\n\nMust Have Tech Skills:\n\n· Demonstrable previous experience as a data engineer.\n• Technical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\n\n· Proficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nNice To Have Tech Skills:\n\n· Familiar with data services in a Lakehouse architecture.\n\n· Familiar with technical design practices, allowing for the creation of scalable, reliable data products that meet both technical and business requirements\n\n· A master’s degree or relevant certifications (e.g., AWS Certified Solutions Architect, Certified Data Analytics) is advantageous\n\nKey Accountabilities:\n• Writes high quality code, ensuring solutions meet business requirements and technical standards.\n• Works with architects, Product Owners, and Development leads to decompose solutions into Epics, assisting the design and planning of these components.\n• Creates clear, comprehensive technical documentation that supports knowledge sharing and compliance. Experience in decomposing solutions into components (Epics, stories) to streamline development.\n• Actively contributes to technical discussions, supporting a culture of continuous learning and innovation.\n\nKey Skills:\n• Proficient in Python and familiar with a variety of development technologies.\n• Previous experience of implementing data pipelines, including use of ETL tools to streamline data ingestion, transformation, and loading.\n• Solid understanding of AWS services and cloud solutions, particularly as they pertain to data engineering practices. Familiar with AWS solutions including IAM, Step Functions, Glue, Lambda, RDS, SQS, API Gateway, Athena.\n• Proficient in quality assurance practices, including code reviews, automated testing, and best practices for data validation.\n• Experienced in Agile development, including sprint planning, reviews, and retrospectives\n\nEducational Background:\n• Bachelor’s degree in computer science, Software Engineering, or related essential.\n\nBonus Skills:\n• Financial Services expertise preferred, working with Equity and Fixed Income asset classes and a working knowledge of Indices.\n• Familiar with implementing and optimizing CI/CD pipelines. Understands the processes that enable rapid, reliable releases, minimizing manual effort and supporting agile development cycles.",
         "eyJqb2JfdGl0bGUiOiJBV1MgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkNvZ25pemFudCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiWjYyVVNZZkhtUHk0dHdwNkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 days ago",
         "Data Engineer"
        ],
        [
         "Engineer III Consultant-Data Engineering",
         "Verizon",
         "Hyderabad, Telangana, India (+2 others)",
         "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat you’ll be doing…\n\nWe are looking for data engineers who can work with world class team members to help drive telecom business to its full potential. We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n\nAs a data engineer you will be collaborating with business product owners, coaches, industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n• Understanding the business requirements and converting them to technical design.\n• Working on Data Ingestion, Preparation and Transformation.\n• Developing data streaming applications.\n• Debugging the production failures and identifying the solution.\n• Working on ETL/ELT development.\n• Understanding devops process and contributing for devops pipelines\n\nWhat we’re looking for...\n\nYou’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n\nYou’ll need to have…\n• Bachelor’s degree or four or more years of work experience.\n• Four or more years of relevant work experience.\n• Experience with Data Warehouse concepts and Data Management life cycle.\n• Experience in Big Data technologies - GCP/Hadoop/Spark/Composer/DataFlow/Bigquery.\n• Experience in complex SQL.\n• Experience working on Streaming ETL pipelines\n• Expertise in Java\n• Experience with MemoryStore / Redis / Spanner\n• Experience in troubleshooting the data issues.\n• Experience with data pipeline and workflow management & Governance tools.\n• Knowledge of Information Systems and their applications to data management processes.\n\nEven better if you have one or more of the following…\n• Three or more years of relevant experience.\n• Any relevant Certification on ETL/ELT developer.\n• Certification in GCP-Data Engineer.\n• Accuracy and attention to detail.\n• Good problem solving, analytical, and research capabilities.\n• Good verbal and written communication.\n• Experience presenting to and influence stakeholders.\n• Experience in driving a small team of 2 or more members for technical delivery\n\n#AI&D\n\nWhere you’ll be working\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.",
         "eyJqb2JfdGl0bGUiOiJFbmdpbmVlciBJSUkgQ29uc3VsdGFudC1EYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiVmVyaXpvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEhtc2E1cThWc3RlSGhKaUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Data Engineer"
        ],
        [
         "Data Engineer (Python, Pyspark, and Azure Databricks) || (4 - 6 Years)",
         "Emids",
         "Bengaluru, Karnataka, India",
         "Hi All,\n\nGreetings for the day!!\n\nWe are currently hiring for Data Engineer (Python, Pyspark, and Azure Databricks) for Emids(MNC) at Bangalore location.\n\nRole: Data Engineer\n\nExp: 5 to 8 Years\n\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\n\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\nNote: Candidate Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.\n\nRole Overview:\n\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\n• Lead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\n• Architect scalable data streaming and processing solutions to support healthcare data workflows.\n• Develop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\n• Ensure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\n• Collaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\n• Troubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\n• Mentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\n• Stay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\nRequired Skills & Qualifications:\n• 4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\n• Expertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\n• Experience with Azure Databricks (or willingness to learn and adopt it quickly).\n• Hands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\n• Proficiency in SQL, NoSQL databases, and data modeling for big data processing.\n• Knowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\n• Experience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\n• Strong analytical skills, problem-solving mindset, and ability to lead complex data projects.\n• Excellent communication and stakeholder management skills.\n\nNote: This is not a contract position, this will be a permanent position with Emids.\n\nInterested candidates Can Share Your Updated Profile with details for below Email.\n\nNAME:\n\nCCTC:\n\nECTC:\n\nNotice Period:\n\nOffers in Hand :\n\nEmail ID: Ravi.chekka@emids.com",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChQeXRob24sIFB5c3BhcmssIGFuZCBBenVyZSBEYXRhYnJpY2tzKSB8fCAoNCAtIDYgWWVhcnMpIiwiY29tcGFueV9uYW1lIjoiRW1pZHMiLCJhZGRyZXNzX2NpdHkiOiJCZW5nYWx1cnUsIEthcm5hdGFrYSwgSW5kaWEiLCJodGlkb2NpZCI6Im9TMEEyREhLeU9wd0lVdnVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "8 hours ago",
         "Data Engineer"
        ],
        [
         "Senior Data Engineer",
         "Mastercard",
         "Pune, Maharashtra, India",
         "Job Title:\n\nSenior Data Engineer\n\nOverview:\n\nPosition Overview:\n\nThe Senior Data Engineer, MyMPA will be part of GBSC’s Automation & Engineering Team, responsible for implementing enhancements and periodic refreshes of an enterprise-wide data platform.\n\nThis role will also work closely with the VP of Analytics & Metrics and Director of FP&A and gather requirements for changes and enhancements to the application and contribute to the technology platform's evolution as it grows to support the rapidly expanding Mastercard business.\n\nThe ideal candidate will have hands-on development skills combined with an ability to analyze and understand end user requirements that are critical success factors within this role. This role requires the skills and desire to work as an individual contributor as well as, collaborate cross functionally with various business constituents.\n\n1. Have you ever worked on an enterprise-wide reporting solution that relied heavily on your own knowledge and abilities to build and maintain the solution?\n2. Are you constantly hungry to learn? Do you have the “growth mindset” as opposed to the “fixed mindset”?\n3. Do you love working with people, helping them, and turning their requirements into something that can make a difference?\n\nRole:\n\n• Skilled at writing performant and reliable SQL queries to source data from Centralized data repositories such as a Data Warehouse.\n• Proficient at designing and developing Relational and Multi-dimensional Databases to host the sourced data and work with Tableau and Power BI developers within and outside the team to support reporting and dashboarding needs of the organization.\n• Develop ETL workflows and macros in Alteryx to source data for projects and build high-level and detailed data validations to ensure accuracy of the sourced data\n• Contribute to initiatives aimed at automating the data extraction, application of data quality checks to ensure datasets are released on time and with 100% accuracy.\n• Develop technical components to ensure department’s compliance with audit requirements such as SOX and other statutory/audit requirements as applicable from time to time.\n• Organize and lead discussions with customers to brainstorm on data quality issues and contribute to discussions to devise business rules to address data quality issues\n• Use MS-Excel and MS-PPT to capture findings and present to customers in an easy-to-understand and impactful manner\n• Liaison with the internal groups in MasterCard Operations and Technology to ensure our solutions remain in compliance with MasterCard technical standards. Navigate O&T requirements around change management and new development.\n\nAll About You:\n\n• Strong understanding of Windows and Linux server.\n• Good understanding of SQL Server or Oracle DB.\n• Solid understanding of Essbase technology – understand how this technology works, for both BSO\nand ASO cubes.\n• Develop BSO and ASO cubes with a strong eye for performance.\n• Strong commitment to quality and error testing code you develop. Strong ability to step in and analyze the code of others on the team.\n• Be able to work within an Agile environment that is highly responsive to the business. Our team is part of the Finance organization – you must be comfortable with working as part of the business with a strong “roll up your sleeves” mentality.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6Ik1hc3RlcmNhcmQiLCJhZGRyZXNzX2NpdHkiOiJQdW5lLCBNYWhhcmFzaHRyYSwgSW5kaWEiLCJodGlkb2NpZCI6IlV1SE5zREk1eXZ2ckFfdVBBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "Data Engineer"
        ],
        [
         "Architect - Data Engineer",
         "PepsiCo",
         "Hyderabad, Telangana, India",
         "Overview\n\nProvide the job title you would like to be displayed on the job posting:\n\nData Platform Engineer – Transformation & Modernization\n\nJob Overview: A Data Platform Engineer to be a key player in our transformation and modernization programs, leading the migration of applications from legacy systems to Azure-based architectures. This role involves designing, implementing, and optimizing scalable, cloud-native data solutions using Databricks, Azure DevOps (ADO), and Agile development methodologies.\n\nAs an active contributor to code development, you will help drive automation, operational excellence, and data quality across our platforms. You will collaborate with data science and product teams to create solutions that enhance our data-driven decision-making capabilities.\n\nResponsibilities\n\nResponsibilities: • Lead the migration and modernization of data platforms, moving applications and pipelines to Azure-based solutions.\n• Actively contribute to code development in projects and services.\n• Manage and scale data pipelines from internal and external data sources to support new product launches and ensure high data quality.\n• Develop automation and monitoring frameworks to capture key metrics and operational KPIs for pipeline performance.\n• Implement best practices around systems integration, security, performance, and data management.\n• Collaborate with internal teams, including data science and product teams, to drive solutioning and proof-of-concept (PoC) discussions.\n• Develop and optimize procedures to transition data into production.\n• Define and manage SLAs for data products and operational processes.\n• Prototype and build scalable solutions for data engineering and analytics.\n• Research and apply state-of-the-art methodologies in data and Platform engineering.\n• Create and maintain technical documentation for knowledge sharing.\n• Develop reusable packages and libraries to enhance development efficiency.\n\nQualifications\n\nQualifications: • Bachelor’s degree in Computer Science, MIS, Business Management, or related field\n• 10 + years’ experience in Information Technology\n• 4 + years of Azure, AWS and Cloud technologies\n• Experience in data platform engineering, with a focus on cloud transformation and modernization.\n• Strong knowledge of Azure services, including Databricks, Azure Data Factory, Synapse Analytics, and Azure DevOps (ADO).\n• Proficiency in SQL, Python, and Spark for data engineering tasks.\n• Hands-on experience building and scaling data pipelines in cloud environments.\n• Experience with CI/CD pipeline management in Azure DevOps (ADO).\n• Understanding of data governance, security, and compliance best practices.\n• Experience working in an Agile development environment.\n• Prior experience in migrating applications from legacy platforms to the cloud.\n• Knowledge of Terraform or Infrastructure-as-Code (IaC) for cloud resource management.\n• Familiarity with Kafka, Event Hubs, or other real-time data streaming solutions.\n• Experience with lagacy RDBMS (Oracl, DB2, Teradata)\n• Background in supporting data science models in production.\n\nDoes the person hired for this job need to be based in a PepsiCo office, or can they be remote?: Employee must be based in a Pepsico office\n\nPrimary Work Location: Hyderabad HUB-IND\n\nIs this role approved for relocation?: No\n\nWould you like to initially post this job internally-only or both internally and externally?: Post both internally and externally",
         "eyJqb2JfdGl0bGUiOiJBcmNoaXRlY3QgLSBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiUGVwc2lDbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNzVXaWFLWW1Xa1V2Vk5VY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Data Engineer"
        ],
        [
         "Senior Analytics Data Engineer",
         "Okta, Inc.",
         "Bengaluru, Karnataka, India",
         "Get to know OktaOkta is The World’s Identity Company. We free everyone to safely use any technology, anywhere, on any device or app. Our flexible and neutral products, Okta Platform and Auth0 Platform, provide secure access, authentication, and automation, placing identity at the core of business security and growth.At Okta, we celebrate a variety of perspectives and experiences. We are not looking for someone who checks every single box - we’re looking for lifelong learners and people who can make us better with their unique experiences. Join our team! We’re building a world where Identity belongs to you.Senior Analytics Engineer\nWe are looking for an experienced Analytics Engineer to join Okta’s enterprise data team. This analyst will have strong background in SaaS subscription and product analytics, a passion for providing customer usage insights to internal stakeholders, and experience organizing complex data into consumable data assets. In this.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHl0aWNzIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJPa3RhLCBJbmMuIiwiYWRkcmVzc19jaXR5IjoiQmVuZ2FsdXJ1LCBLYXJuYXRha2EsIEluZGlhIiwiaHRpZG9jaWQiOiJhZHlINU5lRGFEQWwyUW1aQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "Data Engineer"
        ],
        [
         "Lead Data Engineer - Data Engineering",
         "Cencora",
         "India",
         "Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\n\nJob Details\n\nPRIMARY DUTIES AND RESPONSIBILITIES:\n• Leads design, development, and automation of scalable Analytic data engineering products leveraging Machine Learning, cloud infrastructure\n• Leads the design, build and operationalization of monitoring and tracking of data quality and data flow dynamics\n• Optimizes existing data processes and implements best-in-class data transformation capabilities\n• Leverages advanced statistical and computational methodologies to deliver insights and identify strategic opportunities\n• Implements Data Ingestion Framework and other re-usable components leveraged for delivering data pipeline and supporting ad-hoc data ingestions\n• Assists with development and storage of analytics-ready data for development of analytic deliverables\n• Recommends data products to solve business problems meeting multiple stakeholder requirements\n• Drives project planning processes, delegates non-complex tasks to junior team members\n• Mentors other team members and assists them with priority setting and issue resolution\n• Maintains data and information environment that provides consistent and accurate reporting of the client’s data\n• Leverages Machine Learning to enhance the developed solution\n• Collaborates with Business and Client representatives to target strategic analytics opportunities and define deliverable scope\n• Facilitates the resolution of issues regarding projects being worked on to include proper documentation of recommendations for those issues\n• Manages multiple initiatives in parallel partnering with other Analytics team members and conducts review meetings with stakeholders to drive prioritization effort\n• Analyzes model errors and design strategies to overcome them\n• Implements best practices for data engineering to ensure quality delivery of enterprise solutions\n• Brings expert knowledge of data visualization tools and techniques to drive business analytics and semantic data access requirements.\n• Work closely with business users, vendors, and delivery teams to understand the business requirements that drive the analysis and design of business analytics and reporting solutions.\n• Conceptualize, design, and develop data visualization solutions that synthesize data concepts into clear communications for key business stakeholders.\n• Drives business stakeholder adoption of insights-driven decision making and/or business process innovation.\n• Leads knowledge transfer around using data visualizations to business stakeholders.\n• Assist in developing best practices for data presentation and sharing across the organization.\n• Ensures data visualization standards are maintained and implemented.\n• Demonstrate an ability to reduce data to the bare minimum of what is needed to optimally communicate a message.\n• Provides technical leadership, coaching and mentoring to team members and business users.\n• Participates in POC projects and provides business analytics solutions recommendations.\n• Evaluates new visualization tools and performs research on best practices.\n• Contributes to strategic planning meetings and provides guidance and expertise on system options, risk, cost vs. benefits, and impacts on business processes and goals.\n• Has expertise in multiple technical environments and possesses business knowledge that spans multiple business areas.\n• Responsible for BI Tool administration & security functions as designated\n\n.\n\nEDUCATIONAL QUALIFICATIONS:\n\nBachelor’s Degree in Statistics, Computer Science, Information Technology or any other related discipline or equivalent related experience.\n\nPreferred Certifications:\n• Advanced Data Analytics Certifications\n• AI and ML Certifications\n• SAS Statistical Business Analyst Professional Certification\n\nWORK EXPERIENCE:\n6+ years of directly-related or relevant experience, preferably in healthcare data analytics or data engineering.\n\nWorking Hours:\n\n7PM IST to 2AM IST; Hybrid Working Model\n\nSKILLS & KNOWLEDGE:\n\nBehavioral Skills:\n• Conflict Resolution\n• Creativity & Innovation\n• Decision Making\n• Planning\n• Presentation Skills\n• Risk-taking\n\nTechnical Skills:\n• Advanced Data Visualization Techniques\n• Advanced Statistical Analysis\n• Big Data Analysis Tools and Techniques\n• Data Governance\n• Data Management\n• Data Modelling\n• Data Quality Assurance\n• Machine Learning and AI Fundamentals\n• Programming languages like SQL, R, Python\n\nTools Knowledge:\n• Business Intelligence Software like Tableau, Power BI, Alteryx, QlikSense\n• Data Visualization Tools\n• Microsoft Office Suite\n• Statistical Analytics tools (SAS, SPSS3)\n\nWhat Cencora offers\n\n​Benefit offerings outside the US may vary by country and will be aligned to local market practice. The eligibility and effective date may differ for some benefits and for team members covered under collective bargaining agreements.\n\nFull time\n\nAffiliated Companies\nAffiliated Companies: CENCORA INDIA TECHNOLOGY SERVICES PRIVATE LIMITED\n\nEqual Employment Opportunity\n\nCencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\n\nThe company’s continued success depends on the full and effective utilization of qualified individuals. Therefore, harassment is prohibited and all matters related to recruiting, training, compensation, benefits, promotions and transfers comply with equal opportunity principles and are non-discriminatory.\n\nCencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com. We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned",
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgLSBEYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiQ2VuY29yYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJHdzRzcW9FbWlPdEVnTDAwQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "3 days ago",
         "Data Engineer"
        ],
        [
         "Azure Data Engineer – Azure Databricks",
         "Aiprus Software Private Limited",
         "Bengaluru, Karnataka, India",
         "Job Title: Azure Data Engineer – Azure Databricks\n\nLocation: Bangalore, India\n\nExperience: 5 to 10 Years\n\nJob Summary:\n\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n\nKey Responsibilities:\n• Design and develop scalable data pipelines using Azure Databricks and PySpark.\n• Transform raw data into actionable insights through advanced data engineering techniques.\n• Build, deploy, and maintain machine learning models using MLlib, TensorFlow, and MLflow.\n• Optimize data integration workflows from Azure Blob Storage, Data Lake, and SQL/NoSQL sources.\n• Execute large-scale data processing using Spark Pools, fine-tuning configurations for performance and cost-efficiency.\n• Collaborate with data scientists, analysts, and business stakeholders to deliver robust data solutions.\n• Maintain and enhance Databricks notebooks and Delta Lake architectures.\n\nRequired Skills & Qualifications:\n• Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field.\n• 7+ years of experience in data engineering, with at least 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n• Strong proficiency in:\n• Python, PySpark, Pandas, NumPy, SciPy\n• Spark SQL, DataFrames, RDDs\n• Delta Lake, Databricks Notebooks, MLflow\n• Hands-on experience with:\n• Azure Data Lake, Blob Storage, Synapse Analytics\n• Excellent problem-solving and communication skills.\n• Ability to work independently and in a collaborative team environment.\n\nPreferred Qualifications:\n• Experience with CI/CD pipelines for data workflows.\n• Familiarity with data governance and security best practices in Azure.\n• Knowledge of real-time data processing and streaming technologies.",
         "eyJqb2JfdGl0bGUiOiJBenVyZSBEYXRhIEVuZ2luZWVyIOKAkyBBenVyZSBEYXRhYnJpY2tzIiwiY29tcGFueV9uYW1lIjoiQWlwcnVzIFNvZnR3YXJlIFByaXZhdGUgTGltaXRlZCIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiek5tRUxOMG5Iam5zazl6TkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 hours ago",
         "Data Engineer"
        ],
        [
         "Principle Software Engineer for Data Platform - 31866",
         "Splunk",
         "Bengaluru, Karnataka, India",
         "Splunk, a Cisco company, is building a safer and more resilient digital world with an end-to-end full stack platform made for a hybrid, multi-cloud world. Leading enterprises use our unified security and observability platform to keep their digital systems secure and reliable. Our customers love our technology, but it's our caring employees that make Splunk stand out as an amazing career destination. No matter where in the world or what level of the organization, we approach our work with kindness. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Come help organizations be their best, while you reach new heights with a team that has your back.Role SummarySplunk AppDynamics, a leader in observability solutions for both cloud and on-premises environments, empowers customers with end-to-end visibility across their entire application ecosystems, cloud, on-premises, or hybrid. Our team is dedicated to developing, maintaining, and supporting SaaS and on-premises offerings within the observability product suite.We're looking for a Senior Software Engineer to build upon our application and data platform as we continue to innovate on application observability. We move fast and iterate quickly. We are passionate about solving customers’ problems. We have ambitious goals to build best-of-its-kind products. You will help the team win in a fast-growing market. If you are passionate about innovation and embrace the challenge of working on highly scalable systems that handle large volumes of data, this position is for you.Meet the Team (Optional)Data platform is the heart of the AppDynamics architecture. We process more than 100 billion data packets per day, more than 5 peta byte of data flows through our platform. We are in 8 regions having 40 clusters of 7000 nodes. We ingest, process and store all the data that power the dashboards that our customers rely on for observing their infrastructure and their applications.What you'll get to do\n• Design and build highly scalable solutions\n• Work with a team of exceptionally capable and dedicated peers, all the way from engineering to product management and customer support\n• Work in an open environment, work together to get things done and adapt to the team's changing needs\n• Leverage technologies including Kafka, ElasticSearch, Docker, and Kubernetes across different cloud environments like AWS and Azure\n• lead critical initiatives for the organisation\nMust-have Qualifications\n• 15+ years of full-stack developer experience in designing and developing highly scalable, distributed applications, products, and services.\n• Expertise in Java programming language.\n• Strong proficiency in data structures, algorithms, threads, concurrent programming\n• Extensive knowledge of SQL and at least one relational database engine: MySQL. Hands on experience in RDS or NoSQL (Dynamo, MongoDB) is a big plus\n• Experience of building applications using microservices architecture with expertise in Dockers and Kubernetes.\n• Strong communication skills, both verbal and written. Ability to multi-task and adapt quickly to changing requirements, scope, and priorities.\n• Optimize data pipelines, storage systems, and query engines for performance and efficiency.\n• Work closely with product teams, data engineers, software developers, and product managers to deliver on business goals.\n• Mentor team members in architecture principles, coding best practices, and system design.\n• Ensure robust monitoring, logging, and alerting systems for proactive issue detection and resolution.\n• Support CI/CD processes and automate testing for data systems\n• Stay abreast of emerging trends in big data, machine learning, and distributed systems to recommend innovative solutions.\nNice-to-have QualificationsWe’ve taken special care to separate the must-have qualifications from the nice-to-haves. “Nice-to-have” means just that: Nice. To. Have. So, don’t worry if you can’t check off every box. We’re not hiring a list of bullet points–we’re interested in the whole you.\n• Added advantage of having an experience in working on Cloud Observability Space.\n• experience of other languages like python, etc\n• experience of front-end technologies\nSplunk is an Equal Opportunity EmployerSplunk, a Cisco company, is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.\n\nNote:",
         "eyJqb2JfdGl0bGUiOiJQcmluY2lwbGUgU29mdHdhcmUgRW5naW5lZXIgZm9yIERhdGEgUGxhdGZvcm0gLSAzMTg2NiIsImNvbXBhbnlfbmFtZSI6IlNwbHVuayIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiY2k4emNxSnNNanNSemNYUEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "9 days ago",
         "Data Engineer"
        ],
        [
         "Software Developer- Python",
         "BNP Paribas India Solutions",
         "India",
         "About BNP Paribas India Solutions:\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n\nAbout BNP Paribas Group:\n\nBNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n\nCommitment to Diversity and Inclusion\n\nAt BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n\nAbout Business line/Function:\n\nThe Intermediate Holding Company (“IHC”) program structured at the U.S. level across poles of activities of BNP Paribas provides guidance, supports the analysis, impact assessment and drives adjustments of the U.S. platform’s operating model due to the drastic changes introduced by the Enhanced Prudential Standards (“EPS”) for Foreign Banking Organizations (“FBOs”) finalized by the Federal Reserve in February 2014, implementing Section 165 of U.S. Dodd-Frank Act.\n\nThe IT Transversal Team is part of the Information Technology Group which works simultaneously on a wide range of projects arising from business, strategic initiatives, and regulatory changes and reengineering of existing applications to improve functionality and efficiency.\n\nJob Title:\n\nPython Developer\n\nDate:\n\nJune-25\n\nDepartment:\n\nITG- Fresh\n\nLocation:\n\nChennai, Mumbai\n\nBusiness Line / Function:\n\nFinance Dedicated Solutions\n\nReports to:\n\n(Direct)\n\nGrade:\n\n(if applicable)\n\n(Functional)\n\nNumber of Direct Reports:\n\nNA\n\nDirectorship / Registration:\n\nNA\nPosition Purpose\n\nThe Python Developer will play a critical role in building and maintaining financial applications and tools that support data processing, analysis, and reporting within a fast-paced financial services environment. This position involves developing scalable and secure systems. The developer will collaborate with business analysts, finance users/or finance BA to translate complex business requirements into efficient, high-quality software solutions.\n\nA strong understanding of financial concepts, data integrity, and regulatory compliance is essential. The detailed responsibilities are mentioned below.\n\nResponsibilities\n\nDirect Responsibilities\n\n- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n\n- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n\n- Expertise in PySpark for large-scale data processing and loading into databases.\n\n- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n\n- Strong communication skills to effectively collaborate with team members and stakeholders.\n\n- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\nTechnical & Behavioral Competencies\n\n- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n\n- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n\n- Expertise in PySpark for large-scale data processing and loading into databases.\n\n- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n\n- Strong communication skills to effectively collaborate with team members and stakeholders.\n\n- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n\n- Good analytical, problem solving, & communication skills\n\n- Engage in technical discussions and to help in improving the system, process etc\n\nNice to Have\n\n- Familiarity with Plotly and Matplotlib for data visualization of large datasets.\n\n- Skilled in API programming, handling JSON, CSV, and other unstructured data from various systems.\n\n- Familiarity with JavaScript, CSS, and HTML.\n\n- Experience with cloud architecture applications such as Dataiku or Databricks; competency with ETL tools.\n\n- Knowledge of regulatory frameworks, RISK, CCAR, and GDPR.\nSpecific Qualifications (if required)\n\nSkills Referential\n\nBehavioural Skills: (Please select up to 4 skills)\n\nAbility to collaborate / Teamwork\n\nCritical thinking\n\nAbility to deliver / Results driven\n\nCommunication skills - oral & written\n\nTransversal Skills: (Please select up to 5 skills)\n\nAnalytical Ability\n\nAbility to develop and adapt a process\n\nAbility to understand, explain and support change\n\nAbility to develop others & improve their skills\n\nChoose an item.\n\nEducation Level:\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 5 years\n\nOther/Specific Qualifications (if required)",
         "eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBEZXZlbG9wZXItIFB5dGhvbiIsImNvbXBhbnlfbmFtZSI6IkJOUCBQYXJpYmFzIEluZGlhIFNvbHV0aW9ucyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ6VTdCX1R2emRzczk0M0ZoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "7 days ago",
         "Python Developer"
        ],
        [
         "Freelance Python Developer",
         "Teqlawn",
         "Anywhere",
         "We’re looking for an experienced Python Developer (4–7 years) to build scalable web and application solutions, with opportunities to integrate AI capabilities. Experience in AI integration is a must.\n\nResponsibilities:\n• Develop scalable web and application solutions using Python, with integration of AI/ML components\n• Collaborate with clients to understand project goals and technical requirements\n• Write clean, maintainable, and well-documented code\n• Troubleshoot, debug, and optimize applications for performance and reliability\n• Ensure timely and efficient delivery of milestones and final deliverables\n• Participate in code reviews and contribute to maintaining coding standards and best practices\n• Work with relevant frameworks and libraries such as Django, Flask, FastAPI, NumPy, pandas, and scikit-learn\n\nNote: Please share the link to your portfolio along with your application.\n\nJob Types: Full-time, Contractual / Temporary, Freelance\nContract length: 2 months\n\nPay: ₹50,000.00 - ₹80,000.00 per month\n\nBenefits:\n• Work from home\n\nLocation Type:\n• Remote\n\nSchedule:\n• Day shift\n• Monday to Friday\n\nExperience:\n• Python Development: 4 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlRlcWxhd24iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiY20xcUU4aHhjbFQ3dnVyT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "1 day ago",
         "Python Developer"
        ],
        [
         "Python Developer — Full-Time | 1-2 years Exp | In-Office | Bangalore",
         "SERP Hawk",
         "India",
         "\uD83D\uDE80 We’re Hiring: Python Developer\n\nSERP Hawk is looking for a talented Python Developer to join our dynamic team in Bangalore!\n\n\uD83C\uDF1F About Us\n\nSERP Hawk is a leading digital marketing and technology solutions company based in Bangalore. We specialize in building robust backend systems, scalable applications, and AI-powered solutions for clients across various industries.\n\n\uD83D\uDCCD Location:B-1, Bannerghatta Slip Road, KEB Colony, New Gurappana Palya, 1st Stage, BTM Layout 1, Bengaluru, Karnataka 560029\n\n\uD83C\uDF10 Website: www.serphawk.com\n\n\uD83D\uDCBC What You’ll Do\n• Design and develop scalable backend architectures.\n• Write clean, efficient Python code.\n• Integrate APIs and databases.\n• Implement CI/CD pipelines and automated tests.\n• Ensure high performance, security, and reliability.\n\n✅ What We’re Looking For\n\n✔️ 1–2 years of experience in Python development.\n\n✔️ Proficiency in frameworks like Django, Flask, or FastAPI.\n\n✔️ Strong understanding of APIs and databases.\n\n✔️ Experience with CI/CD tools and best practices.\n\n✔️ Excellent problem-solving skills and a collaborative mindset.\n\n\uD83D\uDCA1 Nice to Have\n\n⭐ Experience with AI/chatbots.\n\n⭐ Knowledge of cloud services and containerization.\n\n\uD83D\uDCB0 Salary\n• ₹20,000 – ₹25,000 per month (based on skills and experience).\n\n\uD83D\uDCCC Additional Details\n\n\uD83D\uDCBC This is a full-time, in-office role in Bangalore.\n\n\uD83C\uDFE2 Candidates must report to the office daily.\n\n\uD83C\uDF10 Should be flexible to work and attend meetings or calls as per client time zones.\n\n✨ Apply now and grow with us!",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAlCBGdWxsLVRpbWUgfCAxLTIgeWVhcnMgRXhwIHwgSW4tT2ZmaWNlIHwgQmFuZ2Fsb3JlIiwiY29tcGFueV9uYW1lIjoiU0VSUCBIYXdrIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjNBMWtiUmowZFhsRVZsWnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 hours ago",
         "Python Developer"
        ],
        [
         "SQL + Python",
         "Wissen Technology",
         "India",
         "Wissen Technology is Hiring for SQL With Python\n\nAbout Wissen Technology:\n\nWissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges.\n\nRole Overview:\n\nWe are looking for a skilled and detail-oriented candidate with a strong foundation in SQL, Python, and data processing techniques. The ideal candidate is passionate about transforming raw data into meaningful insights and has hands-on experience across the data pipeline—from data wrangling to visualization.\n\nExperience: 3-7 Years\n\nLocation: Bengaluru\n\nRequired Skills:\n• Strong experience with SQL (e.g., joins, subqueries, CTEs, window functions).\n• Proficiency in Python for data manipulation (e.g., pandas, NumPy).\n• Experience working with relational databases like MySQL, PostgreSQL, SQL Server, or Oracle.\n• Hands-on experience in data wrangling, cleaning, and feature engineering.\n• Understanding of ETL processes and tools.\n• Familiarity with version control systems like Git.\n• Knowledge of data visualization techniques and tools.\n• Strong problem-solving and analytical skills.\n\nThe Wissen Group was founded in the year 2000. Wissen Technology, a part of Wissen Group, was established in the year 2015. Wissen Technology is a specialized technology company that delivers high-end consulting for organizations in the Banking & Finance, Telecom, and Healthcare domains. We help clients build world class products.\n\nWe offer an array of services including Core Business Application Development, Artificial Intelligence & Machine Learning, Big Data & Analytics, Visualization & Business Intelligence, Robotic Process Automation, Cloud Adoption, Mobility, Digital Adoption, Agile & DevOps, Quality Assurance & Test Automation.\n\nOver the years, Wissen Group has successfully delivered $1 billion worth of projects for more than 20 of the Fortune 500 companies. Wissen Technology provides exceptional value in mission critical projects for its clients, through thought leadership, ownership, and assured on-time deliveries that are always ‘first time right’.\n\nThe technology and thought leadership that the company commands in the industry is the direct result of the kind of people Wissen has been able to attract. Wissen is committed to providing them with the best possible opportunities and careers, which extends to providing the best possible experience and value to our clients.\n\nWe have been certified as a Great Place to Work® company for two consecutive years (2020-2022) and voted as the Top 20 AI/ML vendor by CIO Insider. Great Place to Work® Certification is recognized world over by employees and employers alike and is considered the ‘Gold Standard’. Wissen Technology has created a Great Place to Work by excelling in all dimensions - High-Trust, High-Performance Culture, Credibility, Respect, Fairness, Pride and Camaraderie.\n\nWebsite: www.wissen.com\n\nLinkedIn: https://www.linkedin.com/company/wissen-technology\n\nWissen Leadership: https://www.wissen.com/company/leadership-team/\n\nWissen Live: https://www.linkedin.com/company/wissen-technology/posts/feedView=All\n\nWissen Thought Leadership: https://www.wissen.com/articles/\n\nEmployee Speak:\n\nhttps://www.ambitionbox.com/overview/wissen-technology-overview\n\nhttps://www.glassdoor.com/Reviews/Wissen-Infotech-Reviews-E287365.htm\n\nGreat Place to Work:\n\nhttps://www.wissen.com/blog/wissen-is-a-great-place-to-work-says-the-great-place-to-work-institute-india/\n\nhttps://www.linkedin.com/posts/wissen-infotech_wissen-leadership-wissenites-activity-6935459546131763200-xF2k\n\nAbout Wissen Interview Process:\n\nhttps://www.wissen.com/blog/we-work-on-highly-complex-technology-projects-here-is-how-it-changes-whom-we-hire/\n\nLatest in Wissen in CIO Insider:\n\nhttps://www.cioinsiderindia.com/vendor/wissen-technology-setting-new-benchmarks-in-technology-consulting-cid-1064.html",
         "eyJqb2JfdGl0bGUiOiJTUUwgKyBQeXRob24iLCJjb21wYW55X25hbWUiOiJXaXNzZW4gVGVjaG5vbG9neSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJITW5vZUwxWlF1QWVma0hlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "7 hours ago",
         "Python Developer"
        ],
        [
         "Python Developer Full time",
         "Variance Technologies Private Limited",
         "Anywhere",
         "Job Opportunity: Python Developer at Variance Technologies Private Limited!\n\nRole: Python Developer\n\nDuration: 1 Months\n\nLocation: Hybrid / Remote\n\nResponsibilities:\n\nCollaborate with our development team to develop and maintain Python-based applications tailored for finance-related tasks\n\nImplement object-oriented programming principles to ensure the scalability and maintainability of codebase\n\nGain hands-on experience with Pandas for data manipulation and analysis within finance contexts\n\nSupport integration of REST and WebSocket APIs for seamless communication with financial data providers and platforms\n\nAssist in automating financial modeling and analysis workflows using Python, including Excel automation for enhanced efficiency\n\nRequirements:\n\nCurrently pursuing or recently completed a degree in Computer Science, Finance, or a related field\n\nBasic proficiency in Python programming language, with a strong willingness to learn and grow\n\nExceptional attention to detail and proactive attitude towards problem-solving\n\nGenuine interest in the intersection of finance and technology\n\nBonus Skills:\n\nFamiliarity with fundamental financial concepts and markets\n\nExposure to Python libraries such as Pandas, NumPy, or SciPy\n\nDemonstrated interest in financial data analysis and visualization techniques\n\nBasic understanding of REST and WebSocket APIs\n\nPerks:\n\nHands-on experience working on real-world projects at the forefront of finance and technology\n\nMentorship and guidance from seasoned professionals in the field\n\nNetworking opportunities with industry experts to expand your professional connections\n\nFlexible scheduling to accommodate academic commitments\n\nPotential for transition to a full-time position based on exceptional performance and availability\n\nReady to kickstart your career with a prestigious 1-month internship at Variance Technologies Private Limited? Apply now by sending your resume and a tailored cover letter expressing your interest and qualifications to careers@variancefintech.com. Join our team and embark on a journey towards shaping the future of finance through technology!\n\nVariance Technologies Private Limited is committed to fostering a diverse and inclusive workplace where all individuals are empowered to thrive. We welcome applicants from all backgrounds and identities to apply.\n\nJob Type: Full-time\n\nPay: From ₹35,000.00 per month\n\nBenefits:\n• Work from home\n\nSchedule:\n• Monday to Friday\n\nEducation:\n• Bachelor's (Preferred)\n\nExperience:\n• Python: 1 year (Preferred)\n• total work: 1 year (Preferred)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIEZ1bGwgdGltZSIsImNvbXBhbnlfbmFtZSI6IlZhcmlhbmNlIFRlY2hub2xvZ2llcyBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiaXZCN3JUQTN5YnkxRzV2MUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "5 days ago",
         "Python Developer"
        ],
        [
         "Python Developer – AI & LLM Integrations",
         "Discover WebTech Private Limited",
         "India",
         "We are hiring a skilled Python Developer with a minimum of 3 years of experience, who has hands-on expertise in developing AI modules, building intelligent agents using LLMs, and working with cutting-edge frameworks such as LangChain, LangGraph, and LangSmith.\n\nThe ideal candidate should be capable of designing, developing, and deploying backend services as well as intelligent AI-driven tools and systems.\n\nKey Responsibilities\n• Design and implement intelligent agents using LLM-based frameworks like LangChain, LangGraph, and LangSmith.\n• Build backend systems using Python (Django, FastAPI, or Flask).\n• Develop and integrate APIs, third-party tools, and cloud services.\n• Create AI modules that interact with knowledge bases, APIs, and perform multi-step reasoning.\n• Implement prompt engineering, memory chains, and agent behavior logic.\n• Collaborate with cross-functional teams to deliver robust AI features.\n• Optimize code for scalability, performance, and reliability.\n\nRequired Skills and Qualifications\n• 3+ years of hands-on experience with Python.\n• Proficiency in LangChain, LangGraph, or LangSmith.\n• Strong experience in working with LLMs like OpenAI, Cohere, or Anthropic.\n• Deep understanding of prompt engineering and agent orchestration.\n• Experience with APIs, JSON, and external integrations.\n• Knowledge of data storage systems (PostgreSQL, MongoDB).\n• Familiarity with Docker, Git, and CI/CD tools.\n• Excellent problem-solving and debugging skills.\n\nPreferred Qualifications\n• Knowledge of vector databases (e.g., Pinecone, Weaviate, FAISS).\n• Experience with microservices, asynchronous programming, and message queues (Celery, RabbitMQ).\n• Familiarity with frontend technologies (React.js or Vue.js) is a plus.\n• Exposure to cloud platforms such as AWS, GCP, or Azure.\n\nJob Types: Full-time, Permanent\n\nPay: ₹30,000.00 - ₹70,000.00 per month\n\nBenefits:\n• Health insurance\n\nSchedule:\n• Day shift\n\nWork Location: In person",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBBSSBcdTAwMjYgTExNIEludGVncmF0aW9ucyIsImNvbXBhbnlfbmFtZSI6IkRpc2NvdmVyIFdlYlRlY2ggUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Im1JUlJqRWJSV0pCSkdQR0VBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "3 days ago",
         "Python Developer"
        ],
        [
         "Full Stack Developer (Python / React JS)",
         "Hitachi Careers",
         "India",
         "Our Company\n\nWe're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n\nOur group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n\nImagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n\nThe team\n\nWe are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n\nThe role: Full Stack Developer/Specialist\n\nResponsibilities:\n• Design, develop, and maintain applications.\n• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n• Optimise applications for performance, scalability, and user experience.\n• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n• Provide training and support to end-users and IT staff on functionalities and best practices.\n• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n• Participate in project planning, execution, and post-implementation support.\n• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n• What you'll bring\nQualifications:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n• Expertise in development and customisation.\n• Proficiency in Python and React JS\n• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n• Experience with RESTful APIs and web services.\n• Excellent problem-solving skills and attention to detail.\n• Strong communication and collaboration skills.\n• Ability to work independently and as part of a team in a fast-paced environment.\n• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n• Prior experience in building AI applications is a plus.\n• Prior experience with automation tools like UIPath is a plus.\n• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\nPreferred Skills\n• Certification in\n• Experience with other low-code/no-code platforms.\n• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n• Experience with Agile development methodologies.\nAbout us\n\nWe're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n\n#LI-MS3\n\nChampioning diversity, equity, and inclusion\n\nDiversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n\nHow we look after you\n\nWe help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\nWe're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",
         "eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "14 days ago",
         "Python Developer"
        ],
        [
         "Python Back-End Developer",
         "Goodyear",
         "India",
         "Location: IN - Hyderabad Telangana\n\nGoodyear Talent Acquisition Representative: M Bhavya Sree\n\nSponsorship Available: No\n\nRelocation Assistance Available: No\n\nDuties and Responsibilities:\n\n• Develop and support Data Driven applications\n\n• Help design and develop back-end services and APIs for data-driven applications and simulations.\n\n• Work with our technical partners to collaborate on system requirements and data integration needs for our new applications.\n\n• Support the deployment and scaling of new back-end technologies and cloud-native architectures within the organization.\n\n• Work closely with our data scientists to support model deployment into production environments.\n\n• Develop and maintain server-side components for digital tools and products using Python or other modern back-end technologies and frameworks. Build scalable, secure, and efficient services that support a seamless experience across multiple platforms.\n\n• Design, implement, and maintain robust database systems (SQL and NoSQL), ensuring high availability and performance for critical applications.\n\n• Contribute to DevOps practices including CI/CD pipelines, infrastructure as code, containerization (Docker), and orchestration (Kubernetes).\n\n• Learn about the tire industry and tire manufacturing processes from subject matter experts.\n\n• Be a part of cross-functional teams working together to deliver impactful results.\n\nSkills Required:\n\n• Significant experience in server-side development using Python\n\n• Strong understanding of RESTful API design, microservices architecture, and service-oriented design\n\n• Experience with relational and non-relational databases such as PostgreSQL, MySQL, MongoDB, or DynamoDB\n\n• Application of software design skills and methodologies (algorithms, data structures, design patterns, software architecture and testing)\n\n• Hands-on experience working with cloud platforms such as AWS, Microsoft Azure, or Google Cloud Platform\n\n• Good teamwork skills - ability to work in a team environment and deliver results on time.\n\n• Strong communication skills - capable of conveying information concisely to diverse audiences.\n\n• Exposure to DevOps practices including CI/CD pipelines (e.g., GitHub Actions, Jenkins), containerization (e.g., Docker), and orchestration tools (e.g., Kubernetes)\n\n• Familiarity with front-end technologies like React, HTML, CSS, and JavaScript for API integration purposes\n\nGoodyear is an Equal Employment Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to that individual's race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender identity, age, physical or mental disability, ethnicity, citizenship, or any other characteristic protected by law.\n\nGoodyear is one of the world’s largest tire companies. It employs about 68,000 people and manufactures its products in 53 facilities in 20 countries around the world. Its two Innovation Centers in Akron, Ohio and Colmar-Berg, Luxembourg strive to develop state-of-the-art products and services that set the technology and performance standard for the industry. For more information about Goodyear and its products, go to www.goodyear.com/corporate\n\n#Li-Hybrid",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gQmFjay1FbmQgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiR29vZHllYXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoid0VrdmdmMVREWEhlTFhuQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Python Developer"
        ],
        [
         "Generative AI & Backend Developer(python)",
         "Intellypod",
         "Anywhere",
         "Job Description (JD) For Gen Ai with Python:\n\nWe're Hiring: GenAI & Backend Developer (Python)\n\nWork Location: Remote (Work From Home)\n\nExperience: 2+ Years\n\nImmediate Joiners Preferred\n\nCompany: IntellyPod\n\nApply at: hrd@intellypod.com | hr@intellypod.com\n\nAbout the Role:\n\nIntellyPod is looking for a passionate Backend Developer with hands-on experience in GenAI, LLMs, and Python. If you’re excited about building scalable APIs, integrating AI models, and working with the latest in backend and GenAI technologies — we’d love to connect!\n\nKey Responsibilities:\n\n· Develop and maintain Python-based backend services.\n\n· Design and implement RESTful APIs.\n\n· Integrate GenAI/LLM solutions into applications.\n\n· Manage and optimize SQL/NoSQL databases.\n\n· Collaborate with cross-functional tech teams.\n\nMust-Have Skills:\n\n· 2+ years of experience in backend development (Python).\n\n· Experience with GenAI and Large Language Models (e.g., GPT, LLaMA).\n\n· Strong knowledge of REST APIs and database design.\n\n· Familiarity with Git and backend architecture best practices.\n\nNeed to Have:\n\n· Experience with AWS/GCP/Azure.\n\n· Docker, Kubernetes, or CI/CD exposure.\n\n· Familiarity with vector databases (e.g., Pinecone, FAISS).\n\n· Prompt engineering or LLM fine-tuning knowledge.\n\nWhy Join Us?\n\n· 100% Remote – Flexible work setup\n\n· Work on next-gen AI products\n\n· Fast-growing, collaborative tech team\n\n· Opportunity to innovate with emerging AI tools\n\nReady to build the future with us? Send your resume to: hrd@intellypod.com | hr@intellypod.com #GenAI #BackendDeveloper #PythonJobs #LLM #RemoteJob #HiringNow #IntellyPod\n\nJob Type: Full-time\n\nPay: Up to ₹70,000.00 per month\n\nLocation Type:\n• Remote\n\nApplication Question(s):\n• Are an immediate joiner -\n\nAre on notice period if yes [Then how many days]\n• Write YES or NO\n\n1) Need to ask have you worked on LLM based project -\n\n2) Have you worked on chatbot types apps -\n\n3) Have you strong knowleged of OOps and Python basic -\n\n4) Have you knowledge of Rest APi development -\n\nExperience:\n• 5G: 3 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJHZW5lcmF0aXZlIEFJIFx1MDAyNiBCYWNrZW5kIERldmVsb3BlcihweXRob24pIiwiY29tcGFueV9uYW1lIjoiSW50ZWxseXBvZCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiI4ekp1Sk9ZUFlTbWtOcFRuQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "5 hours ago",
         "Python Developer"
        ],
        [
         "Junior Python Developer",
         "Dehazelabs",
         "Anywhere",
         "Location: Onsite, Kokapet, Hyderabad, Telangana.\n\nJob Type: Full-Time\n\nAbout Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n\nResponsibilities:\n• Develop and maintain Python applications and services\n• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n• Write clean, efficient, and well-documented code\n• Troubleshoot, debug, and optimize applications for performance and scalability\n• Participate in code reviews and maintain coding standards\n• Stay up-to-date with the latest industry trends and technologies\n\nRequirements:\n• Ability to code in Python and SQL\n• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n• Knowledge of version control systems (e.g., Git)\n• Excellent problem-solving skills and attention to detail\n• Strong communication and teamwork abilities.\n\nPreferred Qualifications:\n• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n• Knowledge of RESTful APIs and microservices architecture.",
         "eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Python Developer"
        ],
        [
         "Etl Developer",
         "Vivid Resourcing",
         "Bilaspur, Chhattisgarh, India",
         "Job Title:\nData Engineer / ETL Developer\n\nLocation:\nUS, remote from India\n\nDepartment:\nIT / Data & Analytics\n\nReports To:\nHead of Data & Analytics\n\nEmployment Type:\nContract, 12 months\n\nPay:\n$28-35 per hour payrolled or self-employed\n\nAbout the Role\n\nWe are seeking a Data Engineer / ETL Developer to help design, build, and maintain data pipelines that support our internal reporting and analytics needs. This role will focus on integrating data from systems such as ERP, MES , and production databases into a centralized data platform to enable reliable and timely insights across the business. You will work alongside BI developers, IT staff, and business users to ensure the smooth flow of data and contribute to the foundation of a modern business intelligence environment.\n\nKey Responsibilities\n\nETL & Data Pipeline Development\n• Develop and maintain ETL/ELT pipelines to move data from source systems (ERP, MES, SQL databases, flat files) into our centralized platform.\n• Use tools such as Azure Data Factory , SSIS , or similar to orchestrate and automate data workflows.\n\nData Modeling & Integration\n• Assist in designing and building data models and data marts optimized for Power BI dashboards.\n• Support the creation and maintenance of dataflows and datasets in Power BI Service.\n\nData Quality & Documentation\n• Implement data validation, transformation, and cleansing logic to ensure high-quality, reliable data.\n• Document data processes, business rules, and data mappings to support knowledge sharing and governance.\n\nCross-Functional Collaboration\n• Work with internal teams (production, quality, operations, finance) to understand data requirements and reporting needs.\n• Collaborate with BI developers to ensure the data pipeline supports efficient and user-friendly reporting.\n\nRequired Qualifications\n• Bachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\n• 2+ years of experience in data engineering or ETL development roles.\n• Solid experience with SQL and ETL tools (e.G., Azure Data Factory , SSIS , Informatica , etc.).\n• Familiarity with Power BI , including working with datasets, dataflows, or basic DAX.\n• Understanding of data integration, transformation, and warehousing concepts.\n• Experience working with or integrating manufacturing systems (ERP, MES) is a strong plus.\n\nPreferred Skills\n• Experience with cloud data platforms (especially Microsoft Azure ).\n• Exposure to Python or scripting for automation.\n• Familiarity with data governance and documentation practices.\n• Experience with manufacturing environments or industrial data is beneficial.\n\nSoft Skills\n• Strong attention to detail and a logical, structured approach to problem-solving.\n• Willingness to learn and grow in a fast-paced environment.\n• Good communication and collaboration skills across technical and non-technical teams.\n• Proactive and solutions-oriented mindset.",
         "eyJqb2JfdGl0bGUiOiJFdGwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiVml2aWQgUmVzb3VyY2luZyIsImFkZHJlc3NfY2l0eSI6IkJpbGFzcHVyLCBDaGhhdHRpc2dhcmgsIEluZGlhIiwiaHRpZG9jaWQiOiJueDl3cW1oMXpfcnBSX2dFQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "14 hours ago",
         "ETL Developer"
        ],
        [
         "Senior ETL and Backend Developer (Salesforce)",
         "S&P Global",
         "Hyderabad, Telangana, India (+1 other)",
         "About the Role:\n\nGrade Level (for internal use):\n10\n\nTitle: Senior ETL and Backend Developer (Salesforce)\n\nJob Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n\nThe Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n\nThe team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n\nThe Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n\nResponsibilities:\n• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\nMonitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\nQualifications:\n\nBasic Qualifications:\n• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n• A minimum of 8+ years of experience in software engineering & Architecture.\n• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n• A minimum of 3+ years of Salesforce development, administration/Integration.\n• Proficiency in Informatica PowerCenter and other ETL tools.\n• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n• Experience with Salesforce integration and administration.\n• Proficiency in backend development languages (e.g., Java, Python, C#).\n• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n• Excellent problem-solving skills and attention to detail.\n• Ability to work independently and as part of a team.\n• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n\nPreferred Qualifications:\n• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n• Experience with other ETL tools and data integration platforms.\n• Informatica Certified Professional\nSalesforce Certified Administrator or Developer\n• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n• Excellent problem solving, analytical and technical troubleshooting skills.\n• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat’s In It For You?\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nWe're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n\nOur benefits include:\n• Health & Wellness: Health care coverage designed for the mind and body.\n• Flexible Downtime: Generous time off helps keep you energized for your time on.\n• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring and Opportunity at S&P Global:\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\n-----------------------------------------------------------\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n-----------------------------------------------------------\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n\nJob ID: 316835\nPosted On: 2025-06-03\nLocation: Hyderabad, Telangana, India",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjJlUWpKemNOclM4MXR6QzhBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "30 days ago",
         "ETL Developer"
        ],
        [
         "Senior Etl Developer",
         "Vivid Resourcing",
         "Bilaspur, Chhattisgarh, India",
         "Job Title:\nSenior Data Engineer / ETL Developer\n\nLocation:\nUS, from India\n\nDepartment:\nIT / Data & Analytics\n\nReports To:\nDirector of Data & Analytics\n\nEmployment Type:\nContract, 12 months\n\nPay:\n$30-38 per hour payrolled or self-employed\n\nAbout the Role\n\nWe are seeking a skilled and motivated Senior Data Engineer / ETL Developer to lead the design and implementation of end-to-end data integration solutions powering this new platform. You will work closely with IT, business stakeholders, and BI developers to build scalable data pipelines, robust data models, and a high-performance reporting environment centered around Power BI.\n\nThis role is a key contributor in shaping our data infrastructure and delivering timely, accurate insights across the organization—from the shop floor to the boardroom.\n\nKey Responsibilities\n\nData Engineering & Integration\n• Design, build, and maintain scalable ETL/ELT pipelines to extract, transform, and load data from diverse manufacturing systems (ERP, MES, IoT sensors, SQL databases, flat files, APIs).\n• Develop and manage data workflows using tools such as Azure Data Factory, SQL Server Integration Services (SSIS), or other modern data orchestration platforms.\n\nPlatform & Architecture Support\n• Contribute to the design and architecture of the new internal analytics platform, ensuring flexibility, scalability, and cost-efficiency.\n• Design and optimize data lake and data warehouse solutions using cloud-native tools (preferably Microsoft Azure).\n\nPower BI Enablement\n• Build and optimize semantic data models, dataflows, and datasets for use in Power BI.\n• Collaborate with Power BI developers and business analysts to ensure data models meet reporting requirements, including DAX performance tuning and efficient data structure design.\n\nData Governance & Quality\n• Implement data validation, cleansing, and monitoring processes to ensure data quality, reliability, and accuracy.\n• Contribute to data governance policies, including documentation, lineage tracking, and security controls.\n\nCollaboration & Mentorship\n• Work closely with cross-functional teams (IT, production, quality, finance, supply chain) to gather data requirements and deliver meaningful solutions.\n• Provide technical leadership and mentor junior developers or analysts in best practices for data engineering and analytics.\n\nRequired Qualifications\n• Bachelor’s or Master’s degree in Computer Science, Information Systems, Engineering, or a related field.\n• 5+ years of experience in data engineering, ETL development, or database architecture roles.\n• Proven experience with ETL tools such as Azure Data Factory, SSIS, or Informatica.\n• Advanced SQL skills, including performance tuning, stored procedures, and query optimization.\n• Strong experience with Power BI, including DAX, data modeling, and dataset optimization.\n• Experience with data warehousing (e.G., Azure Synapse, SQL Server, Snowflake) and cloud platforms (preferably Azure).\n• Understanding of manufacturing systems and data (e.G., ERP, MES, shop floor data, SCADA, historians).\n\nPreferred Skills\n• Experience with Python or other scripting languages for automation and data manipulation.\n• Familiarity with time-series data and integration from IoT or edge devices.\n• Knowledge of DevOps practices, CI/CD for data pipelines, and version control using Git.\n• Exposure to data governance frameworks and tools like Purview, Alation, or Collibra.\n• Power BI Service administration experience and integration with Power Platform (e.G., Power Automate, Power Apps)\n\nKey Competencies\n• Strong analytical and problem-solving skills with attention to detail.\n• Excellent communication skills, with the ability to bridge technical and business domains.\n• Self-starter with the ability to lead initiatives and work independently or collaboratively.\n• A passion for continuous improvement and innovation in a manufacturing setting.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRXRsIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlZpdmlkIFJlc291cmNpbmciLCJhZGRyZXNzX2NpdHkiOiJCaWxhc3B1ciwgQ2hoYXR0aXNnYXJoLCBJbmRpYSIsImh0aWRvY2lkIjoiMnQxYXo2bnUzRU1abUlLLUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "14 hours ago",
         "ETL Developer"
        ],
        [
         "ETL Developer – IBM DataStage",
         "Tata Consultancy Services",
         "Hyderabad, Telangana, India",
         "Job Title: ETL Developer – IBM DataStage\n\nExperience: 5 to 10 years\n\nLocation: Hyderabad, Chennai, Mumbai, Bangalore, Ahmedabad, Indore\n\nEmployment Type: Full-time\n\nJob Summary:\n\nWe are seeking a skilled ETL Developer with hands-on experience in IBM DataStage to join our dynamic data engineering team. The ideal candidate will have a strong background in ETL development, data warehousing concepts, and performance optimization. This role involves designing and implementing robust ETL solutions, collaborating with cross-functional teams, and ensuring high data quality and integrity.\n\nKey Responsibilities:\n• Design, develop, and implement ETL processes using IBM DataStage.\n• Work across the full project lifecycle including low-level design, development, testing, and deployment.\n• Develop parallel jobs using various DataStage stages such as Copy, Join, Merge, Lookup, Funnel, Filter, Sort, Remove Duplicates, Aggregator, Change Capture, and Transformer.\n• Integrate data from heterogeneous sources including Oracle, SQL Server, and flat files.\n• Optimize and troubleshoot existing ETL processes for performance improvements.\n• Collaborate with data analysts and architects to understand business requirements and translate them into technical solutions.\n• Ensure data quality and integrity across multiple data sources.\n• Create and maintain technical documentation for ETL processes.\n• Participate in code reviews and adhere to ETL best practices.\n• Work in Agile environments and use tools like JIRA for tracking tasks and issues.\n• Demonstrate strong problem-solving skills and the ability to troubleshoot technical issues effectively.\n• Understand and support operational requirements as part of business delivery.\n\nRequired Skills:\n• Strong experience with IBM DataStage for ETL development and migration.\n• Solid understanding of database and data warehousing concepts.\n• Proficiency in SQL and UNIX.\n• Experience working with large datasets and complex data transformations.\n• Familiarity with Agile methodologies and tools like JIRA.\n• Excellent communication and collaboration skills.",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIOKAkyBJQk0gRGF0YVN0YWdlIiwiY29tcGFueV9uYW1lIjoiVGF0YSBDb25zdWx0YW5jeSBTZXJ2aWNlcyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoibkdQWmp0dFNpdlVKUWVEYkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "7 days ago",
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "Insight Global",
         "Hyderabad, Telangana, India",
         "We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\nResponsibilities:\nLead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\nCollaborate with developers to ensure a smooth transition and integration of the upgraded system.\nEnhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\nProvide support and guidance to the ETL development lead.\nEngage with end-users to ensure their needs are met during and after the upgrade process.\nUtilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\nRequirements:\nMinimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\nProven experience with SQL Server upgrades, particularly from 2012 to 2022.\nStrong SQL Server skills, including building tables, stored procedures, views, and functions.\nExperience with .NET development is highly desirable to be leveraged for other projects\nExcellent problem-solving skills and attention to detail.\nAbility to work effectively in a team environment and communicate with end-users.\nPreferred Qualifications:\nExperience with SQL Server 2016.\nExperience with Oracle\nFamiliarity with the latest features and enhancements in SQL Server 2022.\nStrong understanding of database architecture and migration strategies.\n\nWe are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ikw2UEZlVW5YOU90VExsNnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "sonataOne",
         "Hyderabad, Telangana, India",
         "Real-time data Ingestion, Streaming data, Kafka, AWS Cloud streaming tools, ETL, Semi-structured data formats like JSON, XML\n\nTools: Talend, Kafka, AWS Event Bridge, Lamda and and Strong SQL & Python",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoic29uYXRhT25lIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "EPAM - ETL Developer - SSIS/SSRS",
         "EPAM Systems India Private Limited",
         "Hyderabad, Telangana, India",
         "Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n\nExperience : 5+ Years\n\nLocation : Hyderabad\n\nJob Description :\n\nWe are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n\nYou will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n\nKey Responsibilities :\n\n- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n\n- Develop, deploy, and maintain SSIS packages for ETL processes.\n\n- Work on data warehouse design, development, and maintenance.\n\n- Ensure high performance and reliability of data integration workflows.\n\n- Troubleshoot SQL queries and ETL issues; optimize database performance.\n\n- Collaborate with cross-functional teams to define data solutions.\n\n- Work with complex data structures including JSON and XML.\n\n- Understand and implement different data models (star, snowflake, etc.).",
         "eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IkVQQU0gU3lzdGVtcyBJbmRpYSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InZ6RDdVNWpsc2hzYS03eW5BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "ETL Developer"
        ],
        [
         "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location",
         "Fiserv",
         "India",
         "Calling all innovators – find your future at Fiserv.\n\nWe’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n\nJob Title\n\nETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n\nWork Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n• Participates in client kickoff activities including requirements and data gathering sessions.\n• Ability to analyze and migrate incoming external data into Fiserv solutions.\n• Strong working knowledge of ETL processes and best practices.\n• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n• Documents business processes and identifies opportunities for process redesign.\n• Reports system defects and identifies opportunities for system enhancements.\n• Performs as a technical consulting resource for new clients during the implementation process.\n• Supports several project managers during the pre and post go-live activities\n• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n• Provides technical and analytical guidance to the project team.\n• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n• The position requires working in shifts, 2 PM to 11 PM IST\n\nWhat you will need to have:\n• Minimum of 5 – 10 years of experience in software development.\n• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n• Self-starter who can work effectively, both independently and in a team environment.\n• Strong analytical, organizational, and problem-solving skills.\n• Excellent verbal and written communication skills.\n• Ability to maintain a professional attitude and demeanor in high pressure situations.\n• Ability to multi-task and manage multiple projects simultaneously.\n• Weekend/evening availability and support (10% - 25%).\n\nWhat would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n\nThank you for considering employment with Fiserv. Please:\n• Apply using your legal name\n• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n\nWhat you should know about us:\n\nFiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n\nOur commitment to Diversity and Inclusion:\n\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n\nWarning about fake job posts:\n\nPlease be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n\nAny communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n\nIf you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "ETL Developer/Senior Consultant Specialist",
         "HSBC",
         "India",
         "Some careers shine brighter than others.If you're looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.HSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.We are currently seeking an experienced professional to join our team in the role of Senior Consultant Specialist.In this role, you will:- Communicating effectively with senior stakeholders.- Work with a team of technologists, ensuring prioritization of tasks and supporting removal of blockers.- Working on burning the product backlog and oversee product demo's- Understanding complex business requirements and new technologies and understand how to influence teams to adopt new practice and embrace automation.- Managing of risks and issues\n\nRequirements\n• name : HSBC\n• location : India, IN",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyL1NlbmlvciBDb25zdWx0YW50IFNwZWNpYWxpc3QiLCJjb21wYW55X25hbWUiOiJIU0JDIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRIYmhtVUVOWmF3OGh3Q0lBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "ETL Developer"
        ],
        [
         "Senior Informatica Developer",
         "EverestDX Inc",
         "Hyderabad, Telangana, India",
         "About the Company:\n\nEverest DX – We are a Digital Platform Services company, headquartered in Stamford. Our Platform/Solution includes Orchestration, Intelligent operations with BOTs’, AI-powered analytics for Enterprise IT. Our vision is to enable Digital Transformation for enterprises to deliver seamless customer experience, business efficiency and actionable insights through an integrated set of futuristic digital technologies.\n\nDigital Transformation Services - Specialized in Design, Build, Develop, Integrate, and Manage cloud solutions and modernize Data centers, build a Cloud-native application and migrate existing applications into secure, multi-cloud environments to support digital transformation. Our Digital\n\nPlatform Services enable organizations to reduce IT resource requirements and improve productivity, in addition to lowering costs and speeding digital transformation.\n\nDigital Platform - Cloud Intelligent Management (CiM) - An Autonomous Hybrid Cloud Management Platform that works across multi-cloud environments. helps enterprise Digital Transformation get most out of the cloud strategy while reducing Cost, Risk and Speed.\n\nTo know more please visit: http://www.everestdx.com\n\nResponsibilities:\n• Candidate should hands-on experience on ETL and SQL.\n• Design, develop, and optimize ETL workflows using Informatica PowerCenter.\n• Implement cloud-based ETL solutions using Informatica IDMC and IICS.\n• Should have expertise on all transformations in Power Center and IDMC/IICS.\n• Should have experience or knowledge on the PC to IICS migration using CDI PC tool or some other tool.\n• Lead data migration projects, transitioning data from on-premise to cloud environments.\n• Write complex SQL queries and perform data validation and transformation.\n• Conduct detailed data analysis to ensure accuracy and integrity of migrated data.\n• Troubleshoot and optimize ETL processes for performance and error handling.\n• Collaborate with cross-functional teams to gather requirements and design solutions.\n• Create and maintain documentation for ETL processes and system configurations.\n• Implement industry best practices for data integration and performance tuning.\n\nRequired Skills:\n• Hands-on experience with Informatica Power Center, IDMC and IICS.\n• Strong expertise in writing complex SQL queries and database management.\n• Experience in data migration projects (on-premise to cloud).\n• Strong data analysis skills for large datasets and ensuring accuracy.\n• Solid understanding of ETL design & development concepts.\n• Familiarity with cloud platforms (AWS, Azure).\n• Experience with version control tools (e.g., Git) and deployment processes.\n\nPreferred Skills:\n• Experience with data lakes, data warehousing, or big data platforms.\n• Familiarity with Agile methodologies.\n• Knowledge of other ETL tools",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgSW5mb3JtYXRpY2EgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiRXZlcmVzdERYIEluYyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNDdnQ200aEh5aHpSWlhDOEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "2 days ago",
         "ETL Developer"
        ],
        [
         "Spark Engineer",
         "Staffingine LLC",
         "India",
         "- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred",
         "eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "Spark Engineer"
        ],
        [
         "Data Engineer - Spark/Python",
         "Etelligens Technologies",
         "India",
         "Job Description\n\nWe are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n\nKey Responsibilities\n• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n• Perform data cleansing and migration from diverse sources to target systems.\n• Collaborate with stakeholders to understand requirements and convert them into technical\n\nsolutions.\n• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n• Support, troubleshoot, and maintain data pipelines and workflows.\n• Participate in all phases of software development lifecycle including unit testing, integration\n\ntesting, and performance testing.\n• Contribute to the modernization of data platforms and analytics tools on Azure.\n• Ensure data quality and integrity across all pipelines and systems.\n\nRequired Skills & Qualifications\n• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n• 1+ year of experience in Python and Spark scripting.\n• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n• Good understanding of Data Warehousing (DWH) concepts.\n• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n• Hands-on experience in data cleansing, transformation, and migration projects.\n• Ability to work independently and within a team environment.\n• Microsoft Certified : Azure Data Engineer Associate\n• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n\nLocation : Artha SEZ, Greater Noida West\n\n(ref:hirist.tech)",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Spark Engineer"
        ],
        [
         "Staff Data Engineer (Spark, Python, Hadoop)",
         "Visa",
         "India",
         "Company Description\n\nVisa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n\nWhen you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n\nJoin Visa: A Network Working for Everyone.\nJob Description\n\nThe Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n\nAre you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n\nAs a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n\nEssential Functions\n• Work with manager and clients to fully understand business requirements and desired business outcomes\n• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n• Execute data engineering projects ranging from small to large either individually or as part of a project team\n• Ensure project delivery within timelines and budget requirements\n• Provide coaching and mentoring to junior team members\n\nThis is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\nQualifications\n\n• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n• Minimum of 4 years of hands-on expertise with Java or Scala\n• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n• Experience working in an Agile and Test Driven Development environment.\n• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n• Experience with SAS as a statistical package is preferred\n• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "Spark Engineer"
        ],
        [
         "Databricks Engineer - Spark / PySpark",
         "Enkefalos Technologies LLP",
         "Anywhere",
         "Databricks Engineer – Spark / PySpark\n\nLocation : Remote / Mysore\n\nJoining : Immediate\n\nExperience : 5+ years\n\nResponsibilities :\n\nWill implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n\nRequirements:\n• PySpark (RDDs, DataFrames, performance tuning)\n• Building gold‐layer data models for financial reporting\n• Experience with complex joins, aggregations, GL hierarchies\n• Version handling (Actuals vs Budget), currency conversions\n\nJob Type: Full-time\n\nPay: ₹500,395.35 - ₹1,840,348.25 per year\n\nBenefits:\n• Flexible schedule\n• Paid sick time\n• Provident Fund\n• Work from home\n\nApplication Question(s):\n• Have you worked on ADF/ADLS ?\n• Do you have hands-on experience of Spark / PySpark\n\nExperience:\n• Databricks Engineering: 4 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "15 days ago",
         "Spark Engineer"
        ],
        [
         "Senior PySpark Data Engineer (Big Data, Cloud Data Solutions, & Python)",
         "Synechron",
         "India",
         "Job Summary\n\nSynechron is seeking a skilled PySpark Data Engineer to design, develop, and optimize data processing solutions leveraging modern big data technologies. In this role, you will lead efforts to build scalable data pipelines, support data integration initiatives, and work closely with cross-functional teams to enable data-driven decision-making. Your expertise will contribute to enhancing business insights and operational efficiency, positioning Synechron as a pioneer in adopting emerging data technologies.\n\nSoftware Requirements\n\nRequired Software Skills:\n• PySpark (Apache Spark with Python) – experience in developing data pipelines\n• Apache Spark ecosystem knowledge\n• Python programming (versions 3.7 or higher)\n• SQL and relational database management systems (e.g., PostgreSQL, MySQL)\n• Cloud platforms (preferably AWS or Azure)\n• Version control: GIT\n• Data workflow orchestration tools like Apache Airflow\n• Data management tools: SQL Developer or equivalent\n\nPreferred Software Skills:\n• Experience with Hadoop ecosystem components\n• Knowledge of containerization (Docker, Kubernetes)\n• Familiarity with data lake and data warehouse solutions (e.g., AWS S3, Redshift, Snowflake)\n• Monitoring and logging tools (e.g., Prometheus, Grafana)\n\nOverall Responsibilities\n• Lead the design and implementation of large-scale data processing solutions using PySpark and related technologies\n• Collaborate with data scientists, analysts, and business teams to understand data requirements and deliver scalable pipelines\n• Mentor junior team members on best practices in data engineering and emerging technologies\n• Evaluate new tools and methodologies to optimize data workflows and improve data quality\n• Ensure data solutions are robust, scalable, and aligned with organizational data governance policies\n• Stay informed on industry trends and technological advancements in big data and analytics\n• Support production environment stability and performance tuning of data pipelines\n• Drive innovative approaches to extract value from large and complex datasets\n\nTechnical Skills (By Category)\n\nProgramming Languages:\n• Required: Python (PySpark experience minimum 2 years)\n• Preferred: Scala (for Spark), SQL, Bash scripting\n\nDatabases/Data Management:\n• Relational databases (PostgreSQL, MySQL)\n• Distributed storage solutions (HDFS, cloud object storage like S3 or Azure Blob Storage)\n• Data warehousing platforms (Snowflake, Redshift – preferred)\n\nCloud Technologies:\n• Required: Experience deploying and managing data solutions on AWS or Azure\n• Preferred: Knowledge of cloud-native services like EMR, Data Factory, or Azure Data Lake\n\nFrameworks and Libraries:\n• Apache Spark (PySpark)\n• Airflow or similar orchestration tools\n• Data processing frameworks (Kafka, Spark Streaming – preferred)\n\nDevelopment Tools and Methodologies:\n• Version control with GIT\n• Agile management tools: Jira, Confluence\n• Continuous integration/deployment pipelines (Jenkins, GitLab CI)\n\nSecurity Protocols:\n• Understanding of data security, access controls, and GDPR compliance in cloud environments\n\nExperience Requirements\n• Minimum of 5+ years in data engineering, with hands-on PySpark experience\n• Proven track record of developing, deploying, and maintaining scalable data pipelines\n• Experience working with data lakes, data warehouses, and cloud data services\n• Demonstrated leadership in projects involving big data technologies\n• Experience mentoring junior team members and collaborating across teams\n• Prior experience in financial, healthcare, or retail sectors is beneficial but not mandatory\n\nDay-to-Day Activities\n• Develop, optimize, and deploy big data pipelines using PySpark and related tools\n• Collaborate with data analysts, data scientists, and business teams to define data requirements\n• Conduct code reviews, troubleshoot pipeline issues, and optimize performance\n• Mentor junior team members on best practices and emerging technologies\n• Design solutions for data ingestion, transformation, and storage\n• Evaluate new tools and frameworks for continuous improvement\n• Maintain documentation, monitor system health, and ensure security compliance\n• Participate in sprint planning, daily stand-ups, and project retrospectives to align priorities\n\nQualifications\n• Bachelor’s or Master’s degree in Computer Science, Information Technology, or related discipline\n• Relevant industry certifications (e.g., AWS Data Analytics, GCP Professional Data Engineer) preferred\n• Proven experience working with PySpark and big data ecosystems\n• Strong understanding of software development lifecycle and data governance standards\n• Commitment to continuous learning and professional development in data engineering technologies\n\nProfessional Competencies\n• Analytical mindset and problem-solving acumen for complex data challenges\n• Effective leadership and team management skills\n• Excellent communication skills tailored to technical and non-technical audiences\n• Adaptability in fast-evolving technological landscapes\n• Strong organizational skills to prioritize tasks and manage multiple projects\n• Innovation-driven with a passion for leveraging emerging data technologies\n\nS YNECHRON’S DIVERSITY & INCLUSION STATEMENT\n\nDiversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.\n\nAll employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.\n\nCandidate Application Notice",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgUHlTcGFyayBEYXRhIEVuZ2luZWVyIChCaWcgRGF0YSwgQ2xvdWQgRGF0YSBTb2x1dGlvbnMsIFx1MDAyNiBQeXRob24pIiwiY29tcGFueV9uYW1lIjoiU3luZWNocm9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjJYOEpqZUhEQmxiRndHQklBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "1 day ago",
         "Spark Engineer"
        ],
        [
         "Senior Data Engineer (Delta Lake, Spark & Unity Catalog)",
         "306 - GoTo Technologies India Private Limited",
         "India",
         "Job Description Where you’ll work: India (Remote) Engineering at GoTo We’re the trailblazers of remote work technology. We build powerful, flexible work software that empowers everyone to live their best life, at work and beyond. And blaze even more trails along the way. There’s ample room for growth – so you can blaze your own trail here too. When you join a GoTo product team, you’ll take on a key role in this process and see your work be used by millions of users worldwide. Your Day to Day As a Senior Data Engineer, you would be: Design, develop, and maintain robust, scalable, and efficient ETL/ELT data pipelines to process structured and unstructured data from diverse sources. Architect and implement cloud-native data solutions using AWS services including S3, EMR, Lambda, and EKS. Build and optimize large-scale data processing workflows using Apache Spark and Databricks, with hands-on experience implementing and managing Delta Lake tables for high-volume batch and streaming use cases. Manage and govern data using Unity Catalog, maintaining strong data lineage, access controls, and metadata management. Develop and maintain data models (relational and dimensional) to support analytics, reporting, and machine learning use cases. Schedule, monitor, and orchestrate workflows using Apache Airflow or similar orchestration tools. Implement data quality checks, logging, monitoring, and alerting to ensure reliability and visibility of data pipelines. Collaborate with analysts, data scientists, and business stakeholders to deliver high-quality, trusted data for downstream applications and enable self-service BI tooling. Ensure adherence to best practices in data governance, security, and compliance. Mentor junior engineers and contribute to engineering standards, including CI/CD, automated testing, and documentation What We’re Looking For As an Senior Data Engineer, your background will look like: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. 5+ years of relevant experience in data engineering or software development, with a proven track record designing and maintaining production-grade data pipelines. Proficiency in Python programming and SQL for data transformation and analytics. Strong hands-on experience with Apache Spark and Databricks, including direct exposure to Delta Lake for data lake management, ACID transactions, schema enforcement and evolution, and time travel functionalities. Experience managing and organizing data access with Unity Catalog. In-depth experience with AWS services—specifically S3, EMR, Lambda, and EKS—and an excellent understanding of cloud-based data architecture and security best practices. Strong data modeling skills (dimensional, normalized) and understanding of data warehousing and lakehouse paradigms. Hands-on experience orchestrating workflows using Apache Airflow or similar orchestration frameworks. Familiarity with BI and data visualization tools (e.g., Tableau, Power BI). Experience establishing data quality processes, monitoring, and observability for pipelines. Exceptional communication, collaboration, and problem-solving skills, with the ability to thrive in an agile, multicultural team environment. At GoTo, authenticity and inclusive culture are key to our thriving workplace, where diverse perspectives drive innovation and growth. Our team of GoGetters is passionate about learning, exploring, and working together to achieve success while staying committed to delivering exceptional experiences for our customers. We take pride in supporting our employees with comprehensive benefits, wellness programs, and global opportunities for professional and personal development. By maintaining an inclusive environment, we empower our teams to do their best work, make a meaningful impact, and grow their career. Learn more. At GoTo, we’re bold enough to imagine a world of work without limits—where curiosity and AI-driven innovation fuel our constant growth. As the leader in cloud communications and IT, we solve real-world challenges through practical, cutting-edge solutions and an unwavering customer-first mindset. Our culture is rooted in inclusion, ownership, and transparency, fueling an environment where every voice contributes to both personal and collective achievement. Here, collaboration sparks bold ideas, and authenticity is celebrated—empowering you to adapt, evolve, and make a real impact. Join GoTo, and help shape the future of work while accelerating your own growth alongside exceptional people who are redefining what’s possible.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAoRGVsdGEgTGFrZSwgU3BhcmsgXHUwMDI2IFVuaXR5IENhdGFsb2cpIiwiY29tcGFueV9uYW1lIjoiMzA2IC0gR29UbyBUZWNobm9sb2dpZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlJxSFNWX2s0aVZkRWhBNGRBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "10 days ago",
         "Spark Engineer"
        ],
        [
         "Cloud Data Engineer- Spark & Databricks",
         "Brighttier",
         "Anywhere",
         "This a Full Remote job, the offer is available from: India\n\nJob Title: Cloud Engineer – Spark/Databricks Specialist\nLocation: Remote\nJob Type: Contract\nIndustry: IT/Cloud Engineering\nJob Summary:\nWe are looking for a highly skilled Cloud Engineer with a specialization in Apache Spark and Databricks to join our dynamic team. The ideal candidate will have extensive experience working with cloud platforms such as AWS, Azure, and GCP, and a deep understanding of data engineering, ETL processes, and cloud-native tools. Your primary responsibility will be to design, develop, and maintain scalable data pipelines using Spark and Databricks, while optimizing performance and ensuring data integrity across diverse environments.\nKey Responsibilities:\nDesign and Development:\n• Architect, develop, and maintain scalable ETL pipelines using Databricks, Apache Spark (Scala, Python), and other cloud-native tools such as AWS Glue, Azure Data Factory, and GCP Dataflow.\n• Design and build data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n• Implement efficient data ingestion, transformation, and processing workflows with Spark and Databricks.\n• Optimize the performance of ETL processes for faster data processing and lower costs.\n• Develop and manage data pipelines using other ETL tools such as Informatica, SAP Data Intelligence, and others as needed.\nData Integration and Management:\n• Integrate structured and unstructured data sources (relational databases, APIs, ERP systems) into the cloud data infrastructure.\n• Ensure data quality, validation, and integrity through rigorous testing.\n• Perform data extraction and integration from SAP or ERP systems, ensuring seamless data flow.\nPerformance Optimization:\n• Monitor, troubleshoot, and enhance the performance of Spark/Databricks pipelines.\n• Implement best practices for data governance, security, and compliance across data workflows.\nCollaboration and Communication:\n• Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders, to define data requirements and deliver scalable solutions.\n• Provide technical guidance and recommendations on cloud data engineering processes and tools.\nDocumentation and Maintenance:\n• Document data engineering solutions, ETL pipelines, and workflows.\n• Maintain and support existing data pipelines, ensuring they operate effectively and align with business goals.\nQualifications:\nEducation:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\nExperience:\n• 7+ years of experience in cloud data engineering or similar roles.\n• Expertise in Apache Spark and Databricks for data processing.\n• Proven experience with cloud platforms like AWS, Azure, and GCP.\n• Experience with cloud-native ETL tools such as AWS Glue, Azure Data Factory, Kafka, GCP Dataflow, etc.\n• Hands-on experience with data platforms like Redshift, Snowflake, Azure Synapse, and BigQuery.\n• Experience in extracting data from SAP or ERP systems is preferred.\n• Strong programming skills in Python, Scala, or Java.\n• Proficient in SQL and query optimization techniques.\nSkills:\n• In-depth knowledge of Spark/Scala for high-performance data processing.\n• Strong understanding of data modeling, ETL/ELT processes, and data warehousing concepts.\n• Familiarity with data governance, security, and compliance best practices.\n• Excellent problem-solving, communication, and collaboration skills.\nPreferred Qualifications:\n• Certifications in cloud platforms (e.g., AWS Certified Data Analytics, Google Professional Data Engineer, Azure Data Engineer Associate).\n• Experience with CI/CD pipelines and DevOps practices for data engineering.\n• Exposure to Apache Hadoop, Kafka, or other data frameworks is a plus.\n\nThis offer from \"Brighttier\" has been enriched by Jobgether.com and got a 74% flex score.",
         "eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyLSBTcGFyayBcdTAwMjYgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJyaWdodHRpZXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTzdzcjNWUGpVa2l1R0VpZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Spark Engineer"
        ],
        [
         "Data Engineer (Snowflake, Spark, AWS) - AVP",
         "12542 Citicorp Services India Private Limited",
         "India",
         "The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "Spark Engineer"
        ],
        [
         "Data Engineer-Python,PySpark,SQL ,Spark Architecture,Azure Databricks",
         "Siemens Healthineers",
         "India",
         "jobid\n• 460574\n\njobfamily\n• Research & Development\n\ncompany\n• Siemens Healthcare Private Limited\n\norganization\n• Siemens Healthineers\n\njobType\n• Full-time\n\nexperienceLevel\n• Experienced Professional\n\ncontractType\n• Permanent\n\nAs a Data Engineer , you are required to:\n\nDesign, build, and maintain data pipelines that efficiently process and transport data from various sources to storage systems or processing environments while ensuring data integrity, consistency, and accuracy across the entire data pipeline.\n\nIntegrate data from different systems, often involving data cleaning, transformation (ETL), and validation. Design the structure of databases and data storage systems, including the design of schemas, tables, and relationships between datasets to enable efficient querying. Work closely with data scientists, analysts, and other stakeholders to understand their data needs and ensure that the data is structured in a way that makes it accessible and usable.\n\nStay up-to-date with the latest trends and technologies in the data engineering space, such as new data storage solutions, processing frameworks, and cloud technologies. Evaluate and implement new tools to improve data engineering processes.\n\nQualification : Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Science, Engineering is desirable.\n\nExperience level : At least 3 - 5 years hands-on experience in Data Engineering\n\nDesired Knowledge & Experience :\n• Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\n• Knowing Spark internals: Catalyst/Tungsten/Photon\n• Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\n• IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\n• Test: pytest, Great Expectations\n• CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\n• Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\n• Languages: Python/Functional Programming (FP)\n• SQL : TSQL/Spark SQL/HiveQL\n• Storage : Data Lake and Big Data Storage Design\n\nadditionally it is helpful to know basics of:\n• Data Pipelines : ADF/Synapse Pipelines/Oozie/Airflow\n• Languages: Scala, Java\n• NoSQL : Cosmos, Mongo, Cassandra\n• Cubes : SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\n• SQL Server : TSQL, Stored Procedures\n• Hadoop : HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\n• Data Catalog : Azure Purview, Apache Atlas, Informatica\n\nRequired Soft skills & Other Capabilities :\n\nGreat attention to detail and good analytical abilities.\n\nGood planning and organizational skills\n\nCollaborative approach to sharing ideas and finding solutions\n\nAbility to work independently and also in a global team environment.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVB5dGhvbixQeVNwYXJrLFNRTCAsU3BhcmsgQXJjaGl0ZWN0dXJlLEF6dXJlIERhdGFicmlja3MiLCJjb21wYW55X25hbWUiOiJTaWVtZW5zIEhlYWx0aGluZWVycyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJXVkxrZFF1QU4tSGFXQjIzQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "8 days ago",
         "Spark Engineer"
        ],
        [
         "Cloud Data Engineer (Spark/Databricks)",
         "Antal Job Board",
         "Nagpur, Maharashtra, India",
         "Vacancy No\nVN1228\n\nBusiness Unit\nEMEA\n\nJob Location\nIndia\n\nEmployment Type\nFull Time\n\nJob Details and Responsibilities\nWe are seeking an experienced Cloud Data Engineer with a strong background in AWS, Azure, and GCP. The ideal candidate will have extensive experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, and other ETL tools like Informatica, SAP Data Intelligence, etc. You will be responsible for designing, implementing, and maintaining robust data pipelines and building scalable data lakes. Experience with various data platforms like Redshift, Snowflake, Databricks, Synapse, Snowflake and others is essential. Familiarity with data extraction from SAP or ERP systems is a plus.\n\nKey Responsibilities:\n\nDesign and Development:\n• Design, develop, and maintain scalable ETL pipelines using cloud-native tools (AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.).\n• Architect and implement data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n• Develop and optimize data ingestion, transformation, and loading processes using Databricks, Snowflake, Redshift, BigQuery and Azure Synapse.\n• Implement ETL processes using tools like Informatica, SAP Data Intelligence, and others.\n• Develop and optimize data processing jobs using Spark Scala.\nData Integration and Management:\n• Integrate various data sources, including relational databases, APIs, unstructured data, and ERP systems into the data lake.\n• Ensure data quality and integrity through rigorous testing and validation.\n• Perform data extraction from SAP or ERP systems when necessary.\nPerformance Optimization:\n• Monitor and optimize the performance of data pipelines and ETL processes.\n• Implement best practices for data management, including data governance, security, and compliance.\nCollaboration and Communication:\n• Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\n• Collaborate with cross-functional teams to design and implement data solutions that meet business needs.\nDocumentation and Maintenance:\n• Document technical solutions, processes, and workflows.\n• Maintain and troubleshoot existing ETL pipelines and data integrations.\n\nQualifications\n\nEducation:\n\nBachelor’s degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n\nExperience:\n• 7+ years of experience as a Data Engineer or in a similar role.\n• Proven experience with cloud platforms: AWS, Azure, and GCP.\n• Hands-on experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.\n• Experience with other ETL tools like Informatica, SAP Data Intelligence, etc.\n• Experience in building and managing data lakes and data warehouses.\n• Proficiency with data platforms like Redshift, Snowflake, BigQuery, Databricks, and Azure Synapse.\n• Experience with data extraction from SAP or ERP systems is a plus.\n• Strong experience with Spark and Scala for data processing.\n\nSkills:\n• Strong programming skills in Python, Java, or Scala.\n• Proficient in SQL and query optimization techniques.\n• Familiarity with data modeling, ETL/ELT processes, and data warehousing concepts.\n• Knowledge of data governance, security, and compliance best practices.\n• Excellent problem-solving and analytical skills.\n• Strong communication and collaboration skills.\n\nPreferred Qualifications:\n• Experience with other data tools and technologies such as Apache Spark, or Hadoop.\n• Certifications in cloud platforms (AWS Certified Data Analytics – Specialty, Google Professional Data Engineer, Microsoft Certified: Azure Data Engineer Associate).\n• Experience with CI/CD pipelines and DevOps practices for data engineering\n• Selected applicant will be subject to a background investigation, which will be conducted and the results of which will be used in compliance with applicable law.\n\nWhat we offer in return:\n• Remote Working: Lemongrass always has been and always will offer 100% remote work\n• Flexibility: Work where and when you like most of the time\n• Training: A subscription to A Cloud Guru and generous budget for taking certifications and other resources you’ll find helpful\n• State of the art tech: An opportunity to learn and run the latest industry standard tools\n• Team: Colleagues who will challenge you giving the chance to learn from them and them from you\n\nLemongrass Consulting is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate on the basis of race, religion, color, national origin, religious creed, gender, sexual orientation, gender identity, gender expression, age, genetic information, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics\n\nAbout Lemongrass\nLemongrass (lemongrasscloud.com) is a global leader in SAP consulting, focused on helping organizations transform their business processes through innovative solutions and technologies. With a strong commitment to customer success, Lemongrass partners with companies to drive their digital transformation journeys, enabling them to unlock the full potential of their SAP investments.\n\nWe do this with our continuous innovation, automation, migration and operation, delivered on the world's most comprehensive cloud platforms – AWS, Azure and GCP and SAP Cloud ERP. We have been working with AWS and SAP since 2010 and we are a Premier Amazon Partner Network (APN) Consulting Partner. We are also a Microsoft Gold Partner, a Google Cloud Partner and an SAP Certified Silver Partner.\n\nOur team is what makes Lemongrass exceptional and why we have the excellent reputation in the market that we enjoy today. At Lemongrass, you will work with the smartest and most motivated people in the business. We take pride in our culture of innovation and collaboration that drives us to deliver exceptional benefits to our clients every day.",
         "eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyIChTcGFyay9EYXRhYnJpY2tzKSIsImNvbXBhbnlfbmFtZSI6IkFudGFsIEpvYiBCb2FyZCIsImFkZHJlc3NfY2l0eSI6Ik5hZ3B1ciwgTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJCQXpseTdfU0lsQjByTmF2QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "28 days ago",
         "Spark Engineer"
        ],
        [
         "Data Analyst III",
         "Bristol Myers Squibb",
         "Hyderabad, Telangana, India",
         "Working with Us\nChallenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n\nThe US Commercial Analytics, Market Access Data Analyst will play a crucial role in supporting data-driven decision-making processes, generating insights, and providing strategic guidance to optimize our Market Access & Pricing activities in the US. This position requires a deep expertise in secondary analytics and statistical analytic techniques, along with a firm grasp of the US healthcare landscape, especially market access and pricing dynamics. The Business Analyst III will typically be responsible for leading support for multiple therapeutic areas in Market Access and will collaborate with cross-functional teams to enhance understanding of business performance and drive growth.\n\nRoles and Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Perform statistical analyses, data mining and predictive modelling to uncover trends and drivers of performance\n• Collaborate with stakeholders (US based Market Access Business Insights & Analytics team and US Market Access team members) to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects including payer, PBM and patient level subnational & claims data analyses, formulary and policy level analysis, and ad-hoc analytics based on secondary data to support US Market Access strategy\n• Prepare reports, dashboards and presentations to communicate findings, insights and recommendations to stakeholders\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up-to-date with industry trends (US access and health care landscape), best practices, and emerging technologies\n• Partner with US based Market Access BIA/Commercial team members (for a given project scope)\n• Provide overall project oversight and training/mentorship to junior analysts and team members as required\n• Supports culture of fact-based decision making through application of best-in-class, innovative, and appropriate methodologies to address key business questions\n\nSkills & Competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions, around US pricing and reimbursement strategies\n• Proficiency in statistical analysis techniques and applications of those to US commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and strategic thinking, with the ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome Candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• 5-8 years prior Pharmaceutical industry experience with 1-2 years within US Market Access & Pricing Analytics\n• Experience working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims, medical policy and formulary data), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with reimbursement and regulatory requirements and compliance in the US biopharma industry\n• Certification or training in relevant analytics or business intelligence tools is a plus\n\nIf you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n\nUniquely Interesting Work, Life-changing Careers\nWith a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n\nOn-site Protocol\n\nBMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n\nSite-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n\nBMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n\nIf you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUlJIiwiY29tcGFueV9uYW1lIjoiQnJpc3RvbCBNeWVycyBTcXVpYmIiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZHU3JGRGo4OTcybHdnQWpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "3 days ago",
         "Data Analyst"
        ],
        [
         "Data Science Analyst",
         "IN10 (FCRS = IN010) Novartis Healthcare Private Limited",
         "India",
         "About the Role:Within Insights and Commercial Solutions, below are the key service groups:  Marketing Effectiveness (Business Analytics, Forecasting, Competitive Intelligence)  Field Force Operations  Commercial Consulting  Brand Insights  Patient access services (Pricing and market access, etc.)  Primary Market Research Provide analytics support to Novartis internal customers (CPOs & Regional marketing and sales teams) on various projects • Support and facilitate data enabled decision making for Novartis internal customers using data analysis and data science techniques/methods on internal and external (3rd party) data to solve business problems • Support I&CS business in building capabilities by involving in various initiatives like knowledge sharing, on-boarding and training support, support in all business-related tasks/activities, building process documentation and knowledge repositories.Analyst, Data ScienceLocation – Hyderabad #LI Hybrid Key Responsibilities:Delivering projects and managing internal customer expectations across multi-channel marketing analytics, Portfolio Analytics, Targeting and Segmentation, Predictive Analytics, Resource Allocation and Optimization, Other ad-hoc requests.Create and deliver customer requirements as agreed SLAs (timeliness, accuracy, quality, etc.) and drive excellent customer satisfaction. Deliver advanced analytical and statistical solutions for various projects related to promotion evaluation, multi-channel marketing (MCM) campaign design, return on investment (ROI) analysis, resource allocation, segmentation, targeting, and other ad-hoc business questionsSupport exploratory research to identify new areas of application of advanced analytics/data science in providing enhanced decision-making support. Deliver services through a structured project management approach with appropriate documentation and communication throughout the delivery of servicesAutomation of project codes and development of front-end delivery solutions. Support in creation and maintenance of standard operating procedures (SOPs), quality checklists that will enable excellent quality outputs within the function.Support in developing and maintaining knowledge repositories that captures qualitative and quantitative reports of brands, disease areas, macroeconomic trends of Novartis operating markets, etc.Comply with all internal functional operating procedures like time tracking, KPI tracking and reporting, and other internal systems and processes. Comply to all Novartis operating procedures as per legal/IT/HR requirementsEssential Requirements:Technical Skills: Statistical Modeling Experience (Covering but not limited to, Regression, Classification, Supervised and Unsupervised Learning Methods, Optimization techniques, Predictive Models, Deep learning Models, e.g. Neural Networks, Natural Language Processing, Text Mining etc.)Programming languages/tools – R, SAS, Python, SQLVisualization tools – Qlikview, Qliksense, TableauWorking knowledge of MS-Office (MS Excel, VBA, PowerPoint, Access)Pharmaceutical industry domain/datasets knowledge – desirableEducation:Graduation/Post Graduation in Business Administration, Mathematics, Economics, Statistics, Engineering or Quantitative streamsExperience: Experience (Minimum 2years) in analytics in marketing analytics, experience in pharma industry is preferableShould have strong quantitative and systems backgroundStrong analytical thinking with problem solving approachShould have good ability to understand new data sources in short timeframe and embed them into standard analysesA good understanding of pharmaceutical domain and data would be ideal. Should have worked in an international company with exposure to working in a cross-cultural environmentStrong and proactive business results focus, and proven ability to provide insights that increase productivityWhy Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook. https://www.novartis.com/careers/benefits-rewards Commitment to Diversity and Inclusion: Novartis is committed to building an outstanding, inclusive work environment and diverse teams' representative of the patients and communities we serve. Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVuY2UgQW5hbHlzdCIsImNvbXBhbnlfbmFtZSI6IklOMTAgKEZDUlMgPSBJTjAxMCkgTm92YXJ0aXMgSGVhbHRoY2FyZSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiejA4Y3NEMUZUdmdvZExfWkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "7 days ago",
         "Data Analyst"
        ],
        [
         "Data Sr.Modeler/Data Analyst( Immediate Joiner)",
         "The Talent Quest",
         "Hyderabad, Telangana, India",
         "Vacancy with a company focused on digital transformation, specializing in intelligent automation, digitalization, data science & analytics, and mobile enablement. They help businesses improve cost efficiency, productivity, and agility by reducing turnaround time and errors. The company provides services and solutions including operations digital transformation consulting, next-gen shared services setup consulting, cognitive RPA deployment, and AI-enabled CX enhancement. Founded in 2020 ;with HQ in Gurugram, India; the Company is now operating from Noida, Mumbai, Hyderabad, and Bengaluru as well.\n\nJob Role:We are seeking a highly skilled and detail-oriented Data Modeler to join our Data\n\nManagement team. The ideal candidate will be responsible for designing and\n\nimplementing logical and physical data models to support enterprise data\n\ninitiatives. This role requires close collaboration with business stakeholders, data\n\narchitects, and engineers to ensure data is structured and accessible for analytics,\n\nreporting, and operational needs.\n\nThe successful candidate will:\n\nProvides technical expertise in needs identification, data modelling, data\n\nmovement and transformation mapping (source to target), automation and testing\n\nstrategies, translating business needs into technical solutions with adherence to\n\nestablished data guidelines and approaches from a business unit or project\n\nperspective.\n\n7-10 Years industry implementation experience with one or more data\n\nmodelling tools such as Erwin, ERStudio, PowerDesigner etc.\n\n Minimum of 8 years of data architecture, data modelling or similar\n\nexperience\n\n 5-7 years of management experience required\n\n 5-7 years consulting experience preferred\n\n Experience working with dimensionally modelled data\n\n Bachelor’s degree or equivalent experience, Master’s Degree Preferred\n\n Understanding of cloud (Azure, AWS, GCP, Snowflake preferred) and on\n\npremises architectures\n\nJob Types: Full-time, Permanent\n\nPay: Up to ₹3,000,000.00 per year\n\nBenefits:\n• Cell phone reimbursement\n• Internet reimbursement\n• Life insurance\n• Paid sick time\n• Paid time off\n• Work from home\n\nWork Location: In person",
         "eyJqb2JfdGl0bGUiOiJEYXRhIFNyLk1vZGVsZXIvRGF0YSBBbmFseXN0KCBJbW1lZGlhdGUgSm9pbmVyKSIsImNvbXBhbnlfbmFtZSI6IlRoZSBUYWxlbnQgUXVlc3QiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ino0YkdrY3RHOUdZdmFTRVVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "2 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst - INTL - Mexico or India",
         "Insight Global",
         "Hyderabad, Telangana, India",
         "Project Background:\nMosaic is our financial planning and reporting transformation program, bringing an aligned business process and toolset to all key business units enabling us to get consistent and standardized financial metrics across geographies. Started in 2019 the program is already rolled out to Europe and most of North America and continues across the world.\nThere are three key components to the program:\n1. A standardized planning tool IBM Cognos TM1\n2. A global data warehouse based on Azure Cloud know as SPOT (single point of truth). The SPOT team is made up of functional data analysts & data engineers\n3. An extensive suite of dashboards known as Cockpit using Tableau. The Cockpit team are generally finance analysts with Tableau skills\nRole Background:\nWe are looking for an additional functional/data analyst to join the existing team of functional/data analysts working on the SPOT data warehouse.\nThe role of the functional/data analyst is to be the bridge between the Cockpit team, Data Engineering team, and Cognos TM1 team. The analyst understands the business data requirements and translates these into data definition and data transformation documents that are then build by the data engineers. The analyst can also be involved in testing, data validation and any sustain activity related to the flow and accuracy of the data in SPOT & Cockpit.\nTypically, the functional data analyst receives requirements for new and/or amended data pipelines from the Cockpit team or works with the Cognos TM1 team on new data sets that need to flow to SPOT & Cockpit.\nThe current functional data analysts come from a range of backgrounds from data engineers to business analysts. What unites the team is a passion for data, attention to detail, ability to navigate and understand complex data flows, and a customer-centric mindset. This is a high profile and at times high pressure transformation program and everyone on the team must be responsive to our stakeholders needs.\nKey Accountabilities:\nThis role will primarily work with the North America Cockpit team primarily based in Mexico City, but as part of the team they will be expected to build a good knowledge of all data pipelines and work on any area as needed. The key elements of the role are:\nDevelop and maintain SPOT solution design & data architecture:\no Ensure SPOT solution design & data model is up to date with latest business requirements\no Contribute to the delivery of current technical roadmap that can support the predicted growth in data volumes\nTranslate and communicate business requirements across all IT delivery teams and/or partners:\no Collaborate closely with SPOT Cockpit teams to ensure business requirements received from Global & Regional Planning teams are well-understood and translated into the SPOT solution design (functional & data model)\nAct as an owner in the diagnosis and resolution of any data issues raised with the team, partnering with other teams as needed.\n\nWe are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBJTlRMIC0gTWV4aWNvIG9yIEluZGlhIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Il96ZU5oSUE0Ujg3ak1KZHNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "6 days ago",
         "Data Analyst"
        ],
        [
         "IN Specialist 3- Data Analyst - TRAS -Assurance– Bangalore",
         "PwC",
         "India",
         "Line of ServiceAssuranceIndustry/SectorNot ApplicableSpecialismAssuranceManagement LevelSpecialistJob Description & SummaryAt PwC, our people in audit and assurance focus on providing independent and objective assessments of financial statements, internal controls, and other assurable information enhancing the credibility and reliability of this information with a variety of stakeholders. They evaluate compliance with regulations including assessing governance and risk management processes and related controls.In digital assurance at PwC, you will focus on providing assurance services over clients' digital environment, including processes and controls, cyber security measures, data and AI systems, and their associated governance, to help organisations and their stakeholders build trust in their technology while complying with relevant regulations.*Why PWCAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm’s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations. \"Job Description & SummaryABOUT TECHNOLOGY RISK ASSURANCE SERVICES:The Technology Risk Assurance Services practice in the firm, provides both audit and non-audit services related to controls around the financial reporting process, including financial business process and IT management controls.SPA provides:· Financial and operation applications/business process controls reviews· Database security controls reviews· IT general controls reviews· Infrastructure security reviews· Third party assurance and opinion services· Sarbanes-Oxley readiness, process improvement and sustainability services· Due diligence on systems and controls· Pre- and post-implementation systems reviews· Project assurance services· Data services (e.g., CAATs, data quality reviews)· Computer security reviewsResponsibilities (essential functions and responsibilities of the job):The tasks, roles and responsibilities concerned with the job will include the following:Executing various audit engagements as a support to field in-charge andApplying The Firm’s methodology while executing various job rolesApply current knowledge of IT trends and systems to identify security and risk management issues, and other opportunities for improvementAssist in the planning and execution of business process control reviews, information systems audits, and other Assurance offerings across a variety of industriesA good understanding of clients Business Risks, Audit Risks, IT Risks and the mitigating controls in addressing these risksGain strong comprehension of client operations, processes, and business objectives and utilize that knowledge on engagements.Evaluate and test business processes and controls and identify areas of risks.Perform general computer and application controls reviews including ERPs like SAP, Oracle Application, JD Edwards, MFG Pro, etc.Comply with requirements of ICAI Auditing Standards like AAS 29Data Analysis using CAATs tool like Excel, ACL, SQL etc*Mandatory skill sets - Data Analyst*Preferred Skill Sets – Business Analyst*Years of experience required – 0-1 Years*Education Qualification – Post Graduation in Data ScienceEducation (if blank, degree and/or field of study not specified)Degrees/Field of Study required: Postgraduate (Diploma)Degrees/Field of Study preferred:Certifications (if blank, certifications not specified)Required SkillsData AnalyticsOptional SkillsAccepting Feedback, Accepting Feedback, Active Listening, Artificial Intelligence (AI) Platform, Auditing Methodologies, Cloud Engineering, Communication, Compliance and Governance, Compliance and Standards, Compliance Auditing, Corporate Governance, Cybersecurity Governance, Cybersecurity Risk Management, Data Quality, Data Quality Assessment, Data Quality Assurance Testing, Data Validation, Emotional Regulation, Empathy, Governance Framework, Inclusion, Information Assurance, Information Security Governance, Intellectual Curiosity, Internal Controls {+ 11 more}Desired Languages (If blank, desired languages not specified)Travel RequirementsNot SpecifiedAvailable for Work Visa Sponsorship?NoGovernment Clearance Required?NoJob Posting End Date",
         "eyJqb2JfdGl0bGUiOiJJTiBTcGVjaWFsaXN0IDMtIERhdGEgQW5hbHlzdCAtIFRSQVMgLUFzc3VyYW5jZeKAkyBCYW5nYWxvcmUiLCJjb21wYW55X25hbWUiOiJQd0MiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiZTlNdVBjTnNJWHlmb3ZrSkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "1 day ago",
         "Data Analyst"
        ],
        [
         "Data Analyst 1",
         "Bristol Myers Squibb",
         "Hyderabad, Telangana, India",
         "The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n\nRoles & Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up to date with industry trends, best practices, and emerging technologies\n\nSkills and competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n• commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• Proven experience (1-3 years) in a similar business analyst role\n• Prior Pharmaceutical industry and/or healthcare consulting experience required\n• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with regulatory requirements and compliance in the US biopharma industry\n\nThe US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n\nRoles & Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up to date with industry trends, best practices, and emerging technologies\n\nSkills and competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n• commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• Proven experience (1-3 years) in a similar business analyst role\n• Prior Pharmaceutical industry and/or healthcare consulting experience required\n• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with regulatory requirements and compliance in the US biopharma industry",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ1ZUlVZURnaVJaU1ludkdlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "16 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst Finance (VBA, Power Query, Power BI, Python) 4+ years of experience)",
         "Dupont",
         "Hyderabad, Telangana, India",
         "At DuPont, our purpose is to empower the world with essential innovations to thrive. We work on things that matter. Whether it’s providing clean water to more than a billion people on the planet, producing materials that are essential in everyday technology devices from smartphones to electric vehicles, or protecting workers around the world. Discover the many reasons the world’s most talented people are choosing to work at DuPont. Why Join Us | DuPont Careers\n\nJob Summary:\n\nThe Data Analyst/Developer will play a crucial role in developing automated data solutions, creating comprehensive dashboards, and utilizing advanced Excel functionalities. This position will require collaboration with various teams to translate data needs into actionable intelligence.\n\nKey Areas of Expertise and Responsibilities:\n\n1. Visual Basic for Applications (VBA)\n• Responsibilities:\n• Develop and maintain complex VBA applications to automate repetitive tasks.\n• Incorporate SAP Scripting within VBA to optimize business processes.\n• Troubleshoot and debug existing VBA code to ensure smooth functionality.\n• Criteria:\n• Advanced proficiency in VBA programming.\n• Demonstrated experience with SAP interfaces and scripting.\n• Ability to write modular, efficient, and maintainable code.\n• Knowledge of Excel object model and its functionalities.\n\n2. Power Query\n• Responsibilities:\n• Utilize Power Query for data extraction, transformation, and loading (ETL) processes.\n• Develop and maintain data models in Excel to streamline data preparation.\n• Create and optimize Power Query scripts for efficient data processing.\n• Criteria:\n• Intermediate experience with Power Query including M language for data transformation.\n• Familiarity with connecting Power Query to various data sources (e.g., databases, APIs).\n• Ability to perform data cleansing and manipulation through Power Query.\n\n3. Power BI\n• Responsibilities:\n• Create interactive, user-friendly dashboards and reports using Power BI.\n• Collaborate with stakeholders to gather reporting requirements and deliver insights.\n• Optimize Power BI reports for performance and usability.\n• Criteria:\n• Intermediate knowledge of Power BI Desktop and Power BI Service.\n• Ability to create DAX measures and calculated columns for enhanced analytics.\n• Familiarity with data visualization best practices and techniques.\n\n4. Python\n• Responsibilities:\n• Develop Python scripts to automate data manipulation and Excel-related tasks.\n• Utilize libraries such as Pandas and NumPy for data analysis and automation.\n• Collaborate with the data team to integrate Python solutions with existing tools.\n• Criteria:\n• Intermediate proficiency in Python, especially in data manipulation and automation.\n• Experience with libraries like Pandas, NumPy, and openpyxl for Excel automation.\n• Understanding of APIs and ability to retrieve data programmatically.\n\nQualifications:\n• Bachelor’s degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n• A minimum of 4 of experience in a data-related role, focusing on the technologies mentioned.\n• Strong analytical and problem-solving skills with attention to detail.\n• Excellent communication skills and the ability to work collaboratively with diverse teams.\n\nPreferred Skills:\n• Experience with SQL and relational databases for data querying and data management.\n• Familiarity with other business intelligence tools (like Tableau or QlikView) is a plus.\n• Knowledge of machine learning principles is an advantage.\n• Understanding of data warehousing concepts and methodologies.\n\nJoin our Talent Community to stay connected with us!\n\nOn May 22, 2024, we announced a plan to separate our Electronics and Water businesses in a tax-free manner to its shareholders. On January 15, 2025, we announced that we are targeting November 1, 2025, for the completion of the intended separation of the Electronics business (the “Intended Electronics Separation”)*. We also announced that we would retain the Water business. We are committed to ensuring a smooth and successful separation process for the Future Electronics business. We look forward to welcoming new talent interested in contributing to the continued success and growth of our evolving organization.\n\n(1)The separation transactions are subject to satisfaction of customary conditions, including final approval by DuPont's Board of Directors, receipt of tax opinion from counsel, the filing and effectiveness of Form 10 registration statements with the U.S. Securities and Exchange Commission, applicable regulatory approvals, and satisfactory completion of financing.  For further discussion of risks, uncertainties and assumptions that could impact the achievement, expected timing and intended benefits of the separation transactions, see DuPont’s announcement.\n\nDuPont is an equal opportunity employer. Qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability or any other protected class. If you need a reasonable accommodation to search or apply for a position, please visit our Accessibility Page for Contact Information.\n\nDuPont offers a comprehensive pay and benefits package. To learn more visit the Compensation and Benefits page.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgRmluYW5jZSAoVkJBLCBQb3dlciBRdWVyeSwgUG93ZXIgQkksIFB5dGhvbikgNCsgeWVhcnMgb2YgZXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJEdXBvbnQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImNGeEdTMkhJQXpJRW5kb2ZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "2 days ago",
         "Data Analyst"
        ],
        [
         "Sustainability Data Analyst",
         "Carrier",
         "Hyderabad, Telangana, India",
         "Role: Sustainability Data Analyst\n\nLocation: Hyderabad, India\n\nFull/ Part-time: Full time\n\nBuild a career with confidence\n\nCarrier Global Corporation, global leader in intelligent climate and energy solutions is committed to creating solutions that matter for people and our planet for generations to come. From the beginning, we've led in inventing new technologies and entirely new industries. Today, we continue to lead because we have a world-class, diverse workforce that puts the customer at the center of everything we do.\n\nAbout the role\n\nWe are seeking a results-driven Sustainability Data Analyst to join Carrier’s product sustainability analytics team. The ideal candidate will have a strong background in sustainability practices and data analytics, with a focus on analyzing environmental impact and identifying opportunities for improvement. This role involves conducting comprehensive sustainability/energy audits, analyzing data, and developing strategies to enhance sustainability performance.\n\nKey responsibilities:\n• Perform detailed sustainability audits and analyze data to identify trends and areas for improvement.\n• Develop and implement strategies to enhance sustainability and reduce environmental impact.\n• Monitor and evaluate the performance of sustainability initiatives using data analytics.\n• Collaborate with cross-functional teams to optimize sustainability practices.\n• Prepare reports and presentations on sustainability metrics and audit findings.\n• Stay updated on industry trends and best practices in sustainability and data analytics.\n\nMinimum Requirements:\n\nEducation: Bachelor’s or Master’s degree in Mechanical, Energy Engineering, Environmental Science, Sustainability, Data Analytics, or a related field.\n\nExperience: atleast 6 years in sustainability, data analytics, or BI development roles to be able to drive himself towards business goals independently.\n\nKey Skills:\n• Strong analytical skills, attention to detail and ability to think from first principles.\n• Excellent communication and teamwork abilities.\n• Proficiency in data analytics software and tools. Knowledge of python, SQL, Power-BI, would be added advantage.\n• Knowledge of relevant regulations and standards in sustainability.\n• Familiarity with data visualization tools and techniques.\n• Willingness to be flexible, learn new tools, techniques and deliver.\n\nBenefits\n\nWe are committed to offering competitive benefits programs for all of our employees, and enhancing our programs when necessary.\n• Enjoy your best years with our retirement savings plan\n• Have peace of mind and body with our health insurance\n• Make yourself a priority with flexible schedules, parental leave and our holiday purchase scheme\n• Drive forward your career through professional development opportunities\n• Achieve your personal goals with our Employee Assistance Programme.\n\nOur commitment to you\n\nOur greatest assets are the expertise, creativity and passion of our employees. We strive to provide a great place to work that attracts, develops and retains the best talent, promotes employee engagement, fosters teamwork and ultimately drives innovation for the benefit of our customers. We strive to create an environment where you feel that you belong, with diversity and inclusion as the engine to growth and innovation. We develop and deploy best-in-class programs and practices, providing enriching career opportunities, listening to employee feedback and always challenging ourselves to do better. This is The Carrier Way.\n\nJoin us and make a difference.\n\nApply Now!\n\nCarrier is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.\n\nJob Applicant's Privacy Notice:\n\nClick on this link to read the Job Applicant's Privacy Notice",
         "eyJqb2JfdGl0bGUiOiJTdXN0YWluYWJpbGl0eSBEYXRhIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJDYXJyaWVyIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJPMkpEMHBVcG0xc3dxYnc1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "9 days ago",
         "Data Analyst"
        ],
        [
         "Sr. Data Analyst",
         "iCIMS Talent Acquisition",
         "Rai Durg, Telangana, India",
         "Job Overview\n\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.\n\nAbout Us\n\nWhen you join iCIMS, you join the team helping global companies transform business and the world through the power of talent. Our customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile. As the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent. We’re passionate about helping companies build a diverse, winning workforce and about building our home team. We're dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\n\nResponsibilities\n• Perform various data analysis functions to analyze data from a variety of sources including external labor market data and research and internal data sets from our platforms \n• Incorporate information from a variety of systems to produce comprehensive and compelling narratives for thought-leadership initiatives and customer engagements\n• Demonstrate critical thinking - identify the story in context using multiple data sets, and present results. A strong proficiency in data storytelling will be critical to success in this role.\n• Understand principles of quality data visualization and apply them in Tableau to create and maintain custom dashboards for consumption by other employees\n• Find and investigate data quality issues, root causes and recommend remedies to be implemented by the data scientists and engineers  \n• Liaise with teams around our business to understand their problems, determine how our team can help, then use our database to produce the content they need\n• Identify data mapping and enrichment requirements. Familiarity with SQL, especially the logic behind different types of data joins and writing efficient queries, will be necessary\n• Consistently ensure that business is always conducted with integrity and that behavior aligns with iCIMS policies, procedures, and core competencies\n\nAdditional Job Responsibilities: \n• Produce and adapt data visualizations in response to business requests for internal and external use\n• Shows good judgement in prioritizing their own commitments and those of the larger team, while demonstrating initiative and appropriate urgency when needed\n• Mentor junior team members in best practices for analytics, data visualization, and data storytelling. Exemplify these standards and guide teammates in following them.\n• Think creatively to produce unique, actionable insights from complex datasets, which can deliver value to our business and to our customers.\n\nQualifications\n• 5-10 years professional experience working in an analytics capacity\n• Excellent communication skills, especially with regards to data storytelling – finding insights from complex datasets and sharing those findings with key stakeholders\n• Strong data analytics and visualization skills\n• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n• Advanced knowledge of Excel (Pivot tables, VLOOKUPs, IF statements)\n• Familiarity with data guardrails to ensure compliance with applicable data governance regulations and privacy laws (i.e., GDPR)\n\nPreferred\n• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n\nEEO Statement\n\niCIMS is a place where everyone belongs. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at iCIMS.\n\nWe are proud to be an equal opportunity and affirmative action employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at careers@icims.com.\n\nCompensation and Benefits\n\nCompetitive health and wellness benefits include medical insurance (employee and dependent family members), personal accident and group term life insurance, bonding and parental leave, lifestyle spending account reimbursements, wellness services offerings, sick and casual/emergency days, paid holidays, tuition reimbursement, retirals (PF - employer contribution) and gratuity. Benefits and eligibility may vary by location, role, and tenure. Learn more here: https://careers.icims.com/benefits",
         "eyJqb2JfdGl0bGUiOiJTci4gRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiaUNJTVMgVGFsZW50IEFjcXVpc2l0aW9uIiwiYWRkcmVzc19jaXR5IjoiUmFpIER1cmcsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImZMUlRBQjlESVFWdFdOeGpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "21 days ago",
         "Data Analyst"
        ],
        [
         "Master Data Analyst",
         "C32511 Alfa Laval India Private Limited",
         "India",
         "Purpose of the Job: To provide direct support to the Sales and Order Handling teams by managing customer master data and administering the Order Management System. This role ensures data integrity, system readiness, and compliance with internal controls during order booking and execution processes. Key Responsibilities: Customer Master Data Management Manage the full lifecycle of customer codes, including creation, amendment, and deactivation. Conduct background checks using the Global World Check system and coordinate with the Export Control Super User for compliance validation. Ensure customer data is configured correctly in the system for seamless order booking and execution. Maintain data accuracy and cleanliness in line with Alfa Laval’s global data standards. System Administration Act as the system administrator for the Customer Order Management system. Maintain system master data and troubleshoot operational issues. Ensure the system is up-to-date and aligned with business requirements. Order Booking Compliance Perform basic compliance checks during order booking. Verify completeness and correctness of mandatory information. Ensure all required approvals are in place as per the authorization matrix. Process Activation Initiate internal processes required for order execution. Collaborate with cross-functional teams to ensure timely and accurate order processing. Key Competencies: Strong attention to detail and data accuracy Knowledge of compliance and export control processes Proficiency in ERP and order management systems Analytical and problem-solving skills Effective communication and collaboration across teams Qualifications & Experience: Bachelor’s degree in Business Administration, Information Systems, or related field 3+ years of experience in data management or order processing roles Familiarity with data governance and compliance frameworks Experience with customer master data and ERP systems (e.g., SAP) \"We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\" Every day, we get opportunities to make a positive impact – on our colleagues, partners, customers and society. Together, we’re pioneering the solutions of the future and unlocking the full potential of precious resources. Trusted to act on initiative, we challenge conventional thinking to develop world-leading technologies that inspire progress in vital areas, including energy, food, water and shipping. As we push forward, the innovative, open spirit that fuels our 140-year-old start-up culture and rapid growth also drives our personal growth. So, as we shape a more resourceful, less wasteful world, we build our careers too. Meet our dedicated people who always go the extra mile Alfa Laval Career Site Or sign up for Job Alerts to stay in touch Sign up for Job Alerts",
         "eyJqb2JfdGl0bGUiOiJNYXN0ZXIgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiQzMyNTExIEFsZmEgTGF2YWwgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImhJMnc5MHYxS1E3QXVQTkNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "6 days ago",
         "Data Analyst"
        ],
        [
         "Engr II-Data Engineering",
         "Verizon",
         "India",
         "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat You’ll Be Doing...\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform and analytics teams, you will understand and enable the required data sets from different sources. This includes both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n• Understanding the business requirements.\n• Transforming technical design.\n• Working on data ingestion, preparation and transformation.\n• Developing the scripts for data sourcing and parsing.\n• Developing data streaming applications.\n• Debugging the production failures and identifying the solution.\n• Working on ETL/ELT development.\n\nWhat We’re Looking For...\n\nYou’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems.\n\nYou'll Need To Have\n• Bachelor’s degree or one or more years of relevant experience required, demonstrated through work experience and/or military experience.\n• Experience with Data Warehouse concepts and Data Management life cycle.\n\nEven better if you have one or more of the following:\n• Any related Certification on ETL/ELT developer.\n• Accuracy and attention to detail.\n• Good problem solving, analytical, and research capabilities.\n• Good verbal and written communication.\n• Experience presenting to and influencing partners.\n\nWhy Verizon?\n\nVerizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.\n• We are a ‘pay for performance’ company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.\n• Your benefits are market competitive and delivered by some of the best providers.\n• You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.\n• We offer generous paid time off benefits to help you manage your work life balance and opportunities for flexible working arrangements*.\n• Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.\n• You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.\n\nYour benefits package will vary depending on the country in which you work.\n• subject to business approval\n\nIf Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.\n\nWhere you’ll be working\n\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.",
         "eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkpaY3lnNzJpTUJGcHpfUTJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "1 day ago",
         "Data Engineer"
        ],
        [
         "Associate Analyst - Data Engineer",
         "PepsiCo",
         "Hyderabad, Telangana, India",
         "Overview\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT.  The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics, and new product development.  PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\n\nWhat PepsiCo Data Management and Operations does:\n\nMaintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\n\nResponsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\n\nWork cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\n\nIncrease awareness about available data and democratize access to it across the company.\n\n \n\n               As a data engineer, you will be the key technical expert building PepsiCo's data products to drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help developing very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n\nResponsibilities\n• Act as a subject matter expert across different digital projects.\n• Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\n• Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\n• Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\n• Responsible for implementing best practices around systems integration, security, performance, and data management.\n• Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\n• Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\n• Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\n• Develop and optimize procedures to “productionalize” data science models.\n• Define and manage SLA’s for data products and processes running in production.\n• Support large-scale experimentation done by data scientists.\n• Prototype new approaches and build solutions at scale.\n• Research in state-of-the-art methodologies.\n• Create documentation for learnings and knowledge transfer.\n• Create and audit reusable packages or libraries.\n\nQualifications\n• 4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n• 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n• 3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n• 2+ years in cloud data engineering experience in Azure.\n• Fluent with Azure cloud services. Azure Certification is a plus.\n• Experience in Azure Log Analytics\n• Experience with integration of multi cloud services with on-premises technologies.\n• Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\n• Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\n• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n• Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\n• Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\n• Experience with Statistical/ML techniques is a plus.\n• Experience with building solutions in the retail or in the supply chain space is a plus.\n• Experience with version control systems like Github and deployment & CI tools.\n• Working knowledge of agile development, including DevOps and DataOps concepts.\n• B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.\n\n Skills, Abilities, Knowledge:\n• Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\n• Strong change manager. Comfortable with change, especially that which arises through company growth.\n• Ability to understand and translate business requirements into data and technical requirements.\n• High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\n• Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\n• Strong organizational and interpersonal skills; comfortable managing trade-offs.",
         "eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUgQW5hbHlzdCAtIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJQZXBzaUNvIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJkdUI4NkhSU1JwRWNPdlBCQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "1 month ago",
         "Data Engineer"
        ],
        [
         "Senior Big Data Engineer",
         "Qualcomm",
         "Hyderabad, Telangana, India",
         "Company:\nQualcomm India Private Limited\n\nJob Area:\nEngineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\n• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n• 2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary:\n\nPreferred Qualifications\n• 3+ years of experience as a Data Engineer or in a similar role\n• Experience with data modeling, data warehousing, and building ETL pipelines\n• Solid working experience with Python, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n• Experience with Big Data tools, platforms and architecture with solid working experience with SQL\n• Experience working in a very large data warehousing environment, Distributed System.\n• Solid understanding on various data exchange formats and complexities\n• Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n• Strong data visualization skills\n• Basic understanding of Machine Learning; Prior experience in ML Engineering a plus\n• Ability to manage on-premises data and make it inter-operate with AWS based pipelines\n• Ability to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\nEducation\n• Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n• Preferred Qualifications: Masters in CS/ECE with a Data Science / ML Specialization\n\nMinimum Qualifications:\n• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n• 3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n• Completes assigned coding tasks to specifications on time without significant errors or bugs.\n• Adapts to changes and setbacks in order to manage pressure and meet deadlines.\n• Collaborates with others inside project team to accomplish project objectives.\n• Communicates with project lead to provide status and information about impending obstacles.\n• Quickly resolves complex software issues and bugs.\n• Gathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n• Seeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n• Participates in technical conversations with tech leads/managers.\n• Anticipates and communicates issues with project team to maintain open communication.\n• Makes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n• Prioritizes project deadlines and deliverables with minimal supervision.\n• Resolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n• Writes readable code for large features or significant bug fixes to support collaboration with other engineers.\n• Determines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n• Unit tests own code to verify the stability and functionality of a feature.\n\nApplicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n\nQualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n\nTo all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n\nIf you would like more information about this role, please contact Qualcomm Careers.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQmlnIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJRdWFsY29tbSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieVRVTHdrQjB2Ujk5bmJQUUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 days ago",
         "Data Engineer"
        ],
        [
         "Manager Data Engineer - AWS Databricks",
         "Blend360",
         "Hyderabad, Telangana, India",
         "Company Description\n\nBlend is a premier AI services provider, committed to co-creating meaningful impact for its clients through the power of data science, AI, technology, and people. With a mission to fuel bold visions, Blend tackles significant challenges by seamlessly aligning human expertise with artificial intelligence. The company is dedicated to unlocking value and fostering innovation for its clients by harnessing world-class people and data-driven strategy. We believe that the power of people and AI can have a meaningful impact on your world, creating more fulfilling work and projects for our people and clients. For more information, visit www.blend360.com\n\nJob Description\n\nWe are seeking a seasoned Data Engineering Manager with 8+ years of experience to lead and grow our data engineering capabilities. This role demands strong hands-on expertise in Python, SQL, Spark, and advanced proficiency in AWS and Databricks. As a technical leader, you will be responsible for architecting and optimizing scalable data solutions that enable analytics, data science, and business intelligence across the organization.\n\nKey Responsibilities:\n• Lead the design, development, and optimization of scalable and secure data pipelines using AWS services such as Glue, S3, Lambda, EMR, and Databricks Notebooks, Jobs, and Workflows.\n• Oversee the development and maintenance of data lakes on AWS Databricks, ensuring performance and scalability.\n• Build and manage robust ETL/ELT workflows using Python and SQL, handling both structured and semi-structured data.\n• Implement distributed data processing solutions using Apache Spark/PySpark for large-scale data transformation.\n• Collaborate with cross-functional teams including data scientists, analysts, and product managers to ensure data is accurate, accessible, and well-structured.\n• Enforce best practices for data quality, governance, security, and compliance across the entire data ecosystem.\n• Monitor system performance, troubleshoot issues, and drive continuous improvements in data infrastructure.\n• Conduct code reviews, define coding standards, and promote engineering excellence across the team.\n• Mentor and guide junior data engineers, fostering a culture of technical growth and innovation.\n\nQualifications\n\nRequirements\n• 8+ years of experience in data engineering with proven leadership in managing data projects and teams.\n• Expertise in Python, SQL, Spark (PySpark), and experience with AWS and Databricks in production environments.\n• Strong understanding of modern data architecture, distributed systems, and cloud-native solutions.\n• Excellent problem-solving, communication, and collaboration skills.\n• Prior experience mentoring team members and contributing to strategic technical decisions is highly desirable.",
         "eyJqb2JfdGl0bGUiOiJNYW5hZ2VyIERhdGEgRW5naW5lZXIgLSBBV1MgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJsZW5kMzYwIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJxelhlVGQtWWc2ekE5U3R4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "3 days ago",
         "Data Engineer"
        ],
        [
         "Data Engineer INTL India - EOR 6fb570f8",
         "Insight Global",
         "Hyderabad, Telangana, India",
         "- In this role, you will be building data pipeline solutions by designing, adopting, and applying big data strategies and architectures. You must have knowledge of scalable system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration, data transformation, data modeling, and data cleansing.\n- The Sr. Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for business stakeholders.\n- Design and develop a logical data model for a large multi-tenant application with intent to facilitate consistency and efficiency.\n- Collaborate with analysts and internal clients to understand the goals of the overall system data architecture, and further develop on business needs/issues.\n- Test/troubleshoot problems and conduct root cause analysis.\n- Own communications with technical contacts at software vendors to escalate software issues, recommend enhancements, and find sustainable workarounds to issues.\n- Work in tandem with analysts and other stakeholders to develop and execute necessary processes and controls around the flow of data to meet data governance standards.\n- Verify accuracy of table changes and data transformation processes\n- Deliver fully tested code prior to prod-deployment when appropriate.\n- Recommend and implement enhancements that address system performance requirements, streamline processes and improve data integrity.\n- Create sound technical documentation and train peer developers on this documentation as development completes.\n- Additional duties as assigned to ensure company success.\nThe compensation for this role varies depending on depth and length of experience, and can range from 27-36 LAKH.\n\nWe are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElOVEwgSW5kaWEgLSBFT1IgNmZiNTcwZjgiLCJjb21wYW55X25hbWUiOiJJbnNpZ2h0IEdsb2JhbCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiZUtoaVo5cWozWC1wLXlBVEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 days ago",
         "Data Engineer"
        ],
        [
         "R&D Data Engineer",
         "Sanofi",
         "Hyderabad, Telangana, India",
         "Position Title: R&D Data Engineer\n\nAbout the Job\n\nAt Sanofi, we’re committed to providing the next-gen healthcare that patients and customers need. It’s about harnessing data insights and leveraging AI responsibly to search deeper and solve sooner than ever before. Join our R&D Data & AI Products and Platforms Team as an R&D Data Engineer\n\nand you can help make it happen.\n\nWhat you will be doing:\n\nSanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives.\n\nThe R&D Data & AI Products and Platforms Team is a key team within R&D Digital, focused on developing and delivering Data and AI products for R&D use cases. This team plays a critical role in pursuing broader democratization of data across R&D and providing the foundation to scale AI/ML, advanced analytics, and operational analytics capabilities.\n\nAs an R&D Data Engineer, you will join this dynamic team committed to driving strategic and operational digital priorities and initiatives in R&D. You will work as a part of a Data & AI Product Delivery Pod, lead by a Product Owner, in an agile environment to deliver Data & AI Products. As a part of this team, you will be responsible for the design and development of data pipelines and workflows to ingest, curate, process, and store large volumes of complex structured and unstructured data. You will have the ability to work on multiple data products serving multiple areas of the business.\n\nOur vision for digital, data analytics and AI\n\nJoin us on our journey in enabling Sanofi’s Digital Transformation through becoming an AI first organization. This means:\n• AI Factory - Versatile Teams Operating in Cross Functional Pods: Utilizing digital and data resources to develop AI products, bringing data management, AI and product development skills to products, programs and projects to create an agile, fulfilling and meaningful work environment.\n• Leading Edge Tech Stack: Experience build products that will be deployed globally on a leading-edge tech stack.\n• World Class Mentorship and Training: Working with renown leaders and academics in machine learning to further develop your skillsets.\n\nWe are an innovative global healthcare company with one purpose: to chase the miracles of science to improve people’s lives. We’re also a company where you can flourish and grow your career, with countless opportunities to explore, make connections with people, and stretch the limits of what you thought was possible. Ready to get started?\n\nMain Responsibilities:\n\nData Product Engineering:\n• Provide input into the engineering feasibility of developing specific R&D Data/AI Products\n• Provide input to Data/AI Product Owner and Scrum Master to support with planning, capacity, and resource estimates\n• Design, build, and maintain scalable and reusable ETL / ELT pipelines to ingest, transform, clean, and load data from sources into central platforms / repositories\n• Structure and provision data to support modeling and data discovery, including filtering, tagging, joining, parsing and normalizing data\n• Collaborate with Data/AI Product Owner and Scrum Master to share Progress on engineering activities and inform of any delays, issues, bugs, or risks with proposed remediation plans\n• Design, develop, and deploy APIs, data feeds, or specific features required by product design and user stories\n• Optimize data workflows to drive high performance and reliability of implemented data products\n• Oversee and support junior engineer with Data/AI Product testing requirements and execution\n\nInnovation & Team Collaboration:\n• Stay current on industry trends, emerging technologies, and best practices in data product engineering\n• Contribute to a team culture of of innovation, collaboration, and continuous learning within the product team\n\nAbout You:\n\nKey Functional Requirements & Qualifications:\n• Bachelor’s degree in software engineering or related field, or equivalent work experience\n• 3-5 years of experience in data product engineering, software engineering, or other related field\n• Understanding of R&D business and data environment preferred\n• Excellent communication and collaboration skills\n• Working knowledge and comfort working with Agile methodologies\n\nKey Technical Requirements & Qualifications:\n• Proficiency with data analytics and statistical software (incl. SQL, Python, Java, Excel, AWS, Snowflake, Informatica)\n• Deep understanding and proven track record of developing data pipelines and workflows\n\nWhy Choose Us?\n• Bring the miracles of science to life alongside a supportive, future-focused team\n• Discover endless opportunities to grow your talent and drive your career, whether it’s through a promotion or lateral move, at home or internationally\n• Enjoy a thoughtful, well-crafted rewards package that recognizes your contribution and amplifies your impact\n• Take good care of yourself and your family, with a wide range of health and wellbeing benefits including high-quality healthcare, prevention and wellness programs\n\nPursue Progress. Discover Extraordinary.\n\nProgress doesn’t happen without people – people from different backgrounds, in different locations, doing different roles, all united by one thing: a desire to make miracles happen. You can be one of those people. Chasing change, embracing new ideas and exploring all the opportunities we have to offer. Let’s pursue progress. And let’s discover Extraordinary together.\n\nAt Sanofi, we provide equal opportunities to all regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or gender identity.\n\nWatch our ALL IN video and check out our Diversity Equity and Inclusion actions at sanofi.com!",
         "eyJqb2JfdGl0bGUiOiJSXHUwMDI2RCBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiU2Fub2ZpIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJWUzRuREMyZ2hRajk3SVhDQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "Data Engineer"
        ],
        [
         "Data Engineer-Senior II",
         "Federal Express Corporation AMEA",
         "Hyderabad, Telangana, India",
         "Responsible for supporting the development and maintenance of business intelligence and analytics systems, implementing data warehouse architecture, enabling self-service data exploration, designing data models, implementing data security strategies, optimizing production processes, providing user training and support, and overseeing project management activities as a Data Engineer to facilitate data-driven decision-making and generate insights for business stakeholders.\n\n1. Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making.\n2. Implement of business intelligence and analytics systems, ensuring alignment with business requirements.\n3. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets.\n4. Enable self-service data exploration capabilities for users to analyze and visualize data independently.\n5. Develop reporting and analysis applications to generate insights from data for business stakeholders.\n6. Design and implement data models to organize and structure data for analytical purposes.\n7. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information.\n8. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability.\n9. Assist in training and support to users on business intelligence tools and applications.\n10. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions.\n\nEducation: Bachelors’ degree or equivalent in Computer Science, MIS, or similar discipline.\nAccreditation: Specific business accreditation for Business Intelligence.\n\nExperience: Relevant work experience in data engineering based on the following number of years:\nAssociate: Prior experience not required\nStandard I: Two (2) years\nStandard II: Three (3) years\nSenior I: Four (4) years\nSenior II: Five (5) years\n\nKnowledge, Skills and Abilities\n• Fluency in English\n• Analytical Skills\n• Accuracy & Attention to Detail\n• Numerical Skills\n• Planning & Organizing Skills\n• Presentation Skills\n\nPreferred Qualifications:\n\nPay Transparency:\n\nPay:\n\nAdditional Details:\n\nFedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n\nAll qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\nOur Company\n\nFedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World’s Most Admired Companies by \"Fortune\" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\nOur Philosophy\n\nThe People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\nOur Culture\n\nOur culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970’s. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today’s global marketplace.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVNlbmlvciBJSSIsImNvbXBhbnlfbmFtZSI6IkZlZGVyYWwgRXhwcmVzcyBDb3Jwb3JhdGlvbiBBTUVBIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJhZDU5RmU0Vl94QzdTVGFqQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "1 day ago",
         "Data Engineer"
        ],
        [
         "Lead Data Engineer(Snowflake,PowerBi)",
         "Thomson Reuters",
         "Hyderabad, Telangana, India (+1 other)",
         "Want to be part of the Data & Analytics organization, whose strategic goal is to create a world-class Data & Analytics company by building, embedding, and maturing a data-driven culture across Thomson Reuters.\n\nAbout The Role\nWe are looking for a highly motivated individual with strong organizational and technical skills for the position of Lead Data Engineer/ Data Engineering Manager (Snowflake). You will play a critical role working on cutting edge of Data Engineering and analytics, leveraging predictive models, machine learning and generative AI to drive business insights and facilitating informed decision-making and help Thomson Reuters rapidly scale data-driven initiatives.\n\nEffectively communicate across various levels, including Executives, and functions within the global organization.\nDemonstrate strong leadership skills with ability to drive projects/tasks to delivering value\nEngage with stakeholders, business analysts and project team to understand the data requirements.\nDesign analytical frameworks to provide insights into a business problem.\nExplore and visualize multiple data sets to understand data available and prepare data for problem solving.\nDesign database models (if a data mart or operational data store is required to aggregate data for modeling).\n\nAbout You\nYou're a fit for the Lead Data Engineer/ Data Engineering Manager (Snowflake), if your background includes:\nQualifications: B-Tech/M-Tech/MCA or equivalent\nExperience: 7-9 years of corporate experience\nLocation: Bangalore, India\nHands-on experience in developing data models for large scale data warehouse/data Lake – Snowflake, BW\nMap the data journey from operational system sources through any transformations in transit to its delivery into enterprise repositories (Warehouse, Data Lake, Master Data, etc.)\nEnabling on the overall master and reference data strategy, including the procedures to ensure the consistency and quality of Finance reference data.\nExperience across ETL, SQL and other emerging data technologies with experience in integrations of a cloud-based analytics environment\nBuild and refine end-to-end data workflows to offer actionable insights\nFair understanding of Data Strategy, Data Governance Process\nKnowledge in BI analytics and visualization tools: Power BI, Tableau\n\n#LI-NR1\n\nWhat’s in it For You?\n• Hybrid Work Model: We’ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\n• Flexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\n• Career Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow’s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\n• Industry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\n• Culture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\n• Social Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\n• Making a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\n\nAbout Us\n\nThomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\n\nWe are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\n\nAs a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\n\nWe also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here.\n\nLearn more on how to protect yourself from fraudulent job postings here.\n\nMore information about Thomson Reuters can be found on thomsonreuters.com.",
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIoU25vd2ZsYWtlLFBvd2VyQmkpIiwiY29tcGFueV9uYW1lIjoiVGhvbXNvbiBSZXV0ZXJzIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJzSFl5TXBoOXlWbDl4R0pBQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "13 days ago",
         "Data Engineer"
        ],
        [
         "Engr II-Data Engineering",
         "Verizon",
         "Hyderabad, Telangana, India (+1 other)",
         "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat you’ll be doing…\n\nWe are looking for data engineers who can work with world class team members to help drive telecom business to its full potential . We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n\nAs a data engineer you will be collaborating with business product owners , coaches , industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n\nUnderstanding the business requirements and the technical design.\n\nWorking on Data Ingestion, Preparation and Transformation.\n\nDeveloping data streaming applications.\n\nDebugging the production failures and identifying the solution.\n\nWorking on ETL/ELT development.\n\nWhere you'll be working:\n\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nWhat we’re looking for...\n\nYou’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n\nYou’ll need to have:\n\nBachelor’s degree or one or more years of work experience.\n\nExperience with Data Warehouse concepts and Data Management life cycle.\n\nExperience in any DBMS\n\nExperience in Shell scripting, Spark, Scala.\n\nKnowledge in GCP/BigQuery.\n\nEven better if you have:\n\nTwo or more years of relevant experience.\n\nAny relevant Certification on ETL/ELT developer.\n\nCertification in GCP-Data Engineer.\n\nAccuracy and attention to detail.\n\nGood problem solving, analytical, and research capabilities.\n\nGood verbal and written communication.\n\nExperience presenting to and influence stakeholders.\n\n#AI&D\n\nWhere you’ll be working\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.",
         "eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJJZ05hWTZ2NWxkNUFrcWRyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "Data Engineer"
        ],
        [
         "Senior Data Engineer - Data Integration",
         "EPAM Systems",
         "Hyderabad, Telangana, India",
         "EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.\n\nOur company is looking for an experienced Senior Data Engineer to join our team.\n\nAs a Senior Data Engineer, you will be working on a project that focuses on data integration and ETL for cloud-based platforms. You will be responsible for designing and implementing complex data solutions, ensuring that the data is accurate, reliable, and easily accessible.\n\nRESPONSIBILITIES\n• Design and implement complex data solutions for cloud-based platforms\n• Develop ETL processes using SQL, Python, and other relevant technologies\n• Ensure that data is accurate, reliable, and easily accessible for all stakeholders\n• Collaborate with cross-functional teams to understand data integration needs and requirements\n• Develop and maintain documentation, including technical specifications, data flow diagrams, and data mappings\n• Monitor and optimize data integration processes for performance and efficiency, ensuring data accuracy and integrity\n\nREQUIREMENTS\n• Bachelor's degree in Computer Science, Electrical Engineering, or a related field\n• 5-8 years of experience in data engineering\n• Experience with cloud-native or Spark-based ETL tools such as AWS Glue, Azure Data Factory, or GCP Dataflow\n• Strong knowledge of SQL for data querying and manipulation\n• Experience with Snowflake for data warehousing\n• Experience with cloud platforms such as AWS, GCP, or Azure for data storage and processing\n• Excellent problem-solving skills and attention to detail\n• Good verbal and written communication skills in English at a B2 level\n\nNICE TO HAVE\n• Experience with ETL using Python\n\nWE OFFER\n• Opportunity to work on technical challenges that may impact across geographies\n• Vast opportunities for self-development: online university, knowledge sharing opportunities globally, learning opportunities through external certifications\n• Opportunity to share your ideas on international platforms\n• Sponsored Tech Talks & Hackathons\n• Unlimited access to LinkedIn learning solutions\n• Possibility to relocate to any EPAM office for short and long-term projects\n• Focused individual development\n• Benefit package\n• Health benefits\n• Retirement benefits\n• Paid time off\n• Flexible benefits\n• Forums to explore beyond work passion (CSR, photography, painting, sports, etc.)",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAtIERhdGEgSW50ZWdyYXRpb24iLCJjb21wYW55X25hbWUiOiJFUEFNIFN5c3RlbXMiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZnZkJvSVM5OFhDSUw3NWNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "Data Engineer"
        ],
        [
         "Python Developer – Telegram Bot Integration & Excel Automation",
         "SANGA & ASSOCIATES - EQUIDOTE",
         "Anywhere",
         "Job Title:\n\nPython Developer – Telegram Bot Integration & Excel Automation\n\nJob Description:\n\nWe are seeking a skilled and detail-oriented Python Developer to help automate the process of sending structured trade updates from an Excel file to a Telegram channel using a Telegram bot.\n\nThis is a freelance / part-time project with the potential for ongoing work based on performance.\n\nResponsibilities:\n• Read data from an Excel file that is regularly updated using Python.\n• Format and send messages to a Telegram channel or group using the Telegram Bot API.\n• Implement conditions to filter or trigger messages based on specific columns (e.g., P&L thresholds).\n• Ensure the messages are well-formatted and synchronized.\n• (Optional) Schedule the script to run at regular intervals (e.g., every 5 mins or on update).\n• Ensure error handling, avoid duplicate messages, and maintain clean logs.\n\nRequired Skills:\n• Strong experience with Python scripting\n• Proficiency in using pandas for Excel/CSV handling\n• Working knowledge of the Telegram Bot API\n• Experience with HTTP requests (requests library)\n• Ability to format dynamic messages (Markdown/HTML for Telegram)\n• (Optional but a plus) Familiarity with scheduling tools like schedule, cron jobs, or apscheduler\n\nNice to Have:\n• Understanding of stock market data or options trading (for better context)\n• Experience integrating with trading APIs or using TradingView alerts\n• Basic knowledge of Excel automation or VBA\n\nProject Details:\n• Project Type: One-time setup, with possible ongoing maintenance\n• Location: Remote (India preferred)\n• Start Date: Immediate\n\nHow to Apply:\n\nPlease apply with:\n• A short summary of your experience with Python + Telegram Bots\n• A link to any relevant projects or GitHub repos\n• Your expected rate and estimated time to complete the task\n\nJob Type: Freelance\n\nBenefits:\n• Health insurance\n• Provident Fund\n• Work from home\n\nSchedule:\n• Day shift\n\nSupplemental Pay:\n• Performance bonus\n• Yearly bonus\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBUZWxlZ3JhbSBCb3QgSW50ZWdyYXRpb24gXHUwMDI2IEV4Y2VsIEF1dG9tYXRpb24iLCJjb21wYW55X25hbWUiOiJTQU5HQSBcdTAwMjYgQVNTT0NJQVRFUyAtIEVRVUlET1RFIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNBNXdGSEplRUV3c2lQVWJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "1 day ago",
         "Python Developer"
        ],
        [
         "Python Developer - Remote",
         "Xpress Health",
         "Anywhere",
         "Job Title: Python Developer\nLocation: Remote\nSalary: Up to ₹12 LPA (based on experience and skillset)\nExperience: 3–6 years (preferred)\nEmployment Type: Full-time\n\nAbout Xpress Health\n\nXpress Health is a healthtech platform transforming clinical workforce management by connecting healthcare professionals to facilities in real time. With operations across Ireland and rapid expansion underway, we’re solving critical staffing challenges in the healthcare sector using advanced technology and automation.\n\nRole Overview\n\nWe are looking for a Python Developer with solid experience in backend development and building scalable, secure systems. You will work closely with cross-functional teams to build backend services, automate operations, and power data-driven features in our real-time healthcare staffing platform. If you're passionate about solving real-world problems through code, this role is for you.\n\nKey Responsibilities\n• Design, develop, and maintain backend services and APIs using Python (preferably with frameworks like Django or Flask).\n• Build scalable systems for real-time scheduling, user management, and analytics.\n• Integrate third-party APIs and internal services securely and efficiently.\n• Work with databases (SQL and NoSQL) to design efficient schemas and queries.\n• Optimize performance and ensure system reliability under scale.\n• Collaborate with frontend, product, and QA teams to deliver complete features.\n• Write clean, maintainable, and well-documented code.\n• Participate in code reviews, system design discussions, and architecture planning.\n\nRequirements\n• 3–6 years of professional experience with Python backend development.\n• Strong knowledge of Django, Flask, or other web frameworks.\n• Proficient in working with relational databases (MySQL/PostgreSQL) and REST APIs.\n• Experience with Git, CI/CD pipelines, and working in an Agile environment.\n• Strong debugging, testing, and problem-solving skills.\n• Good communication and ability to collaborate with remote teams.\n\nPreferred Qualifications\n• Experience in healthcare, staffing, or enterprise SaaS platforms.\n• Familiarity with containerization tools like Docker and orchestration platforms (Kubernetes).\n• Exposure to cloud platforms like AWS, GCP, or Azure.\n• Knowledge of async programming and task queues (e.g., Celery, Redis).\n• Experience working with frontend teams using React/Vue (a plus).\n\nWhat We Offer\n• Competitive salary up to ₹12 LPA, depending on experience.\n• A mission-driven environment working on meaningful, real-world problems.\n• Opportunity to shape a rapidly scaling healthtech product.\n• Flexible work culture with remote options and learning opportunities.\n• Collaborative, cross-functional team with international exposure.\n\nBe part of a product-first team building real-time tech that supports critical healthcare systems. Join Xpress Health as a Python Developer !!!\n\nJob Type: Full-time\n\nPay: Up to ₹1,200,000.00 per year\n\nBenefits:\n• Paid time off\n• Work from home\n\nLocation Type:\n• Remote\n\nSchedule:\n• Evening shift\n• Fixed shift\n• Monday to Friday\n• UK shift\n\nApplication Question(s):\n• What is your current and expected CTC?\n• Are you currently working? If yes, what is your notice period?\n\nExperience:\n• Python : 5 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIC0gUmVtb3RlIiwiY29tcGFueV9uYW1lIjoiWHByZXNzIEhlYWx0aCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJGWjc0LW4wM2NjU2xGWVVOQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "2 days ago",
         "Python Developer"
        ],
        [
         "Full Stack Developer (Python / React JS)",
         "Hitachi Careers",
         "India",
         "Our Company\n\nWe're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n\nOur group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n\nImagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n\nThe team\n\nWe are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n\nThe role: Full Stack Developer/Specialist\n\nResponsibilities:\n• Design, develop, and maintain applications.\n• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n• Optimise applications for performance, scalability, and user experience.\n• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n• Provide training and support to end-users and IT staff on functionalities and best practices.\n• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n• Participate in project planning, execution, and post-implementation support.\n• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n• What you'll bring\nQualifications:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n• Expertise in development and customisation.\n• Proficiency in Python and React JS\n• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n• Experience with RESTful APIs and web services.\n• Excellent problem-solving skills and attention to detail.\n• Strong communication and collaboration skills.\n• Ability to work independently and as part of a team in a fast-paced environment.\n• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n• Prior experience in building AI applications is a plus.\n• Prior experience with automation tools like UIPath is a plus.\n• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\nPreferred Skills\n• Certification in\n• Experience with other low-code/no-code platforms.\n• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n• Experience with Agile development methodologies.\nAbout us\n\nWe're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n\n#LI-MS3\n\nChampioning diversity, equity, and inclusion\n\nDiversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n\nHow we look after you\n\nWe help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\nWe're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",
         "eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "Python Developer"
        ],
        [
         "Python Developer Role",
         "Pitangent Analytics and Technology Solutions Pvt. Ltd.",
         "India",
         "Overview\n\nPi tangent Group of Companies is an ISO:9001: 2015 Certified, CMMIL-3, Award winning Software Development Company in Eastern India. It caters to areas like AI/ML to Web development to SAAS engineering. The Group is actively seeking out talented, enthusiastic Python Developers as a value addition to the growing team at Kolkata.\n\nKey Responsibilities\n• Design and develop robust backend applications using Python.\n• Collaborate with front-end developers to integrate user-facing elements with server-side logic.\n• Implement RESTful APIs for seamless communication between server and client.\n• Write reusable, testable, and efficient code following best practices.\n• Manage and optimize multiple databases and data storage solutions.\n• Perform unit and integration testing to ensure software reliability.\n• Participate in code reviews and maintain version control in Git.\n• Gather and analyze user requirements to provide optimal solutions\n• Contribute to project documentation and specifications.\n• Collaborate with QA engineers to troubleshoot and resolve issues.\n• Maintain quality assurance processes to ensure best practices are enforced.\n• Engage in agile development practices, participating in sprints and meetings.\n• Mentor junior developers and provide guidance as needed.\n\nRequired Qualifications\n• Bachelor's degree in computer science or related field.\n• 1-2 yrs of experience in Python development.\n• Strong understanding of Django or Flask web frameworks.\n• Proficient with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB).\n• Experience with version control systems, preferably Git.\n• Solid understanding of RESTful API design principles.\n• Familiarity with front-end technologies (HTML, CSS, JavaScript).\n• Experience with containerization tools such as Docker.\n• Strong communication and teamwork abilities.\n• Familiarity with cloud services (AWS, Azure) is a plus.\n• Understanding of security principles and best practices.\n• Experience with Agile/Scrum methodologies.\n• Proven ability to manage multiple tasks and meet deadlines.\n\nSkills: agile,communication,nosql,scrum,aws lambda,docker,sql,azure,html,javascript,mongodb,css,django,aws,git,postgresql,restful apis,python,flask",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIFJvbGUiLCJjb21wYW55X25hbWUiOiJQaXRhbmdlbnQgQW5hbHl0aWNzIGFuZCBUZWNobm9sb2d5IFNvbHV0aW9ucyBQdnQuIEx0ZC4iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiLUNHQ2RNdUVNQ2V4X3F2MkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "2 days ago",
         "Python Developer"
        ],
        [
         "Python and Groovy Framework Developer",
         "Aptita",
         "India",
         "Urgent Hiring!!!\n\nRole : Python and Groovy Framework Developer\n\nMandatory Skills: Python, Appium, Groovy, Git\n\nExperience: 3 to 8 Years\n\nLocation: Bengaluru\n\nContract - 1Year\n\nJob Description:\n\nQualifications\n\n Bachelor’s or master’s degree in Computer Science, Software Engineering, or a\n\nrelated field\n\n 6 to 8 years of relevant experience in quality assurance, with a proven track record in\n\nWebKit or browser engine testing, including team leadership responsibilities.\n\n Proficiency in CI/CD and test automation frameworks such as Appium, and scripting\n\nlanguages like Python, or Shell.\n\nJob Overview\n\nWe are seeking a dynamic and experienced Lead Python & Groovy Framework developer\n\nto join our team You will be part of a fast-paced, Agile development team and work on a\n\nvariety of projects, from building new tools and solutions to improving existing ones.\n\nIn this role, you will have the chance to grow your skills and take your career to the next\n\nlevel. We offer a supportive, challenging, and exciting work environment, with\n\nopportunities for professional development, training, and advancement.\n\nIf you are a Python & Groovy Framework Developer Engineer with a passion for\n\ntechnology and a drive to continuously improve processes, we want to hear from you!\n\nIf you are passionate about browser engine technologies, performance optimization, and\n\nleadership, we encourage you to apply!\n\nPrimary Skills:\n\n Strong experience in Python Framework development, with the ability to automate\n\nand optimize processes using Jenkins Pipeline script\n\n Good knowledge in Groovy scripting\n\n Expertise in CI/CD tools such as Jenkins, CircleCI, or GitLab\n\n Good understanding of Appium.\n\nStrong Problem solving and debugging skills.\n\n Excellent communication and collaboration skills, both with technical and non-\n\ntechnical stakeholders\n\n Version Control: Familiarity with version control systems such as Git for reviewing\n\nchanges and ensuring test coverage.\n\n Communication: Strong communication and collaboration skills for working with\n\ncross-functional teams.\n\n Agile Methodologies: Experience with Agile Scrum methodologies\n\nNotice Period: Immediate- 30 Days\n\nEmail to : sharmila.m@aptita.com\n\n·",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gYW5kIEdyb292eSBGcmFtZXdvcmsgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQXB0aXRhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Imx1ckt5NFVRSi0wb3JkMi1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "2 days ago",
         "Python Developer"
        ],
        [
         "AI Python Developer",
         "Allianz Insurance",
         "India",
         "We are seeking a skilled AI Developer proficient in Python to support various AI use cases within our dynamic team. The ideal candidate will possess a strong understanding of Python programming and a basic familiarity with PyTest. This role will also provide opportunities to work with cloud computing technologies, particularly Azure, and develop APIs.\n\nKey Responsibilities:\n• Utilize advanced Python skills to support and enhance AI use cases, ensuring the efficient and effective implementation of AI solutions.\n• Develop and execute comprehensive test scripts using PyTest to ensure the reliability and accuracy of AI models and applications.\n• Collaborate with cross-functional teams, including data scientists, engineers, and product managers, to integrate AI capabilities into existing systems and workflows.\n• Analyze and optimize AI algorithms for performance and scalability, ensuring they meet business requirements.\n• Stay updated with the latest advancements in AI and machine learning technologies and apply them to improve existing solutions.\n\nRequirements:\n\nMust-Have\n• Python: Advanced proficiency with extensive experience in writing efficient, maintainable, and scalable code. Demonstrated ability to solve complex problems using Python.\n• PyTest: Basic experience in writing and executing unit tests, with a fundamental understanding of test-driven development practices.\n\nGood-to-Have\n• Cloud Computing: Familiarity with cloud platforms and services, with hands-on experience in deploying and managing AI applications in a cloud environment.\n• Azure: Experience with Microsoft Azure, particularly in using its AI and machine learning services.\n• APIs: Knowledge of designing, developing, and consuming APIs, particularly RESTful APIs, for integrating AI solutions with other systems and applications\n\nAbout Allianz Technology\n\nAllianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 13,000 employees located in 22 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry. We oversee the full digitalization spectrum – from one of the industry’s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n\nD&I statement\n\nAllianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, race or ethnicity, age, nationality, religion, disability, or philosophy of life. Join us. Let´s care for tomorrow.",
         "eyJqb2JfdGl0bGUiOiJBSSBQeXRob24gRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQWxsaWFueiBJbnN1cmFuY2UiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQkg5Z0tqUFM2TjBGeExOb0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "24 days ago",
         "Python Developer"
        ],
        [
         "Developer- Angular, Python & Azure",
         "The Value Maximizer",
         "India",
         "About the Role :\n\nAs a Senior Developer, you will play a critical role in designing, implementing, and optimizing IoT applications, Salesforce development, and web solutions. This role is ideal for experienced developers eager to lead projects, mentor junior team members, and shape technical strategies across diverse platforms and technologies. You will be an important part of our international project teams, will work closely with team members from the headquarters and if qualified can lead international software development projects.\n\nKey Responsibilities :\n• Central role and expert for development of IoT frontend applications with Angular, Capacitor, and PrimeNG\n• Design and implement IoT backend systems using C#, .Net 8.0, and the Azure platform\n• Management and optimization of CI/CD pipelines using Azure DevOps and Bicep\n• Drive advanced Salesforce development efforts, including customizations, integrations, and automation\n• Manage and enhance the SharePoint Online intranet platform\n• Architect and implement Power Platform solutions tailored to business needs\n• Develop and maintain complex web applications using Django (Python) and PHP\n• Supervise website development and optimization with Typo3, PHP, and SolR, collaborating closely with the marketing department\n• Monitor and ensure the performance, compliance, and SEO optimization of global websites and portals\n\nKey Requirements :\n• 6-8 years of experience\n• Strong expertise in Angular, Python, and C#\n• Advanced experience with CI/CD pipelines and version control systems (e.g., Azure DevOps or Git)\n• Proficiency in web technologies such as PHP, MySQL, and modern frameworks\n• In-depth knowledge of Salesforce, Azure cloud solutions, and SharePoint Online\n• Proven ability to lead technical projects, mentor team members, and collaborate across departments\n• Excellent problem-solving skills, attention to detail, and a proactive approach to innovation\n• Proficiency in German is an advantage but not a requirement. Candidates must, however, have a minimum of B2-level English proficiency to communicate effectively within the international teams and our headquarters",
         "eyJqb2JfdGl0bGUiOiJEZXZlbG9wZXItIEFuZ3VsYXIsIFB5dGhvbiBcdTAwMjYgQXp1cmUiLCJjb21wYW55X25hbWUiOiJUaGUgVmFsdWUgTWF4aW1pemVyIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRteXhMclAxTG5oaWFJZEVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "Python Developer"
        ],
        [
         "Freelance Python Developer",
         "GBIM Technologies Pvt.Ltd.",
         "Anywhere",
         "We’re Hiring – Freelance Python Developer (Experienced)\nWe are currently looking for a freelance Python developer who brings hands-on experience and can help us build and debug efficient backend systems, automate processes, and solve critical issues with speed and precision.\nKey Expertise Required:\n\nPython (Backend Development)\n\nWeb Scraping & Data Extraction\n\nWeb Automation\n\nFlask | Pandas | ETL\n\nAWS (Basic to Intermediate)\n\nGoogle / Meta / LinkedIn / Third-Party API Integration\n\nProblem-solving mindset – quick in identifying & fixing bugs/errors\n\nIf you are a solution-driven individual with a solid track record in handling Python-based backend projects and automation tasks, we’d love to connect with you!\nPlease DM or share your portfolio/work samples at [hr@gbim.com / sarabjeet.mann@gbim.in ].\n#PythonDeveloper #FreelanceOpportunity #WebScraping #Automation #BackendDevelopment #Flask #AWS #APIIntegration #DataExtraction #HiringNow #LinkedInJobs\n\nJob Type: Full-time\n\nPay: ₹500.00 - ₹10,000.00 per hour\n\nLocation Type:\n• Remote\n\nSchedule:\n• Day shift\n• Monday to Friday\n\nWork Location: Remote\n\nSpeak with the employer\n+91-XXXXXXXXXX",
         "eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkdCSU0gVGVjaG5vbG9naWVzIFB2dC5MdGQuIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNtMXFFOGh4Y2xUN3Z1ck9BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "3 days ago",
         "Python Developer"
        ],
        [
         "Junior Python Developer",
         "Dehazelabs",
         "Anywhere",
         "Location: Onsite, Kokapet, Hyderabad, Telangana.\n\nJob Type: Full-Time\n\nAbout Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n\nResponsibilities:\n• Develop and maintain Python applications and services\n• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n• Write clean, efficient, and well-documented code\n• Troubleshoot, debug, and optimize applications for performance and scalability\n• Participate in code reviews and maintain coding standards\n• Stay up-to-date with the latest industry trends and technologies\n\nRequirements:\n• Ability to code in Python and SQL\n• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n• Knowledge of version control systems (e.g., Git)\n• Excellent problem-solving skills and attention to detail\n• Strong communication and teamwork abilities.\n\nPreferred Qualifications:\n• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n• Knowledge of RESTful APIs and microservices architecture.",
         "eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Python Developer"
        ],
        [
         "DET-Senior GIG Python Developer-GDSNF02",
         "EY",
         "India",
         "At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. Senior Python Developer – Client – Merck, H and M, TIAA, MINT Job Description Bachelor’s or master’s degree with 3+ years of strong Python development experience Design, develop, and maintain high-performance web applications using Python and related frameworks. Strong understanding of Python OOPs, Data type, Data Structure and algorithm , Exception handling, Decarotor , Generator, Iterator, Automation. Strong understanding of Python Libraries (Pandas, TensorFlow, Numpy, SciPy) Experience in Cloud Azure / AWS Develop, optimize, and manage complex APIs (RESTful or GraphQL). Collaborate with cross-functional teams to define, design, and ship new features. Troubleshoot and resolve advanced technical issues in development and production environments. Conduct technical evaluations of new tools and frameworks, recommending their adoption when appropriate. Stay ahead of emerging trends in Python development, ensuring the team remains at the forefront of innovation. Advanced proficiency in Python and frameworks like Django,Flask, or FastAPI. Good understanding of Database Postgres / MySQL & ORM Library i.e. SQL Alchemy/ any ORM libraries Understanding of Code Repository tools i.e. GIT , SVN Strong understanding of DevOps principles(Docker, Kubernetes and microservices) EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",
         "eyJqb2JfdGl0bGUiOiJERVQtU2VuaW9yIEdJRyBQeXRob24gRGV2ZWxvcGVyLUdEU05GMDIiLCJjb21wYW55X25hbWUiOiJFWSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ4Mk5pUFZHVVJ3cjczaVE1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "18 hours ago",
         "Python Developer"
        ],
        [
         "Informatica ETL Developer: Agile Dev Team Member IV",
         "Capgemini",
         "Hyderabad, Telangana, India",
         "The ideal candidate will be responsible for designing, developing, and deploying scalable ETL processes using Informatica PowerCenter to support our data warehousing and analytics initiatives. You will collaborate with business and technical stakeholders to ensure high data quality, availability, and performance. Key Responsibilities:Design, develop, and maintain ETL workflows and mappings using Informatica PowerCenter or Informatica Intelligent Cloud Services (IICS).Extract, transform, and load data from various source systems (e.g., SQL Server, Oracle, flat files, cloud APIs) into data warehouses or operational data stores.Optimize ETL performance, conduct tuning, and ensure error handling and logging.Collaborate with data architects and analysts to understand data requirements and deliver high-quality data solutions.Work with QA teams to support data validation and testing efforts.Support data integration, migration, and transformation initiatives.Document ETL processes, data flows, and job schedules.Monitor daily ETL jobs and resolve production issues in a timely manner.RequirementsBachelor’s degree in Computer Science, Information Systems, or a related field (or equivalent work experience).3+ years of experience with Informatica PowerCenter or Informatica IICS.Strong SQL skills and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).Solid understanding of data warehousing concepts and dimensional modeling.Experience in performance tuning and troubleshooting ETL processes.Hands-on experience with job scheduling tools (e.g., Autosys, Control-M, Tidal).Familiarity with version control systems and DevOps practices.Preferred Qualifications:Experience with cloud data platforms (e.g., Snowflake, AWS Redshift, Azure Synapse).Exposure to data governance and data quality tools.Knowledge of scripting languages (e.g., Shell, Python).Experience working in Agile/Scrum environments.Familiarity with BI tools (e.g., Tableau, Power BI) is a plus. BenefitsThis position comes with competitive compensation and benefits package: Competitive salary and performance-based bonuses Comprehensive benefits package Home Office model Career development and training opportunities Flexible work arrangements (remote and/or office-based) Dynamic and inclusive work culture within a globally known group Private Health Insurance Pension Plan Paid Time Off Training & Development *Note: Benefits differ based on employee level",
         "eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyOiBBZ2lsZSBEZXYgVGVhbSBNZW1iZXIgSVYiLCJjb21wYW55X25hbWUiOiJDYXBnZW1pbmkiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InFZVlFqNTFEM2VzZ0lZSExBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 days ago",
         "ETL Developer"
        ],
        [
         "Senior ETL and Backend Developer (Salesforce)",
         "S&P Global",
         "Hyderabad, Telangana, India (+1 other)",
         "About the Role:\n\nGrade Level (for internal use):\n10\n\nTitle: Senior ETL and Backend Developer (Salesforce)\n\nJob Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n\nThe Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n\nThe team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n\nThe Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n\nResponsibilities:\n• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\nMonitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\nQualifications:\n\nBasic Qualifications:\n• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n• A minimum of 8+ years of experience in software engineering & Architecture.\n• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n• A minimum of 3+ years of Salesforce development, administration/Integration.\n• Proficiency in Informatica PowerCenter and other ETL tools.\n• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n• Experience with Salesforce integration and administration.\n• Proficiency in backend development languages (e.g., Java, Python, C#).\n• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n• Excellent problem-solving skills and attention to detail.\n• Ability to work independently and as part of a team.\n• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n\nPreferred Qualifications:\n• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n• Experience with other ETL tools and data integration platforms.\n• Informatica Certified Professional\nSalesforce Certified Administrator or Developer\n• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n• Excellent problem solving, analytical and technical troubleshooting skills.\n• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat’s In It For You?\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nWe're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n\nOur benefits include:\n• Health & Wellness: Health care coverage designed for the mind and body.\n• Flexible Downtime: Generous time off helps keep you energized for your time on.\n• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring and Opportunity at S&P Global:\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\n-----------------------------------------------------------\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n-----------------------------------------------------------\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n\nJob ID: 316835\nPosted On: 2025-06-03\nLocation: Hyderabad, Telangana, India",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjVGNUtpak5fRjROQXhDcERBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "19 days ago",
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "Zensar Technologies",
         "Madhavaram, Telangana, India",
         "Job Description\n\nPrimary Skill Set\n• ETL Informatica\n• SQL\n• Unix\n• Realtime Data Integration (CDC) using Power Exchange (Can be optional if candidate good in other skills)\n\nGood to Have\n\nExperience on working with Mainframe Databases/files\n\nETL Batch Scheduling tools like TWS/Tidal\n\nRoles & Responsibilities\n\nInformatica PowerCenter, Unix scripting, SQL/PLSQL\n\nKnowledge of Informatica Power Exchange is preferred\n\nExperience With Mainframe Sources/targets Is Preferred\n• Bachelor’s degree in Computer Science or similar field or equivalent work experience.\n• 5-8 years of development experience with Informatica Power Center on Data Integration projects.\n• Strong analytic, problem-solving and organizational skills.\n• Excellent SQL knowledge and ability to write the complex queries keeping performance aspect in mind.\n• Experience with analysis of business requirements, designing and writing technical specifications to design.\n• Hands-on experience to process mainframe files using Informatica Power Exchange.\n• Hands-on experience with UNIX shell scripting.\n• Participate in testing and issue resolution to validate functionality and performance.\n• Hands-on experience on any job scheduling tool, TWS is preferred.\n• Good written and verbal communication skills.\n\nLocation\n\n1 st Preference: Noida\n\n2 nd Preference: Hyderabad\n\n3 rd Preference: Gurgaon",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiWmVuc2FyIFRlY2hub2xvZ2llcyIsImFkZHJlc3NfY2l0eSI6Ik1hZGhhdmFyYW0sIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InJTQzlGVjVoRHhXZ1h3QURBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "11 days ago",
         "ETL Developer"
        ],
        [
         "Insight Global",
         "Insight Global",
         "Hyderabad, Telangana, India",
         "We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\nResponsibilities:\nLead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\nCollaborate with developers to ensure a smooth transition and integration of the upgraded system.\nEnhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\nProvide support and guidance to the ETL development lead.\nEngage with end-users to ensure their needs are met during and after the upgrade process.\nUtilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\nRequirements:\nMinimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\nProven experience with SQL Server upgrades, particularly from 2012 to 2022.\nStrong SQL Server skills, including building tables, stored procedures, views, and functions.\nExperience with .NET development is highly desirable to be leveraged for other projects\nExcellent problem-solving skills and attention to detail.\nAbility to work effectively in a team environment and communicate with end-users.\nPreferred Qualifications:\nExperience with SQL Server 2016.\nExperience with Oracle\nFamiliarity with the latest features and enhancements in SQL Server 2022.\nStrong understanding of database architecture and migration strategies.\n\nWe are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "eyJqb2JfdGl0bGUiOiJJbnNpZ2h0IEdsb2JhbCIsImNvbXBhbnlfbmFtZSI6Ikluc2lnaHQgR2xvYmFsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location",
         "Fiserv",
         "India",
         "Calling all innovators – find your future at Fiserv.\n\nWe’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n\nJob Title\n\nETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n\nWork Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n• Participates in client kickoff activities including requirements and data gathering sessions.\n• Ability to analyze and migrate incoming external data into Fiserv solutions.\n• Strong working knowledge of ETL processes and best practices.\n• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n• Documents business processes and identifies opportunities for process redesign.\n• Reports system defects and identifies opportunities for system enhancements.\n• Performs as a technical consulting resource for new clients during the implementation process.\n• Supports several project managers during the pre and post go-live activities\n• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n• Provides technical and analytical guidance to the project team.\n• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n• The position requires working in shifts, 2 PM to 11 PM IST\n\nWhat you will need to have:\n• Minimum of 5 – 10 years of experience in software development.\n• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n• Self-starter who can work effectively, both independently and in a team environment.\n• Strong analytical, organizational, and problem-solving skills.\n• Excellent verbal and written communication skills.\n• Ability to maintain a professional attitude and demeanor in high pressure situations.\n• Ability to multi-task and manage multiple projects simultaneously.\n• Weekend/evening availability and support (10% - 25%).\n\nWhat would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n\nThank you for considering employment with Fiserv. Please:\n• Apply using your legal name\n• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n\nWhat you should know about us:\n\nFiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n\nOur commitment to Diversity and Inclusion:\n\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n\nWarning about fake job posts:\n\nPlease be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n\nAny communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n\nIf you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "EPAM - ETL Developer - SSIS/SSRS",
         "Swathi V",
         "Hyderabad, Telangana, India",
         "Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n\nExperience : 5+ Years\n\nLocation : Hyderabad\n\nJob Description :\n\nWe are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n\nYou will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n\nKey Responsibilities :\n\n- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n\n- Develop, deploy, and maintain SSIS packages for ETL processes.\n\n- Work on data warehouse design, development, and maintenance.\n\n- Ensure high performance and reliability of data integration workflows.\n\n- Troubleshoot SQL queries and ETL issues; optimize database performance.\n\n- Collaborate with cross-functional teams to define data solutions.\n\n- Work with complex data structures including JSON and XML.\n\n- Understand and implement different data models (star, snowflake, etc.).",
         "eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IlN3YXRoaSBWIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ2ekQ3VTVqbHNoc2EtN3luQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "Data ETL Developer / BI Engineer",
         "American Express Global Business Travel",
         "India",
         "ETL Developer\n\nAmex GBT is a place where colleagues find inspiration in travel as a force for good and – through their work – can make an impact on our industry. We’re here to help our colleagues achieve success and offer an inclusive and collaborative culture where your voice is valued.\n\nWe are looking for an experienced Data ETL Developer / BI Engineer who loves solving complex problems across a full spectrum of data & technologies. You will lead the building effort of GBT's new BI platform and manage the legacy platform to seamlessly support our business function around data and analytics. You will create dashboards, databases, and other platforms that allow for the efficient collection and evaluation of BI data.\n\nWhat You’ll Do on a Typical Day:\n• Design, implement, and maintain systems that collect and analyze business intelligence data.\n• Design and architect an analytical data store or cluster for the enterprise and implement data pipelines that extract, transform, and load data into an information product that helps the organization reach strategic goals.\n• Create physical and logical data models to store and share data that can be easily consumed for different BI needs.\n• Develop Tableau dashboards and features.\n• Create scalable and high-performance data load and management process to make data available near real-time to support on-demand analytics and insights.\n• Translate complex technical and functional requirements into detailed designs.\n• Investigate and analyze alternative solutions to data storing, processing, etc., to ensure the most streamlined approaches are implemented.\n• Serve as a mentor to junior staff by conducting technical training sessions and reviewing project outputs\n• Design & develop, and maintain a data model implementing ETL processes.\n• Manage and maintain the database, warehouse, & cluster with other dependent infrastructure.\n• Work closely with data, products, and another team to implement data analytic solutions.\n• Support production application and Incident management.\n• Help define data governance policies and support data versioning processes\n• Maintain security and data privacy by working closely with the Data Protection Officer internally.\n• Analyze a vast number of data stores and uncover insights\n\nWhat We’re Looking For:\n• Degree in computer sciences or engineering\n• Overall, 3-5 years of experience in data & data warehouse, ETL, and data modeling.\n• 2+ years of experience working and managing large data stores, complex data pipelines, and BI solutions.\n• Strong experience in SQL and writing complex queries.\n• Hands-on experience with Tableau development.\n• Hands-on working experience on Redshift, data modeling, data warehouse, ETL tool, Python, and Shell scripting.\n• Understanding of data warehousing and data modeling techniques\n• Strong data engineering skills on the AWS Cloud Platform are essential.\n• Knowledge of Linux, SQL, and any scripting language\n• Good interpersonal skills and a positive attitude\n• Experience in travel data would be a plus.\n\nLocation\nGurgaon, India\n\nThe #TeamGBT Experience\n\nWork and life: Find your happy medium at Amex GBT.\n• Flexible benefits are tailored to each country and start the day you do. These include health and welfare insurance plans, retirement programs, parental leave, adoption assistance, and wellbeing resources to support you and your immediate family.\n• Travel perks: get a choice of deals each week from major travel providers on everything from flights to hotels to cruises and car rentals.\n• Develop the skills you want when the time is right for you, with access to over 20,000 courses on our learning platform, leadership courses, and new job openings available to internal candidates first.\n• We strive to champion Inclusion in every aspect of our business at Amex GBT. You can connect with colleagues through our global INclusion Groups, centered around common identities or initiatives, to discuss challenges, obstacles, achievements, and drive company awareness and action.\n• And much more!\n\nAll applicants will receive equal consideration for employment without regard to age, sex, gender (and characteristics related to sex and gender), pregnancy (and related medical conditions), race, color, citizenship, religion, disability, or any other class or characteristic protected by law.\n\nClick Here for Additional Disclosures in Accordance with the LA County Fair Chance Ordinance.\n\nFurthermore, we are committed to providing reasonable accommodation to qualified individuals with disabilities. Please let your recruiter know if you need an accommodation at any point during the hiring process. For details regarding how we protect your data, please consult the Amex GBT Recruitment Privacy Statement.\n\nWhat if I don’t meet every requirement? If you’re passionate about our mission and believe you’d be a phenomenal addition to our team, don’t worry about “checking every box;\" please apply anyway. You may be exactly the person we’re looking for!\nExperience Level\nMid Level\n\nMore about this Data ETL Developer / BI Engineer job\n\nAmerican Express Global Business Travel is aggressively hiring for the job profile of Data ETL Developer / BI Engineer at undefined in null locality. Kindly go through the FAQs below to get all answers related to the given job.\n\n1. How much salary can I expect as a Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n\nAns. You can expect a minimum salary of 0 INR. The salary offered will depend on your skills, experience and performance in the interview.\n\n2. What is the eligibility criteria to apply for Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n\nAns. The candidate should have completed undefined degree and people who have 3 to 5 years are eligible to apply for this job. You can apply for more jobs in undefined to get hired quickly.\n\n3. Is there any specific skill required for this job?\n\nAns. The candidate should have undefined skills and sound communication skills for this job.\n\n4. Who can apply for this job?\n\nAns. Both Male and Female candidates can apply for this job.\n\n5. Is it a work from home job?\n\nAns. No, it’s not a work from home job and can’t be done online. You can explore and apply for other work from home jobs in undefined at apna.\n\n6. Are there any charges or deposits required while applying for the role or while joining?\n\nAns. No work-related deposit needs to be made during your employment with the company.\n\n7. How can I apply for this job?\n\nAns. Go to the apna app and apply for this job. Click on the apply button and call HR directly to schedule your interview.\n\n8. What is the last date to apply?\n\nAns. The last date to apply for this job is .\n\nFor more details, download apna app and find Full Time jobs in undefined. Through apna, you can find jobs in 74 cities across India. Join NOW!",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVUTCBEZXZlbG9wZXIgLyBCSSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkFtZXJpY2FuIEV4cHJlc3MgR2xvYmFsIEJ1c2luZXNzIFRyYXZlbCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJnY3VjckFjcm5KVmRmdjRxQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "6 days ago",
         "ETL Developer"
        ],
        [
         "Informatica ETL Developer - SQL/Power Center",
         "Renovision Automation Services Pvt.Ltd.",
         "Telangana, India",
         "Experience - 6+ yearsWork Mode - HybridJob Summary : We are seeking a skilled Informatica ETL Developer with 5+ years of experience in ETL and Business Intelligence projects. The ideal candidate will have a strong background in Informatica PowerCenter, a solid understanding of data warehousing concepts, and hands-on experience in SQL, performance tuning, and production support. This role involves designing and maintaining robust ETL pipelines to support digital transformation initiatives for clients in manufacturing, automotive, transportation, and engineering domains.Key Responsibilities : - Design, develop, and maintain ETL workflows using Informatica PowerCenter.- Troubleshoot and optimize ETL jobs for performance and reliability.- Analyze complex data sets and write advanced SQL queries for data validation and transformation.- Collaborate with data architects and business analysts to implement data warehousing solutions.- Apply SDLC methodologies throughout the ETL development lifecycle.- Support production environments by identifying and resolving data and performance issues.- Work with Unix shell scripting for job automation and scheduling.Required Skills : - 35 years of hands-on experience with Informatica PowerCenter.- Proficiency in SQL and familiarity with NoSQL platforms.- Experience in ETL performance tuning and troubleshooting.- Solid understanding of Unix/Linux environments and scripting.- Excellent verbal and written communication skills.Preferred Qualifications : - AWS Certification or experience with cloud-based data integration is a plus.- Exposure to data modeling and data governance practices. (ref: hirist.tech)",
         "eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyIC0gU1FML1Bvd2VyIENlbnRlciIsImNvbXBhbnlfbmFtZSI6IlJlbm92aXNpb24gQXV0b21hdGlvbiBTZXJ2aWNlcyBQdnQuTHRkLiIsImFkZHJlc3NfY2l0eSI6IlRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlVqTnczajVrcnJpS21VQjdBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "ETL Developer"
        ],
        [
         "ETL Developer- Hyderabad (2-3+ Years of Experience)",
         "A Client of Analytics Vidhya",
         "Hyderabad, Telangana, India",
         "Role Summary:\n\n•ETL Developer is responsible for Design and Development of ETL Jobs which follow standards, best practices and are maintainable, modular and reusable.\n\n•ETL Developer will analyze and review complex object and data models and the metadata repository in order to structure the processes and data for better management and efficient access.\n\n•Working on multiple projects, and delegating work to Junior Analysts to deliver projects on time.\n\n•Training and mentoring Junior Analysts and building their proficiency in the ETL process.Job Duties:\n\n•Preparing mapping document to extract, transform, and load data ensuring compatibility with all tables and requirement specifications.\n\n•Experience in ETL system design and development with Talend / Pentaho PDI is essential.\n\n•Create quality rules in Talend.\n\n•Tune Talend jobs for performance optimization.\n\n•Write relational and multidimensional database queries.\n\n•Functional Knowledge of Talend Adminstration Center, Job Servers & Load balancing setup, and all its administrative fucntions.\n\n•Develop, maintain, and enhance unit test suites to verify the accuracy of ETL processes, dimensional data, OLAP cubes and various forms of BI content including reports, dashboards, and analytical models.\n\n•Exposure in Map Reduce components of Talend / Pentaho PDI.\n\n•Creating and deploying Talend / Pentaho custom components is an add-on advantage.\n\nJob Specification:\n\n•BE, B.Tech / MS Degree in Computer Science, Engineering or a related subject.\n\n•Having an experience of 2 – 3+ years.\n\n•Comprehensive understanding and working knowledge in Data Warehouse loading, tuning, and maintenance.\n\n•Proficiency with Talend or related tools like Pentaho Data Integration / Kettle.\n\n•Working knowledge of relational database theory and dimensional database models.\n\n•Ability to write complex SQL database queries.\n\n•Ability to work independently.",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyLSBIeWRlcmFiYWQgKDItMysgWWVhcnMgb2YgRXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJBIENsaWVudCBvZiBBbmFseXRpY3MgVmlkaHlhIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJYOGlLR3lLZ0p6MDQ5UkdIQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "Luxoft",
         "Maharashtra, India",
         "Project Description:\n\nOur client is an EU subsidiary of a Global Financial Bank working in multiple markets and asset classes.\n\nDWH/ETL developer will work closely with the Development Lead to design, build interfaces and integrate data from a variety from internal and external data sources into the new Enterprise Data Warehouse environment. The ETL Developer will be responsible for developing ETL primarily utilizing Microsoft & Azure technologies within industry recognized ETL standards, architecture, and best practices.\n\nResponsibilities:\n• Act as a technical expert in the designing, coding, unit testing, supporting, and debugging of data warehouse software components in all aspects of SDLC\n• Apply cloud and ETL engineering skills to solve problems and design approaches\n• Troubleshoot and debug ETL pipelines and creating unit tests for ETL pipelines.\n• Assess query performance and actively contribute to optimizing the code\n• Write technical documentation and specifications\n• Support internal audit by submitting required evidence\n• Create reports and dashboards in the BI portal\n• Work with Development Lead, DWH Architect and QA Engineers to plan, implement and deliver best ETL strategies\n• Work with business analysts to understand requirements to create technical design specifications, gaining a sound understanding of business processes for related applications so that integration processes fulfill the end-user requirements\n• Communicate effectively in a collaborative, complex and high performing team environment as per Agile principles\n\nMandatory Skills Description:\n• Proven work experience as an ETL Developer\n• Advanced knowledge of relational databases and dimensional Data Warehouse modelling concepts\n• Good understanding of physical and logical data modeling\n• Very good understanding of modern SaaS/PaaS data solutions in a cost conscious approach\n• Expert level of knowledge of Microsoft Data stack\n• Experience in developing and deploying data oriented solutions in Cloud (Azure/Synapse Analytics/Fabric)\n• Experience in designing and implementing data transformation and ETL layers using Data Factory, Notebooks\n• Experience with PowerBI for report & dashboard creation. PowerQuery and/or DAX is an advantage.\n• Experience in/understanding of Azure Data Lake Storage\n• Knowledge/use of CI/CD tools and principles, preferably Azure DevOps or Bamboo\n• Strong SQL knowledge, able to create complex SQL queries and good understanding of stored procedures, views, indexes, functions, etc.\n• Good working knowledge of at least one Scripting language. Python is an advantage.\n• Experience with GIT repositories and working with branches. GitHub, Azure DevOps or Bitbucket experience are preferable.\n• Ability to troubleshoot and solve complex technical problems\n• Good understanding of software development best practices\n• Working experience in Agile projects; preferably using JIRA\n• Experience in working in high priority projects preferably greenfield project experience\n• Able to communicate complex information clearly and concisely.\n• Able to work independently and also to collaborate across the organization\n• Highly developed problem-solving skills with minimal supervision\n• Understanding of data governance and enterprise concepts preferably in banking environment\n• Verbal and written communication skills in English are essential.\n\nNice-to-Have Skills Description:\n• Microsoft Fabric\n• Snowflake\n• Background in SSIS/SSAS/SSRS\n• Azure DevTest Labs, ARM templates\n• Azure PurView\n• Banking/finance experience",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiTHV4b2Z0IiwiYWRkcmVzc19jaXR5IjoiTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJlZF9YUGZ2UVNVcTZXdXltQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "ETL Developer"
        ],
        [
         "Data Engineer (Hadoop, Spark, Scala, Hive)",
         "Visa",
         "India",
         "Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n\nMake an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n\nJob Description\n\nTranslate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\n\nGood to have GenAI Exposure and Agentic AI Knowledge.\n\nWork with business partners directly to seek clarity on requirements.\n\nDefine solutions in terms of components, modules, and algorithms.\n\nDesign, develop, document, and implement new programs and subprograms, as well as enhancements, modifications and corrections to existing software.\n\nCreate technical documentation and procedures for installation and maintenance.\n\nWrite Unit Tests covering known use cases using appropriate tools.\n\nIntegrate test frameworks in the development process.\n\nWork with operations to get the solutions deployed.\n\nTake ownership of production deployment of code.\n\nCome up with Coding and Design best practices.\n\nThrive in a self-motivated, internal-innovation driven environment.\n\nAdapt quickly to new application knowledge and changes.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\nQualifications\n\nBasic Qualifications\n\n-Minimum of 6 months of work experience or a Bachelor's Degree\n\nPreferred Qualifications\n\n-Bachelor degree in Computer Science.\n\n-Minimum of 1 plus years of software development experience in Hadoop using\n\nSpark, Scala, Hive.\n\n-Expertise in Object Oriented Programming Language Java, Python.\n\n-Experience using CI CD Process, version control and bug tracking tools.\n\n-Result-oriented with strong analytical and problem-solving skills.\n\n-Experience with automation of job execution, validation and comparison of data\n\nfiles on Hadoop Environment at the field level.\n\n-Experience in leading a small team and being a team player.\n\n-Strong communication skills with proven ability to present complex ideas and\n\ndocument them in a clear and concise way.\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChIYWRvb3AsIFNwYXJrLCBTY2FsYSwgSGl2ZSkiLCJjb21wYW55X25hbWUiOiJWaXNhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImVULTZKTlc2NHVzTUxhdS1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 days ago",
         "Spark Engineer"
        ],
        [
         "Data Engineer - Spark/Python",
         "Etelligens Technologies",
         "India",
         "Job Description\n\nWe are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n\nKey Responsibilities\n• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n• Perform data cleansing and migration from diverse sources to target systems.\n• Collaborate with stakeholders to understand requirements and convert them into technical\n\nsolutions.\n• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n• Support, troubleshoot, and maintain data pipelines and workflows.\n• Participate in all phases of software development lifecycle including unit testing, integration\n\ntesting, and performance testing.\n• Contribute to the modernization of data platforms and analytics tools on Azure.\n• Ensure data quality and integrity across all pipelines and systems.\n\nRequired Skills & Qualifications\n• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n• 1+ year of experience in Python and Spark scripting.\n• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n• Good understanding of Data Warehousing (DWH) concepts.\n• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n• Hands-on experience in data cleansing, transformation, and migration projects.\n• Ability to work independently and within a team environment.\n• Microsoft Certified : Azure Data Engineer Associate\n• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n\nLocation : Artha SEZ, Greater Noida West\n\n(ref:hirist.tech)",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Spark Engineer"
        ],
        [
         "Spark Engineer",
         "Staffingine LLC",
         "India",
         "- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred",
         "eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "1 day ago",
         "Spark Engineer"
        ],
        [
         "Staff Data Engineer (Spark, Python, Hadoop)",
         "Visa",
         "India",
         "Company Description\n\nVisa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n\nWhen you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n\nJoin Visa: A Network Working for Everyone.\nJob Description\n\nThe Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n\nAre you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n\nAs a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n\nEssential Functions\n• Work with manager and clients to fully understand business requirements and desired business outcomes\n• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n• Execute data engineering projects ranging from small to large either individually or as part of a project team\n• Ensure project delivery within timelines and budget requirements\n• Provide coaching and mentoring to junior team members\n\nThis is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\nQualifications\n\n• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n• Minimum of 4 years of hands-on expertise with Java or Scala\n• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n• Experience working in an Agile and Test Driven Development environment.\n• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n• Experience with SAS as a statistical package is preferred\n• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "Spark Engineer"
        ],
        [
         "Databricks Engineer - Spark / PySpark",
         "Enkefalos Technologies LLP",
         "Anywhere",
         "Databricks Engineer – Spark / PySpark\n\nLocation : Remote / Mysore\n\nJoining : Immediate\n\nExperience : 5+ years\n\nResponsibilities :\n\nWill implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n\nRequirements:\n• PySpark (RDDs, DataFrames, performance tuning)\n• Building gold‐layer data models for financial reporting\n• Experience with complex joins, aggregations, GL hierarchies\n• Version handling (Actuals vs Budget), currency conversions\n\nJob Type: Full-time\n\nPay: ₹500,395.35 - ₹1,840,348.25 per year\n\nBenefits:\n• Flexible schedule\n• Paid sick time\n• Provident Fund\n• Work from home\n\nApplication Question(s):\n• Have you worked on ADF/ADLS ?\n• Do you have hands-on experience of Spark / PySpark\n\nExperience:\n• Databricks Engineering: 4 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 days ago",
         "Spark Engineer"
        ],
        [
         "Pi Square Technologies - Spark & Scala Engineer",
         "sandeep raja",
         "India",
         "Job Summary :\n\nWe are seeking a skilled and motivated Spark & Scala Engineer with 46 years of hands-on experience in building scalable data processing applications. The ideal candidate will have a strong background in big data ecosystems and a solid understanding of distributed computing using Apache Spark with Scala.\n\nRoles and Responsibilities :\n\n- Design, build, and maintain efficient, reusable, and reliable Apache Spark applications.\n\n- Optimize Spark applications for maximum speed and scalability.\n\n- Implement data ingestion and ETL processes.\n\n- Collaborate with data scientists and architects to implement complex big data solutions.\n\n- Debug and resolve issues in Spark applications.\n\n- Stay up to date with the latest trends in big data technologies and Apache Spark.\n\n- Write clean, readable, and maintainable code.\n\n- Participate in code reviews and contribute to team knowledge sharing.\n\nRequired Skills :\n\n- 46 years of experience working with Apache Spark (core, SQL, streaming).\n\n- Strong proficiency in Scala programming.\n\n- Experience in building and optimizing data pipelines and ETL workflows.\n\n- Proficient in working with big data tools and frameworks (e.g., Hive, HDFS, Kafka).",
         "eyJqb2JfdGl0bGUiOiJQaSBTcXVhcmUgVGVjaG5vbG9naWVzIC0gU3BhcmsgXHUwMDI2IFNjYWxhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoic2FuZGVlcCByYWphIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlFMNDBNZDljZDlqQkxBX3BBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "27 days ago",
         "Spark Engineer"
        ],
        [
         "Spark Developer",
         "Infosys",
         "India",
         "• Primary skills:Technology->Big Data - Data Processing->Spark\n\nA day in the life of an Infoscion\n• As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\n• You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\n• You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\n• You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\n• You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n• Knowledge of more than one technology\n• Basics of Architecture and Design fundamentals\n• Knowledge of Testing tools\n• Knowledge of agile methodologies\n• Understanding of Project life cycle activities on development and maintenance projects\n• Understanding of one or more Estimation methodologies, Knowledge of Quality processes\n• Basics of business domain to understand the business requirements\n• Analytical abilities, Strong Technical Skills, Good communication skills\n• Good understanding of the technology and domain\n• Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\n• Awareness of latest technologies and trends\n• Excellent problem solving, analytical and debugging skills",
         "eyJqb2JfdGl0bGUiOiJTcGFyayBEZXZlbG9wZXIiLCJjb21wYW55X25hbWUiOiJJbmZvc3lzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkZ2VmRvVC1NQnFvWHRRUmJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "16 days ago",
         "Spark Engineer"
        ],
        [
         "Data Engineer (Snowflake, Spark, AWS) - AVP",
         "12542 Citicorp Services India Private Limited",
         "India",
         "The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "Spark Engineer"
        ],
        [
         "SW Engineer (Java and Bigdata/Hadoop/Spark) 1yr",
         "VISA",
         "India",
         "Job Description\n\nThis position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. They will be an integral part of the Payment Products Development team, focusing on the development of software solutions that leverage data to address business challenges. They will be extensively involved in hands-on activities including proof of concepts, development, and testing. They should demonstrate adaptability and readiness to change tasks according to the team's requirements.\n\nKey Responsibilities\n• Deliver high-quality products and features, taking a holistic view from the initial idea to final delivery. This involves design, development, testing, and accountability for release deployments as well as providing support post-deployment.\n• Understand the product in its entirety and ensure the timely delivery of your assigned module within an agile team setting.\n• Accurately scope efforts, identify risks, and clearly communicate trade-offs with team members and other stakeholders.\n• Coordinate and participate in Continuous Integration activities, testing automation frameworks, and other related tasks, in addition to contributing to core product code.\n• Effectively communicate status updates, issues, and potential risks accurately and promptly.\n• Perform other tasks related to data governance and system infrastructure as required.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\nQualifications\n\nBasic Qualifications\n\n-Bachelor's degree in Computer Science or equivalent field\n\n-Relevant working experience of up to 2 years in the industry\n\n-Proven experience in software development, particularly in data-centric\n\nprojects, demonstrating adherence to standard development best practices\n\n-Strong understanding and practical experience with data structures and\n\nalgorithms, with a passion for tackling complex problems\n\n-Proficiency in Java programming\n\n-Hands-on experience with Big Data technologies such as Hadoop, Spark, and\n\nHive\n\n-Strong knowledge of Unix/Linux operating systems and Shell Scripting\n\n-Proficiency in working with RDBMS and SQL\n\n-Basic knowledge of manual and automated testing\n\n-Familiarity with version control systems, specifically Git\n\n-Awareness of and experience with software design patterns\n\n-Experience working within an Agile framework\n\nPreferred Qualifications\n\n-Proficiency in Scala & Kafka programming is a good to have\n\n-Experience with Airflow for workflow management\n\n-Familiarity with AI concepts and tools, including GitHub Copilot for code\n\ndevelopment\n\n-Exposure to AI/ML development is an added advantage\n\n-Proficiency in working with In-memory Databases like Redis\n\n-Good knowledge of API development is highly advantageous\n\n-Strong verbal and written communication skills, with a proactive and self-\n\nmotivated approach to improving existing processes to enable faster\n\niterations.\n\n-Demonstrated intellectual and analytical rigor, with a keen attention to detail\n\n-Team-oriented, energetic, and collaborative approach to work, coupled with a\n\ndiplomatic and adaptable style\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "eyJqb2JfdGl0bGUiOiJTVyBFbmdpbmVlciAoSmF2YSBhbmQgQmlnZGF0YS9IYWRvb3AvU3BhcmspIDF5ciIsImNvbXBhbnlfbmFtZSI6IlZJU0EiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoidHV1TkJsMDF3amRIc0xnSEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "5 days ago",
         "Spark Engineer"
        ],
        [
         "Big Data Lead/ Lead Data Engineer/Spark Tech Lead",
         "Tanisha Systems  Inc",
         "India",
         "Job Description:Mandatory:10+ years of experience in solution, design and development of applications using Java 8+/J2EE, Spring, Spring-Boot, Micro Services, RESTful Services and with experience in Big Data and with experience working in heavy data background needed.Develop, program, and maintain applications using the Apache Spark open source frameworkWork with different aspects of the Spark ecosystem, including Spark SQL, Data Frames, Datasets, and streaming.Spark Developer must have strong programming skills in Java, Scala, or PythonFamiliar with big data processing tools and techniquesProven experience as a Spark Developer or a related roleStrong programming skills in Java, Scala, or PythonFamiliarity with big data processing tools and techniquesExperience with the Hadoop ecosystemGood understanding of distributed systemsExperience with streaming data platformsMust have strong experience in Big Data and with experience working in heavy data background needed.Must be strong in Cloud AWS event-based architecture, Kubernetes, ELK (Elasticsearch, Logstash & Kibana)Must have excellent experience in designing and Implementing cloud-based solutions in various AWS Services (: s3, Lambda, Step Function, AMQ, SNS, SQS, CloudWatch Events, etc.)Must be well experienced in design and development of Microservice using Spring-Boot and REST API and with GraphQLMust have solid knowledge and experience in NoSQL (MongoDB)Good knowledge and experience in any Queue based implementations.Strong knowledge/experience in ORM Framework - JPA / HibernateGood knowledge in technical concepts – Security, Transaction, Monitoring, PerformanceShould we well have versed with TDD/ATDDShould have experience on Java, Python and Spark2+ years of experience in designing and Implementing cloud-based solutions in various AWS ServicesStrong experience in DevOps tool chain (Jenkins, Artifactory, Ansible/Chef/Puppet/Spinnaker, Maven/Gradle, Atlassian Tool suite)Very Good knowledge and experience in Non-Functional (Technical) Requirements like Security, Transaction, Performance, etc.Excellent analytical and problem-solving skills Nice to have:· Experience in Experience with OAuth implementation using Ping Identity· Experience in API Management (Apigee) or Service Mesh (Istio)· Good knowledge and experience in Queue/Topic (Active-MQ) based implementations.· Good knowledge and experience in Scheduler and Batch Jobs· Experience with scripting languages using Unix· Preferably certified in AWS",
         "eyJqb2JfdGl0bGUiOiJCaWcgRGF0YSBMZWFkLyBMZWFkIERhdGEgRW5naW5lZXIvU3BhcmsgVGVjaCBMZWFkIiwiY29tcGFueV9uYW1lIjoiVGFuaXNoYSBTeXN0ZW1zICBJbmMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQmRWaDV5cnl3aTREUFNOT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 hours ago",
         "Spark Engineer"
        ],
        [
         "Data Insights Analyst",
         "IN10 (FCRS = IN010) Novartis Healthcare Private Limited",
         "India",
         "Responsible for leveraging Google Analytics 4 (GA4) to extract, analyze, and interpret data to support business objectives and decision-making processes. Requires proficient understanding and translating business requirements into actionable insights and recommendations derived from GA4 data.Key Responsibilities:1.Data Extraction & Analysis: Extract and analyze GA4 data to provide meaningful insights into user behavior, website performance, and digital marketing effectiveness.2.Reporting & Dashboarding: Develop custom reports, dashboards, and data visualizations using GA4 data to monitor key metrics and performance indicators.3.Insights and Recommendations: Providing actionable insights and recommendations to optimize website performance, user experience, and marketing campaigns.4.Customization & Configurations: Configure GA4 data streams, events, and conversions to align with business goals and KPIs (key performance indicators). Utilize custom dimensions, metrics, and attribution models to capture and analyze data relevant to unique business needs.5.Pharma knowledge: Understanding of the Pharma industry landscape, including regulatory compliance, patient journey mapping, and healthcare data privacy considerationsMinimum Requirement:·2-4 Years of experience in digital analytics, hands on experience on Google Analytics 4.·Proficiency in Google Analytics and Google Tag Manager.·Strong analytical skills and ability to interpret data.·Understanding of digital marketing concepts and website optimization techniques.·Excellent communication skills to present findings and recommendations effectively·GA4 Certification is strongly preferred",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEluc2lnaHRzIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJJTjEwIChGQ1JTID0gSU4wMTApIE5vdmFydGlzIEhlYWx0aGNhcmUgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImFiMU5uYTFGN1dHbFd4eC1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 days ago",
         "Data Analyst"
        ],
        [
         "Senior Data Management Analyst",
         "Wells Fargo",
         "Hyderabad, Telangana, India",
         "About this role:\n\nWells Fargo is seeking a Senior Data Management Analyst\n\nIn this role, you will:\n• Lead or participate in moderately complex programs and initiatives for data quality, governance, and metadata activities\n• Design and conduct moderately complex analysis to identify and remediate data quality, data integrity, process, and control gaps\n• Analyze, assess, and test data controls and data systems to ensure quality and risk compliance standards are met and adhere to data governance standards and procedures\n• Identify data quality metrics and execute data quality audits to benchmark the state of data quality\n• Develop recommendations for optimal approaches to resolve data quality issues and implement plans for assessing the quality of new data sources leveraging domain expertise and data, business, or process analysis to inform and support solution design\n• Lead project teams and mentor less experienced staff members\n• Drive planning and coordination on moderately complex remediation efforts acting as central point of contact\n• Consult with clients to assess the current state of data and metadata quality within area of assigned responsibility\n• Participate in cross-functional groups to develop companywide data governance strategies\n• Provide input into communication routines with stakeholders, business partners, and experienced leaders\n\nRequired Qualifications:\n• 4+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\n• Experience in large enterprise data initiatives\n• Contact center business or technology experience\n• Experience implementing, managing, or utilizing big data environments (Hadoop, Big Query, etc.)\n• Experience using standard BI tools (Tableau, Power BI, MicroStrategy, etc.) preferably from big data environments\n\nPosting End Date:\n24 Jun 2025\n• Job posting may come down early due to volume of applicants.\n\nWe Value Equal Opportunity\n\nWells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n\nEmployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n\nCandidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n\nApplicants with Disabilities\n\nTo request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n\nDrug and Alcohol Policy\n\nWells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n\nWells Fargo Recruitment and Hiring Requirements:\n\na. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n\nb. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBNYW5hZ2VtZW50IEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJXZWxscyBGYXJnbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEFWbXpFcV9ySTNSUG1faUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Associate/Analyst - Data Analytics",
         "D. E. Shaw India",
         "Hyderabad, Telangana, India",
         "The Financial Research (FinRes) group provides research and analysis support to the entire investment life cycle of the firm’s global proprietary trading strategies. The group caters to almost all the quantitative and qualitative strategies of the firm and its research support covers multiple asset classes, sectors, and geographies.",
         "eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUvQW5hbHlzdCAtIERhdGEgQW5hbHl0aWNzIiwiY29tcGFueV9uYW1lIjoiRC4gRS4gU2hhdyBJbmRpYSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiak9McEJja1dWaXhiWjJ4c0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "14 hours ago",
         "Data Analyst"
        ],
        [
         "Senior Analyst- Data Risk Office",
         "Bristol Myers Squibb",
         "Hyderabad, Telangana, India",
         "Working with Us\nChallenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n\nRoles & Responsibilities\n\nFunctional and Technical\n• Execution and monitoring of data privacy office key activties.\n• Assist in the development and implementation of data privacy policies and procedures to ensure compliance with international, federal, and state regulations, including GDPR, CCPA, and HIPAA.\n• Work closely with legal, IT, HR, and other departments to address data privacy concerns and implement best practices.\n• Assist in the development and delivery of data privacy training and awareness programs across the organization. Keeps up to date with evolution of regulations impacting privacy, ethics, and data.\n• Experienced in configuring and working with various industry leading data risk and privacy tools like OneTrust, TrustArc, Microsoft Purview etc.\n• Demonstrate critical thinking around interpreting business and industry challenges and recommending best practices-based solutions to improve products, processes, systems and reduce risk.\n• Implement organizational IT controls in accordance with applicable regulations and the ability to evaluate and understand the impact of new regulations and requirements.\n\nPeople Management:\n• Responsible for training and mentoring junior staff to meet BMS standards.\n• Preferred experience with working in a multi-cultural, multi-location and diverse environments.\n\nQualifications & Experience\n• B.E./B.Tech. or equivalent in computer science, engineering, life science field\n• Recognized privacy/DLP certifications and experience preferred.\n• At least years of privacy program management, compliance, or strong operations management experience (regulated or healthcare or tech preferred).\n• Knowledge of HIPAA, GDPR, CPRA, PIPL etc., and other privacy regulations is a must.\n• Ability to make decisions that impact own work and other groups/teams and works under minimal supervision.\n• Demonstrates openness to learning and developing. Takes a responsibility for their own and team’s development and growth.\n• Demonstrates an understanding of factors driving team performance and how they contribute to the team's overall success.\n• Excellent English Oral and written communication skills including the ability to deliver clear and articulate presentations.\n• Ability to use PowerPoint, Excel, Word, or other technologies to communicate complex topics to stakeholders, manage personal workload, and track projects and issues.\n\nIf you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n\nUniquely Interesting Work, Life-changing Careers\nWith a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n\nOn-site Protocol\n\nBMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n\nSite-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n\nBMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n\nIf you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHlzdC0gRGF0YSBSaXNrIE9mZmljZSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJsWm9Rdlc1dnAyWjZyYVlMQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst II – Product Information Capabilities | Digital & Technology",
         "General Mills India",
         "India",
         "India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.\n\nPosition Title\n\nSoftware Engineer II – Product Information Capability\n\nFunction/Group\n\nDigital & Technology\n\nLocation\n\nMumbai\n\nShift Timing\n\nRegular\n\nRole Reports to\n\nD&T Manager – Product Information Capability\n\nRemote/Hybrid/in-Office\n\nHybrid\n\nAbout General Mills\n\nWe make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and Häagen-Dazs, we’ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\n\nHow we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\n\nus into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\n\nGeneral Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\n\nWith our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital & Technology (D&T) Innovation, Technology & Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy & Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\n\nWe advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\n\nJob Overview\n\nFunction Overview\n\nThe Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The team's expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\n\nThe team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the \"Work with Heart\" philosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the following Link\n\nPurpose of the role\n\nThis is an exciting time to work in General Mills' Supply Chain & ITQ Organization! We are accelerating Digital Transformation of our Product Information Capabilities (PIC) capabilities to provide a competitive advantage to our business. To this end, we are seeking a skilled and motivated STIBO Developer to join our Product Information Capabilities team. As a STIBO Developer, you will play a crucial role in designing, developing, and implementing solutions within our STIBO STEP platform to support our product information management (PIM) and master data management (MDM) processes. You will collaborate with cross-functional teams to understand business requirements, translate them into technical specifications, and deliver high-quality solutions that meet our business needs.\n\nKey Accountabilities\n• Design, develop, and maintain STIBO STEP solutions to support product information management processes.\n• Write and maintain code for business rules to ensure data quality and consistency.\n• Configure outbound and inbound integrations to exchange data with other systems.\n• Configure gateway endpoints for seamless data flow.\n• Develop and maintain data models within STIBO STEP to accurately represent product information.\n• Build web UI screens for data entry, validation, and reporting.\n• Develop solutions based on documented requirements and specifications.\n• Participate in agile project development, including sprint planning, daily stand-ups, and retrospectives.\n• Collaborate with business analysts, data architects, and other developers to ensure solutions align with business needs and technical standards.\n• Troubleshoot and resolve issues related to STIBO STEP implementations.\n• Stay up-to-date with the latest STIBO STEP features and best practices.\n• Create and maintain technical documentation for STIBO STEP solutions.\n\nMinimum Qualifications\n• Education – Full time graduation from an accredited university (Mandatory- Note: This is the minimum education criteria which cannot be altered)\n• Experience with tools and concepts related to MDM and/or Digital Product Publishing, such as data governance, data quality, data integration and data exporting\n• Exposure to Product Information Management Systems (PIM/MDM)\n• Technical expertise into Stibo platform\n• Experience with Data Syndication partners such as 1WorldSync, Syndigo and Salsify.\n• Exposure to GDSN Standards\n• Strong team player and collaborator with core focus on solution making; Effective analytical and technical skills; Ability to work in a cross functional team environment\n\nPreferred Qualifications\n• Product Information Management / Master Data Management\n• STIBO STEP certification\n• Business Analysis skills\n• SQL, Cloud GCP\n• Agile / SCRUM Delivery\n• Familiarity with Service Bus Integration\n• Preferably experience in Consumer Goods industry.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUkg4oCTIFByb2R1Y3QgSW5mb3JtYXRpb24gQ2FwYWJpbGl0aWVzIHwgRGlnaXRhbCBcdTAwMjYgVGVjaG5vbG9neSIsImNvbXBhbnlfbmFtZSI6IkdlbmVyYWwgTWlsbHMgSW5kaWEiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTWplT1ByZWI4UVZPLVpDVUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "2 days ago",
         "Data Analyst"
        ],
        [
         "Lead Data Management Analyst",
         "Wells Fargo",
         "Hyderabad, Telangana, India",
         "About this role:\n\nWells Fargo is seeking a Lead Data Management Analyst\n\nIn this role, you will:\n• Organize and lead complex companywide initiatives to ensure that data quality is maintained so that data can effectively support business processes\n• Oversee analysis and reporting in support of regulatory requirements\n• Identify and recommend analysis of data quality or integrity issues\n• Evaluate data quality metrics and data quality audits to benchmark the state of data quality\n• Make decisions in complex and multi-faceted situations requiring solid understanding of data governance standards and procedures\n• Identify new data sources and develop recommendations for assessing the quality of new data\n• Lead project teams and mentor less experienced staff members\n• Recommend remediation of process or control gaps that align to management strategy\n• Serve as relationship manager for a line of business\n• Consult with and provide recommendations to senior management regarding assessments of the current state of data and metadata quality within area of assigned responsibility\n• Represent client in cross-functional groups to develop companywide data governance strategies\n• Strategically collaborate and consult with peers, colleagues, and mid-level to senior managers to coordinate and drive collaboration on solution design and remediation execution\n\nRequired Qualifications:\n• 5+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\n• Experience in Data Management, Business Analysis, Analytics, Project Management.\n\nPosting End Date:\n24 Jun 2025\n• Job posting may come down early due to volume of applicants.\n\nWe Value Equal Opportunity\n\nWells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n\nEmployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n\nCandidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n\nApplicants with Disabilities\n\nTo request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n\nDrug and Alcohol Policy\n\nWells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n\nWells Fargo Recruitment and Hiring Requirements:\n\na. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n\nb. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgTWFuYWdlbWVudCBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiV2VsbHMgRmFyZ28iLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ik5YbnE1ZlFtc3M5WXNVQ0JBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Senior Data Analyst, Marketing Science",
         "Crunchyroll",
         "Hyderabad, Telangana, India",
         "About the role\n\nWe are looking for a hands-on Senior Marketing Science Analyst with a passion for understanding data, tracking business trends, and storytelling to join the Center for Data and Insights team. This position will report to the Vice President, Business Intelligence & Analytics, and will collaborate with data engineers, campaign managers, and marketing leaders to investigate campaign trends, build business insights, and recommend proactive measures to enable better business decisions every day. The right person for this position is a service-minded, empathetic problem-solver who will be motivated by the opportunity to build a centralized insights service team from the ground up!\n• Partner with Branch/ Adjust MMPs, direct response, and media buying agencies to define and implement campaign measurement of integrated marketing campaigns, including TV, OOH, Digital, and Social Media across a wide range of business lines (theatrical, streaming, e-commerce, etc.)\n• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n• Evaluating media measurement leveraging MMM (Marketing Mix Modeling), MTA (Multi-Touch Attribution), and Incrementality A/B Testing\n• Lead / Assist in development of testing roadmaps, measurement plans, KPI & KLI reporting based on set marketing objectives linked to key campaigns and always-on digital marketing tactics.\n• Deliver ad hoc analysis for client stakeholders throughout marketing, working with the channel leads, planning, and client service departments.\n• Partner with paid marketing teams in building weekly insight decks for the Marketing leadership team by collaborating effectively across teams, documenting the impact of strategic initiatives on benchmarks, and tracking the progress towards company goals.\n• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n• Connect ideas into cohesive, well-grounded recommendations, using creative, structured, and analytical thinking with the help of effective data visualization\n• Work with offshore and onsite teams and lead the sprint planning/management\n• Maintain a culture of high-quality output and outstanding customer service by effectively communicating at all levels, ensuring that work gets done, and responding effectively to\n\nAbout You\n• 5+ years of experience with data analysis, paid campaign analytics, statistics, experimentation, and optimization.\n• 3+ years of experience in writing complex SQL queries, experience in marketing data automation.\n• 3+ years of experience with data visualization tools like Tableau, Superset, etc.\n• Understanding of install and user-level paid campaign tracking to support multi-touch attribution via UTMs and MMP data for all major marketing channels\n• Proficiency in data analysis, including defining critical metrics, statistical and predictive modeling concepts, descriptive statistics, and experimental design\n• Experience in Marketing analytics tools like Google Analytics, Adjust, Braze, Branch, etc.\n• Experience working with large data sets (Terabytes of data/ billions of records).\n• Deep expertise in measuring marketing performance against lifetime value metrics.\n• Outstanding teamwork skills: You have excellent interpersonal skills. You’re a good listener. You place the success of the company ahead of any particular idea. Proven ability to work across a globally, matrixed organization.\n• BS in Statistics, Computer Science, Information Systems, or a related field\n\nAbout the Team\n\nThe Center for Data and Insights (CDI) is the centralized team of data engineering, BI, analytics, and data science experts, passionate about servicing the organization with timely and certified reports and insights! The mission of the group is to inspire, support, and guide our stakeholders to be data-aware as well as build out the systems of intelligence to discover insights and act on them.\n\nWhy you will love working at Crunchyroll\n\nIn addition to getting to work with fun, passionate and inspired colleagues, you will also enjoy the following benefits and perks:\n• Best-in class medical, dental, and vision private insurance healthcare coverage\n• Access to counseling & mental health sessions 24/7 through our Employee Assistance Program (EAP)\n• Free premium access to Crunchyroll\n• Professional Development\n• Company's Paid Parental Leave\n• up to 26 weeks for birthing parents\n• up to 12 weeks for non-birthing parents\n• Hybrid Work Schedule\n• Paid Time Off\n• Flex Time Off\n• 5 Yasumi Days\n• Half-Day Fridays during the summer\n• Winter Break\n\n#LifeAtCrunchyroll #LI-Hybrid",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBBbmFseXN0LCBNYXJrZXRpbmcgU2NpZW5jZSIsImNvbXBhbnlfbmFtZSI6IkNydW5jaHlyb2xsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiI3QU16TGktYmRTM1gybEV4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Principal Data Analyst",
         "Storable",
         "Serilingampalle (M), Hyderabad, Telangana, India",
         "About the Role:\nWe’re seeking a strategic, data-driven professional to lead our Business Analysis initiatives for our Marketplace product within the Self-Storage vertical. This is a ‘hands on keyboard’ high-impact role that bridges business strategy, analytical rigor, and data visualization —ideal for someone who thrives on translating data into actionable insights that drive performance. As the Principal Data Analyst, you will own our marketplace performance metrics, lead the development of world-class dashboards and reporting frameworks, and partner closely with Product, Marketing, Operations, and Finance teams to inform strategic decisions. You’ll drive both hands-on analysis and organizational data literacy while mentoring a team of analysts and collaborating with engineering on data infrastructure.\nKey Responsibilities:\n\nOwn and continuously optimize key Marketplace KPIs (conversion, revenue yield, inventory, pricing, etc.).\nServe as the steward of metric definitions and business logic—ensuring consistency, accuracy, and alignment across stakeholders.\nHave a deep understanding of how to run a BI environment. Proactive, insightful, curious.\nBuild and scale world class self-service dashboards and Tableau reports for internal and executive stakeholders.\nLead cross-functional analysis projects to uncover trends, diagnose problems, and identify growth and efficiency opportunities.\nTranslate complex data into clear, actionable insights and concise narratives for business and executive audiences.\nDrive rigorous SQL-based analysis from raw datasets, ensuring high-quality, reproducible outputs.\nGuide and collaborate with junior analysts in other groups and collaborate with engineering to shape data pipelines and definitions.\nCollaborate with cross functional partners and teams (Product,Marketing, Finance, Business Insights) to test hypotheses, evaluate feature/campaign performance, and measure outcomes.\nIdentify data gaps or integrity risks and work with stakeholders to improve reliability and accuracy.\nChampion a culture of curiosity, experimentation, and evidence-based decision-making.\nProactively keep other teams in the loop on new data capabilities unlocked and changes being made.\n\nRequirements:\n\n5-7 years of experience in business analysis, data analytics, or related roles, ideally with at least 2+ years in a leadership capacity.\nAdvanced proficiency in SQL with experience working with complex data models and the ability to query complex relational datasets.\nOther data engineering experience is a significant plus to facilitate sourcing/formating of data.\nDeep understanding of performance metrics and business levers in a supply/demand or marketplace environment.\nExperience leading analytics efforts in partnership with cross-functional teams including Product, Marketing, Finance, and Engineering.\nProven track record of developing intuitive, scalable dashboards using Tableau (or similar BI platforms).\nStrong business acumen with the ability to connect the dots between business objectives and analytical frameworks.\nExceptional communication skills with the ability to distill technical findings for non-technical audiences.\nCapable of influencing and informing executive stakeholders with clear, concise insights.\nDemonstrated experience owning KPIs, building data products, and drive insights into action in a fast paced environment.\nAbility to navigate ambiguity, manage and prioritize competing needs, and work cross-functionally.\n\nPreferred Qualifications:\n\nExperience in the storage, real estate, or marketplace industries strongly preferred\nFamiliarity with modern data stack tools such as Snowflake, dbt, or similar Experience analyzing marketplace dynamics or supply/demand business models\nExposure to experimentation frameworks, A/B testing, or uplift modeling\nPrior exposure to high-growth SaaS or Marketplace operations\nData engineering capabilities—especially in sourcing, transforming, or centralizing disparate datasets.\n\nAbout Us:\nAt Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\nWe leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.\nImportant Notice: Protect Yourself from Fraudulent Activities Targeting Job Seekers\nWe’ve been made aware of fraudulent activities where an individual or group is pretending to represent Storable, attempting to deceive job seekers by using our company’s good name and stellar reputation.\nTo protect yourself, please consider the following guidelines:\n– Official Communication: All genuine communication from Storable will come from official email addresses, ending in “@storable.com.” Be cautious of any communication that doesn’t match this criteria.– No Unsolicited Offers: We do not extend job offers without a formal interview process. If you receive an unsolicited job offer claiming to be from Storable or any of its representatives, it’s a red flag.– Verification: If you’re uncertain about the legitimacy of any job offer or communication claiming to be from Storable, please directly contact our HR department directly at POps@storable.com for verification.\nYour security and trust are paramount to us. If you suspect you’ve been contacted by someone falsely claiming to be from Storable or using our company’s name for any dubious purpose, please immediately report the incident to POps@storable.com\nWe’re committed to ensuring a transparent and secure hiring process.\nThank you for your vigilance and interest in joining our team.",
         "eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiU3RvcmFibGUiLCJhZGRyZXNzX2NpdHkiOiJTZXJpbGluZ2FtcGFsbGUgKE0pLCBIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InljaTdhWW0wWnR2MEMzbkZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "18 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst 1",
         "UnitedHealth Group",
         "India",
         "At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.\n\nPrimary Responsibilities:\n• Validate data with administrative source systems (source of truth)\n• Analyze complex datasets\n• Generate actionable insights and recommendations based on data analysis\n• Database Management:\n• Develop and maintain data models, data dictionaries, and other documentation\n• Troubleshoot and resolve database-related issues\n• Data Extraction and Transformation:\n• Support capital project that will, in part, import and transform data from various sources for broker bonus calculations\n• Ensure data integrity and quality through rigorous validation and testing\n• Data Visualization and Reporting:\n• Create visually appealing and informative dashboards and reports\n• Present findings and insights to both technical and non-technical stakeholders in a clear and concise manner\n• Continuous Learning and Improvement:\n• Stay up to date with the latest data analysis techniques and tools\n• Identify opportunities to improve data analysis processes and methodologies\n• Actively participate in knowledge sharing and mentoring within the team\n• Comply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\n• Undergraduate degree or equivalent experience\n• 4+ years Experience as SAS Data Analyst\n• Experience with relational database management systems (e.g., MySQL, Oracle, SQL Server)\n• Experience with statistical analysis\n• Familiarity with data visualization tools (e.g., Tableau, Power BI)\n• Proven excellent problem-solving and critical thinking skills\n• Proven solid communication and presentation skills to effectively convey complex data analysis findings to both technical and non-technical stakeholders\n• Proven ability to work independently and collaboratively in a fast-paced, deadline-driven environment\n• Proven detail-oriented with a focus on accuracy and data integrity\n\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone - of every race, gender, sexuality, age, location and income - deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.\n\n#NTRQ",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IlVuaXRlZEhlYWx0aCBHcm91cCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJhdHFPMEswckdadGwwUy1xQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "3 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst – Competitive Benchmarking & Reporting",
         "Reputation",
         "Hyderabad, Telangana, India",
         "Why Work at Reputation?\n• Reputation reached over $100m in Annual Recurring Revenue (ARR) in 2022 and continues to grow worldwide.\n• We've raised over $200 million in funding from A-list venture capital firms such as Bessemer Venture Partner and Kleiner Perkins, including $150 million in equity financing from Marlin Equity Partners in January 2022.\n• Reputation is trusted by more than 250 partners, including Google, Facebook, Salesforce, J.D. Power, Amazon and Web.com.\n• Our industry leading platform has been recognized by Forrester and Gartner as a vendor of choice in Voice of the Customer, Customer Feedback Management, and Social Suites research reports.\n• The platform is used by 10+ major automotive OEMs and 16,000 auto dealerships, more than 250 healthcare systems, and over 100 leading property management firms.\n• Our executive management team is committed to building a performance-based culture where excellence is rewarded and careers are developed.\n• Who thrives at Reputation? Managers who embody a player-coach mentality. Employees who value teamwork and cross-functional collaboration. People who emphasize perseverance and hustle over quick wins and luck.\n• Our Mission: We exist to forge relationships between companies and communities.\n\nWe are seeking a Data Analyst with expertise in data stewardship and analysis to manage our competitive benchmark data tenants and produce customer-facing reports for marketing and sales. The ideal candidate is proficient in BigQuery (SQL), detail-oriented, and capable of working independently on complex data projects.\n\nResponsibilities:\n• Data Stewardship & Governance: Ensure data accuracy, integrity, and accessibility by managing competitive benchmark data tenants, implementing verification processes, and maintaining governance best practices.\n• Data Pipeline Management: Develop and maintain data ingestion pipelines, including API integrations and web scraping, ensuring timely updates and scalability.\n• Data Validation & Quality Control: Oversee large-scale data input and validation, balancing hands-on data manipulation with automation strategies. Collaborate with data owners to resolve quality issues.\n• Advanced Analytics & Reporting: Conduct in-depth data analysis to uncover trends, patterns, and insights that drive business decisions. Scope, design, and execute analysis projects with minimal supervision.\n• Industry & Market Insights: Develop data-driven industry reports for marketing and sales, leveraging analytics to provide insights on market trends and competitive landscapes.\n• Cross-Functional Collaboration: Work closely with product managers, engineers, marketing, and leadership to resolve data issues, provide training, and ensure alignment on data best practices.\n• Tool & Process Optimization: Design scalable internal tools and reporting solutions to enhance data accessibility, automation, and usability across teams.\n• Stakeholder Communication: Translate complex data insights into actionable recommendations for key stakeholders, including non-technical audiences.\n\nQualifications:\n• 5+ years of proven experience in data analysis, data stewardship, and managing large-scale datasets, with a strong focus on ensuring data accuracy, integrity, and accessibility.\n• Must be proficient with SQL and NoSQL (BigQuery), as well as have the ability to thrive in an environment that often requires patience and creativity to query, extract and validate data in which there are high levels of uncertainty.\n• Strong prior professional experience managing databases and using applicable tools is required.\n• Experience with and knowledge of ETL processes and data migration.\n• Understanding of and prior experience with General Data Protection Regulation.\n• Demonstrated experience and proficiency in using Python in real-world, business applications are substantial advantages to candidates.\n• Highly articulate with outstanding verbal and written communication skills; you are able to explain data-driven insights and analyses intuitively to clients, executives, and technical and non-technical coworkers in both large and small groups.\n• You should be comfortable taking feedback from everyone – Product Managers, Engineers, customers, the CEO, etc. – to help understand business objectives and develop the most powerful analyses and tools possible.\n• Proven ability to operate in a fast-paced, data-driven environment.\n\nWhen you join Reputation, you can expect:\n• Flexible working arrangements.\n• Career growth with paid training tuition opportunities.\n• Active Employee Resource Groups (ERGs) to engage with.\n• An equitable work environment.\n\nOur employees say it best:\n\nAccording to Glassdoor, 94% of our employees approve of our CEO, Joe Burton.\n\nOur employees highlight our:\n• Ample Opportunities- “There are many opportunities to learn and grow. Many open roles are replaced with internal promotions.”\n• Positive Culture- \"Great opportunity and exceptional culture.\" “You will never have a better culture anywhere else. Period.”\n• Training and Tools- “All managers truly want you to succeed, and you are given great tools and training to be successful in your role.”\n• Balance- “Great work life balance and awesome team environment!”\n\nDiversity Programs & Initiatives:\n\nOur Reputation Nation spans around the world. This global perspective allows us to intentionally unlock the magic that comes from diversity of experience to contribute to our success.\n\nAt Reputation, we believe in:\n• Diversity: Embracing a culture that values uniqueness.\n• Inclusion: Inviting diverse groups to take part in company life.\n• Belonging: Helping each individual feel accepted for who they are.\n\n\"At Reputation, we see diversity and inclusion as the foundation for an equitable workplace. Our goal is to empower all of our employees, regardless of their background, to make an impact in their work each and every day.\" - Joe Burton, CEO, Reputation\n\nAdditionally, we offer a variety of benefits and perks, such as:\n• Health Insurance & Wellness Benefits: Group Health Insurance: Medical Insurance with floater policy of up to 10,00,000 for employee + spouse + 2 dependent children + 2 parents / parent-in-laws\n• Maternity Benefits: Medical insurance up to 75,000 INR, 26 weeks of leave for birth, adoption or surrogacy\n• Life Insurance: Insurance at 3x annual cost to the company (Term Insurance, GPA)\n• Accident/Disability Insurance: Insured at 3x base salary for permanent total disability, permanent partial disability and temporary total disability (GPA)\n• OPD: of 7500 per annum per employee\n\nLeaves\n• 10 Company observed holidays a year (Refer to the Holiday Calendar for the Year)\n• 12 Casual/Sick leaves (Pro-rata calculated)\n• 02 Earned Leaves per Month (Pro-rata calculated)\n• 04 Employee Recharge days (aka company holiday/office closed)\n• Maternity & Paternity (6 months)\n• Bereavement Leave (10 Days)\n\nCar Lease:\nReputation offers Car Lease Program that allows employees to lease a car with no upfront cost or down payment. They benefit from a fixed monthly lease rental and 20-30% tax savings.\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\nTo learn more about how we handle the personal data of applicants, visit our Candidate Privacy Notice.\n\nApplicants only - No 3rd party agency candidates.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3Qg4oCTIENvbXBldGl0aXZlIEJlbmNobWFya2luZyBcdTAwMjYgUmVwb3J0aW5nIiwiY29tcGFueV9uYW1lIjoiUmVwdXRhdGlvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiT0NPR3UtU1dxWjh4X0gzS0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 days ago",
         "Data Analyst"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "job_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "posted_at",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "search_role",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "silver_df=df.selectExpr(\n",
    "    \"title\",\n",
    "    \"company_name\",\n",
    "    \"location\",\n",
    "    \"description\",\n",
    "    \"job_id\",  \n",
    "    \"detected_extensions.posted_at as posted_at\",\n",
    "    \"search_role\"\n",
    ").dropna(subset=[\"title\", \"company_name\", \"location\"])\n",
    "\n",
    "#display(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37011888-2765-4680-af24-89c721386ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- ### Converting company_name to upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9aed540-ff81-415d-b8df-1ef3bda636d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>company_name</th><th>location</th><th>description</th><th>job_id</th><th>posted_at</th><th>search_role</th></tr></thead><tbody><tr><td>Lead Consultant - Technical Lead - Fullstack Data Engineer</td><td>ASTRAZENECA</td><td>Chennai, Tamil Nadu, India</td><td>Job Title: Lead Consultant - Technical Lead - Data, Analytics & AI\n",
       "Career Level: E\n",
       "\n",
       "Introduction to role: Are you ready to redefine an industry and change lives? Join our Operations IT organization, where your work will have a direct impact on patients by redefining our ability to develop life-changing medicines. We empower the business to perform at its peak, combining innovative science with leading digital technology platforms and data. As part of the Data Analytics and AI (DA&AI) group, you'll deliver innovative solutions that drive the transformation of medicine development. As the Technical Lead, you'll oversee the technical delivery of products and projects aligned with business objectives, tapping into your expertise in Data and Cloud Engineering, Software Engineering, AI, and more. Collaborate with stakeholders to ensure seamless delivery and maximum business impact!\n",
       "\n",
       "Accountabilities:\n",
       "• Bridge business needs with technical solutions by leading IT application design and implementation.\n",
       "• Collaborate with stakeholders to define and deliver requirements, translating them into detailed specifications.\n",
       "• Own the technical vision of products and compile detailed technical designs, refining user Epics/stories.\n",
       "• Optimize performance, mitigate risks, and ensure alignment to timelines and resource allocations.\n",
       "• Advise on industry trends and standard methodologies to enhance performance and business outcomes.\n",
       "• Provide technical direction and guidance to IT teams and business units.\n",
       "• Contribute to Data & Software Engineering standards and best practices.\n",
       "• Research new technologies to boost system performance and scalability.\n",
       "• Lead and mentor technical teams (Data Engineering, Cloud Engineering, Software Engineering) and work with AI/GenAI leads to foster collaboration and innovation.\n",
       "• Ensure platform stability, scalability, and simplicity while adhering to regulatory requirements and data security standards.\n",
       "• Foster continuous improvement and innovation.\n",
       "• Supervise technical leadership throughout the software development lifecycle and provide hands-on support in development, technical delivery, review, and testing while collaborating with global teams.\n",
       "• Apply central Enterprise Data Platforms and guide DevOps, DataOps, and MLOps teams to ensure standard methodologies.\n",
       "• Ensure data solutions align with FAIR principles and support end-to-end data science and machine learning.\n",
       "\n",
       "Essential Skills/Experience:\n",
       "• Minimum 10 years of experience in the design, development, and delivery of software and data engineering solutions.\n",
       "• Extensive technical expertise in Data Engineering, Software Engineering, and Cloud Engineering.\n",
       "• Strong foundational knowledge of AI Engineering principles and practices.\n",
       "• Deep understanding of DevOps, MLOps, and DataOps methodologies with practical implementation experience.\n",
       "• Demonstrated success in product development and/or product management, delivering complex solutions end-to-end.\n",
       "• Demonstrable ability to provide technical thought leadership across Data, Analytics, and AI domains.\n",
       "• Exceptional communication, customer management, and multi-functional collaboration skills.\n",
       "• Robust analytical and problem-solving abilities with a collaborative, team-oriented approach.\n",
       "• Hands-on experience driving innovation throughout the full product development lifecycle.\n",
       "• Solid understanding of Data Mesh and Data Product concepts and architectures.\n",
       "• Proficiency in Agile methodologies and facilitating iterative, multi-functional team delivery.\n",
       "• Hands-on experience designing, implementing, and optimizing data pipelines using leading ETL tools.\n",
       "• Skilled in architecting, deploying, and managing scalable, secure AWS cloud environments.\n",
       "• Proficient in workflow orchestration tools such as Apache Airflow.\n",
       "• Practical experience implementing DataOps practices with tools like DataOps.Live.\n",
       "• Strong expertise in data storage and analytics platforms such as Snowflake.\n",
       "• Ability to deliver actionable insights through business intelligence tools, including Power BI.\n",
       "• Extensive full-stack development experience, including backend proficiency with Node.js and Python and frontend expertise with ReactJS or NextJS.\n",
       "• Experience designing and deploying Generative AI solutions.\n",
       "• Hands-on implementation of AI/ML models using platforms such as Amazon SageMaker.\n",
       "• Advanced programming skills, especially in Python.\n",
       "• Solid knowledge of both SQL and NoSQL database technologies.\n",
       "• Familiarity with agile ways of working and iterative development environments.\n",
       "• Experience working in large, multinational organizations or pharmaceutical environments is highly desirable.\n",
       "• Demonstrated leadership and mentoring skills, with a demonstrable ability to develop high-performing technical teams.\n",
       "\n",
       "Desirable Skills/Experience:\n",
       "• Bachelor's or master's degree in health sciences, Life Sciences, Data Management, IT, or a related field.\n",
       "• Experience in the pharmaceutical industry or a similar multinational environment.\n",
       "• AWS Cloud or relevant data/software engineering certifications.\n",
       "• Awareness of use case specific GenAI tools available in the market and their application in day-to-day work scenarios.\n",
       "• Possess working knowledge of basic prompting techniques and continuously improve these skills.\n",
       "• Stay up to date with developments in AI and GenAI, applying new insights to work-related situations.\n",
       "\n",
       "When we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That's why we work, on average, a minimum of three days per week from the office. But that doesn't mean we're not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\n",
       "\n",
       "At AstraZeneca, we are at a crucial stage of our journey to become a digital and data-led enterprise. Our commitment to innovation empowers us to make the impossible possible by building partnerships and ecosystems that drive scale and speed for exponential growth. With investment backing us all the way, we are focused on disrupting the industry while making a meaningful impact through our work. Here you'll find countless opportunities to learn, grow, and contribute to developing life-changing medicines.\n",
       "\n",
       "Ready to make a difference? Apply now to join our team!\n",
       "\n",
       "Date Posted\n",
       "30-Jun-2025\n",
       "\n",
       "Closing Date\n",
       "\n",
       "AstraZeneca embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills. We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements.</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIENvbnN1bHRhbnQgLSBUZWNobmljYWwgTGVhZCAtIEZ1bGxzdGFjayBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiQXN0cmFaZW5lY2EiLCJhZGRyZXNzX2NpdHkiOiJDaGVubmFpLCBUYW1pbCBOYWR1LCBJbmRpYSIsImh0aWRvY2lkIjoiRGVXa2pFZDgxNXcwSUpJY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>3 days ago</td><td>Data Engineer</td></tr><tr><td>AWS Data Engineer</td><td>COGNIZANT</td><td>Hyderabad, Telangana, India (+1 other)</td><td>Job Summary:\n",
       "\n",
       "Experience : 4 - 8 years\n",
       "\n",
       "Location : Bangalore\n",
       "\n",
       "The Data Engineer will contribute to building state-of-the-art data Lakehouse platforms in AWS, leveraging Python and Spark. You will be part of a dynamic team, building innovative and scalable data solutions in a supportive and hybrid work environment. You will design, implement, and optimize workflows using Python and Spark, contributing to our robust data Lakehouse architecture on AWS. Success in this role requires previous experience of building data products using AWS services, familiarity with Python and Spark, problem-solving skills, and the ability to collaborate effectively within an agile team.\n",
       "\n",
       "Must Have Tech Skills:\n",
       "\n",
       "· Demonstrable previous experience as a data engineer.\n",
       "• Technical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\n",
       "\n",
       "· Proficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n",
       "\n",
       "Nice To Have Tech Skills:\n",
       "\n",
       "· Familiar with data services in a Lakehouse architecture.\n",
       "\n",
       "· Familiar with technical design practices, allowing for the creation of scalable, reliable data products that meet both technical and business requirements\n",
       "\n",
       "· A master’s degree or relevant certifications (e.g., AWS Certified Solutions Architect, Certified Data Analytics) is advantageous\n",
       "\n",
       "Key Accountabilities:\n",
       "• Writes high quality code, ensuring solutions meet business requirements and technical standards.\n",
       "• Works with architects, Product Owners, and Development leads to decompose solutions into Epics, assisting the design and planning of these components.\n",
       "• Creates clear, comprehensive technical documentation that supports knowledge sharing and compliance. Experience in decomposing solutions into components (Epics, stories) to streamline development.\n",
       "• Actively contributes to technical discussions, supporting a culture of continuous learning and innovation.\n",
       "\n",
       "Key Skills:\n",
       "• Proficient in Python and familiar with a variety of development technologies.\n",
       "• Previous experience of implementing data pipelines, including use of ETL tools to streamline data ingestion, transformation, and loading.\n",
       "• Solid understanding of AWS services and cloud solutions, particularly as they pertain to data engineering practices. Familiar with AWS solutions including IAM, Step Functions, Glue, Lambda, RDS, SQS, API Gateway, Athena.\n",
       "• Proficient in quality assurance practices, including code reviews, automated testing, and best practices for data validation.\n",
       "• Experienced in Agile development, including sprint planning, reviews, and retrospectives\n",
       "\n",
       "Educational Background:\n",
       "• Bachelor’s degree in computer science, Software Engineering, or related essential.\n",
       "\n",
       "Bonus Skills:\n",
       "• Financial Services expertise preferred, working with Equity and Fixed Income asset classes and a working knowledge of Indices.\n",
       "• Familiar with implementing and optimizing CI/CD pipelines. Understands the processes that enable rapid, reliable releases, minimizing manual effort and supporting agile development cycles.</td><td>eyJqb2JfdGl0bGUiOiJBV1MgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkNvZ25pemFudCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiWjYyVVNZZkhtUHk0dHdwNkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 days ago</td><td>Data Engineer</td></tr><tr><td>Engineer III Consultant-Data Engineering</td><td>VERIZON</td><td>Hyderabad, Telangana, India (+2 others)</td><td>When you join Verizon\n",
       "\n",
       "You want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n",
       "\n",
       "What you’ll be doing…\n",
       "\n",
       "We are looking for data engineers who can work with world class team members to help drive telecom business to its full potential. We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n",
       "\n",
       "As a data engineer you will be collaborating with business product owners, coaches, industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n",
       "\n",
       "As a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n",
       "• Understanding the business requirements and converting them to technical design.\n",
       "• Working on Data Ingestion, Preparation and Transformation.\n",
       "• Developing data streaming applications.\n",
       "• Debugging the production failures and identifying the solution.\n",
       "• Working on ETL/ELT development.\n",
       "• Understanding devops process and contributing for devops pipelines\n",
       "\n",
       "What we’re looking for...\n",
       "\n",
       "You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n",
       "\n",
       "You’ll need to have…\n",
       "• Bachelor’s degree or four or more years of work experience.\n",
       "• Four or more years of relevant work experience.\n",
       "• Experience with Data Warehouse concepts and Data Management life cycle.\n",
       "• Experience in Big Data technologies - GCP/Hadoop/Spark/Composer/DataFlow/Bigquery.\n",
       "• Experience in complex SQL.\n",
       "• Experience working on Streaming ETL pipelines\n",
       "• Expertise in Java\n",
       "• Experience with MemoryStore / Redis / Spanner\n",
       "• Experience in troubleshooting the data issues.\n",
       "• Experience with data pipeline and workflow management & Governance tools.\n",
       "• Knowledge of Information Systems and their applications to data management processes.\n",
       "\n",
       "Even better if you have one or more of the following…\n",
       "• Three or more years of relevant experience.\n",
       "• Any relevant Certification on ETL/ELT developer.\n",
       "• Certification in GCP-Data Engineer.\n",
       "• Accuracy and attention to detail.\n",
       "• Good problem solving, analytical, and research capabilities.\n",
       "• Good verbal and written communication.\n",
       "• Experience presenting to and influence stakeholders.\n",
       "• Experience in driving a small team of 2 or more members for technical delivery\n",
       "\n",
       "#AI&D\n",
       "\n",
       "Where you’ll be working\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "Scheduled Weekly Hours\n",
       "40\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Verizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.</td><td>eyJqb2JfdGl0bGUiOiJFbmdpbmVlciBJSUkgQ29uc3VsdGFudC1EYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiVmVyaXpvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEhtc2E1cThWc3RlSGhKaUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Data Engineer</td></tr><tr><td>Data Engineer (Python, Pyspark, and Azure Databricks) || (4 - 6 Years)</td><td>EMIDS</td><td>Bengaluru, Karnataka, India</td><td>Hi All,\n",
       "\n",
       "Greetings for the day!!\n",
       "\n",
       "We are currently hiring for Data Engineer (Python, Pyspark, and Azure Databricks) for Emids(MNC) at Bangalore location.\n",
       "\n",
       "Role: Data Engineer\n",
       "\n",
       "Exp: 5 to 8 Years\n",
       "\n",
       "Location: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\n",
       "\n",
       "NP: Immediate to 15 Days (Try to find only immediate joiners)\n",
       "\n",
       "Note: Candidate Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.\n",
       "\n",
       "Role Overview:\n",
       "\n",
       "We are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Lead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\n",
       "• Architect scalable data streaming and processing solutions to support healthcare data workflows.\n",
       "• Develop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\n",
       "• Ensure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\n",
       "• Collaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\n",
       "• Troubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\n",
       "• Mentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\n",
       "• Stay updated with the latest cloud technologies, big data frameworks, and industry trends.\n",
       "\n",
       "Required Skills & Qualifications:\n",
       "• 4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\n",
       "• Expertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\n",
       "• Experience with Azure Databricks (or willingness to learn and adopt it quickly).\n",
       "• Hands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\n",
       "• Proficiency in SQL, NoSQL databases, and data modeling for big data processing.\n",
       "• Knowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\n",
       "• Experience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\n",
       "• Strong analytical skills, problem-solving mindset, and ability to lead complex data projects.\n",
       "• Excellent communication and stakeholder management skills.\n",
       "\n",
       "Note: This is not a contract position, this will be a permanent position with Emids.\n",
       "\n",
       "Interested candidates Can Share Your Updated Profile with details for below Email.\n",
       "\n",
       "NAME:\n",
       "\n",
       "CCTC:\n",
       "\n",
       "ECTC:\n",
       "\n",
       "Notice Period:\n",
       "\n",
       "Offers in Hand :\n",
       "\n",
       "Email ID: Ravi.chekka@emids.com</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChQeXRob24sIFB5c3BhcmssIGFuZCBBenVyZSBEYXRhYnJpY2tzKSB8fCAoNCAtIDYgWWVhcnMpIiwiY29tcGFueV9uYW1lIjoiRW1pZHMiLCJhZGRyZXNzX2NpdHkiOiJCZW5nYWx1cnUsIEthcm5hdGFrYSwgSW5kaWEiLCJodGlkb2NpZCI6Im9TMEEyREhLeU9wd0lVdnVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>8 hours ago</td><td>Data Engineer</td></tr><tr><td>Senior Data Engineer</td><td>MASTERCARD</td><td>Pune, Maharashtra, India</td><td>Job Title:\n",
       "\n",
       "Senior Data Engineer\n",
       "\n",
       "Overview:\n",
       "\n",
       "Position Overview:\n",
       "\n",
       "The Senior Data Engineer, MyMPA will be part of GBSC’s Automation & Engineering Team, responsible for implementing enhancements and periodic refreshes of an enterprise-wide data platform.\n",
       "\n",
       "This role will also work closely with the VP of Analytics & Metrics and Director of FP&A and gather requirements for changes and enhancements to the application and contribute to the technology platform's evolution as it grows to support the rapidly expanding Mastercard business.\n",
       "\n",
       "The ideal candidate will have hands-on development skills combined with an ability to analyze and understand end user requirements that are critical success factors within this role. This role requires the skills and desire to work as an individual contributor as well as, collaborate cross functionally with various business constituents.\n",
       "\n",
       "1. Have you ever worked on an enterprise-wide reporting solution that relied heavily on your own knowledge and abilities to build and maintain the solution?\n",
       "2. Are you constantly hungry to learn? Do you have the “growth mindset” as opposed to the “fixed mindset”?\n",
       "3. Do you love working with people, helping them, and turning their requirements into something that can make a difference?\n",
       "\n",
       "Role:\n",
       "\n",
       "• Skilled at writing performant and reliable SQL queries to source data from Centralized data repositories such as a Data Warehouse.\n",
       "• Proficient at designing and developing Relational and Multi-dimensional Databases to host the sourced data and work with Tableau and Power BI developers within and outside the team to support reporting and dashboarding needs of the organization.\n",
       "• Develop ETL workflows and macros in Alteryx to source data for projects and build high-level and detailed data validations to ensure accuracy of the sourced data\n",
       "• Contribute to initiatives aimed at automating the data extraction, application of data quality checks to ensure datasets are released on time and with 100% accuracy.\n",
       "• Develop technical components to ensure department’s compliance with audit requirements such as SOX and other statutory/audit requirements as applicable from time to time.\n",
       "• Organize and lead discussions with customers to brainstorm on data quality issues and contribute to discussions to devise business rules to address data quality issues\n",
       "• Use MS-Excel and MS-PPT to capture findings and present to customers in an easy-to-understand and impactful manner\n",
       "• Liaison with the internal groups in MasterCard Operations and Technology to ensure our solutions remain in compliance with MasterCard technical standards. Navigate O&T requirements around change management and new development.\n",
       "\n",
       "All About You:\n",
       "\n",
       "• Strong understanding of Windows and Linux server.\n",
       "• Good understanding of SQL Server or Oracle DB.\n",
       "• Solid understanding of Essbase technology – understand how this technology works, for both BSO\n",
       "and ASO cubes.\n",
       "• Develop BSO and ASO cubes with a strong eye for performance.\n",
       "• Strong commitment to quality and error testing code you develop. Strong ability to step in and analyze the code of others on the team.\n",
       "• Be able to work within an Agile environment that is highly responsive to the business. Our team is part of the Finance organization – you must be comfortable with working as part of the business with a strong “roll up your sleeves” mentality.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6Ik1hc3RlcmNhcmQiLCJhZGRyZXNzX2NpdHkiOiJQdW5lLCBNYWhhcmFzaHRyYSwgSW5kaWEiLCJodGlkb2NpZCI6IlV1SE5zREk1eXZ2ckFfdVBBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>Data Engineer</td></tr><tr><td>Architect - Data Engineer</td><td>PEPSICO</td><td>Hyderabad, Telangana, India</td><td>Overview\n",
       "\n",
       "Provide the job title you would like to be displayed on the job posting:\n",
       "\n",
       "Data Platform Engineer – Transformation & Modernization\n",
       "\n",
       "Job Overview: A Data Platform Engineer to be a key player in our transformation and modernization programs, leading the migration of applications from legacy systems to Azure-based architectures. This role involves designing, implementing, and optimizing scalable, cloud-native data solutions using Databricks, Azure DevOps (ADO), and Agile development methodologies.\n",
       "\n",
       "As an active contributor to code development, you will help drive automation, operational excellence, and data quality across our platforms. You will collaborate with data science and product teams to create solutions that enhance our data-driven decision-making capabilities.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Responsibilities: • Lead the migration and modernization of data platforms, moving applications and pipelines to Azure-based solutions.\n",
       "• Actively contribute to code development in projects and services.\n",
       "• Manage and scale data pipelines from internal and external data sources to support new product launches and ensure high data quality.\n",
       "• Develop automation and monitoring frameworks to capture key metrics and operational KPIs for pipeline performance.\n",
       "• Implement best practices around systems integration, security, performance, and data management.\n",
       "• Collaborate with internal teams, including data science and product teams, to drive solutioning and proof-of-concept (PoC) discussions.\n",
       "• Develop and optimize procedures to transition data into production.\n",
       "• Define and manage SLAs for data products and operational processes.\n",
       "• Prototype and build scalable solutions for data engineering and analytics.\n",
       "• Research and apply state-of-the-art methodologies in data and Platform engineering.\n",
       "• Create and maintain technical documentation for knowledge sharing.\n",
       "• Develop reusable packages and libraries to enhance development efficiency.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Qualifications: • Bachelor’s degree in Computer Science, MIS, Business Management, or related field\n",
       "• 10 + years’ experience in Information Technology\n",
       "• 4 + years of Azure, AWS and Cloud technologies\n",
       "• Experience in data platform engineering, with a focus on cloud transformation and modernization.\n",
       "• Strong knowledge of Azure services, including Databricks, Azure Data Factory, Synapse Analytics, and Azure DevOps (ADO).\n",
       "• Proficiency in SQL, Python, and Spark for data engineering tasks.\n",
       "• Hands-on experience building and scaling data pipelines in cloud environments.\n",
       "• Experience with CI/CD pipeline management in Azure DevOps (ADO).\n",
       "• Understanding of data governance, security, and compliance best practices.\n",
       "• Experience working in an Agile development environment.\n",
       "• Prior experience in migrating applications from legacy platforms to the cloud.\n",
       "• Knowledge of Terraform or Infrastructure-as-Code (IaC) for cloud resource management.\n",
       "• Familiarity with Kafka, Event Hubs, or other real-time data streaming solutions.\n",
       "• Experience with lagacy RDBMS (Oracl, DB2, Teradata)\n",
       "• Background in supporting data science models in production.\n",
       "\n",
       "Does the person hired for this job need to be based in a PepsiCo office, or can they be remote?: Employee must be based in a Pepsico office\n",
       "\n",
       "Primary Work Location: Hyderabad HUB-IND\n",
       "\n",
       "Is this role approved for relocation?: No\n",
       "\n",
       "Would you like to initially post this job internally-only or both internally and externally?: Post both internally and externally</td><td>eyJqb2JfdGl0bGUiOiJBcmNoaXRlY3QgLSBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiUGVwc2lDbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNzVXaWFLWW1Xa1V2Vk5VY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Data Engineer</td></tr><tr><td>Senior Analytics Data Engineer</td><td>OKTA, INC.</td><td>Bengaluru, Karnataka, India</td><td>Get to know OktaOkta is The World’s Identity Company. We free everyone to safely use any technology, anywhere, on any device or app. Our flexible and neutral products, Okta Platform and Auth0 Platform, provide secure access, authentication, and automation, placing identity at the core of business security and growth.At Okta, we celebrate a variety of perspectives and experiences. We are not looking for someone who checks every single box - we’re looking for lifelong learners and people who can make us better with their unique experiences. Join our team! We’re building a world where Identity belongs to you.Senior Analytics Engineer\n",
       "We are looking for an experienced Analytics Engineer to join Okta’s enterprise data team. This analyst will have strong background in SaaS subscription and product analytics, a passion for providing customer usage insights to internal stakeholders, and experience organizing complex data into consumable data assets. In this.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHl0aWNzIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJPa3RhLCBJbmMuIiwiYWRkcmVzc19jaXR5IjoiQmVuZ2FsdXJ1LCBLYXJuYXRha2EsIEluZGlhIiwiaHRpZG9jaWQiOiJhZHlINU5lRGFEQWwyUW1aQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>Data Engineer</td></tr><tr><td>Lead Data Engineer - Data Engineering</td><td>CENCORA</td><td>India</td><td>Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\n",
       "\n",
       "Job Details\n",
       "\n",
       "PRIMARY DUTIES AND RESPONSIBILITIES:\n",
       "• Leads design, development, and automation of scalable Analytic data engineering products leveraging Machine Learning, cloud infrastructure\n",
       "• Leads the design, build and operationalization of monitoring and tracking of data quality and data flow dynamics\n",
       "• Optimizes existing data processes and implements best-in-class data transformation capabilities\n",
       "• Leverages advanced statistical and computational methodologies to deliver insights and identify strategic opportunities\n",
       "• Implements Data Ingestion Framework and other re-usable components leveraged for delivering data pipeline and supporting ad-hoc data ingestions\n",
       "• Assists with development and storage of analytics-ready data for development of analytic deliverables\n",
       "• Recommends data products to solve business problems meeting multiple stakeholder requirements\n",
       "• Drives project planning processes, delegates non-complex tasks to junior team members\n",
       "• Mentors other team members and assists them with priority setting and issue resolution\n",
       "• Maintains data and information environment that provides consistent and accurate reporting of the client’s data\n",
       "• Leverages Machine Learning to enhance the developed solution\n",
       "• Collaborates with Business and Client representatives to target strategic analytics opportunities and define deliverable scope\n",
       "• Facilitates the resolution of issues regarding projects being worked on to include proper documentation of recommendations for those issues\n",
       "• Manages multiple initiatives in parallel partnering with other Analytics team members and conducts review meetings with stakeholders to drive prioritization effort\n",
       "• Analyzes model errors and design strategies to overcome them\n",
       "• Implements best practices for data engineering to ensure quality delivery of enterprise solutions\n",
       "• Brings expert knowledge of data visualization tools and techniques to drive business analytics and semantic data access requirements.\n",
       "• Work closely with business users, vendors, and delivery teams to understand the business requirements that drive the analysis and design of business analytics and reporting solutions.\n",
       "• Conceptualize, design, and develop data visualization solutions that synthesize data concepts into clear communications for key business stakeholders.\n",
       "• Drives business stakeholder adoption of insights-driven decision making and/or business process innovation.\n",
       "• Leads knowledge transfer around using data visualizations to business stakeholders.\n",
       "• Assist in developing best practices for data presentation and sharing across the organization.\n",
       "• Ensures data visualization standards are maintained and implemented.\n",
       "• Demonstrate an ability to reduce data to the bare minimum of what is needed to optimally communicate a message.\n",
       "• Provides technical leadership, coaching and mentoring to team members and business users.\n",
       "• Participates in POC projects and provides business analytics solutions recommendations.\n",
       "• Evaluates new visualization tools and performs research on best practices.\n",
       "• Contributes to strategic planning meetings and provides guidance and expertise on system options, risk, cost vs. benefits, and impacts on business processes and goals.\n",
       "• Has expertise in multiple technical environments and possesses business knowledge that spans multiple business areas.\n",
       "• Responsible for BI Tool administration & security functions as designated\n",
       "\n",
       ".\n",
       "\n",
       "EDUCATIONAL QUALIFICATIONS:\n",
       "\n",
       "Bachelor’s Degree in Statistics, Computer Science, Information Technology or any other related discipline or equivalent related experience.\n",
       "\n",
       "Preferred Certifications:\n",
       "• Advanced Data Analytics Certifications\n",
       "• AI and ML Certifications\n",
       "• SAS Statistical Business Analyst Professional Certification\n",
       "\n",
       "WORK EXPERIENCE:\n",
       "6+ years of directly-related or relevant experience, preferably in healthcare data analytics or data engineering.\n",
       "\n",
       "Working Hours:\n",
       "\n",
       "7PM IST to 2AM IST; Hybrid Working Model\n",
       "\n",
       "SKILLS & KNOWLEDGE:\n",
       "\n",
       "Behavioral Skills:\n",
       "• Conflict Resolution\n",
       "• Creativity & Innovation\n",
       "• Decision Making\n",
       "• Planning\n",
       "• Presentation Skills\n",
       "• Risk-taking\n",
       "\n",
       "Technical Skills:\n",
       "• Advanced Data Visualization Techniques\n",
       "• Advanced Statistical Analysis\n",
       "• Big Data Analysis Tools and Techniques\n",
       "• Data Governance\n",
       "• Data Management\n",
       "• Data Modelling\n",
       "• Data Quality Assurance\n",
       "• Machine Learning and AI Fundamentals\n",
       "• Programming languages like SQL, R, Python\n",
       "\n",
       "Tools Knowledge:\n",
       "• Business Intelligence Software like Tableau, Power BI, Alteryx, QlikSense\n",
       "• Data Visualization Tools\n",
       "• Microsoft Office Suite\n",
       "• Statistical Analytics tools (SAS, SPSS3)\n",
       "\n",
       "What Cencora offers\n",
       "\n",
       "​Benefit offerings outside the US may vary by country and will be aligned to local market practice. The eligibility and effective date may differ for some benefits and for team members covered under collective bargaining agreements.\n",
       "\n",
       "Full time\n",
       "\n",
       "Affiliated Companies\n",
       "Affiliated Companies: CENCORA INDIA TECHNOLOGY SERVICES PRIVATE LIMITED\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Cencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\n",
       "\n",
       "The company’s continued success depends on the full and effective utilization of qualified individuals. Therefore, harassment is prohibited and all matters related to recruiting, training, compensation, benefits, promotions and transfers comply with equal opportunity principles and are non-discriminatory.\n",
       "\n",
       "Cencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com. We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgLSBEYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiQ2VuY29yYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJHdzRzcW9FbWlPdEVnTDAwQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>3 days ago</td><td>Data Engineer</td></tr><tr><td>Azure Data Engineer – Azure Databricks</td><td>AIPRUS SOFTWARE PRIVATE LIMITED</td><td>Bengaluru, Karnataka, India</td><td>Job Title: Azure Data Engineer – Azure Databricks\n",
       "\n",
       "Location: Bangalore, India\n",
       "\n",
       "Experience: 5 to 10 Years\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "As a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Design and develop scalable data pipelines using Azure Databricks and PySpark.\n",
       "• Transform raw data into actionable insights through advanced data engineering techniques.\n",
       "• Build, deploy, and maintain machine learning models using MLlib, TensorFlow, and MLflow.\n",
       "• Optimize data integration workflows from Azure Blob Storage, Data Lake, and SQL/NoSQL sources.\n",
       "• Execute large-scale data processing using Spark Pools, fine-tuning configurations for performance and cost-efficiency.\n",
       "• Collaborate with data scientists, analysts, and business stakeholders to deliver robust data solutions.\n",
       "• Maintain and enhance Databricks notebooks and Delta Lake architectures.\n",
       "\n",
       "Required Skills & Qualifications:\n",
       "• Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field.\n",
       "• 7+ years of experience in data engineering, with at least 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n",
       "• Strong proficiency in:\n",
       "• Python, PySpark, Pandas, NumPy, SciPy\n",
       "• Spark SQL, DataFrames, RDDs\n",
       "• Delta Lake, Databricks Notebooks, MLflow\n",
       "• Hands-on experience with:\n",
       "• Azure Data Lake, Blob Storage, Synapse Analytics\n",
       "• Excellent problem-solving and communication skills.\n",
       "• Ability to work independently and in a collaborative team environment.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with CI/CD pipelines for data workflows.\n",
       "• Familiarity with data governance and security best practices in Azure.\n",
       "• Knowledge of real-time data processing and streaming technologies.</td><td>eyJqb2JfdGl0bGUiOiJBenVyZSBEYXRhIEVuZ2luZWVyIOKAkyBBenVyZSBEYXRhYnJpY2tzIiwiY29tcGFueV9uYW1lIjoiQWlwcnVzIFNvZnR3YXJlIFByaXZhdGUgTGltaXRlZCIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiek5tRUxOMG5Iam5zazl6TkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 hours ago</td><td>Data Engineer</td></tr><tr><td>Principle Software Engineer for Data Platform - 31866</td><td>SPLUNK</td><td>Bengaluru, Karnataka, India</td><td>Splunk, a Cisco company, is building a safer and more resilient digital world with an end-to-end full stack platform made for a hybrid, multi-cloud world. Leading enterprises use our unified security and observability platform to keep their digital systems secure and reliable. Our customers love our technology, but it's our caring employees that make Splunk stand out as an amazing career destination. No matter where in the world or what level of the organization, we approach our work with kindness. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Come help organizations be their best, while you reach new heights with a team that has your back.Role SummarySplunk AppDynamics, a leader in observability solutions for both cloud and on-premises environments, empowers customers with end-to-end visibility across their entire application ecosystems, cloud, on-premises, or hybrid. Our team is dedicated to developing, maintaining, and supporting SaaS and on-premises offerings within the observability product suite.We're looking for a Senior Software Engineer to build upon our application and data platform as we continue to innovate on application observability. We move fast and iterate quickly. We are passionate about solving customers’ problems. We have ambitious goals to build best-of-its-kind products. You will help the team win in a fast-growing market. If you are passionate about innovation and embrace the challenge of working on highly scalable systems that handle large volumes of data, this position is for you.Meet the Team (Optional)Data platform is the heart of the AppDynamics architecture. We process more than 100 billion data packets per day, more than 5 peta byte of data flows through our platform. We are in 8 regions having 40 clusters of 7000 nodes. We ingest, process and store all the data that power the dashboards that our customers rely on for observing their infrastructure and their applications.What you'll get to do\n",
       "• Design and build highly scalable solutions\n",
       "• Work with a team of exceptionally capable and dedicated peers, all the way from engineering to product management and customer support\n",
       "• Work in an open environment, work together to get things done and adapt to the team's changing needs\n",
       "• Leverage technologies including Kafka, ElasticSearch, Docker, and Kubernetes across different cloud environments like AWS and Azure\n",
       "• lead critical initiatives for the organisation\n",
       "Must-have Qualifications\n",
       "• 15+ years of full-stack developer experience in designing and developing highly scalable, distributed applications, products, and services.\n",
       "• Expertise in Java programming language.\n",
       "• Strong proficiency in data structures, algorithms, threads, concurrent programming\n",
       "• Extensive knowledge of SQL and at least one relational database engine: MySQL. Hands on experience in RDS or NoSQL (Dynamo, MongoDB) is a big plus\n",
       "• Experience of building applications using microservices architecture with expertise in Dockers and Kubernetes.\n",
       "• Strong communication skills, both verbal and written. Ability to multi-task and adapt quickly to changing requirements, scope, and priorities.\n",
       "• Optimize data pipelines, storage systems, and query engines for performance and efficiency.\n",
       "• Work closely with product teams, data engineers, software developers, and product managers to deliver on business goals.\n",
       "• Mentor team members in architecture principles, coding best practices, and system design.\n",
       "• Ensure robust monitoring, logging, and alerting systems for proactive issue detection and resolution.\n",
       "• Support CI/CD processes and automate testing for data systems\n",
       "• Stay abreast of emerging trends in big data, machine learning, and distributed systems to recommend innovative solutions.\n",
       "Nice-to-have QualificationsWe’ve taken special care to separate the must-have qualifications from the nice-to-haves. “Nice-to-have” means just that: Nice. To. Have. So, don’t worry if you can’t check off every box. We’re not hiring a list of bullet points–we’re interested in the whole you.\n",
       "• Added advantage of having an experience in working on Cloud Observability Space.\n",
       "• experience of other languages like python, etc\n",
       "• experience of front-end technologies\n",
       "Splunk is an Equal Opportunity EmployerSplunk, a Cisco company, is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.\n",
       "\n",
       "Note:</td><td>eyJqb2JfdGl0bGUiOiJQcmluY2lwbGUgU29mdHdhcmUgRW5naW5lZXIgZm9yIERhdGEgUGxhdGZvcm0gLSAzMTg2NiIsImNvbXBhbnlfbmFtZSI6IlNwbHVuayIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiY2k4emNxSnNNanNSemNYUEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>9 days ago</td><td>Data Engineer</td></tr><tr><td>Software Developer- Python</td><td>BNP PARIBAS INDIA SOLUTIONS</td><td>India</td><td>About BNP Paribas India Solutions:\n",
       "\n",
       "Established in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n",
       "\n",
       "About BNP Paribas Group:\n",
       "\n",
       "BNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n",
       "\n",
       "Commitment to Diversity and Inclusion\n",
       "\n",
       "At BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n",
       "\n",
       "About Business line/Function:\n",
       "\n",
       "The Intermediate Holding Company (“IHC”) program structured at the U.S. level across poles of activities of BNP Paribas provides guidance, supports the analysis, impact assessment and drives adjustments of the U.S. platform’s operating model due to the drastic changes introduced by the Enhanced Prudential Standards (“EPS”) for Foreign Banking Organizations (“FBOs”) finalized by the Federal Reserve in February 2014, implementing Section 165 of U.S. Dodd-Frank Act.\n",
       "\n",
       "The IT Transversal Team is part of the Information Technology Group which works simultaneously on a wide range of projects arising from business, strategic initiatives, and regulatory changes and reengineering of existing applications to improve functionality and efficiency.\n",
       "\n",
       "Job Title:\n",
       "\n",
       "Python Developer\n",
       "\n",
       "Date:\n",
       "\n",
       "June-25\n",
       "\n",
       "Department:\n",
       "\n",
       "ITG- Fresh\n",
       "\n",
       "Location:\n",
       "\n",
       "Chennai, Mumbai\n",
       "\n",
       "Business Line / Function:\n",
       "\n",
       "Finance Dedicated Solutions\n",
       "\n",
       "Reports to:\n",
       "\n",
       "(Direct)\n",
       "\n",
       "Grade:\n",
       "\n",
       "(if applicable)\n",
       "\n",
       "(Functional)\n",
       "\n",
       "Number of Direct Reports:\n",
       "\n",
       "NA\n",
       "\n",
       "Directorship / Registration:\n",
       "\n",
       "NA\n",
       "Position Purpose\n",
       "\n",
       "The Python Developer will play a critical role in building and maintaining financial applications and tools that support data processing, analysis, and reporting within a fast-paced financial services environment. This position involves developing scalable and secure systems. The developer will collaborate with business analysts, finance users/or finance BA to translate complex business requirements into efficient, high-quality software solutions.\n",
       "\n",
       "A strong understanding of financial concepts, data integrity, and regulatory compliance is essential. The detailed responsibilities are mentioned below.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Direct Responsibilities\n",
       "\n",
       "- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n",
       "\n",
       "- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n",
       "\n",
       "- Expertise in PySpark for large-scale data processing and loading into databases.\n",
       "\n",
       "- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n",
       "\n",
       "- Strong communication skills to effectively collaborate with team members and stakeholders.\n",
       "\n",
       "- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n",
       "Technical & Behavioral Competencies\n",
       "\n",
       "- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n",
       "\n",
       "- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n",
       "\n",
       "- Expertise in PySpark for large-scale data processing and loading into databases.\n",
       "\n",
       "- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n",
       "\n",
       "- Strong communication skills to effectively collaborate with team members and stakeholders.\n",
       "\n",
       "- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n",
       "\n",
       "- Good analytical, problem solving, & communication skills\n",
       "\n",
       "- Engage in technical discussions and to help in improving the system, process etc\n",
       "\n",
       "Nice to Have\n",
       "\n",
       "- Familiarity with Plotly and Matplotlib for data visualization of large datasets.\n",
       "\n",
       "- Skilled in API programming, handling JSON, CSV, and other unstructured data from various systems.\n",
       "\n",
       "- Familiarity with JavaScript, CSS, and HTML.\n",
       "\n",
       "- Experience with cloud architecture applications such as Dataiku or Databricks; competency with ETL tools.\n",
       "\n",
       "- Knowledge of regulatory frameworks, RISK, CCAR, and GDPR.\n",
       "Specific Qualifications (if required)\n",
       "\n",
       "Skills Referential\n",
       "\n",
       "Behavioural Skills: (Please select up to 4 skills)\n",
       "\n",
       "Ability to collaborate / Teamwork\n",
       "\n",
       "Critical thinking\n",
       "\n",
       "Ability to deliver / Results driven\n",
       "\n",
       "Communication skills - oral & written\n",
       "\n",
       "Transversal Skills: (Please select up to 5 skills)\n",
       "\n",
       "Analytical Ability\n",
       "\n",
       "Ability to develop and adapt a process\n",
       "\n",
       "Ability to understand, explain and support change\n",
       "\n",
       "Ability to develop others & improve their skills\n",
       "\n",
       "Choose an item.\n",
       "\n",
       "Education Level:\n",
       "\n",
       "Bachelor Degree or equivalent\n",
       "\n",
       "Experience Level\n",
       "\n",
       "At least 5 years\n",
       "\n",
       "Other/Specific Qualifications (if required)</td><td>eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBEZXZlbG9wZXItIFB5dGhvbiIsImNvbXBhbnlfbmFtZSI6IkJOUCBQYXJpYmFzIEluZGlhIFNvbHV0aW9ucyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ6VTdCX1R2emRzczk0M0ZoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>7 days ago</td><td>Python Developer</td></tr><tr><td>Freelance Python Developer</td><td>TEQLAWN</td><td>Anywhere</td><td>We’re looking for an experienced Python Developer (4–7 years) to build scalable web and application solutions, with opportunities to integrate AI capabilities. Experience in AI integration is a must.\n",
       "\n",
       "Responsibilities:\n",
       "• Develop scalable web and application solutions using Python, with integration of AI/ML components\n",
       "• Collaborate with clients to understand project goals and technical requirements\n",
       "• Write clean, maintainable, and well-documented code\n",
       "• Troubleshoot, debug, and optimize applications for performance and reliability\n",
       "• Ensure timely and efficient delivery of milestones and final deliverables\n",
       "• Participate in code reviews and contribute to maintaining coding standards and best practices\n",
       "• Work with relevant frameworks and libraries such as Django, Flask, FastAPI, NumPy, pandas, and scikit-learn\n",
       "\n",
       "Note: Please share the link to your portfolio along with your application.\n",
       "\n",
       "Job Types: Full-time, Contractual / Temporary, Freelance\n",
       "Contract length: 2 months\n",
       "\n",
       "Pay: ₹50,000.00 - ₹80,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Work from home\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "• Monday to Friday\n",
       "\n",
       "Experience:\n",
       "• Python Development: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlRlcWxhd24iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiY20xcUU4aHhjbFQ3dnVyT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>1 day ago</td><td>Python Developer</td></tr><tr><td>Python Developer — Full-Time | 1-2 years Exp | In-Office | Bangalore</td><td>SERP HAWK</td><td>India</td><td>\uD83D\uDE80 We’re Hiring: Python Developer\n",
       "\n",
       "SERP Hawk is looking for a talented Python Developer to join our dynamic team in Bangalore!\n",
       "\n",
       "\uD83C\uDF1F About Us\n",
       "\n",
       "SERP Hawk is a leading digital marketing and technology solutions company based in Bangalore. We specialize in building robust backend systems, scalable applications, and AI-powered solutions for clients across various industries.\n",
       "\n",
       "\uD83D\uDCCD Location:B-1, Bannerghatta Slip Road, KEB Colony, New Gurappana Palya, 1st Stage, BTM Layout 1, Bengaluru, Karnataka 560029\n",
       "\n",
       "\uD83C\uDF10 Website: www.serphawk.com\n",
       "\n",
       "\uD83D\uDCBC What You’ll Do\n",
       "• Design and develop scalable backend architectures.\n",
       "• Write clean, efficient Python code.\n",
       "• Integrate APIs and databases.\n",
       "• Implement CI/CD pipelines and automated tests.\n",
       "• Ensure high performance, security, and reliability.\n",
       "\n",
       "✅ What We’re Looking For\n",
       "\n",
       "✔️ 1–2 years of experience in Python development.\n",
       "\n",
       "✔️ Proficiency in frameworks like Django, Flask, or FastAPI.\n",
       "\n",
       "✔️ Strong understanding of APIs and databases.\n",
       "\n",
       "✔️ Experience with CI/CD tools and best practices.\n",
       "\n",
       "✔️ Excellent problem-solving skills and a collaborative mindset.\n",
       "\n",
       "\uD83D\uDCA1 Nice to Have\n",
       "\n",
       "⭐ Experience with AI/chatbots.\n",
       "\n",
       "⭐ Knowledge of cloud services and containerization.\n",
       "\n",
       "\uD83D\uDCB0 Salary\n",
       "• ₹20,000 – ₹25,000 per month (based on skills and experience).\n",
       "\n",
       "\uD83D\uDCCC Additional Details\n",
       "\n",
       "\uD83D\uDCBC This is a full-time, in-office role in Bangalore.\n",
       "\n",
       "\uD83C\uDFE2 Candidates must report to the office daily.\n",
       "\n",
       "\uD83C\uDF10 Should be flexible to work and attend meetings or calls as per client time zones.\n",
       "\n",
       "✨ Apply now and grow with us!</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAlCBGdWxsLVRpbWUgfCAxLTIgeWVhcnMgRXhwIHwgSW4tT2ZmaWNlIHwgQmFuZ2Fsb3JlIiwiY29tcGFueV9uYW1lIjoiU0VSUCBIYXdrIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjNBMWtiUmowZFhsRVZsWnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 hours ago</td><td>Python Developer</td></tr><tr><td>SQL + Python</td><td>WISSEN TECHNOLOGY</td><td>India</td><td>Wissen Technology is Hiring for SQL With Python\n",
       "\n",
       "About Wissen Technology:\n",
       "\n",
       "Wissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges.\n",
       "\n",
       "Role Overview:\n",
       "\n",
       "We are looking for a skilled and detail-oriented candidate with a strong foundation in SQL, Python, and data processing techniques. The ideal candidate is passionate about transforming raw data into meaningful insights and has hands-on experience across the data pipeline—from data wrangling to visualization.\n",
       "\n",
       "Experience: 3-7 Years\n",
       "\n",
       "Location: Bengaluru\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with SQL (e.g., joins, subqueries, CTEs, window functions).\n",
       "• Proficiency in Python for data manipulation (e.g., pandas, NumPy).\n",
       "• Experience working with relational databases like MySQL, PostgreSQL, SQL Server, or Oracle.\n",
       "• Hands-on experience in data wrangling, cleaning, and feature engineering.\n",
       "• Understanding of ETL processes and tools.\n",
       "• Familiarity with version control systems like Git.\n",
       "• Knowledge of data visualization techniques and tools.\n",
       "• Strong problem-solving and analytical skills.\n",
       "\n",
       "The Wissen Group was founded in the year 2000. Wissen Technology, a part of Wissen Group, was established in the year 2015. Wissen Technology is a specialized technology company that delivers high-end consulting for organizations in the Banking & Finance, Telecom, and Healthcare domains. We help clients build world class products.\n",
       "\n",
       "We offer an array of services including Core Business Application Development, Artificial Intelligence & Machine Learning, Big Data & Analytics, Visualization & Business Intelligence, Robotic Process Automation, Cloud Adoption, Mobility, Digital Adoption, Agile & DevOps, Quality Assurance & Test Automation.\n",
       "\n",
       "Over the years, Wissen Group has successfully delivered $1 billion worth of projects for more than 20 of the Fortune 500 companies. Wissen Technology provides exceptional value in mission critical projects for its clients, through thought leadership, ownership, and assured on-time deliveries that are always ‘first time right’.\n",
       "\n",
       "The technology and thought leadership that the company commands in the industry is the direct result of the kind of people Wissen has been able to attract. Wissen is committed to providing them with the best possible opportunities and careers, which extends to providing the best possible experience and value to our clients.\n",
       "\n",
       "We have been certified as a Great Place to Work® company for two consecutive years (2020-2022) and voted as the Top 20 AI/ML vendor by CIO Insider. Great Place to Work® Certification is recognized world over by employees and employers alike and is considered the ‘Gold Standard’. Wissen Technology has created a Great Place to Work by excelling in all dimensions - High-Trust, High-Performance Culture, Credibility, Respect, Fairness, Pride and Camaraderie.\n",
       "\n",
       "Website: www.wissen.com\n",
       "\n",
       "LinkedIn: https://www.linkedin.com/company/wissen-technology\n",
       "\n",
       "Wissen Leadership: https://www.wissen.com/company/leadership-team/\n",
       "\n",
       "Wissen Live: https://www.linkedin.com/company/wissen-technology/posts/feedView=All\n",
       "\n",
       "Wissen Thought Leadership: https://www.wissen.com/articles/\n",
       "\n",
       "Employee Speak:\n",
       "\n",
       "https://www.ambitionbox.com/overview/wissen-technology-overview\n",
       "\n",
       "https://www.glassdoor.com/Reviews/Wissen-Infotech-Reviews-E287365.htm\n",
       "\n",
       "Great Place to Work:\n",
       "\n",
       "https://www.wissen.com/blog/wissen-is-a-great-place-to-work-says-the-great-place-to-work-institute-india/\n",
       "\n",
       "https://www.linkedin.com/posts/wissen-infotech_wissen-leadership-wissenites-activity-6935459546131763200-xF2k\n",
       "\n",
       "About Wissen Interview Process:\n",
       "\n",
       "https://www.wissen.com/blog/we-work-on-highly-complex-technology-projects-here-is-how-it-changes-whom-we-hire/\n",
       "\n",
       "Latest in Wissen in CIO Insider:\n",
       "\n",
       "https://www.cioinsiderindia.com/vendor/wissen-technology-setting-new-benchmarks-in-technology-consulting-cid-1064.html</td><td>eyJqb2JfdGl0bGUiOiJTUUwgKyBQeXRob24iLCJjb21wYW55X25hbWUiOiJXaXNzZW4gVGVjaG5vbG9neSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJITW5vZUwxWlF1QWVma0hlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>7 hours ago</td><td>Python Developer</td></tr><tr><td>Python Developer Full time</td><td>VARIANCE TECHNOLOGIES PRIVATE LIMITED</td><td>Anywhere</td><td>Job Opportunity: Python Developer at Variance Technologies Private Limited!\n",
       "\n",
       "Role: Python Developer\n",
       "\n",
       "Duration: 1 Months\n",
       "\n",
       "Location: Hybrid / Remote\n",
       "\n",
       "Responsibilities:\n",
       "\n",
       "Collaborate with our development team to develop and maintain Python-based applications tailored for finance-related tasks\n",
       "\n",
       "Implement object-oriented programming principles to ensure the scalability and maintainability of codebase\n",
       "\n",
       "Gain hands-on experience with Pandas for data manipulation and analysis within finance contexts\n",
       "\n",
       "Support integration of REST and WebSocket APIs for seamless communication with financial data providers and platforms\n",
       "\n",
       "Assist in automating financial modeling and analysis workflows using Python, including Excel automation for enhanced efficiency\n",
       "\n",
       "Requirements:\n",
       "\n",
       "Currently pursuing or recently completed a degree in Computer Science, Finance, or a related field\n",
       "\n",
       "Basic proficiency in Python programming language, with a strong willingness to learn and grow\n",
       "\n",
       "Exceptional attention to detail and proactive attitude towards problem-solving\n",
       "\n",
       "Genuine interest in the intersection of finance and technology\n",
       "\n",
       "Bonus Skills:\n",
       "\n",
       "Familiarity with fundamental financial concepts and markets\n",
       "\n",
       "Exposure to Python libraries such as Pandas, NumPy, or SciPy\n",
       "\n",
       "Demonstrated interest in financial data analysis and visualization techniques\n",
       "\n",
       "Basic understanding of REST and WebSocket APIs\n",
       "\n",
       "Perks:\n",
       "\n",
       "Hands-on experience working on real-world projects at the forefront of finance and technology\n",
       "\n",
       "Mentorship and guidance from seasoned professionals in the field\n",
       "\n",
       "Networking opportunities with industry experts to expand your professional connections\n",
       "\n",
       "Flexible scheduling to accommodate academic commitments\n",
       "\n",
       "Potential for transition to a full-time position based on exceptional performance and availability\n",
       "\n",
       "Ready to kickstart your career with a prestigious 1-month internship at Variance Technologies Private Limited? Apply now by sending your resume and a tailored cover letter expressing your interest and qualifications to careers@variancefintech.com. Join our team and embark on a journey towards shaping the future of finance through technology!\n",
       "\n",
       "Variance Technologies Private Limited is committed to fostering a diverse and inclusive workplace where all individuals are empowered to thrive. We welcome applicants from all backgrounds and identities to apply.\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: From ₹35,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Work from home\n",
       "\n",
       "Schedule:\n",
       "• Monday to Friday\n",
       "\n",
       "Education:\n",
       "• Bachelor's (Preferred)\n",
       "\n",
       "Experience:\n",
       "• Python: 1 year (Preferred)\n",
       "• total work: 1 year (Preferred)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIEZ1bGwgdGltZSIsImNvbXBhbnlfbmFtZSI6IlZhcmlhbmNlIFRlY2hub2xvZ2llcyBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiaXZCN3JUQTN5YnkxRzV2MUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>5 days ago</td><td>Python Developer</td></tr><tr><td>Python Developer – AI & LLM Integrations</td><td>DISCOVER WEBTECH PRIVATE LIMITED</td><td>India</td><td>We are hiring a skilled Python Developer with a minimum of 3 years of experience, who has hands-on expertise in developing AI modules, building intelligent agents using LLMs, and working with cutting-edge frameworks such as LangChain, LangGraph, and LangSmith.\n",
       "\n",
       "The ideal candidate should be capable of designing, developing, and deploying backend services as well as intelligent AI-driven tools and systems.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design and implement intelligent agents using LLM-based frameworks like LangChain, LangGraph, and LangSmith.\n",
       "• Build backend systems using Python (Django, FastAPI, or Flask).\n",
       "• Develop and integrate APIs, third-party tools, and cloud services.\n",
       "• Create AI modules that interact with knowledge bases, APIs, and perform multi-step reasoning.\n",
       "• Implement prompt engineering, memory chains, and agent behavior logic.\n",
       "• Collaborate with cross-functional teams to deliver robust AI features.\n",
       "• Optimize code for scalability, performance, and reliability.\n",
       "\n",
       "Required Skills and Qualifications\n",
       "• 3+ years of hands-on experience with Python.\n",
       "• Proficiency in LangChain, LangGraph, or LangSmith.\n",
       "• Strong experience in working with LLMs like OpenAI, Cohere, or Anthropic.\n",
       "• Deep understanding of prompt engineering and agent orchestration.\n",
       "• Experience with APIs, JSON, and external integrations.\n",
       "• Knowledge of data storage systems (PostgreSQL, MongoDB).\n",
       "• Familiarity with Docker, Git, and CI/CD tools.\n",
       "• Excellent problem-solving and debugging skills.\n",
       "\n",
       "Preferred Qualifications\n",
       "• Knowledge of vector databases (e.g., Pinecone, Weaviate, FAISS).\n",
       "• Experience with microservices, asynchronous programming, and message queues (Celery, RabbitMQ).\n",
       "• Familiarity with frontend technologies (React.js or Vue.js) is a plus.\n",
       "• Exposure to cloud platforms such as AWS, GCP, or Azure.\n",
       "\n",
       "Job Types: Full-time, Permanent\n",
       "\n",
       "Pay: ₹30,000.00 - ₹70,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Health insurance\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "\n",
       "Work Location: In person</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBBSSBcdTAwMjYgTExNIEludGVncmF0aW9ucyIsImNvbXBhbnlfbmFtZSI6IkRpc2NvdmVyIFdlYlRlY2ggUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Im1JUlJqRWJSV0pCSkdQR0VBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>3 days ago</td><td>Python Developer</td></tr><tr><td>Full Stack Developer (Python / React JS)</td><td>HITACHI CAREERS</td><td>India</td><td>Our Company\n",
       "\n",
       "We're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n",
       "\n",
       "Our group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n",
       "\n",
       "Imagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n",
       "\n",
       "The team\n",
       "\n",
       "We are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "\n",
       "The role: Full Stack Developer/Specialist\n",
       "\n",
       "Responsibilities:\n",
       "• Design, develop, and maintain applications.\n",
       "• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n",
       "• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n",
       "• Optimise applications for performance, scalability, and user experience.\n",
       "• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n",
       "• Provide training and support to end-users and IT staff on functionalities and best practices.\n",
       "• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n",
       "• Participate in project planning, execution, and post-implementation support.\n",
       "• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n",
       "• What you'll bring\n",
       "Qualifications:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n",
       "• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n",
       "• Expertise in development and customisation.\n",
       "• Proficiency in Python and React JS\n",
       "• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n",
       "• Experience with RESTful APIs and web services.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Strong communication and collaboration skills.\n",
       "• Ability to work independently and as part of a team in a fast-paced environment.\n",
       "• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n",
       "• Prior experience in building AI applications is a plus.\n",
       "• Prior experience with automation tools like UIPath is a plus.\n",
       "• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\n",
       "Preferred Skills\n",
       "• Certification in\n",
       "• Experience with other low-code/no-code platforms.\n",
       "• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n",
       "• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n",
       "• Experience with Agile development methodologies.\n",
       "About us\n",
       "\n",
       "We're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n",
       "\n",
       "#LI-MS3\n",
       "\n",
       "Championing diversity, equity, and inclusion\n",
       "\n",
       "Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n",
       "\n",
       "How we look after you\n",
       "\n",
       "We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n",
       "\n",
       "We're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.</td><td>eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>14 days ago</td><td>Python Developer</td></tr><tr><td>Python Back-End Developer</td><td>GOODYEAR</td><td>India</td><td>Location: IN - Hyderabad Telangana\n",
       "\n",
       "Goodyear Talent Acquisition Representative: M Bhavya Sree\n",
       "\n",
       "Sponsorship Available: No\n",
       "\n",
       "Relocation Assistance Available: No\n",
       "\n",
       "Duties and Responsibilities:\n",
       "\n",
       "• Develop and support Data Driven applications\n",
       "\n",
       "• Help design and develop back-end services and APIs for data-driven applications and simulations.\n",
       "\n",
       "• Work with our technical partners to collaborate on system requirements and data integration needs for our new applications.\n",
       "\n",
       "• Support the deployment and scaling of new back-end technologies and cloud-native architectures within the organization.\n",
       "\n",
       "• Work closely with our data scientists to support model deployment into production environments.\n",
       "\n",
       "• Develop and maintain server-side components for digital tools and products using Python or other modern back-end technologies and frameworks. Build scalable, secure, and efficient services that support a seamless experience across multiple platforms.\n",
       "\n",
       "• Design, implement, and maintain robust database systems (SQL and NoSQL), ensuring high availability and performance for critical applications.\n",
       "\n",
       "• Contribute to DevOps practices including CI/CD pipelines, infrastructure as code, containerization (Docker), and orchestration (Kubernetes).\n",
       "\n",
       "• Learn about the tire industry and tire manufacturing processes from subject matter experts.\n",
       "\n",
       "• Be a part of cross-functional teams working together to deliver impactful results.\n",
       "\n",
       "Skills Required:\n",
       "\n",
       "• Significant experience in server-side development using Python\n",
       "\n",
       "• Strong understanding of RESTful API design, microservices architecture, and service-oriented design\n",
       "\n",
       "• Experience with relational and non-relational databases such as PostgreSQL, MySQL, MongoDB, or DynamoDB\n",
       "\n",
       "• Application of software design skills and methodologies (algorithms, data structures, design patterns, software architecture and testing)\n",
       "\n",
       "• Hands-on experience working with cloud platforms such as AWS, Microsoft Azure, or Google Cloud Platform\n",
       "\n",
       "• Good teamwork skills - ability to work in a team environment and deliver results on time.\n",
       "\n",
       "• Strong communication skills - capable of conveying information concisely to diverse audiences.\n",
       "\n",
       "• Exposure to DevOps practices including CI/CD pipelines (e.g., GitHub Actions, Jenkins), containerization (e.g., Docker), and orchestration tools (e.g., Kubernetes)\n",
       "\n",
       "• Familiarity with front-end technologies like React, HTML, CSS, and JavaScript for API integration purposes\n",
       "\n",
       "Goodyear is an Equal Employment Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to that individual's race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender identity, age, physical or mental disability, ethnicity, citizenship, or any other characteristic protected by law.\n",
       "\n",
       "Goodyear is one of the world’s largest tire companies. It employs about 68,000 people and manufactures its products in 53 facilities in 20 countries around the world. Its two Innovation Centers in Akron, Ohio and Colmar-Berg, Luxembourg strive to develop state-of-the-art products and services that set the technology and performance standard for the industry. For more information about Goodyear and its products, go to www.goodyear.com/corporate\n",
       "\n",
       "#Li-Hybrid</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gQmFjay1FbmQgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiR29vZHllYXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoid0VrdmdmMVREWEhlTFhuQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Python Developer</td></tr><tr><td>Generative AI & Backend Developer(python)</td><td>INTELLYPOD</td><td>Anywhere</td><td>Job Description (JD) For Gen Ai with Python:\n",
       "\n",
       "We're Hiring: GenAI & Backend Developer (Python)\n",
       "\n",
       "Work Location: Remote (Work From Home)\n",
       "\n",
       "Experience: 2+ Years\n",
       "\n",
       "Immediate Joiners Preferred\n",
       "\n",
       "Company: IntellyPod\n",
       "\n",
       "Apply at: hrd@intellypod.com | hr@intellypod.com\n",
       "\n",
       "About the Role:\n",
       "\n",
       "IntellyPod is looking for a passionate Backend Developer with hands-on experience in GenAI, LLMs, and Python. If you’re excited about building scalable APIs, integrating AI models, and working with the latest in backend and GenAI technologies — we’d love to connect!\n",
       "\n",
       "Key Responsibilities:\n",
       "\n",
       "· Develop and maintain Python-based backend services.\n",
       "\n",
       "· Design and implement RESTful APIs.\n",
       "\n",
       "· Integrate GenAI/LLM solutions into applications.\n",
       "\n",
       "· Manage and optimize SQL/NoSQL databases.\n",
       "\n",
       "· Collaborate with cross-functional tech teams.\n",
       "\n",
       "Must-Have Skills:\n",
       "\n",
       "· 2+ years of experience in backend development (Python).\n",
       "\n",
       "· Experience with GenAI and Large Language Models (e.g., GPT, LLaMA).\n",
       "\n",
       "· Strong knowledge of REST APIs and database design.\n",
       "\n",
       "· Familiarity with Git and backend architecture best practices.\n",
       "\n",
       "Need to Have:\n",
       "\n",
       "· Experience with AWS/GCP/Azure.\n",
       "\n",
       "· Docker, Kubernetes, or CI/CD exposure.\n",
       "\n",
       "· Familiarity with vector databases (e.g., Pinecone, FAISS).\n",
       "\n",
       "· Prompt engineering or LLM fine-tuning knowledge.\n",
       "\n",
       "Why Join Us?\n",
       "\n",
       "· 100% Remote – Flexible work setup\n",
       "\n",
       "· Work on next-gen AI products\n",
       "\n",
       "· Fast-growing, collaborative tech team\n",
       "\n",
       "· Opportunity to innovate with emerging AI tools\n",
       "\n",
       "Ready to build the future with us? Send your resume to: hrd@intellypod.com | hr@intellypod.com #GenAI #BackendDeveloper #PythonJobs #LLM #RemoteJob #HiringNow #IntellyPod\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: Up to ₹70,000.00 per month\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Application Question(s):\n",
       "• Are an immediate joiner -\n",
       "\n",
       "Are on notice period if yes [Then how many days]\n",
       "• Write YES or NO\n",
       "\n",
       "1) Need to ask have you worked on LLM based project -\n",
       "\n",
       "2) Have you worked on chatbot types apps -\n",
       "\n",
       "3) Have you strong knowleged of OOps and Python basic -\n",
       "\n",
       "4) Have you knowledge of Rest APi development -\n",
       "\n",
       "Experience:\n",
       "• 5G: 3 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJHZW5lcmF0aXZlIEFJIFx1MDAyNiBCYWNrZW5kIERldmVsb3BlcihweXRob24pIiwiY29tcGFueV9uYW1lIjoiSW50ZWxseXBvZCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiI4ekp1Sk9ZUFlTbWtOcFRuQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>5 hours ago</td><td>Python Developer</td></tr><tr><td>Junior Python Developer</td><td>DEHAZELABS</td><td>Anywhere</td><td>Location: Onsite, Kokapet, Hyderabad, Telangana.\n",
       "\n",
       "Job Type: Full-Time\n",
       "\n",
       "About Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n",
       "\n",
       "Responsibilities:\n",
       "• Develop and maintain Python applications and services\n",
       "• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n",
       "• Write clean, efficient, and well-documented code\n",
       "• Troubleshoot, debug, and optimize applications for performance and scalability\n",
       "• Participate in code reviews and maintain coding standards\n",
       "• Stay up-to-date with the latest industry trends and technologies\n",
       "\n",
       "Requirements:\n",
       "• Ability to code in Python and SQL\n",
       "• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n",
       "• Knowledge of version control systems (e.g., Git)\n",
       "• Excellent problem-solving skills and attention to detail\n",
       "• Strong communication and teamwork abilities.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n",
       "• Knowledge of RESTful APIs and microservices architecture.</td><td>eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Python Developer</td></tr><tr><td>Etl Developer</td><td>VIVID RESOURCING</td><td>Bilaspur, Chhattisgarh, India</td><td>Job Title:\n",
       "Data Engineer / ETL Developer\n",
       "\n",
       "Location:\n",
       "US, remote from India\n",
       "\n",
       "Department:\n",
       "IT / Data & Analytics\n",
       "\n",
       "Reports To:\n",
       "Head of Data & Analytics\n",
       "\n",
       "Employment Type:\n",
       "Contract, 12 months\n",
       "\n",
       "Pay:\n",
       "$28-35 per hour payrolled or self-employed\n",
       "\n",
       "About the Role\n",
       "\n",
       "We are seeking a Data Engineer / ETL Developer to help design, build, and maintain data pipelines that support our internal reporting and analytics needs. This role will focus on integrating data from systems such as ERP, MES , and production databases into a centralized data platform to enable reliable and timely insights across the business. You will work alongside BI developers, IT staff, and business users to ensure the smooth flow of data and contribute to the foundation of a modern business intelligence environment.\n",
       "\n",
       "Key Responsibilities\n",
       "\n",
       "ETL & Data Pipeline Development\n",
       "• Develop and maintain ETL/ELT pipelines to move data from source systems (ERP, MES, SQL databases, flat files) into our centralized platform.\n",
       "• Use tools such as Azure Data Factory , SSIS , or similar to orchestrate and automate data workflows.\n",
       "\n",
       "Data Modeling & Integration\n",
       "• Assist in designing and building data models and data marts optimized for Power BI dashboards.\n",
       "• Support the creation and maintenance of dataflows and datasets in Power BI Service.\n",
       "\n",
       "Data Quality & Documentation\n",
       "• Implement data validation, transformation, and cleansing logic to ensure high-quality, reliable data.\n",
       "• Document data processes, business rules, and data mappings to support knowledge sharing and governance.\n",
       "\n",
       "Cross-Functional Collaboration\n",
       "• Work with internal teams (production, quality, operations, finance) to understand data requirements and reporting needs.\n",
       "• Collaborate with BI developers to ensure the data pipeline supports efficient and user-friendly reporting.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\n",
       "• 2+ years of experience in data engineering or ETL development roles.\n",
       "• Solid experience with SQL and ETL tools (e.G., Azure Data Factory , SSIS , Informatica , etc.).\n",
       "• Familiarity with Power BI , including working with datasets, dataflows, or basic DAX.\n",
       "• Understanding of data integration, transformation, and warehousing concepts.\n",
       "• Experience working with or integrating manufacturing systems (ERP, MES) is a strong plus.\n",
       "\n",
       "Preferred Skills\n",
       "• Experience with cloud data platforms (especially Microsoft Azure ).\n",
       "• Exposure to Python or scripting for automation.\n",
       "• Familiarity with data governance and documentation practices.\n",
       "• Experience with manufacturing environments or industrial data is beneficial.\n",
       "\n",
       "Soft Skills\n",
       "• Strong attention to detail and a logical, structured approach to problem-solving.\n",
       "• Willingness to learn and grow in a fast-paced environment.\n",
       "• Good communication and collaboration skills across technical and non-technical teams.\n",
       "• Proactive and solutions-oriented mindset.</td><td>eyJqb2JfdGl0bGUiOiJFdGwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiVml2aWQgUmVzb3VyY2luZyIsImFkZHJlc3NfY2l0eSI6IkJpbGFzcHVyLCBDaGhhdHRpc2dhcmgsIEluZGlhIiwiaHRpZG9jaWQiOiJueDl3cW1oMXpfcnBSX2dFQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>14 hours ago</td><td>ETL Developer</td></tr><tr><td>Senior ETL and Backend Developer (Salesforce)</td><td>S&P GLOBAL</td><td>Hyderabad, Telangana, India (+1 other)</td><td>About the Role:\n",
       "\n",
       "Grade Level (for internal use):\n",
       "10\n",
       "\n",
       "Title: Senior ETL and Backend Developer (Salesforce)\n",
       "\n",
       "Job Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n",
       "\n",
       "The Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n",
       "\n",
       "The team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n",
       "\n",
       "The Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n",
       "\n",
       "Responsibilities:\n",
       "• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n",
       "• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n",
       "• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n",
       "• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n",
       "• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n",
       "• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\n",
       "Monitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n",
       "• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\n",
       "Qualifications:\n",
       "\n",
       "Basic Qualifications:\n",
       "• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n",
       "• A minimum of 8+ years of experience in software engineering & Architecture.\n",
       "• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n",
       "• A minimum of 3+ years of Salesforce development, administration/Integration.\n",
       "• Proficiency in Informatica PowerCenter and other ETL tools.\n",
       "• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n",
       "• Experience with Salesforce integration and administration.\n",
       "• Proficiency in backend development languages (e.g., Java, Python, C#).\n",
       "• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Ability to work independently and as part of a team.\n",
       "• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n",
       "• Experience with other ETL tools and data integration platforms.\n",
       "• Informatica Certified Professional\n",
       "Salesforce Certified Administrator or Developer\n",
       "• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n",
       "• Excellent problem solving, analytical and technical troubleshooting skills.\n",
       "• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n",
       "• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n",
       "\n",
       "About S&P Global Commodity Insights\n",
       "At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n",
       "\n",
       "We’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n",
       "\n",
       "S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n",
       "\n",
       "For more information, visit http://www.spglobal.com/commodity-insights.\n",
       "\n",
       "What’s In It For You?\n",
       "\n",
       "Our Purpose:\n",
       "\n",
       "Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n",
       "\n",
       "Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n",
       "\n",
       "Our People:\n",
       "\n",
       "We're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n",
       "\n",
       "From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n",
       "\n",
       "Our Values:\n",
       "\n",
       "Integrity, Discovery, Partnership\n",
       "\n",
       "At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n",
       "\n",
       "Benefits:\n",
       "\n",
       "We take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n",
       "\n",
       "Our benefits include:\n",
       "• Health & Wellness: Health care coverage designed for the mind and body.\n",
       "• Flexible Downtime: Generous time off helps keep you energized for your time on.\n",
       "• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n",
       "• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n",
       "• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n",
       "• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n",
       "\n",
       "For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n",
       "\n",
       "Global Hiring and Opportunity at S&P Global:\n",
       "\n",
       "At S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "Equal Opportunity Employer\n",
       "\n",
       "S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n",
       "\n",
       "If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n",
       "\n",
       "US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n",
       "\n",
       "Job ID: 316835\n",
       "Posted On: 2025-06-03\n",
       "Location: Hyderabad, Telangana, India</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjJlUWpKemNOclM4MXR6QzhBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>30 days ago</td><td>ETL Developer</td></tr><tr><td>Senior Etl Developer</td><td>VIVID RESOURCING</td><td>Bilaspur, Chhattisgarh, India</td><td>Job Title:\n",
       "Senior Data Engineer / ETL Developer\n",
       "\n",
       "Location:\n",
       "US, from India\n",
       "\n",
       "Department:\n",
       "IT / Data & Analytics\n",
       "\n",
       "Reports To:\n",
       "Director of Data & Analytics\n",
       "\n",
       "Employment Type:\n",
       "Contract, 12 months\n",
       "\n",
       "Pay:\n",
       "$30-38 per hour payrolled or self-employed\n",
       "\n",
       "About the Role\n",
       "\n",
       "We are seeking a skilled and motivated Senior Data Engineer / ETL Developer to lead the design and implementation of end-to-end data integration solutions powering this new platform. You will work closely with IT, business stakeholders, and BI developers to build scalable data pipelines, robust data models, and a high-performance reporting environment centered around Power BI.\n",
       "\n",
       "This role is a key contributor in shaping our data infrastructure and delivering timely, accurate insights across the organization—from the shop floor to the boardroom.\n",
       "\n",
       "Key Responsibilities\n",
       "\n",
       "Data Engineering & Integration\n",
       "• Design, build, and maintain scalable ETL/ELT pipelines to extract, transform, and load data from diverse manufacturing systems (ERP, MES, IoT sensors, SQL databases, flat files, APIs).\n",
       "• Develop and manage data workflows using tools such as Azure Data Factory, SQL Server Integration Services (SSIS), or other modern data orchestration platforms.\n",
       "\n",
       "Platform & Architecture Support\n",
       "• Contribute to the design and architecture of the new internal analytics platform, ensuring flexibility, scalability, and cost-efficiency.\n",
       "• Design and optimize data lake and data warehouse solutions using cloud-native tools (preferably Microsoft Azure).\n",
       "\n",
       "Power BI Enablement\n",
       "• Build and optimize semantic data models, dataflows, and datasets for use in Power BI.\n",
       "• Collaborate with Power BI developers and business analysts to ensure data models meet reporting requirements, including DAX performance tuning and efficient data structure design.\n",
       "\n",
       "Data Governance & Quality\n",
       "• Implement data validation, cleansing, and monitoring processes to ensure data quality, reliability, and accuracy.\n",
       "• Contribute to data governance policies, including documentation, lineage tracking, and security controls.\n",
       "\n",
       "Collaboration & Mentorship\n",
       "• Work closely with cross-functional teams (IT, production, quality, finance, supply chain) to gather data requirements and deliver meaningful solutions.\n",
       "• Provide technical leadership and mentor junior developers or analysts in best practices for data engineering and analytics.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor’s or Master’s degree in Computer Science, Information Systems, Engineering, or a related field.\n",
       "• 5+ years of experience in data engineering, ETL development, or database architecture roles.\n",
       "• Proven experience with ETL tools such as Azure Data Factory, SSIS, or Informatica.\n",
       "• Advanced SQL skills, including performance tuning, stored procedures, and query optimization.\n",
       "• Strong experience with Power BI, including DAX, data modeling, and dataset optimization.\n",
       "• Experience with data warehousing (e.G., Azure Synapse, SQL Server, Snowflake) and cloud platforms (preferably Azure).\n",
       "• Understanding of manufacturing systems and data (e.G., ERP, MES, shop floor data, SCADA, historians).\n",
       "\n",
       "Preferred Skills\n",
       "• Experience with Python or other scripting languages for automation and data manipulation.\n",
       "• Familiarity with time-series data and integration from IoT or edge devices.\n",
       "• Knowledge of DevOps practices, CI/CD for data pipelines, and version control using Git.\n",
       "• Exposure to data governance frameworks and tools like Purview, Alation, or Collibra.\n",
       "• Power BI Service administration experience and integration with Power Platform (e.G., Power Automate, Power Apps)\n",
       "\n",
       "Key Competencies\n",
       "• Strong analytical and problem-solving skills with attention to detail.\n",
       "• Excellent communication skills, with the ability to bridge technical and business domains.\n",
       "• Self-starter with the ability to lead initiatives and work independently or collaboratively.\n",
       "• A passion for continuous improvement and innovation in a manufacturing setting.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRXRsIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlZpdmlkIFJlc291cmNpbmciLCJhZGRyZXNzX2NpdHkiOiJCaWxhc3B1ciwgQ2hoYXR0aXNnYXJoLCBJbmRpYSIsImh0aWRvY2lkIjoiMnQxYXo2bnUzRU1abUlLLUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>14 hours ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer – IBM DataStage</td><td>TATA CONSULTANCY SERVICES</td><td>Hyderabad, Telangana, India</td><td>Job Title: ETL Developer – IBM DataStage\n",
       "\n",
       "Experience: 5 to 10 years\n",
       "\n",
       "Location: Hyderabad, Chennai, Mumbai, Bangalore, Ahmedabad, Indore\n",
       "\n",
       "Employment Type: Full-time\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "We are seeking a skilled ETL Developer with hands-on experience in IBM DataStage to join our dynamic data engineering team. The ideal candidate will have a strong background in ETL development, data warehousing concepts, and performance optimization. This role involves designing and implementing robust ETL solutions, collaborating with cross-functional teams, and ensuring high data quality and integrity.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Design, develop, and implement ETL processes using IBM DataStage.\n",
       "• Work across the full project lifecycle including low-level design, development, testing, and deployment.\n",
       "• Develop parallel jobs using various DataStage stages such as Copy, Join, Merge, Lookup, Funnel, Filter, Sort, Remove Duplicates, Aggregator, Change Capture, and Transformer.\n",
       "• Integrate data from heterogeneous sources including Oracle, SQL Server, and flat files.\n",
       "• Optimize and troubleshoot existing ETL processes for performance improvements.\n",
       "• Collaborate with data analysts and architects to understand business requirements and translate them into technical solutions.\n",
       "• Ensure data quality and integrity across multiple data sources.\n",
       "• Create and maintain technical documentation for ETL processes.\n",
       "• Participate in code reviews and adhere to ETL best practices.\n",
       "• Work in Agile environments and use tools like JIRA for tracking tasks and issues.\n",
       "• Demonstrate strong problem-solving skills and the ability to troubleshoot technical issues effectively.\n",
       "• Understand and support operational requirements as part of business delivery.\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with IBM DataStage for ETL development and migration.\n",
       "• Solid understanding of database and data warehousing concepts.\n",
       "• Proficiency in SQL and UNIX.\n",
       "• Experience working with large datasets and complex data transformations.\n",
       "• Familiarity with Agile methodologies and tools like JIRA.\n",
       "• Excellent communication and collaboration skills.</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIOKAkyBJQk0gRGF0YVN0YWdlIiwiY29tcGFueV9uYW1lIjoiVGF0YSBDb25zdWx0YW5jeSBTZXJ2aWNlcyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoibkdQWmp0dFNpdlVKUWVEYkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>7 days ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>INSIGHT GLOBAL</td><td>Hyderabad, Telangana, India</td><td>We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\n",
       "Responsibilities:\n",
       "Lead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\n",
       "Collaborate with developers to ensure a smooth transition and integration of the upgraded system.\n",
       "Enhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\n",
       "Provide support and guidance to the ETL development lead.\n",
       "Engage with end-users to ensure their needs are met during and after the upgrade process.\n",
       "Utilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\n",
       "Requirements:\n",
       "Minimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\n",
       "Proven experience with SQL Server upgrades, particularly from 2012 to 2022.\n",
       "Strong SQL Server skills, including building tables, stored procedures, views, and functions.\n",
       "Experience with .NET development is highly desirable to be leveraged for other projects\n",
       "Excellent problem-solving skills and attention to detail.\n",
       "Ability to work effectively in a team environment and communicate with end-users.\n",
       "Preferred Qualifications:\n",
       "Experience with SQL Server 2016.\n",
       "Experience with Oracle\n",
       "Familiarity with the latest features and enhancements in SQL Server 2022.\n",
       "Strong understanding of database architecture and migration strategies.\n",
       "\n",
       "We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ikw2UEZlVW5YOU90VExsNnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>SONATAONE</td><td>Hyderabad, Telangana, India</td><td>Real-time data Ingestion, Streaming data, Kafka, AWS Cloud streaming tools, ETL, Semi-structured data formats like JSON, XML\n",
       "\n",
       "Tools: Talend, Kafka, AWS Event Bridge, Lamda and and Strong SQL & Python</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoic29uYXRhT25lIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>EPAM - ETL Developer - SSIS/SSRS</td><td>EPAM SYSTEMS INDIA PRIVATE LIMITED</td><td>Hyderabad, Telangana, India</td><td>Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n",
       "\n",
       "Experience : 5+ Years\n",
       "\n",
       "Location : Hyderabad\n",
       "\n",
       "Job Description :\n",
       "\n",
       "We are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n",
       "\n",
       "You will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n",
       "\n",
       "Key Responsibilities :\n",
       "\n",
       "- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n",
       "\n",
       "- Develop, deploy, and maintain SSIS packages for ETL processes.\n",
       "\n",
       "- Work on data warehouse design, development, and maintenance.\n",
       "\n",
       "- Ensure high performance and reliability of data integration workflows.\n",
       "\n",
       "- Troubleshoot SQL queries and ETL issues; optimize database performance.\n",
       "\n",
       "- Collaborate with cross-functional teams to define data solutions.\n",
       "\n",
       "- Work with complex data structures including JSON and XML.\n",
       "\n",
       "- Understand and implement different data models (star, snowflake, etc.).</td><td>eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IkVQQU0gU3lzdGVtcyBJbmRpYSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InZ6RDdVNWpsc2hzYS03eW5BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>ETL Developer</td></tr><tr><td>ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location</td><td>FISERV</td><td>India</td><td>Calling all innovators – find your future at Fiserv.\n",
       "\n",
       "We’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n",
       "\n",
       "Job Title\n",
       "\n",
       "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n",
       "\n",
       "Work Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n",
       "• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n",
       "• Participates in client kickoff activities including requirements and data gathering sessions.\n",
       "• Ability to analyze and migrate incoming external data into Fiserv solutions.\n",
       "• Strong working knowledge of ETL processes and best practices.\n",
       "• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n",
       "• Documents business processes and identifies opportunities for process redesign.\n",
       "• Reports system defects and identifies opportunities for system enhancements.\n",
       "• Performs as a technical consulting resource for new clients during the implementation process.\n",
       "• Supports several project managers during the pre and post go-live activities\n",
       "• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n",
       "• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n",
       "• Provides technical and analytical guidance to the project team.\n",
       "• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n",
       "• The position requires working in shifts, 2 PM to 11 PM IST\n",
       "\n",
       "What you will need to have:\n",
       "• Minimum of 5 – 10 years of experience in software development.\n",
       "• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n",
       "• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n",
       "• Self-starter who can work effectively, both independently and in a team environment.\n",
       "• Strong analytical, organizational, and problem-solving skills.\n",
       "• Excellent verbal and written communication skills.\n",
       "• Ability to maintain a professional attitude and demeanor in high pressure situations.\n",
       "• Ability to multi-task and manage multiple projects simultaneously.\n",
       "• Weekend/evening availability and support (10% - 25%).\n",
       "\n",
       "What would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n",
       "\n",
       "Thank you for considering employment with Fiserv. Please:\n",
       "• Apply using your legal name\n",
       "• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n",
       "\n",
       "What you should know about us:\n",
       "\n",
       "Fiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n",
       "\n",
       "Our commitment to Diversity and Inclusion:\n",
       "\n",
       "We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n",
       "\n",
       "Warning about fake job posts:\n",
       "\n",
       "Please be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n",
       "\n",
       "Any communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n",
       "\n",
       "If you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>ETL Developer/Senior Consultant Specialist</td><td>HSBC</td><td>India</td><td>Some careers shine brighter than others.If you're looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.HSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.We are currently seeking an experienced professional to join our team in the role of Senior Consultant Specialist.In this role, you will:- Communicating effectively with senior stakeholders.- Work with a team of technologists, ensuring prioritization of tasks and supporting removal of blockers.- Working on burning the product backlog and oversee product demo's- Understanding complex business requirements and new technologies and understand how to influence teams to adopt new practice and embrace automation.- Managing of risks and issues\n",
       "\n",
       "Requirements\n",
       "• name : HSBC\n",
       "• location : India, IN</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyL1NlbmlvciBDb25zdWx0YW50IFNwZWNpYWxpc3QiLCJjb21wYW55X25hbWUiOiJIU0JDIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRIYmhtVUVOWmF3OGh3Q0lBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>ETL Developer</td></tr><tr><td>Senior Informatica Developer</td><td>EVERESTDX INC</td><td>Hyderabad, Telangana, India</td><td>About the Company:\n",
       "\n",
       "Everest DX – We are a Digital Platform Services company, headquartered in Stamford. Our Platform/Solution includes Orchestration, Intelligent operations with BOTs’, AI-powered analytics for Enterprise IT. Our vision is to enable Digital Transformation for enterprises to deliver seamless customer experience, business efficiency and actionable insights through an integrated set of futuristic digital technologies.\n",
       "\n",
       "Digital Transformation Services - Specialized in Design, Build, Develop, Integrate, and Manage cloud solutions and modernize Data centers, build a Cloud-native application and migrate existing applications into secure, multi-cloud environments to support digital transformation. Our Digital\n",
       "\n",
       "Platform Services enable organizations to reduce IT resource requirements and improve productivity, in addition to lowering costs and speeding digital transformation.\n",
       "\n",
       "Digital Platform - Cloud Intelligent Management (CiM) - An Autonomous Hybrid Cloud Management Platform that works across multi-cloud environments. helps enterprise Digital Transformation get most out of the cloud strategy while reducing Cost, Risk and Speed.\n",
       "\n",
       "To know more please visit: http://www.everestdx.com\n",
       "\n",
       "Responsibilities:\n",
       "• Candidate should hands-on experience on ETL and SQL.\n",
       "• Design, develop, and optimize ETL workflows using Informatica PowerCenter.\n",
       "• Implement cloud-based ETL solutions using Informatica IDMC and IICS.\n",
       "• Should have expertise on all transformations in Power Center and IDMC/IICS.\n",
       "• Should have experience or knowledge on the PC to IICS migration using CDI PC tool or some other tool.\n",
       "• Lead data migration projects, transitioning data from on-premise to cloud environments.\n",
       "• Write complex SQL queries and perform data validation and transformation.\n",
       "• Conduct detailed data analysis to ensure accuracy and integrity of migrated data.\n",
       "• Troubleshoot and optimize ETL processes for performance and error handling.\n",
       "• Collaborate with cross-functional teams to gather requirements and design solutions.\n",
       "• Create and maintain documentation for ETL processes and system configurations.\n",
       "• Implement industry best practices for data integration and performance tuning.\n",
       "\n",
       "Required Skills:\n",
       "• Hands-on experience with Informatica Power Center, IDMC and IICS.\n",
       "• Strong expertise in writing complex SQL queries and database management.\n",
       "• Experience in data migration projects (on-premise to cloud).\n",
       "• Strong data analysis skills for large datasets and ensuring accuracy.\n",
       "• Solid understanding of ETL design & development concepts.\n",
       "• Familiarity with cloud platforms (AWS, Azure).\n",
       "• Experience with version control tools (e.g., Git) and deployment processes.\n",
       "\n",
       "Preferred Skills:\n",
       "• Experience with data lakes, data warehousing, or big data platforms.\n",
       "• Familiarity with Agile methodologies.\n",
       "• Knowledge of other ETL tools</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgSW5mb3JtYXRpY2EgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiRXZlcmVzdERYIEluYyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNDdnQ200aEh5aHpSWlhDOEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>2 days ago</td><td>ETL Developer</td></tr><tr><td>Spark Engineer</td><td>STAFFINGINE LLC</td><td>India</td><td>- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>Spark Engineer</td></tr><tr><td>Data Engineer - Spark/Python</td><td>ETELLIGENS TECHNOLOGIES</td><td>India</td><td>Job Description\n",
       "\n",
       "We are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n",
       "• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n",
       "• Perform data cleansing and migration from diverse sources to target systems.\n",
       "• Collaborate with stakeholders to understand requirements and convert them into technical\n",
       "\n",
       "solutions.\n",
       "• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n",
       "• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n",
       "• Support, troubleshoot, and maintain data pipelines and workflows.\n",
       "• Participate in all phases of software development lifecycle including unit testing, integration\n",
       "\n",
       "testing, and performance testing.\n",
       "• Contribute to the modernization of data platforms and analytics tools on Azure.\n",
       "• Ensure data quality and integrity across all pipelines and systems.\n",
       "\n",
       "Required Skills & Qualifications\n",
       "• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n",
       "• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n",
       "• 1+ year of experience in Python and Spark scripting.\n",
       "• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n",
       "• Good understanding of Data Warehousing (DWH) concepts.\n",
       "• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n",
       "• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n",
       "• Hands-on experience in data cleansing, transformation, and migration projects.\n",
       "• Ability to work independently and within a team environment.\n",
       "• Microsoft Certified : Azure Data Engineer Associate\n",
       "• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n",
       "\n",
       "Location : Artha SEZ, Greater Noida West\n",
       "\n",
       "(ref:hirist.tech)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Staff Data Engineer (Spark, Python, Hadoop)</td><td>VISA</td><td>India</td><td>Company Description\n",
       "\n",
       "Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n",
       "\n",
       "When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n",
       "\n",
       "Join Visa: A Network Working for Everyone.\n",
       "Job Description\n",
       "\n",
       "The Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n",
       "\n",
       "Are you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n",
       "\n",
       "As a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n",
       "\n",
       "Essential Functions\n",
       "• Work with manager and clients to fully understand business requirements and desired business outcomes\n",
       "• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n",
       "• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n",
       "• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n",
       "• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n",
       "• Execute data engineering projects ranging from small to large either individually or as part of a project team\n",
       "• Ensure project delivery within timelines and budget requirements\n",
       "• Provide coaching and mentoring to junior team members\n",
       "\n",
       "This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\n",
       "Qualifications\n",
       "\n",
       "• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n",
       "• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n",
       "• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n",
       "• Minimum of 4 years of hands-on expertise with Java or Scala\n",
       "• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n",
       "• Experience working in an Agile and Test Driven Development environment.\n",
       "• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n",
       "• Experience with SAS as a statistical package is preferred\n",
       "• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Databricks Engineer - Spark / PySpark</td><td>ENKEFALOS TECHNOLOGIES LLP</td><td>Anywhere</td><td>Databricks Engineer – Spark / PySpark\n",
       "\n",
       "Location : Remote / Mysore\n",
       "\n",
       "Joining : Immediate\n",
       "\n",
       "Experience : 5+ years\n",
       "\n",
       "Responsibilities :\n",
       "\n",
       "Will implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n",
       "\n",
       "Requirements:\n",
       "• PySpark (RDDs, DataFrames, performance tuning)\n",
       "• Building gold‐layer data models for financial reporting\n",
       "• Experience with complex joins, aggregations, GL hierarchies\n",
       "• Version handling (Actuals vs Budget), currency conversions\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500,395.35 - ₹1,840,348.25 per year\n",
       "\n",
       "Benefits:\n",
       "• Flexible schedule\n",
       "• Paid sick time\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Application Question(s):\n",
       "• Have you worked on ADF/ADLS ?\n",
       "• Do you have hands-on experience of Spark / PySpark\n",
       "\n",
       "Experience:\n",
       "• Databricks Engineering: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>15 days ago</td><td>Spark Engineer</td></tr><tr><td>Senior PySpark Data Engineer (Big Data, Cloud Data Solutions, & Python)</td><td>SYNECHRON</td><td>India</td><td>Job Summary\n",
       "\n",
       "Synechron is seeking a skilled PySpark Data Engineer to design, develop, and optimize data processing solutions leveraging modern big data technologies. In this role, you will lead efforts to build scalable data pipelines, support data integration initiatives, and work closely with cross-functional teams to enable data-driven decision-making. Your expertise will contribute to enhancing business insights and operational efficiency, positioning Synechron as a pioneer in adopting emerging data technologies.\n",
       "\n",
       "Software Requirements\n",
       "\n",
       "Required Software Skills:\n",
       "• PySpark (Apache Spark with Python) – experience in developing data pipelines\n",
       "• Apache Spark ecosystem knowledge\n",
       "• Python programming (versions 3.7 or higher)\n",
       "• SQL and relational database management systems (e.g., PostgreSQL, MySQL)\n",
       "• Cloud platforms (preferably AWS or Azure)\n",
       "• Version control: GIT\n",
       "• Data workflow orchestration tools like Apache Airflow\n",
       "• Data management tools: SQL Developer or equivalent\n",
       "\n",
       "Preferred Software Skills:\n",
       "• Experience with Hadoop ecosystem components\n",
       "• Knowledge of containerization (Docker, Kubernetes)\n",
       "• Familiarity with data lake and data warehouse solutions (e.g., AWS S3, Redshift, Snowflake)\n",
       "• Monitoring and logging tools (e.g., Prometheus, Grafana)\n",
       "\n",
       "Overall Responsibilities\n",
       "• Lead the design and implementation of large-scale data processing solutions using PySpark and related technologies\n",
       "• Collaborate with data scientists, analysts, and business teams to understand data requirements and deliver scalable pipelines\n",
       "• Mentor junior team members on best practices in data engineering and emerging technologies\n",
       "• Evaluate new tools and methodologies to optimize data workflows and improve data quality\n",
       "• Ensure data solutions are robust, scalable, and aligned with organizational data governance policies\n",
       "• Stay informed on industry trends and technological advancements in big data and analytics\n",
       "• Support production environment stability and performance tuning of data pipelines\n",
       "• Drive innovative approaches to extract value from large and complex datasets\n",
       "\n",
       "Technical Skills (By Category)\n",
       "\n",
       "Programming Languages:\n",
       "• Required: Python (PySpark experience minimum 2 years)\n",
       "• Preferred: Scala (for Spark), SQL, Bash scripting\n",
       "\n",
       "Databases/Data Management:\n",
       "• Relational databases (PostgreSQL, MySQL)\n",
       "• Distributed storage solutions (HDFS, cloud object storage like S3 or Azure Blob Storage)\n",
       "• Data warehousing platforms (Snowflake, Redshift – preferred)\n",
       "\n",
       "Cloud Technologies:\n",
       "• Required: Experience deploying and managing data solutions on AWS or Azure\n",
       "• Preferred: Knowledge of cloud-native services like EMR, Data Factory, or Azure Data Lake\n",
       "\n",
       "Frameworks and Libraries:\n",
       "• Apache Spark (PySpark)\n",
       "• Airflow or similar orchestration tools\n",
       "• Data processing frameworks (Kafka, Spark Streaming – preferred)\n",
       "\n",
       "Development Tools and Methodologies:\n",
       "• Version control with GIT\n",
       "• Agile management tools: Jira, Confluence\n",
       "• Continuous integration/deployment pipelines (Jenkins, GitLab CI)\n",
       "\n",
       "Security Protocols:\n",
       "• Understanding of data security, access controls, and GDPR compliance in cloud environments\n",
       "\n",
       "Experience Requirements\n",
       "• Minimum of 5+ years in data engineering, with hands-on PySpark experience\n",
       "• Proven track record of developing, deploying, and maintaining scalable data pipelines\n",
       "• Experience working with data lakes, data warehouses, and cloud data services\n",
       "• Demonstrated leadership in projects involving big data technologies\n",
       "• Experience mentoring junior team members and collaborating across teams\n",
       "• Prior experience in financial, healthcare, or retail sectors is beneficial but not mandatory\n",
       "\n",
       "Day-to-Day Activities\n",
       "• Develop, optimize, and deploy big data pipelines using PySpark and related tools\n",
       "• Collaborate with data analysts, data scientists, and business teams to define data requirements\n",
       "• Conduct code reviews, troubleshoot pipeline issues, and optimize performance\n",
       "• Mentor junior team members on best practices and emerging technologies\n",
       "• Design solutions for data ingestion, transformation, and storage\n",
       "• Evaluate new tools and frameworks for continuous improvement\n",
       "• Maintain documentation, monitor system health, and ensure security compliance\n",
       "• Participate in sprint planning, daily stand-ups, and project retrospectives to align priorities\n",
       "\n",
       "Qualifications\n",
       "• Bachelor’s or Master’s degree in Computer Science, Information Technology, or related discipline\n",
       "• Relevant industry certifications (e.g., AWS Data Analytics, GCP Professional Data Engineer) preferred\n",
       "• Proven experience working with PySpark and big data ecosystems\n",
       "• Strong understanding of software development lifecycle and data governance standards\n",
       "• Commitment to continuous learning and professional development in data engineering technologies\n",
       "\n",
       "Professional Competencies\n",
       "• Analytical mindset and problem-solving acumen for complex data challenges\n",
       "• Effective leadership and team management skills\n",
       "• Excellent communication skills tailored to technical and non-technical audiences\n",
       "• Adaptability in fast-evolving technological landscapes\n",
       "• Strong organizational skills to prioritize tasks and manage multiple projects\n",
       "• Innovation-driven with a passion for leveraging emerging data technologies\n",
       "\n",
       "S YNECHRON’S DIVERSITY & INCLUSION STATEMENT\n",
       "\n",
       "Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.\n",
       "\n",
       "All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.\n",
       "\n",
       "Candidate Application Notice</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgUHlTcGFyayBEYXRhIEVuZ2luZWVyIChCaWcgRGF0YSwgQ2xvdWQgRGF0YSBTb2x1dGlvbnMsIFx1MDAyNiBQeXRob24pIiwiY29tcGFueV9uYW1lIjoiU3luZWNocm9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjJYOEpqZUhEQmxiRndHQklBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>1 day ago</td><td>Spark Engineer</td></tr><tr><td>Senior Data Engineer (Delta Lake, Spark & Unity Catalog)</td><td>306 - GOTO TECHNOLOGIES INDIA PRIVATE LIMITED</td><td>India</td><td>Job Description Where you’ll work: India (Remote) Engineering at GoTo We’re the trailblazers of remote work technology. We build powerful, flexible work software that empowers everyone to live their best life, at work and beyond. And blaze even more trails along the way. There’s ample room for growth – so you can blaze your own trail here too. When you join a GoTo product team, you’ll take on a key role in this process and see your work be used by millions of users worldwide. Your Day to Day As a Senior Data Engineer, you would be: Design, develop, and maintain robust, scalable, and efficient ETL/ELT data pipelines to process structured and unstructured data from diverse sources. Architect and implement cloud-native data solutions using AWS services including S3, EMR, Lambda, and EKS. Build and optimize large-scale data processing workflows using Apache Spark and Databricks, with hands-on experience implementing and managing Delta Lake tables for high-volume batch and streaming use cases. Manage and govern data using Unity Catalog, maintaining strong data lineage, access controls, and metadata management. Develop and maintain data models (relational and dimensional) to support analytics, reporting, and machine learning use cases. Schedule, monitor, and orchestrate workflows using Apache Airflow or similar orchestration tools. Implement data quality checks, logging, monitoring, and alerting to ensure reliability and visibility of data pipelines. Collaborate with analysts, data scientists, and business stakeholders to deliver high-quality, trusted data for downstream applications and enable self-service BI tooling. Ensure adherence to best practices in data governance, security, and compliance. Mentor junior engineers and contribute to engineering standards, including CI/CD, automated testing, and documentation What We’re Looking For As an Senior Data Engineer, your background will look like: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. 5+ years of relevant experience in data engineering or software development, with a proven track record designing and maintaining production-grade data pipelines. Proficiency in Python programming and SQL for data transformation and analytics. Strong hands-on experience with Apache Spark and Databricks, including direct exposure to Delta Lake for data lake management, ACID transactions, schema enforcement and evolution, and time travel functionalities. Experience managing and organizing data access with Unity Catalog. In-depth experience with AWS services—specifically S3, EMR, Lambda, and EKS—and an excellent understanding of cloud-based data architecture and security best practices. Strong data modeling skills (dimensional, normalized) and understanding of data warehousing and lakehouse paradigms. Hands-on experience orchestrating workflows using Apache Airflow or similar orchestration frameworks. Familiarity with BI and data visualization tools (e.g., Tableau, Power BI). Experience establishing data quality processes, monitoring, and observability for pipelines. Exceptional communication, collaboration, and problem-solving skills, with the ability to thrive in an agile, multicultural team environment. At GoTo, authenticity and inclusive culture are key to our thriving workplace, where diverse perspectives drive innovation and growth. Our team of GoGetters is passionate about learning, exploring, and working together to achieve success while staying committed to delivering exceptional experiences for our customers. We take pride in supporting our employees with comprehensive benefits, wellness programs, and global opportunities for professional and personal development. By maintaining an inclusive environment, we empower our teams to do their best work, make a meaningful impact, and grow their career. Learn more. At GoTo, we’re bold enough to imagine a world of work without limits—where curiosity and AI-driven innovation fuel our constant growth. As the leader in cloud communications and IT, we solve real-world challenges through practical, cutting-edge solutions and an unwavering customer-first mindset. Our culture is rooted in inclusion, ownership, and transparency, fueling an environment where every voice contributes to both personal and collective achievement. Here, collaboration sparks bold ideas, and authenticity is celebrated—empowering you to adapt, evolve, and make a real impact. Join GoTo, and help shape the future of work while accelerating your own growth alongside exceptional people who are redefining what’s possible.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAoRGVsdGEgTGFrZSwgU3BhcmsgXHUwMDI2IFVuaXR5IENhdGFsb2cpIiwiY29tcGFueV9uYW1lIjoiMzA2IC0gR29UbyBUZWNobm9sb2dpZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlJxSFNWX2s0aVZkRWhBNGRBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>10 days ago</td><td>Spark Engineer</td></tr><tr><td>Cloud Data Engineer- Spark & Databricks</td><td>BRIGHTTIER</td><td>Anywhere</td><td>This a Full Remote job, the offer is available from: India\n",
       "\n",
       "Job Title: Cloud Engineer – Spark/Databricks Specialist\n",
       "Location: Remote\n",
       "Job Type: Contract\n",
       "Industry: IT/Cloud Engineering\n",
       "Job Summary:\n",
       "We are looking for a highly skilled Cloud Engineer with a specialization in Apache Spark and Databricks to join our dynamic team. The ideal candidate will have extensive experience working with cloud platforms such as AWS, Azure, and GCP, and a deep understanding of data engineering, ETL processes, and cloud-native tools. Your primary responsibility will be to design, develop, and maintain scalable data pipelines using Spark and Databricks, while optimizing performance and ensuring data integrity across diverse environments.\n",
       "Key Responsibilities:\n",
       "Design and Development:\n",
       "• Architect, develop, and maintain scalable ETL pipelines using Databricks, Apache Spark (Scala, Python), and other cloud-native tools such as AWS Glue, Azure Data Factory, and GCP Dataflow.\n",
       "• Design and build data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n",
       "• Implement efficient data ingestion, transformation, and processing workflows with Spark and Databricks.\n",
       "• Optimize the performance of ETL processes for faster data processing and lower costs.\n",
       "• Develop and manage data pipelines using other ETL tools such as Informatica, SAP Data Intelligence, and others as needed.\n",
       "Data Integration and Management:\n",
       "• Integrate structured and unstructured data sources (relational databases, APIs, ERP systems) into the cloud data infrastructure.\n",
       "• Ensure data quality, validation, and integrity through rigorous testing.\n",
       "• Perform data extraction and integration from SAP or ERP systems, ensuring seamless data flow.\n",
       "Performance Optimization:\n",
       "• Monitor, troubleshoot, and enhance the performance of Spark/Databricks pipelines.\n",
       "• Implement best practices for data governance, security, and compliance across data workflows.\n",
       "Collaboration and Communication:\n",
       "• Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders, to define data requirements and deliver scalable solutions.\n",
       "• Provide technical guidance and recommendations on cloud data engineering processes and tools.\n",
       "Documentation and Maintenance:\n",
       "• Document data engineering solutions, ETL pipelines, and workflows.\n",
       "• Maintain and support existing data pipelines, ensuring they operate effectively and align with business goals.\n",
       "Qualifications:\n",
       "Education:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n",
       "Experience:\n",
       "• 7+ years of experience in cloud data engineering or similar roles.\n",
       "• Expertise in Apache Spark and Databricks for data processing.\n",
       "• Proven experience with cloud platforms like AWS, Azure, and GCP.\n",
       "• Experience with cloud-native ETL tools such as AWS Glue, Azure Data Factory, Kafka, GCP Dataflow, etc.\n",
       "• Hands-on experience with data platforms like Redshift, Snowflake, Azure Synapse, and BigQuery.\n",
       "• Experience in extracting data from SAP or ERP systems is preferred.\n",
       "• Strong programming skills in Python, Scala, or Java.\n",
       "• Proficient in SQL and query optimization techniques.\n",
       "Skills:\n",
       "• In-depth knowledge of Spark/Scala for high-performance data processing.\n",
       "• Strong understanding of data modeling, ETL/ELT processes, and data warehousing concepts.\n",
       "• Familiarity with data governance, security, and compliance best practices.\n",
       "• Excellent problem-solving, communication, and collaboration skills.\n",
       "Preferred Qualifications:\n",
       "• Certifications in cloud platforms (e.g., AWS Certified Data Analytics, Google Professional Data Engineer, Azure Data Engineer Associate).\n",
       "• Experience with CI/CD pipelines and DevOps practices for data engineering.\n",
       "• Exposure to Apache Hadoop, Kafka, or other data frameworks is a plus.\n",
       "\n",
       "This offer from \"Brighttier\" has been enriched by Jobgether.com and got a 74% flex score.</td><td>eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyLSBTcGFyayBcdTAwMjYgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJyaWdodHRpZXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTzdzcjNWUGpVa2l1R0VpZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Data Engineer (Snowflake, Spark, AWS) - AVP</td><td>12542 CITICORP SERVICES INDIA PRIVATE LIMITED</td><td>India</td><td>The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Data Engineer-Python,PySpark,SQL ,Spark Architecture,Azure Databricks</td><td>SIEMENS HEALTHINEERS</td><td>India</td><td>jobid\n",
       "• 460574\n",
       "\n",
       "jobfamily\n",
       "• Research & Development\n",
       "\n",
       "company\n",
       "• Siemens Healthcare Private Limited\n",
       "\n",
       "organization\n",
       "• Siemens Healthineers\n",
       "\n",
       "jobType\n",
       "• Full-time\n",
       "\n",
       "experienceLevel\n",
       "• Experienced Professional\n",
       "\n",
       "contractType\n",
       "• Permanent\n",
       "\n",
       "As a Data Engineer , you are required to:\n",
       "\n",
       "Design, build, and maintain data pipelines that efficiently process and transport data from various sources to storage systems or processing environments while ensuring data integrity, consistency, and accuracy across the entire data pipeline.\n",
       "\n",
       "Integrate data from different systems, often involving data cleaning, transformation (ETL), and validation. Design the structure of databases and data storage systems, including the design of schemas, tables, and relationships between datasets to enable efficient querying. Work closely with data scientists, analysts, and other stakeholders to understand their data needs and ensure that the data is structured in a way that makes it accessible and usable.\n",
       "\n",
       "Stay up-to-date with the latest trends and technologies in the data engineering space, such as new data storage solutions, processing frameworks, and cloud technologies. Evaluate and implement new tools to improve data engineering processes.\n",
       "\n",
       "Qualification : Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Science, Engineering is desirable.\n",
       "\n",
       "Experience level : At least 3 - 5 years hands-on experience in Data Engineering\n",
       "\n",
       "Desired Knowledge & Experience :\n",
       "• Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\n",
       "• Knowing Spark internals: Catalyst/Tungsten/Photon\n",
       "• Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\n",
       "• IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\n",
       "• Test: pytest, Great Expectations\n",
       "• CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\n",
       "• Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\n",
       "• Languages: Python/Functional Programming (FP)\n",
       "• SQL : TSQL/Spark SQL/HiveQL\n",
       "• Storage : Data Lake and Big Data Storage Design\n",
       "\n",
       "additionally it is helpful to know basics of:\n",
       "• Data Pipelines : ADF/Synapse Pipelines/Oozie/Airflow\n",
       "• Languages: Scala, Java\n",
       "• NoSQL : Cosmos, Mongo, Cassandra\n",
       "• Cubes : SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\n",
       "• SQL Server : TSQL, Stored Procedures\n",
       "• Hadoop : HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\n",
       "• Data Catalog : Azure Purview, Apache Atlas, Informatica\n",
       "\n",
       "Required Soft skills & Other Capabilities :\n",
       "\n",
       "Great attention to detail and good analytical abilities.\n",
       "\n",
       "Good planning and organizational skills\n",
       "\n",
       "Collaborative approach to sharing ideas and finding solutions\n",
       "\n",
       "Ability to work independently and also in a global team environment.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVB5dGhvbixQeVNwYXJrLFNRTCAsU3BhcmsgQXJjaGl0ZWN0dXJlLEF6dXJlIERhdGFicmlja3MiLCJjb21wYW55X25hbWUiOiJTaWVtZW5zIEhlYWx0aGluZWVycyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJXVkxrZFF1QU4tSGFXQjIzQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>8 days ago</td><td>Spark Engineer</td></tr><tr><td>Cloud Data Engineer (Spark/Databricks)</td><td>ANTAL JOB BOARD</td><td>Nagpur, Maharashtra, India</td><td>Vacancy No\n",
       "VN1228\n",
       "\n",
       "Business Unit\n",
       "EMEA\n",
       "\n",
       "Job Location\n",
       "India\n",
       "\n",
       "Employment Type\n",
       "Full Time\n",
       "\n",
       "Job Details and Responsibilities\n",
       "We are seeking an experienced Cloud Data Engineer with a strong background in AWS, Azure, and GCP. The ideal candidate will have extensive experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, and other ETL tools like Informatica, SAP Data Intelligence, etc. You will be responsible for designing, implementing, and maintaining robust data pipelines and building scalable data lakes. Experience with various data platforms like Redshift, Snowflake, Databricks, Synapse, Snowflake and others is essential. Familiarity with data extraction from SAP or ERP systems is a plus.\n",
       "\n",
       "Key Responsibilities:\n",
       "\n",
       "Design and Development:\n",
       "• Design, develop, and maintain scalable ETL pipelines using cloud-native tools (AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.).\n",
       "• Architect and implement data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n",
       "• Develop and optimize data ingestion, transformation, and loading processes using Databricks, Snowflake, Redshift, BigQuery and Azure Synapse.\n",
       "• Implement ETL processes using tools like Informatica, SAP Data Intelligence, and others.\n",
       "• Develop and optimize data processing jobs using Spark Scala.\n",
       "Data Integration and Management:\n",
       "• Integrate various data sources, including relational databases, APIs, unstructured data, and ERP systems into the data lake.\n",
       "• Ensure data quality and integrity through rigorous testing and validation.\n",
       "• Perform data extraction from SAP or ERP systems when necessary.\n",
       "Performance Optimization:\n",
       "• Monitor and optimize the performance of data pipelines and ETL processes.\n",
       "• Implement best practices for data management, including data governance, security, and compliance.\n",
       "Collaboration and Communication:\n",
       "• Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\n",
       "• Collaborate with cross-functional teams to design and implement data solutions that meet business needs.\n",
       "Documentation and Maintenance:\n",
       "• Document technical solutions, processes, and workflows.\n",
       "• Maintain and troubleshoot existing ETL pipelines and data integrations.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Education:\n",
       "\n",
       "Bachelor’s degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n",
       "\n",
       "Experience:\n",
       "• 7+ years of experience as a Data Engineer or in a similar role.\n",
       "• Proven experience with cloud platforms: AWS, Azure, and GCP.\n",
       "• Hands-on experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.\n",
       "• Experience with other ETL tools like Informatica, SAP Data Intelligence, etc.\n",
       "• Experience in building and managing data lakes and data warehouses.\n",
       "• Proficiency with data platforms like Redshift, Snowflake, BigQuery, Databricks, and Azure Synapse.\n",
       "• Experience with data extraction from SAP or ERP systems is a plus.\n",
       "• Strong experience with Spark and Scala for data processing.\n",
       "\n",
       "Skills:\n",
       "• Strong programming skills in Python, Java, or Scala.\n",
       "• Proficient in SQL and query optimization techniques.\n",
       "• Familiarity with data modeling, ETL/ELT processes, and data warehousing concepts.\n",
       "• Knowledge of data governance, security, and compliance best practices.\n",
       "• Excellent problem-solving and analytical skills.\n",
       "• Strong communication and collaboration skills.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with other data tools and technologies such as Apache Spark, or Hadoop.\n",
       "• Certifications in cloud platforms (AWS Certified Data Analytics – Specialty, Google Professional Data Engineer, Microsoft Certified: Azure Data Engineer Associate).\n",
       "• Experience with CI/CD pipelines and DevOps practices for data engineering\n",
       "• Selected applicant will be subject to a background investigation, which will be conducted and the results of which will be used in compliance with applicable law.\n",
       "\n",
       "What we offer in return:\n",
       "• Remote Working: Lemongrass always has been and always will offer 100% remote work\n",
       "• Flexibility: Work where and when you like most of the time\n",
       "• Training: A subscription to A Cloud Guru and generous budget for taking certifications and other resources you’ll find helpful\n",
       "• State of the art tech: An opportunity to learn and run the latest industry standard tools\n",
       "• Team: Colleagues who will challenge you giving the chance to learn from them and them from you\n",
       "\n",
       "Lemongrass Consulting is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate on the basis of race, religion, color, national origin, religious creed, gender, sexual orientation, gender identity, gender expression, age, genetic information, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics\n",
       "\n",
       "About Lemongrass\n",
       "Lemongrass (lemongrasscloud.com) is a global leader in SAP consulting, focused on helping organizations transform their business processes through innovative solutions and technologies. With a strong commitment to customer success, Lemongrass partners with companies to drive their digital transformation journeys, enabling them to unlock the full potential of their SAP investments.\n",
       "\n",
       "We do this with our continuous innovation, automation, migration and operation, delivered on the world's most comprehensive cloud platforms – AWS, Azure and GCP and SAP Cloud ERP. We have been working with AWS and SAP since 2010 and we are a Premier Amazon Partner Network (APN) Consulting Partner. We are also a Microsoft Gold Partner, a Google Cloud Partner and an SAP Certified Silver Partner.\n",
       "\n",
       "Our team is what makes Lemongrass exceptional and why we have the excellent reputation in the market that we enjoy today. At Lemongrass, you will work with the smartest and most motivated people in the business. We take pride in our culture of innovation and collaboration that drives us to deliver exceptional benefits to our clients every day.</td><td>eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyIChTcGFyay9EYXRhYnJpY2tzKSIsImNvbXBhbnlfbmFtZSI6IkFudGFsIEpvYiBCb2FyZCIsImFkZHJlc3NfY2l0eSI6Ik5hZ3B1ciwgTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJCQXpseTdfU0lsQjByTmF2QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>28 days ago</td><td>Spark Engineer</td></tr><tr><td>Data Analyst III</td><td>BRISTOL MYERS SQUIBB</td><td>Hyderabad, Telangana, India</td><td>Working with Us\n",
       "Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n",
       "\n",
       "Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n",
       "\n",
       "The US Commercial Analytics, Market Access Data Analyst will play a crucial role in supporting data-driven decision-making processes, generating insights, and providing strategic guidance to optimize our Market Access & Pricing activities in the US. This position requires a deep expertise in secondary analytics and statistical analytic techniques, along with a firm grasp of the US healthcare landscape, especially market access and pricing dynamics. The Business Analyst III will typically be responsible for leading support for multiple therapeutic areas in Market Access and will collaborate with cross-functional teams to enhance understanding of business performance and drive growth.\n",
       "\n",
       "Roles and Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Perform statistical analyses, data mining and predictive modelling to uncover trends and drivers of performance\n",
       "• Collaborate with stakeholders (US based Market Access Business Insights & Analytics team and US Market Access team members) to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects including payer, PBM and patient level subnational & claims data analyses, formulary and policy level analysis, and ad-hoc analytics based on secondary data to support US Market Access strategy\n",
       "• Prepare reports, dashboards and presentations to communicate findings, insights and recommendations to stakeholders\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up-to-date with industry trends (US access and health care landscape), best practices, and emerging technologies\n",
       "• Partner with US based Market Access BIA/Commercial team members (for a given project scope)\n",
       "• Provide overall project oversight and training/mentorship to junior analysts and team members as required\n",
       "• Supports culture of fact-based decision making through application of best-in-class, innovative, and appropriate methodologies to address key business questions\n",
       "\n",
       "Skills & Competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions, around US pricing and reimbursement strategies\n",
       "• Proficiency in statistical analysis techniques and applications of those to US commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and strategic thinking, with the ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome Candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• 5-8 years prior Pharmaceutical industry experience with 1-2 years within US Market Access & Pricing Analytics\n",
       "• Experience working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims, medical policy and formulary data), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with reimbursement and regulatory requirements and compliance in the US biopharma industry\n",
       "• Certification or training in relevant analytics or business intelligence tools is a plus\n",
       "\n",
       "If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n",
       "\n",
       "Uniquely Interesting Work, Life-changing Careers\n",
       "With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n",
       "\n",
       "On-site Protocol\n",
       "\n",
       "BMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n",
       "\n",
       "Site-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n",
       "\n",
       "BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n",
       "\n",
       "BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n",
       "\n",
       "BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n",
       "\n",
       "If you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n",
       "\n",
       "Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUlJIiwiY29tcGFueV9uYW1lIjoiQnJpc3RvbCBNeWVycyBTcXVpYmIiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZHU3JGRGo4OTcybHdnQWpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>3 days ago</td><td>Data Analyst</td></tr><tr><td>Data Science Analyst</td><td>IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED</td><td>India</td><td>About the Role:Within Insights and Commercial Solutions, below are the key service groups:  Marketing Effectiveness (Business Analytics, Forecasting, Competitive Intelligence)  Field Force Operations  Commercial Consulting  Brand Insights  Patient access services (Pricing and market access, etc.)  Primary Market Research Provide analytics support to Novartis internal customers (CPOs & Regional marketing and sales teams) on various projects • Support and facilitate data enabled decision making for Novartis internal customers using data analysis and data science techniques/methods on internal and external (3rd party) data to solve business problems • Support I&CS business in building capabilities by involving in various initiatives like knowledge sharing, on-boarding and training support, support in all business-related tasks/activities, building process documentation and knowledge repositories.Analyst, Data ScienceLocation – Hyderabad #LI Hybrid Key Responsibilities:Delivering projects and managing internal customer expectations across multi-channel marketing analytics, Portfolio Analytics, Targeting and Segmentation, Predictive Analytics, Resource Allocation and Optimization, Other ad-hoc requests.Create and deliver customer requirements as agreed SLAs (timeliness, accuracy, quality, etc.) and drive excellent customer satisfaction. Deliver advanced analytical and statistical solutions for various projects related to promotion evaluation, multi-channel marketing (MCM) campaign design, return on investment (ROI) analysis, resource allocation, segmentation, targeting, and other ad-hoc business questionsSupport exploratory research to identify new areas of application of advanced analytics/data science in providing enhanced decision-making support. Deliver services through a structured project management approach with appropriate documentation and communication throughout the delivery of servicesAutomation of project codes and development of front-end delivery solutions. Support in creation and maintenance of standard operating procedures (SOPs), quality checklists that will enable excellent quality outputs within the function.Support in developing and maintaining knowledge repositories that captures qualitative and quantitative reports of brands, disease areas, macroeconomic trends of Novartis operating markets, etc.Comply with all internal functional operating procedures like time tracking, KPI tracking and reporting, and other internal systems and processes. Comply to all Novartis operating procedures as per legal/IT/HR requirementsEssential Requirements:Technical Skills: Statistical Modeling Experience (Covering but not limited to, Regression, Classification, Supervised and Unsupervised Learning Methods, Optimization techniques, Predictive Models, Deep learning Models, e.g. Neural Networks, Natural Language Processing, Text Mining etc.)Programming languages/tools – R, SAS, Python, SQLVisualization tools – Qlikview, Qliksense, TableauWorking knowledge of MS-Office (MS Excel, VBA, PowerPoint, Access)Pharmaceutical industry domain/datasets knowledge – desirableEducation:Graduation/Post Graduation in Business Administration, Mathematics, Economics, Statistics, Engineering or Quantitative streamsExperience: Experience (Minimum 2years) in analytics in marketing analytics, experience in pharma industry is preferableShould have strong quantitative and systems backgroundStrong analytical thinking with problem solving approachShould have good ability to understand new data sources in short timeframe and embed them into standard analysesA good understanding of pharmaceutical domain and data would be ideal. Should have worked in an international company with exposure to working in a cross-cultural environmentStrong and proactive business results focus, and proven ability to provide insights that increase productivityWhy Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook. https://www.novartis.com/careers/benefits-rewards Commitment to Diversity and Inclusion: Novartis is committed to building an outstanding, inclusive work environment and diverse teams' representative of the patients and communities we serve. Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVuY2UgQW5hbHlzdCIsImNvbXBhbnlfbmFtZSI6IklOMTAgKEZDUlMgPSBJTjAxMCkgTm92YXJ0aXMgSGVhbHRoY2FyZSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiejA4Y3NEMUZUdmdvZExfWkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>7 days ago</td><td>Data Analyst</td></tr><tr><td>Data Sr.Modeler/Data Analyst( Immediate Joiner)</td><td>THE TALENT QUEST</td><td>Hyderabad, Telangana, India</td><td>Vacancy with a company focused on digital transformation, specializing in intelligent automation, digitalization, data science & analytics, and mobile enablement. They help businesses improve cost efficiency, productivity, and agility by reducing turnaround time and errors. The company provides services and solutions including operations digital transformation consulting, next-gen shared services setup consulting, cognitive RPA deployment, and AI-enabled CX enhancement. Founded in 2020 ;with HQ in Gurugram, India; the Company is now operating from Noida, Mumbai, Hyderabad, and Bengaluru as well.\n",
       "\n",
       "Job Role:We are seeking a highly skilled and detail-oriented Data Modeler to join our Data\n",
       "\n",
       "Management team. The ideal candidate will be responsible for designing and\n",
       "\n",
       "implementing logical and physical data models to support enterprise data\n",
       "\n",
       "initiatives. This role requires close collaboration with business stakeholders, data\n",
       "\n",
       "architects, and engineers to ensure data is structured and accessible for analytics,\n",
       "\n",
       "reporting, and operational needs.\n",
       "\n",
       "The successful candidate will:\n",
       "\n",
       "Provides technical expertise in needs identification, data modelling, data\n",
       "\n",
       "movement and transformation mapping (source to target), automation and testing\n",
       "\n",
       "strategies, translating business needs into technical solutions with adherence to\n",
       "\n",
       "established data guidelines and approaches from a business unit or project\n",
       "\n",
       "perspective.\n",
       "\n",
       "7-10 Years industry implementation experience with one or more data\n",
       "\n",
       "modelling tools such as Erwin, ERStudio, PowerDesigner etc.\n",
       "\n",
       " Minimum of 8 years of data architecture, data modelling or similar\n",
       "\n",
       "experience\n",
       "\n",
       " 5-7 years of management experience required\n",
       "\n",
       " 5-7 years consulting experience preferred\n",
       "\n",
       " Experience working with dimensionally modelled data\n",
       "\n",
       " Bachelor’s degree or equivalent experience, Master’s Degree Preferred\n",
       "\n",
       " Understanding of cloud (Azure, AWS, GCP, Snowflake preferred) and on\n",
       "\n",
       "premises architectures\n",
       "\n",
       "Job Types: Full-time, Permanent\n",
       "\n",
       "Pay: Up to ₹3,000,000.00 per year\n",
       "\n",
       "Benefits:\n",
       "• Cell phone reimbursement\n",
       "• Internet reimbursement\n",
       "• Life insurance\n",
       "• Paid sick time\n",
       "• Paid time off\n",
       "• Work from home\n",
       "\n",
       "Work Location: In person</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIFNyLk1vZGVsZXIvRGF0YSBBbmFseXN0KCBJbW1lZGlhdGUgSm9pbmVyKSIsImNvbXBhbnlfbmFtZSI6IlRoZSBUYWxlbnQgUXVlc3QiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ino0YkdrY3RHOUdZdmFTRVVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>2 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst - INTL - Mexico or India</td><td>INSIGHT GLOBAL</td><td>Hyderabad, Telangana, India</td><td>Project Background:\n",
       "Mosaic is our financial planning and reporting transformation program, bringing an aligned business process and toolset to all key business units enabling us to get consistent and standardized financial metrics across geographies. Started in 2019 the program is already rolled out to Europe and most of North America and continues across the world.\n",
       "There are three key components to the program:\n",
       "1. A standardized planning tool IBM Cognos TM1\n",
       "2. A global data warehouse based on Azure Cloud know as SPOT (single point of truth). The SPOT team is made up of functional data analysts & data engineers\n",
       "3. An extensive suite of dashboards known as Cockpit using Tableau. The Cockpit team are generally finance analysts with Tableau skills\n",
       "Role Background:\n",
       "We are looking for an additional functional/data analyst to join the existing team of functional/data analysts working on the SPOT data warehouse.\n",
       "The role of the functional/data analyst is to be the bridge between the Cockpit team, Data Engineering team, and Cognos TM1 team. The analyst understands the business data requirements and translates these into data definition and data transformation documents that are then build by the data engineers. The analyst can also be involved in testing, data validation and any sustain activity related to the flow and accuracy of the data in SPOT & Cockpit.\n",
       "Typically, the functional data analyst receives requirements for new and/or amended data pipelines from the Cockpit team or works with the Cognos TM1 team on new data sets that need to flow to SPOT & Cockpit.\n",
       "The current functional data analysts come from a range of backgrounds from data engineers to business analysts. What unites the team is a passion for data, attention to detail, ability to navigate and understand complex data flows, and a customer-centric mindset. This is a high profile and at times high pressure transformation program and everyone on the team must be responsive to our stakeholders needs.\n",
       "Key Accountabilities:\n",
       "This role will primarily work with the North America Cockpit team primarily based in Mexico City, but as part of the team they will be expected to build a good knowledge of all data pipelines and work on any area as needed. The key elements of the role are:\n",
       "Develop and maintain SPOT solution design & data architecture:\n",
       "o Ensure SPOT solution design & data model is up to date with latest business requirements\n",
       "o Contribute to the delivery of current technical roadmap that can support the predicted growth in data volumes\n",
       "Translate and communicate business requirements across all IT delivery teams and/or partners:\n",
       "o Collaborate closely with SPOT Cockpit teams to ensure business requirements received from Global & Regional Planning teams are well-understood and translated into the SPOT solution design (functional & data model)\n",
       "Act as an owner in the diagnosis and resolution of any data issues raised with the team, partnering with other teams as needed.\n",
       "\n",
       "We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBJTlRMIC0gTWV4aWNvIG9yIEluZGlhIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Il96ZU5oSUE0Ujg3ak1KZHNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>6 days ago</td><td>Data Analyst</td></tr><tr><td>IN Specialist 3- Data Analyst - TRAS -Assurance– Bangalore</td><td>PWC</td><td>India</td><td>Line of ServiceAssuranceIndustry/SectorNot ApplicableSpecialismAssuranceManagement LevelSpecialistJob Description & SummaryAt PwC, our people in audit and assurance focus on providing independent and objective assessments of financial statements, internal controls, and other assurable information enhancing the credibility and reliability of this information with a variety of stakeholders. They evaluate compliance with regulations including assessing governance and risk management processes and related controls.In digital assurance at PwC, you will focus on providing assurance services over clients' digital environment, including processes and controls, cyber security measures, data and AI systems, and their associated governance, to help organisations and their stakeholders build trust in their technology while complying with relevant regulations.*Why PWCAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm’s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations. \"Job Description & SummaryABOUT TECHNOLOGY RISK ASSURANCE SERVICES:The Technology Risk Assurance Services practice in the firm, provides both audit and non-audit services related to controls around the financial reporting process, including financial business process and IT management controls.SPA provides:· Financial and operation applications/business process controls reviews· Database security controls reviews· IT general controls reviews· Infrastructure security reviews· Third party assurance and opinion services· Sarbanes-Oxley readiness, process improvement and sustainability services· Due diligence on systems and controls· Pre- and post-implementation systems reviews· Project assurance services· Data services (e.g., CAATs, data quality reviews)· Computer security reviewsResponsibilities (essential functions and responsibilities of the job):The tasks, roles and responsibilities concerned with the job will include the following:Executing various audit engagements as a support to field in-charge andApplying The Firm’s methodology while executing various job rolesApply current knowledge of IT trends and systems to identify security and risk management issues, and other opportunities for improvementAssist in the planning and execution of business process control reviews, information systems audits, and other Assurance offerings across a variety of industriesA good understanding of clients Business Risks, Audit Risks, IT Risks and the mitigating controls in addressing these risksGain strong comprehension of client operations, processes, and business objectives and utilize that knowledge on engagements.Evaluate and test business processes and controls and identify areas of risks.Perform general computer and application controls reviews including ERPs like SAP, Oracle Application, JD Edwards, MFG Pro, etc.Comply with requirements of ICAI Auditing Standards like AAS 29Data Analysis using CAATs tool like Excel, ACL, SQL etc*Mandatory skill sets - Data Analyst*Preferred Skill Sets – Business Analyst*Years of experience required – 0-1 Years*Education Qualification – Post Graduation in Data ScienceEducation (if blank, degree and/or field of study not specified)Degrees/Field of Study required: Postgraduate (Diploma)Degrees/Field of Study preferred:Certifications (if blank, certifications not specified)Required SkillsData AnalyticsOptional SkillsAccepting Feedback, Accepting Feedback, Active Listening, Artificial Intelligence (AI) Platform, Auditing Methodologies, Cloud Engineering, Communication, Compliance and Governance, Compliance and Standards, Compliance Auditing, Corporate Governance, Cybersecurity Governance, Cybersecurity Risk Management, Data Quality, Data Quality Assessment, Data Quality Assurance Testing, Data Validation, Emotional Regulation, Empathy, Governance Framework, Inclusion, Information Assurance, Information Security Governance, Intellectual Curiosity, Internal Controls {+ 11 more}Desired Languages (If blank, desired languages not specified)Travel RequirementsNot SpecifiedAvailable for Work Visa Sponsorship?NoGovernment Clearance Required?NoJob Posting End Date</td><td>eyJqb2JfdGl0bGUiOiJJTiBTcGVjaWFsaXN0IDMtIERhdGEgQW5hbHlzdCAtIFRSQVMgLUFzc3VyYW5jZeKAkyBCYW5nYWxvcmUiLCJjb21wYW55X25hbWUiOiJQd0MiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiZTlNdVBjTnNJWHlmb3ZrSkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>1 day ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst 1</td><td>BRISTOL MYERS SQUIBB</td><td>Hyderabad, Telangana, India</td><td>The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n",
       "\n",
       "Roles & Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up to date with industry trends, best practices, and emerging technologies\n",
       "\n",
       "Skills and competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n",
       "• commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• Proven experience (1-3 years) in a similar business analyst role\n",
       "• Prior Pharmaceutical industry and/or healthcare consulting experience required\n",
       "• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with regulatory requirements and compliance in the US biopharma industry\n",
       "\n",
       "The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n",
       "\n",
       "Roles & Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up to date with industry trends, best practices, and emerging technologies\n",
       "\n",
       "Skills and competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n",
       "• commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• Proven experience (1-3 years) in a similar business analyst role\n",
       "• Prior Pharmaceutical industry and/or healthcare consulting experience required\n",
       "• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with regulatory requirements and compliance in the US biopharma industry</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ1ZUlVZURnaVJaU1ludkdlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>16 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst Finance (VBA, Power Query, Power BI, Python) 4+ years of experience)</td><td>DUPONT</td><td>Hyderabad, Telangana, India</td><td>At DuPont, our purpose is to empower the world with essential innovations to thrive. We work on things that matter. Whether it’s providing clean water to more than a billion people on the planet, producing materials that are essential in everyday technology devices from smartphones to electric vehicles, or protecting workers around the world. Discover the many reasons the world’s most talented people are choosing to work at DuPont. Why Join Us | DuPont Careers\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "The Data Analyst/Developer will play a crucial role in developing automated data solutions, creating comprehensive dashboards, and utilizing advanced Excel functionalities. This position will require collaboration with various teams to translate data needs into actionable intelligence.\n",
       "\n",
       "Key Areas of Expertise and Responsibilities:\n",
       "\n",
       "1. Visual Basic for Applications (VBA)\n",
       "• Responsibilities:\n",
       "• Develop and maintain complex VBA applications to automate repetitive tasks.\n",
       "• Incorporate SAP Scripting within VBA to optimize business processes.\n",
       "• Troubleshoot and debug existing VBA code to ensure smooth functionality.\n",
       "• Criteria:\n",
       "• Advanced proficiency in VBA programming.\n",
       "• Demonstrated experience with SAP interfaces and scripting.\n",
       "• Ability to write modular, efficient, and maintainable code.\n",
       "• Knowledge of Excel object model and its functionalities.\n",
       "\n",
       "2. Power Query\n",
       "• Responsibilities:\n",
       "• Utilize Power Query for data extraction, transformation, and loading (ETL) processes.\n",
       "• Develop and maintain data models in Excel to streamline data preparation.\n",
       "• Create and optimize Power Query scripts for efficient data processing.\n",
       "• Criteria:\n",
       "• Intermediate experience with Power Query including M language for data transformation.\n",
       "• Familiarity with connecting Power Query to various data sources (e.g., databases, APIs).\n",
       "• Ability to perform data cleansing and manipulation through Power Query.\n",
       "\n",
       "3. Power BI\n",
       "• Responsibilities:\n",
       "• Create interactive, user-friendly dashboards and reports using Power BI.\n",
       "• Collaborate with stakeholders to gather reporting requirements and deliver insights.\n",
       "• Optimize Power BI reports for performance and usability.\n",
       "• Criteria:\n",
       "• Intermediate knowledge of Power BI Desktop and Power BI Service.\n",
       "• Ability to create DAX measures and calculated columns for enhanced analytics.\n",
       "• Familiarity with data visualization best practices and techniques.\n",
       "\n",
       "4. Python\n",
       "• Responsibilities:\n",
       "• Develop Python scripts to automate data manipulation and Excel-related tasks.\n",
       "• Utilize libraries such as Pandas and NumPy for data analysis and automation.\n",
       "• Collaborate with the data team to integrate Python solutions with existing tools.\n",
       "• Criteria:\n",
       "• Intermediate proficiency in Python, especially in data manipulation and automation.\n",
       "• Experience with libraries like Pandas, NumPy, and openpyxl for Excel automation.\n",
       "• Understanding of APIs and ability to retrieve data programmatically.\n",
       "\n",
       "Qualifications:\n",
       "• Bachelor’s degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n",
       "• A minimum of 4 of experience in a data-related role, focusing on the technologies mentioned.\n",
       "• Strong analytical and problem-solving skills with attention to detail.\n",
       "• Excellent communication skills and the ability to work collaboratively with diverse teams.\n",
       "\n",
       "Preferred Skills:\n",
       "• Experience with SQL and relational databases for data querying and data management.\n",
       "• Familiarity with other business intelligence tools (like Tableau or QlikView) is a plus.\n",
       "• Knowledge of machine learning principles is an advantage.\n",
       "• Understanding of data warehousing concepts and methodologies.\n",
       "\n",
       "Join our Talent Community to stay connected with us!\n",
       "\n",
       "On May 22, 2024, we announced a plan to separate our Electronics and Water businesses in a tax-free manner to its shareholders. On January 15, 2025, we announced that we are targeting November 1, 2025, for the completion of the intended separation of the Electronics business (the “Intended Electronics Separation”)*. We also announced that we would retain the Water business. We are committed to ensuring a smooth and successful separation process for the Future Electronics business. We look forward to welcoming new talent interested in contributing to the continued success and growth of our evolving organization.\n",
       "\n",
       "(1)The separation transactions are subject to satisfaction of customary conditions, including final approval by DuPont's Board of Directors, receipt of tax opinion from counsel, the filing and effectiveness of Form 10 registration statements with the U.S. Securities and Exchange Commission, applicable regulatory approvals, and satisfactory completion of financing.  For further discussion of risks, uncertainties and assumptions that could impact the achievement, expected timing and intended benefits of the separation transactions, see DuPont’s announcement.\n",
       "\n",
       "DuPont is an equal opportunity employer. Qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability or any other protected class. If you need a reasonable accommodation to search or apply for a position, please visit our Accessibility Page for Contact Information.\n",
       "\n",
       "DuPont offers a comprehensive pay and benefits package. To learn more visit the Compensation and Benefits page.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgRmluYW5jZSAoVkJBLCBQb3dlciBRdWVyeSwgUG93ZXIgQkksIFB5dGhvbikgNCsgeWVhcnMgb2YgZXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJEdXBvbnQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImNGeEdTMkhJQXpJRW5kb2ZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>2 days ago</td><td>Data Analyst</td></tr><tr><td>Sustainability Data Analyst</td><td>CARRIER</td><td>Hyderabad, Telangana, India</td><td>Role: Sustainability Data Analyst\n",
       "\n",
       "Location: Hyderabad, India\n",
       "\n",
       "Full/ Part-time: Full time\n",
       "\n",
       "Build a career with confidence\n",
       "\n",
       "Carrier Global Corporation, global leader in intelligent climate and energy solutions is committed to creating solutions that matter for people and our planet for generations to come. From the beginning, we've led in inventing new technologies and entirely new industries. Today, we continue to lead because we have a world-class, diverse workforce that puts the customer at the center of everything we do.\n",
       "\n",
       "About the role\n",
       "\n",
       "We are seeking a results-driven Sustainability Data Analyst to join Carrier’s product sustainability analytics team. The ideal candidate will have a strong background in sustainability practices and data analytics, with a focus on analyzing environmental impact and identifying opportunities for improvement. This role involves conducting comprehensive sustainability/energy audits, analyzing data, and developing strategies to enhance sustainability performance.\n",
       "\n",
       "Key responsibilities:\n",
       "• Perform detailed sustainability audits and analyze data to identify trends and areas for improvement.\n",
       "• Develop and implement strategies to enhance sustainability and reduce environmental impact.\n",
       "• Monitor and evaluate the performance of sustainability initiatives using data analytics.\n",
       "• Collaborate with cross-functional teams to optimize sustainability practices.\n",
       "• Prepare reports and presentations on sustainability metrics and audit findings.\n",
       "• Stay updated on industry trends and best practices in sustainability and data analytics.\n",
       "\n",
       "Minimum Requirements:\n",
       "\n",
       "Education: Bachelor’s or Master’s degree in Mechanical, Energy Engineering, Environmental Science, Sustainability, Data Analytics, or a related field.\n",
       "\n",
       "Experience: atleast 6 years in sustainability, data analytics, or BI development roles to be able to drive himself towards business goals independently.\n",
       "\n",
       "Key Skills:\n",
       "• Strong analytical skills, attention to detail and ability to think from first principles.\n",
       "• Excellent communication and teamwork abilities.\n",
       "• Proficiency in data analytics software and tools. Knowledge of python, SQL, Power-BI, would be added advantage.\n",
       "• Knowledge of relevant regulations and standards in sustainability.\n",
       "• Familiarity with data visualization tools and techniques.\n",
       "• Willingness to be flexible, learn new tools, techniques and deliver.\n",
       "\n",
       "Benefits\n",
       "\n",
       "We are committed to offering competitive benefits programs for all of our employees, and enhancing our programs when necessary.\n",
       "• Enjoy your best years with our retirement savings plan\n",
       "• Have peace of mind and body with our health insurance\n",
       "• Make yourself a priority with flexible schedules, parental leave and our holiday purchase scheme\n",
       "• Drive forward your career through professional development opportunities\n",
       "• Achieve your personal goals with our Employee Assistance Programme.\n",
       "\n",
       "Our commitment to you\n",
       "\n",
       "Our greatest assets are the expertise, creativity and passion of our employees. We strive to provide a great place to work that attracts, develops and retains the best talent, promotes employee engagement, fosters teamwork and ultimately drives innovation for the benefit of our customers. We strive to create an environment where you feel that you belong, with diversity and inclusion as the engine to growth and innovation. We develop and deploy best-in-class programs and practices, providing enriching career opportunities, listening to employee feedback and always challenging ourselves to do better. This is The Carrier Way.\n",
       "\n",
       "Join us and make a difference.\n",
       "\n",
       "Apply Now!\n",
       "\n",
       "Carrier is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.\n",
       "\n",
       "Job Applicant's Privacy Notice:\n",
       "\n",
       "Click on this link to read the Job Applicant's Privacy Notice</td><td>eyJqb2JfdGl0bGUiOiJTdXN0YWluYWJpbGl0eSBEYXRhIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJDYXJyaWVyIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJPMkpEMHBVcG0xc3dxYnc1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>9 days ago</td><td>Data Analyst</td></tr><tr><td>Sr. Data Analyst</td><td>ICIMS TALENT ACQUISITION</td><td>Rai Durg, Telangana, India</td><td>Job Overview\n",
       "\n",
       "The Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.\n",
       "\n",
       "About Us\n",
       "\n",
       "When you join iCIMS, you join the team helping global companies transform business and the world through the power of talent. Our customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile. As the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent. We’re passionate about helping companies build a diverse, winning workforce and about building our home team. We're dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\n",
       "\n",
       "Responsibilities\n",
       "• Perform various data analysis functions to analyze data from a variety of sources including external labor market data and research and internal data sets from our platforms \n",
       "• Incorporate information from a variety of systems to produce comprehensive and compelling narratives for thought-leadership initiatives and customer engagements\n",
       "• Demonstrate critical thinking - identify the story in context using multiple data sets, and present results. A strong proficiency in data storytelling will be critical to success in this role.\n",
       "• Understand principles of quality data visualization and apply them in Tableau to create and maintain custom dashboards for consumption by other employees\n",
       "• Find and investigate data quality issues, root causes and recommend remedies to be implemented by the data scientists and engineers  \n",
       "• Liaise with teams around our business to understand their problems, determine how our team can help, then use our database to produce the content they need\n",
       "• Identify data mapping and enrichment requirements. Familiarity with SQL, especially the logic behind different types of data joins and writing efficient queries, will be necessary\n",
       "• Consistently ensure that business is always conducted with integrity and that behavior aligns with iCIMS policies, procedures, and core competencies\n",
       "\n",
       "Additional Job Responsibilities: \n",
       "• Produce and adapt data visualizations in response to business requests for internal and external use\n",
       "• Shows good judgement in prioritizing their own commitments and those of the larger team, while demonstrating initiative and appropriate urgency when needed\n",
       "• Mentor junior team members in best practices for analytics, data visualization, and data storytelling. Exemplify these standards and guide teammates in following them.\n",
       "• Think creatively to produce unique, actionable insights from complex datasets, which can deliver value to our business and to our customers.\n",
       "\n",
       "Qualifications\n",
       "• 5-10 years professional experience working in an analytics capacity\n",
       "• Excellent communication skills, especially with regards to data storytelling – finding insights from complex datasets and sharing those findings with key stakeholders\n",
       "• Strong data analytics and visualization skills\n",
       "• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n",
       "• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n",
       "• Advanced knowledge of Excel (Pivot tables, VLOOKUPs, IF statements)\n",
       "• Familiarity with data guardrails to ensure compliance with applicable data governance regulations and privacy laws (i.e., GDPR)\n",
       "\n",
       "Preferred\n",
       "• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n",
       "• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n",
       "\n",
       "EEO Statement\n",
       "\n",
       "iCIMS is a place where everyone belongs. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at iCIMS.\n",
       "\n",
       "We are proud to be an equal opportunity and affirmative action employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at careers@icims.com.\n",
       "\n",
       "Compensation and Benefits\n",
       "\n",
       "Competitive health and wellness benefits include medical insurance (employee and dependent family members), personal accident and group term life insurance, bonding and parental leave, lifestyle spending account reimbursements, wellness services offerings, sick and casual/emergency days, paid holidays, tuition reimbursement, retirals (PF - employer contribution) and gratuity. Benefits and eligibility may vary by location, role, and tenure. Learn more here: https://careers.icims.com/benefits</td><td>eyJqb2JfdGl0bGUiOiJTci4gRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiaUNJTVMgVGFsZW50IEFjcXVpc2l0aW9uIiwiYWRkcmVzc19jaXR5IjoiUmFpIER1cmcsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImZMUlRBQjlESVFWdFdOeGpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>21 days ago</td><td>Data Analyst</td></tr><tr><td>Master Data Analyst</td><td>C32511 ALFA LAVAL INDIA PRIVATE LIMITED</td><td>India</td><td>Purpose of the Job: To provide direct support to the Sales and Order Handling teams by managing customer master data and administering the Order Management System. This role ensures data integrity, system readiness, and compliance with internal controls during order booking and execution processes. Key Responsibilities: Customer Master Data Management Manage the full lifecycle of customer codes, including creation, amendment, and deactivation. Conduct background checks using the Global World Check system and coordinate with the Export Control Super User for compliance validation. Ensure customer data is configured correctly in the system for seamless order booking and execution. Maintain data accuracy and cleanliness in line with Alfa Laval’s global data standards. System Administration Act as the system administrator for the Customer Order Management system. Maintain system master data and troubleshoot operational issues. Ensure the system is up-to-date and aligned with business requirements. Order Booking Compliance Perform basic compliance checks during order booking. Verify completeness and correctness of mandatory information. Ensure all required approvals are in place as per the authorization matrix. Process Activation Initiate internal processes required for order execution. Collaborate with cross-functional teams to ensure timely and accurate order processing. Key Competencies: Strong attention to detail and data accuracy Knowledge of compliance and export control processes Proficiency in ERP and order management systems Analytical and problem-solving skills Effective communication and collaboration across teams Qualifications & Experience: Bachelor’s degree in Business Administration, Information Systems, or related field 3+ years of experience in data management or order processing roles Familiarity with data governance and compliance frameworks Experience with customer master data and ERP systems (e.g., SAP) \"We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\" Every day, we get opportunities to make a positive impact – on our colleagues, partners, customers and society. Together, we’re pioneering the solutions of the future and unlocking the full potential of precious resources. Trusted to act on initiative, we challenge conventional thinking to develop world-leading technologies that inspire progress in vital areas, including energy, food, water and shipping. As we push forward, the innovative, open spirit that fuels our 140-year-old start-up culture and rapid growth also drives our personal growth. So, as we shape a more resourceful, less wasteful world, we build our careers too. Meet our dedicated people who always go the extra mile Alfa Laval Career Site Or sign up for Job Alerts to stay in touch Sign up for Job Alerts</td><td>eyJqb2JfdGl0bGUiOiJNYXN0ZXIgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiQzMyNTExIEFsZmEgTGF2YWwgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImhJMnc5MHYxS1E3QXVQTkNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>6 days ago</td><td>Data Analyst</td></tr><tr><td>Engr II-Data Engineering</td><td>VERIZON</td><td>India</td><td>When you join Verizon\n",
       "\n",
       "You want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n",
       "\n",
       "What You’ll Be Doing...\n",
       "\n",
       "As a Data Engineer with ETL/ELT expertise for our growing data platform and analytics teams, you will understand and enable the required data sets from different sources. This includes both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n",
       "• Understanding the business requirements.\n",
       "• Transforming technical design.\n",
       "• Working on data ingestion, preparation and transformation.\n",
       "• Developing the scripts for data sourcing and parsing.\n",
       "• Developing data streaming applications.\n",
       "• Debugging the production failures and identifying the solution.\n",
       "• Working on ETL/ELT development.\n",
       "\n",
       "What We’re Looking For...\n",
       "\n",
       "You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems.\n",
       "\n",
       "You'll Need To Have\n",
       "• Bachelor’s degree or one or more years of relevant experience required, demonstrated through work experience and/or military experience.\n",
       "• Experience with Data Warehouse concepts and Data Management life cycle.\n",
       "\n",
       "Even better if you have one or more of the following:\n",
       "• Any related Certification on ETL/ELT developer.\n",
       "• Accuracy and attention to detail.\n",
       "• Good problem solving, analytical, and research capabilities.\n",
       "• Good verbal and written communication.\n",
       "• Experience presenting to and influencing partners.\n",
       "\n",
       "Why Verizon?\n",
       "\n",
       "Verizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.\n",
       "• We are a ‘pay for performance’ company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.\n",
       "• Your benefits are market competitive and delivered by some of the best providers.\n",
       "• You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.\n",
       "• We offer generous paid time off benefits to help you manage your work life balance and opportunities for flexible working arrangements*.\n",
       "• Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.\n",
       "• You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.\n",
       "\n",
       "Your benefits package will vary depending on the country in which you work.\n",
       "• subject to business approval\n",
       "\n",
       "If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.\n",
       "\n",
       "Where you’ll be working\n",
       "\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "Scheduled Weekly Hours\n",
       "\n",
       "40\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Verizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.</td><td>eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkpaY3lnNzJpTUJGcHpfUTJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>1 day ago</td><td>Data Engineer</td></tr><tr><td>Associate Analyst - Data Engineer</td><td>PEPSICO</td><td>Hyderabad, Telangana, India</td><td>Overview\n",
       "\n",
       "PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT.  The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics, and new product development.  PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\n",
       "\n",
       "What PepsiCo Data Management and Operations does:\n",
       "\n",
       "Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\n",
       "\n",
       "Responsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\n",
       "\n",
       "Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\n",
       "\n",
       "Increase awareness about available data and democratize access to it across the company.\n",
       "\n",
       " \n",
       "\n",
       "               As a data engineer, you will be the key technical expert building PepsiCo's data products to drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help developing very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n",
       "\n",
       "Responsibilities\n",
       "• Act as a subject matter expert across different digital projects.\n",
       "• Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\n",
       "• Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\n",
       "• Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\n",
       "• Responsible for implementing best practices around systems integration, security, performance, and data management.\n",
       "• Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\n",
       "• Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\n",
       "• Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\n",
       "• Develop and optimize procedures to “productionalize” data science models.\n",
       "• Define and manage SLA’s for data products and processes running in production.\n",
       "• Support large-scale experimentation done by data scientists.\n",
       "• Prototype new approaches and build solutions at scale.\n",
       "• Research in state-of-the-art methodologies.\n",
       "• Create documentation for learnings and knowledge transfer.\n",
       "• Create and audit reusable packages or libraries.\n",
       "\n",
       "Qualifications\n",
       "• 4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n",
       "• 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n",
       "• 3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n",
       "• 2+ years in cloud data engineering experience in Azure.\n",
       "• Fluent with Azure cloud services. Azure Certification is a plus.\n",
       "• Experience in Azure Log Analytics\n",
       "• Experience with integration of multi cloud services with on-premises technologies.\n",
       "• Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\n",
       "• Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\n",
       "• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n",
       "• Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\n",
       "• Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\n",
       "• Experience with Statistical/ML techniques is a plus.\n",
       "• Experience with building solutions in the retail or in the supply chain space is a plus.\n",
       "• Experience with version control systems like Github and deployment & CI tools.\n",
       "• Working knowledge of agile development, including DevOps and DataOps concepts.\n",
       "• B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.\n",
       "\n",
       " Skills, Abilities, Knowledge:\n",
       "• Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\n",
       "• Strong change manager. Comfortable with change, especially that which arises through company growth.\n",
       "• Ability to understand and translate business requirements into data and technical requirements.\n",
       "• High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\n",
       "• Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\n",
       "• Strong organizational and interpersonal skills; comfortable managing trade-offs.</td><td>eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUgQW5hbHlzdCAtIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJQZXBzaUNvIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJkdUI4NkhSU1JwRWNPdlBCQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>1 month ago</td><td>Data Engineer</td></tr><tr><td>Senior Big Data Engineer</td><td>QUALCOMM</td><td>Hyderabad, Telangana, India</td><td>Company:\n",
       "Qualcomm India Private Limited\n",
       "\n",
       "Job Area:\n",
       "Engineering Group, Engineering Group > Software Engineering\n",
       "\n",
       "General Summary:\n",
       "\n",
       "As a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n",
       "\n",
       "Minimum Qualifications:\n",
       "• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n",
       "OR\n",
       "Master's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n",
       "OR\n",
       "PhD in Engineering, Information Systems, Computer Science, or related field.\n",
       "\n",
       "• 2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n",
       "\n",
       "General Summary:\n",
       "\n",
       "Preferred Qualifications\n",
       "• 3+ years of experience as a Data Engineer or in a similar role\n",
       "• Experience with data modeling, data warehousing, and building ETL pipelines\n",
       "• Solid working experience with Python, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n",
       "• Experience with Big Data tools, platforms and architecture with solid working experience with SQL\n",
       "• Experience working in a very large data warehousing environment, Distributed System.\n",
       "• Solid understanding on various data exchange formats and complexities\n",
       "• Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n",
       "• Strong data visualization skills\n",
       "• Basic understanding of Machine Learning; Prior experience in ML Engineering a plus\n",
       "• Ability to manage on-premises data and make it inter-operate with AWS based pipelines\n",
       "• Ability to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n",
       "\n",
       "Education\n",
       "• Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n",
       "• Preferred Qualifications: Masters in CS/ECE with a Data Science / ML Specialization\n",
       "\n",
       "Minimum Qualifications:\n",
       "• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n",
       "\n",
       "OR\n",
       "\n",
       "Master's degree in Engineering, Information Systems, Computer Science, or related field\n",
       "\n",
       "OR\n",
       "\n",
       "PhD in Engineering, Information Systems, Computer Science, or related field.\n",
       "• 3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n",
       "\n",
       "Develops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n",
       "\n",
       "Principal Duties and Responsibilities:\n",
       "• Completes assigned coding tasks to specifications on time without significant errors or bugs.\n",
       "• Adapts to changes and setbacks in order to manage pressure and meet deadlines.\n",
       "• Collaborates with others inside project team to accomplish project objectives.\n",
       "• Communicates with project lead to provide status and information about impending obstacles.\n",
       "• Quickly resolves complex software issues and bugs.\n",
       "• Gathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n",
       "• Seeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n",
       "• Participates in technical conversations with tech leads/managers.\n",
       "• Anticipates and communicates issues with project team to maintain open communication.\n",
       "• Makes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n",
       "• Prioritizes project deadlines and deliverables with minimal supervision.\n",
       "• Resolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n",
       "• Writes readable code for large features or significant bug fixes to support collaboration with other engineers.\n",
       "• Determines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n",
       "• Unit tests own code to verify the stability and functionality of a feature.\n",
       "\n",
       "Applicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n",
       "\n",
       "Qualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n",
       "\n",
       "To all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n",
       "\n",
       "If you would like more information about this role, please contact Qualcomm Careers.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQmlnIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJRdWFsY29tbSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieVRVTHdrQjB2Ujk5bmJQUUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 days ago</td><td>Data Engineer</td></tr><tr><td>Manager Data Engineer - AWS Databricks</td><td>BLEND360</td><td>Hyderabad, Telangana, India</td><td>Company Description\n",
       "\n",
       "Blend is a premier AI services provider, committed to co-creating meaningful impact for its clients through the power of data science, AI, technology, and people. With a mission to fuel bold visions, Blend tackles significant challenges by seamlessly aligning human expertise with artificial intelligence. The company is dedicated to unlocking value and fostering innovation for its clients by harnessing world-class people and data-driven strategy. We believe that the power of people and AI can have a meaningful impact on your world, creating more fulfilling work and projects for our people and clients. For more information, visit www.blend360.com\n",
       "\n",
       "Job Description\n",
       "\n",
       "We are seeking a seasoned Data Engineering Manager with 8+ years of experience to lead and grow our data engineering capabilities. This role demands strong hands-on expertise in Python, SQL, Spark, and advanced proficiency in AWS and Databricks. As a technical leader, you will be responsible for architecting and optimizing scalable data solutions that enable analytics, data science, and business intelligence across the organization.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Lead the design, development, and optimization of scalable and secure data pipelines using AWS services such as Glue, S3, Lambda, EMR, and Databricks Notebooks, Jobs, and Workflows.\n",
       "• Oversee the development and maintenance of data lakes on AWS Databricks, ensuring performance and scalability.\n",
       "• Build and manage robust ETL/ELT workflows using Python and SQL, handling both structured and semi-structured data.\n",
       "• Implement distributed data processing solutions using Apache Spark/PySpark for large-scale data transformation.\n",
       "• Collaborate with cross-functional teams including data scientists, analysts, and product managers to ensure data is accurate, accessible, and well-structured.\n",
       "• Enforce best practices for data quality, governance, security, and compliance across the entire data ecosystem.\n",
       "• Monitor system performance, troubleshoot issues, and drive continuous improvements in data infrastructure.\n",
       "• Conduct code reviews, define coding standards, and promote engineering excellence across the team.\n",
       "• Mentor and guide junior data engineers, fostering a culture of technical growth and innovation.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Requirements\n",
       "• 8+ years of experience in data engineering with proven leadership in managing data projects and teams.\n",
       "• Expertise in Python, SQL, Spark (PySpark), and experience with AWS and Databricks in production environments.\n",
       "• Strong understanding of modern data architecture, distributed systems, and cloud-native solutions.\n",
       "• Excellent problem-solving, communication, and collaboration skills.\n",
       "• Prior experience mentoring team members and contributing to strategic technical decisions is highly desirable.</td><td>eyJqb2JfdGl0bGUiOiJNYW5hZ2VyIERhdGEgRW5naW5lZXIgLSBBV1MgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJsZW5kMzYwIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJxelhlVGQtWWc2ekE5U3R4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>3 days ago</td><td>Data Engineer</td></tr><tr><td>Data Engineer INTL India - EOR 6fb570f8</td><td>INSIGHT GLOBAL</td><td>Hyderabad, Telangana, India</td><td>- In this role, you will be building data pipeline solutions by designing, adopting, and applying big data strategies and architectures. You must have knowledge of scalable system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration, data transformation, data modeling, and data cleansing.\n",
       "- The Sr. Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for business stakeholders.\n",
       "- Design and develop a logical data model for a large multi-tenant application with intent to facilitate consistency and efficiency.\n",
       "- Collaborate with analysts and internal clients to understand the goals of the overall system data architecture, and further develop on business needs/issues.\n",
       "- Test/troubleshoot problems and conduct root cause analysis.\n",
       "- Own communications with technical contacts at software vendors to escalate software issues, recommend enhancements, and find sustainable workarounds to issues.\n",
       "- Work in tandem with analysts and other stakeholders to develop and execute necessary processes and controls around the flow of data to meet data governance standards.\n",
       "- Verify accuracy of table changes and data transformation processes\n",
       "- Deliver fully tested code prior to prod-deployment when appropriate.\n",
       "- Recommend and implement enhancements that address system performance requirements, streamline processes and improve data integrity.\n",
       "- Create sound technical documentation and train peer developers on this documentation as development completes.\n",
       "- Additional duties as assigned to ensure company success.\n",
       "The compensation for this role varies depending on depth and length of experience, and can range from 27-36 LAKH.\n",
       "\n",
       "We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElOVEwgSW5kaWEgLSBFT1IgNmZiNTcwZjgiLCJjb21wYW55X25hbWUiOiJJbnNpZ2h0IEdsb2JhbCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiZUtoaVo5cWozWC1wLXlBVEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 days ago</td><td>Data Engineer</td></tr><tr><td>R&D Data Engineer</td><td>SANOFI</td><td>Hyderabad, Telangana, India</td><td>Position Title: R&D Data Engineer\n",
       "\n",
       "About the Job\n",
       "\n",
       "At Sanofi, we’re committed to providing the next-gen healthcare that patients and customers need. It’s about harnessing data insights and leveraging AI responsibly to search deeper and solve sooner than ever before. Join our R&D Data & AI Products and Platforms Team as an R&D Data Engineer\n",
       "\n",
       "and you can help make it happen.\n",
       "\n",
       "What you will be doing:\n",
       "\n",
       "Sanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives.\n",
       "\n",
       "The R&D Data & AI Products and Platforms Team is a key team within R&D Digital, focused on developing and delivering Data and AI products for R&D use cases. This team plays a critical role in pursuing broader democratization of data across R&D and providing the foundation to scale AI/ML, advanced analytics, and operational analytics capabilities.\n",
       "\n",
       "As an R&D Data Engineer, you will join this dynamic team committed to driving strategic and operational digital priorities and initiatives in R&D. You will work as a part of a Data & AI Product Delivery Pod, lead by a Product Owner, in an agile environment to deliver Data & AI Products. As a part of this team, you will be responsible for the design and development of data pipelines and workflows to ingest, curate, process, and store large volumes of complex structured and unstructured data. You will have the ability to work on multiple data products serving multiple areas of the business.\n",
       "\n",
       "Our vision for digital, data analytics and AI\n",
       "\n",
       "Join us on our journey in enabling Sanofi’s Digital Transformation through becoming an AI first organization. This means:\n",
       "• AI Factory - Versatile Teams Operating in Cross Functional Pods: Utilizing digital and data resources to develop AI products, bringing data management, AI and product development skills to products, programs and projects to create an agile, fulfilling and meaningful work environment.\n",
       "• Leading Edge Tech Stack: Experience build products that will be deployed globally on a leading-edge tech stack.\n",
       "• World Class Mentorship and Training: Working with renown leaders and academics in machine learning to further develop your skillsets.\n",
       "\n",
       "We are an innovative global healthcare company with one purpose: to chase the miracles of science to improve people’s lives. We’re also a company where you can flourish and grow your career, with countless opportunities to explore, make connections with people, and stretch the limits of what you thought was possible. Ready to get started?\n",
       "\n",
       "Main Responsibilities:\n",
       "\n",
       "Data Product Engineering:\n",
       "• Provide input into the engineering feasibility of developing specific R&D Data/AI Products\n",
       "• Provide input to Data/AI Product Owner and Scrum Master to support with planning, capacity, and resource estimates\n",
       "• Design, build, and maintain scalable and reusable ETL / ELT pipelines to ingest, transform, clean, and load data from sources into central platforms / repositories\n",
       "• Structure and provision data to support modeling and data discovery, including filtering, tagging, joining, parsing and normalizing data\n",
       "• Collaborate with Data/AI Product Owner and Scrum Master to share Progress on engineering activities and inform of any delays, issues, bugs, or risks with proposed remediation plans\n",
       "• Design, develop, and deploy APIs, data feeds, or specific features required by product design and user stories\n",
       "• Optimize data workflows to drive high performance and reliability of implemented data products\n",
       "• Oversee and support junior engineer with Data/AI Product testing requirements and execution\n",
       "\n",
       "Innovation & Team Collaboration:\n",
       "• Stay current on industry trends, emerging technologies, and best practices in data product engineering\n",
       "• Contribute to a team culture of of innovation, collaboration, and continuous learning within the product team\n",
       "\n",
       "About You:\n",
       "\n",
       "Key Functional Requirements & Qualifications:\n",
       "• Bachelor’s degree in software engineering or related field, or equivalent work experience\n",
       "• 3-5 years of experience in data product engineering, software engineering, or other related field\n",
       "• Understanding of R&D business and data environment preferred\n",
       "• Excellent communication and collaboration skills\n",
       "• Working knowledge and comfort working with Agile methodologies\n",
       "\n",
       "Key Technical Requirements & Qualifications:\n",
       "• Proficiency with data analytics and statistical software (incl. SQL, Python, Java, Excel, AWS, Snowflake, Informatica)\n",
       "• Deep understanding and proven track record of developing data pipelines and workflows\n",
       "\n",
       "Why Choose Us?\n",
       "• Bring the miracles of science to life alongside a supportive, future-focused team\n",
       "• Discover endless opportunities to grow your talent and drive your career, whether it’s through a promotion or lateral move, at home or internationally\n",
       "• Enjoy a thoughtful, well-crafted rewards package that recognizes your contribution and amplifies your impact\n",
       "• Take good care of yourself and your family, with a wide range of health and wellbeing benefits including high-quality healthcare, prevention and wellness programs\n",
       "\n",
       "Pursue Progress. Discover Extraordinary.\n",
       "\n",
       "Progress doesn’t happen without people – people from different backgrounds, in different locations, doing different roles, all united by one thing: a desire to make miracles happen. You can be one of those people. Chasing change, embracing new ideas and exploring all the opportunities we have to offer. Let’s pursue progress. And let’s discover Extraordinary together.\n",
       "\n",
       "At Sanofi, we provide equal opportunities to all regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or gender identity.\n",
       "\n",
       "Watch our ALL IN video and check out our Diversity Equity and Inclusion actions at sanofi.com!</td><td>eyJqb2JfdGl0bGUiOiJSXHUwMDI2RCBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiU2Fub2ZpIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJWUzRuREMyZ2hRajk3SVhDQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>Data Engineer</td></tr><tr><td>Data Engineer-Senior II</td><td>FEDERAL EXPRESS CORPORATION AMEA</td><td>Hyderabad, Telangana, India</td><td>Responsible for supporting the development and maintenance of business intelligence and analytics systems, implementing data warehouse architecture, enabling self-service data exploration, designing data models, implementing data security strategies, optimizing production processes, providing user training and support, and overseeing project management activities as a Data Engineer to facilitate data-driven decision-making and generate insights for business stakeholders.\n",
       "\n",
       "1. Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making.\n",
       "2. Implement of business intelligence and analytics systems, ensuring alignment with business requirements.\n",
       "3. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets.\n",
       "4. Enable self-service data exploration capabilities for users to analyze and visualize data independently.\n",
       "5. Develop reporting and analysis applications to generate insights from data for business stakeholders.\n",
       "6. Design and implement data models to organize and structure data for analytical purposes.\n",
       "7. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information.\n",
       "8. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability.\n",
       "9. Assist in training and support to users on business intelligence tools and applications.\n",
       "10. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions.\n",
       "\n",
       "Education: Bachelors’ degree or equivalent in Computer Science, MIS, or similar discipline.\n",
       "Accreditation: Specific business accreditation for Business Intelligence.\n",
       "\n",
       "Experience: Relevant work experience in data engineering based on the following number of years:\n",
       "Associate: Prior experience not required\n",
       "Standard I: Two (2) years\n",
       "Standard II: Three (3) years\n",
       "Senior I: Four (4) years\n",
       "Senior II: Five (5) years\n",
       "\n",
       "Knowledge, Skills and Abilities\n",
       "• Fluency in English\n",
       "• Analytical Skills\n",
       "• Accuracy & Attention to Detail\n",
       "• Numerical Skills\n",
       "• Planning & Organizing Skills\n",
       "• Presentation Skills\n",
       "\n",
       "Preferred Qualifications:\n",
       "\n",
       "Pay Transparency:\n",
       "\n",
       "Pay:\n",
       "\n",
       "Additional Details:\n",
       "\n",
       "FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n",
       "\n",
       "All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\n",
       "Our Company\n",
       "\n",
       "FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World’s Most Admired Companies by \"Fortune\" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\n",
       "Our Philosophy\n",
       "\n",
       "The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\n",
       "Our Culture\n",
       "\n",
       "Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970’s. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today’s global marketplace.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVNlbmlvciBJSSIsImNvbXBhbnlfbmFtZSI6IkZlZGVyYWwgRXhwcmVzcyBDb3Jwb3JhdGlvbiBBTUVBIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJhZDU5RmU0Vl94QzdTVGFqQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>1 day ago</td><td>Data Engineer</td></tr><tr><td>Lead Data Engineer(Snowflake,PowerBi)</td><td>THOMSON REUTERS</td><td>Hyderabad, Telangana, India (+1 other)</td><td>Want to be part of the Data & Analytics organization, whose strategic goal is to create a world-class Data & Analytics company by building, embedding, and maturing a data-driven culture across Thomson Reuters.\n",
       "\n",
       "About The Role\n",
       "We are looking for a highly motivated individual with strong organizational and technical skills for the position of Lead Data Engineer/ Data Engineering Manager (Snowflake). You will play a critical role working on cutting edge of Data Engineering and analytics, leveraging predictive models, machine learning and generative AI to drive business insights and facilitating informed decision-making and help Thomson Reuters rapidly scale data-driven initiatives.\n",
       "\n",
       "Effectively communicate across various levels, including Executives, and functions within the global organization.\n",
       "Demonstrate strong leadership skills with ability to drive projects/tasks to delivering value\n",
       "Engage with stakeholders, business analysts and project team to understand the data requirements.\n",
       "Design analytical frameworks to provide insights into a business problem.\n",
       "Explore and visualize multiple data sets to understand data available and prepare data for problem solving.\n",
       "Design database models (if a data mart or operational data store is required to aggregate data for modeling).\n",
       "\n",
       "About You\n",
       "You're a fit for the Lead Data Engineer/ Data Engineering Manager (Snowflake), if your background includes:\n",
       "Qualifications: B-Tech/M-Tech/MCA or equivalent\n",
       "Experience: 7-9 years of corporate experience\n",
       "Location: Bangalore, India\n",
       "Hands-on experience in developing data models for large scale data warehouse/data Lake – Snowflake, BW\n",
       "Map the data journey from operational system sources through any transformations in transit to its delivery into enterprise repositories (Warehouse, Data Lake, Master Data, etc.)\n",
       "Enabling on the overall master and reference data strategy, including the procedures to ensure the consistency and quality of Finance reference data.\n",
       "Experience across ETL, SQL and other emerging data technologies with experience in integrations of a cloud-based analytics environment\n",
       "Build and refine end-to-end data workflows to offer actionable insights\n",
       "Fair understanding of Data Strategy, Data Governance Process\n",
       "Knowledge in BI analytics and visualization tools: Power BI, Tableau\n",
       "\n",
       "#LI-NR1\n",
       "\n",
       "What’s in it For You?\n",
       "• Hybrid Work Model: We’ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\n",
       "• Flexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\n",
       "• Career Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow’s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\n",
       "• Industry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\n",
       "• Culture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\n",
       "• Social Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\n",
       "• Making a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\n",
       "\n",
       "About Us\n",
       "\n",
       "Thomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\n",
       "\n",
       "We are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\n",
       "\n",
       "As a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\n",
       "\n",
       "We also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here.\n",
       "\n",
       "Learn more on how to protect yourself from fraudulent job postings here.\n",
       "\n",
       "More information about Thomson Reuters can be found on thomsonreuters.com.</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIoU25vd2ZsYWtlLFBvd2VyQmkpIiwiY29tcGFueV9uYW1lIjoiVGhvbXNvbiBSZXV0ZXJzIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJzSFl5TXBoOXlWbDl4R0pBQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>13 days ago</td><td>Data Engineer</td></tr><tr><td>Engr II-Data Engineering</td><td>VERIZON</td><td>Hyderabad, Telangana, India (+1 other)</td><td>When you join Verizon\n",
       "\n",
       "You want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n",
       "\n",
       "What you’ll be doing…\n",
       "\n",
       "We are looking for data engineers who can work with world class team members to help drive telecom business to its full potential . We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n",
       "\n",
       "As a data engineer you will be collaborating with business product owners , coaches , industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n",
       "\n",
       "As a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n",
       "\n",
       "Understanding the business requirements and the technical design.\n",
       "\n",
       "Working on Data Ingestion, Preparation and Transformation.\n",
       "\n",
       "Developing data streaming applications.\n",
       "\n",
       "Debugging the production failures and identifying the solution.\n",
       "\n",
       "Working on ETL/ELT development.\n",
       "\n",
       "Where you'll be working:\n",
       "\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "What we’re looking for...\n",
       "\n",
       "You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n",
       "\n",
       "You’ll need to have:\n",
       "\n",
       "Bachelor’s degree or one or more years of work experience.\n",
       "\n",
       "Experience with Data Warehouse concepts and Data Management life cycle.\n",
       "\n",
       "Experience in any DBMS\n",
       "\n",
       "Experience in Shell scripting, Spark, Scala.\n",
       "\n",
       "Knowledge in GCP/BigQuery.\n",
       "\n",
       "Even better if you have:\n",
       "\n",
       "Two or more years of relevant experience.\n",
       "\n",
       "Any relevant Certification on ETL/ELT developer.\n",
       "\n",
       "Certification in GCP-Data Engineer.\n",
       "\n",
       "Accuracy and attention to detail.\n",
       "\n",
       "Good problem solving, analytical, and research capabilities.\n",
       "\n",
       "Good verbal and written communication.\n",
       "\n",
       "Experience presenting to and influence stakeholders.\n",
       "\n",
       "#AI&D\n",
       "\n",
       "Where you’ll be working\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "Scheduled Weekly Hours\n",
       "40\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Verizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.</td><td>eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJJZ05hWTZ2NWxkNUFrcWRyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>Data Engineer</td></tr><tr><td>Senior Data Engineer - Data Integration</td><td>EPAM SYSTEMS</td><td>Hyderabad, Telangana, India</td><td>EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.\n",
       "\n",
       "Our company is looking for an experienced Senior Data Engineer to join our team.\n",
       "\n",
       "As a Senior Data Engineer, you will be working on a project that focuses on data integration and ETL for cloud-based platforms. You will be responsible for designing and implementing complex data solutions, ensuring that the data is accurate, reliable, and easily accessible.\n",
       "\n",
       "RESPONSIBILITIES\n",
       "• Design and implement complex data solutions for cloud-based platforms\n",
       "• Develop ETL processes using SQL, Python, and other relevant technologies\n",
       "• Ensure that data is accurate, reliable, and easily accessible for all stakeholders\n",
       "• Collaborate with cross-functional teams to understand data integration needs and requirements\n",
       "• Develop and maintain documentation, including technical specifications, data flow diagrams, and data mappings\n",
       "• Monitor and optimize data integration processes for performance and efficiency, ensuring data accuracy and integrity\n",
       "\n",
       "REQUIREMENTS\n",
       "• Bachelor's degree in Computer Science, Electrical Engineering, or a related field\n",
       "• 5-8 years of experience in data engineering\n",
       "• Experience with cloud-native or Spark-based ETL tools such as AWS Glue, Azure Data Factory, or GCP Dataflow\n",
       "• Strong knowledge of SQL for data querying and manipulation\n",
       "• Experience with Snowflake for data warehousing\n",
       "• Experience with cloud platforms such as AWS, GCP, or Azure for data storage and processing\n",
       "• Excellent problem-solving skills and attention to detail\n",
       "• Good verbal and written communication skills in English at a B2 level\n",
       "\n",
       "NICE TO HAVE\n",
       "• Experience with ETL using Python\n",
       "\n",
       "WE OFFER\n",
       "• Opportunity to work on technical challenges that may impact across geographies\n",
       "• Vast opportunities for self-development: online university, knowledge sharing opportunities globally, learning opportunities through external certifications\n",
       "• Opportunity to share your ideas on international platforms\n",
       "• Sponsored Tech Talks & Hackathons\n",
       "• Unlimited access to LinkedIn learning solutions\n",
       "• Possibility to relocate to any EPAM office for short and long-term projects\n",
       "• Focused individual development\n",
       "• Benefit package\n",
       "• Health benefits\n",
       "• Retirement benefits\n",
       "• Paid time off\n",
       "• Flexible benefits\n",
       "• Forums to explore beyond work passion (CSR, photography, painting, sports, etc.)</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAtIERhdGEgSW50ZWdyYXRpb24iLCJjb21wYW55X25hbWUiOiJFUEFNIFN5c3RlbXMiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZnZkJvSVM5OFhDSUw3NWNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>Data Engineer</td></tr><tr><td>Python Developer – Telegram Bot Integration & Excel Automation</td><td>SANGA & ASSOCIATES - EQUIDOTE</td><td>Anywhere</td><td>Job Title:\n",
       "\n",
       "Python Developer – Telegram Bot Integration & Excel Automation\n",
       "\n",
       "Job Description:\n",
       "\n",
       "We are seeking a skilled and detail-oriented Python Developer to help automate the process of sending structured trade updates from an Excel file to a Telegram channel using a Telegram bot.\n",
       "\n",
       "This is a freelance / part-time project with the potential for ongoing work based on performance.\n",
       "\n",
       "Responsibilities:\n",
       "• Read data from an Excel file that is regularly updated using Python.\n",
       "• Format and send messages to a Telegram channel or group using the Telegram Bot API.\n",
       "• Implement conditions to filter or trigger messages based on specific columns (e.g., P&L thresholds).\n",
       "• Ensure the messages are well-formatted and synchronized.\n",
       "• (Optional) Schedule the script to run at regular intervals (e.g., every 5 mins or on update).\n",
       "• Ensure error handling, avoid duplicate messages, and maintain clean logs.\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with Python scripting\n",
       "• Proficiency in using pandas for Excel/CSV handling\n",
       "• Working knowledge of the Telegram Bot API\n",
       "• Experience with HTTP requests (requests library)\n",
       "• Ability to format dynamic messages (Markdown/HTML for Telegram)\n",
       "• (Optional but a plus) Familiarity with scheduling tools like schedule, cron jobs, or apscheduler\n",
       "\n",
       "Nice to Have:\n",
       "• Understanding of stock market data or options trading (for better context)\n",
       "• Experience integrating with trading APIs or using TradingView alerts\n",
       "• Basic knowledge of Excel automation or VBA\n",
       "\n",
       "Project Details:\n",
       "• Project Type: One-time setup, with possible ongoing maintenance\n",
       "• Location: Remote (India preferred)\n",
       "• Start Date: Immediate\n",
       "\n",
       "How to Apply:\n",
       "\n",
       "Please apply with:\n",
       "• A short summary of your experience with Python + Telegram Bots\n",
       "• A link to any relevant projects or GitHub repos\n",
       "• Your expected rate and estimated time to complete the task\n",
       "\n",
       "Job Type: Freelance\n",
       "\n",
       "Benefits:\n",
       "• Health insurance\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "\n",
       "Supplemental Pay:\n",
       "• Performance bonus\n",
       "• Yearly bonus\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBUZWxlZ3JhbSBCb3QgSW50ZWdyYXRpb24gXHUwMDI2IEV4Y2VsIEF1dG9tYXRpb24iLCJjb21wYW55X25hbWUiOiJTQU5HQSBcdTAwMjYgQVNTT0NJQVRFUyAtIEVRVUlET1RFIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNBNXdGSEplRUV3c2lQVWJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>1 day ago</td><td>Python Developer</td></tr><tr><td>Python Developer - Remote</td><td>XPRESS HEALTH</td><td>Anywhere</td><td>Job Title: Python Developer\n",
       "Location: Remote\n",
       "Salary: Up to ₹12 LPA (based on experience and skillset)\n",
       "Experience: 3–6 years (preferred)\n",
       "Employment Type: Full-time\n",
       "\n",
       "About Xpress Health\n",
       "\n",
       "Xpress Health is a healthtech platform transforming clinical workforce management by connecting healthcare professionals to facilities in real time. With operations across Ireland and rapid expansion underway, we’re solving critical staffing challenges in the healthcare sector using advanced technology and automation.\n",
       "\n",
       "Role Overview\n",
       "\n",
       "We are looking for a Python Developer with solid experience in backend development and building scalable, secure systems. You will work closely with cross-functional teams to build backend services, automate operations, and power data-driven features in our real-time healthcare staffing platform. If you're passionate about solving real-world problems through code, this role is for you.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and maintain backend services and APIs using Python (preferably with frameworks like Django or Flask).\n",
       "• Build scalable systems for real-time scheduling, user management, and analytics.\n",
       "• Integrate third-party APIs and internal services securely and efficiently.\n",
       "• Work with databases (SQL and NoSQL) to design efficient schemas and queries.\n",
       "• Optimize performance and ensure system reliability under scale.\n",
       "• Collaborate with frontend, product, and QA teams to deliver complete features.\n",
       "• Write clean, maintainable, and well-documented code.\n",
       "• Participate in code reviews, system design discussions, and architecture planning.\n",
       "\n",
       "Requirements\n",
       "• 3–6 years of professional experience with Python backend development.\n",
       "• Strong knowledge of Django, Flask, or other web frameworks.\n",
       "• Proficient in working with relational databases (MySQL/PostgreSQL) and REST APIs.\n",
       "• Experience with Git, CI/CD pipelines, and working in an Agile environment.\n",
       "• Strong debugging, testing, and problem-solving skills.\n",
       "• Good communication and ability to collaborate with remote teams.\n",
       "\n",
       "Preferred Qualifications\n",
       "• Experience in healthcare, staffing, or enterprise SaaS platforms.\n",
       "• Familiarity with containerization tools like Docker and orchestration platforms (Kubernetes).\n",
       "• Exposure to cloud platforms like AWS, GCP, or Azure.\n",
       "• Knowledge of async programming and task queues (e.g., Celery, Redis).\n",
       "• Experience working with frontend teams using React/Vue (a plus).\n",
       "\n",
       "What We Offer\n",
       "• Competitive salary up to ₹12 LPA, depending on experience.\n",
       "• A mission-driven environment working on meaningful, real-world problems.\n",
       "• Opportunity to shape a rapidly scaling healthtech product.\n",
       "• Flexible work culture with remote options and learning opportunities.\n",
       "• Collaborative, cross-functional team with international exposure.\n",
       "\n",
       "Be part of a product-first team building real-time tech that supports critical healthcare systems. Join Xpress Health as a Python Developer !!!\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: Up to ₹1,200,000.00 per year\n",
       "\n",
       "Benefits:\n",
       "• Paid time off\n",
       "• Work from home\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Evening shift\n",
       "• Fixed shift\n",
       "• Monday to Friday\n",
       "• UK shift\n",
       "\n",
       "Application Question(s):\n",
       "• What is your current and expected CTC?\n",
       "• Are you currently working? If yes, what is your notice period?\n",
       "\n",
       "Experience:\n",
       "• Python : 5 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIC0gUmVtb3RlIiwiY29tcGFueV9uYW1lIjoiWHByZXNzIEhlYWx0aCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJGWjc0LW4wM2NjU2xGWVVOQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>2 days ago</td><td>Python Developer</td></tr><tr><td>Full Stack Developer (Python / React JS)</td><td>HITACHI CAREERS</td><td>India</td><td>Our Company\n",
       "\n",
       "We're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n",
       "\n",
       "Our group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n",
       "\n",
       "Imagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n",
       "\n",
       "The team\n",
       "\n",
       "We are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "\n",
       "The role: Full Stack Developer/Specialist\n",
       "\n",
       "Responsibilities:\n",
       "• Design, develop, and maintain applications.\n",
       "• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n",
       "• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n",
       "• Optimise applications for performance, scalability, and user experience.\n",
       "• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n",
       "• Provide training and support to end-users and IT staff on functionalities and best practices.\n",
       "• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n",
       "• Participate in project planning, execution, and post-implementation support.\n",
       "• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n",
       "• What you'll bring\n",
       "Qualifications:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n",
       "• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n",
       "• Expertise in development and customisation.\n",
       "• Proficiency in Python and React JS\n",
       "• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n",
       "• Experience with RESTful APIs and web services.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Strong communication and collaboration skills.\n",
       "• Ability to work independently and as part of a team in a fast-paced environment.\n",
       "• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n",
       "• Prior experience in building AI applications is a plus.\n",
       "• Prior experience with automation tools like UIPath is a plus.\n",
       "• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\n",
       "Preferred Skills\n",
       "• Certification in\n",
       "• Experience with other low-code/no-code platforms.\n",
       "• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n",
       "• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n",
       "• Experience with Agile development methodologies.\n",
       "About us\n",
       "\n",
       "We're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n",
       "\n",
       "#LI-MS3\n",
       "\n",
       "Championing diversity, equity, and inclusion\n",
       "\n",
       "Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n",
       "\n",
       "How we look after you\n",
       "\n",
       "We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n",
       "\n",
       "We're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.</td><td>eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>Python Developer</td></tr><tr><td>Python Developer Role</td><td>PITANGENT ANALYTICS AND TECHNOLOGY SOLUTIONS PVT. LTD.</td><td>India</td><td>Overview\n",
       "\n",
       "Pi tangent Group of Companies is an ISO:9001: 2015 Certified, CMMIL-3, Award winning Software Development Company in Eastern India. It caters to areas like AI/ML to Web development to SAAS engineering. The Group is actively seeking out talented, enthusiastic Python Developers as a value addition to the growing team at Kolkata.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design and develop robust backend applications using Python.\n",
       "• Collaborate with front-end developers to integrate user-facing elements with server-side logic.\n",
       "• Implement RESTful APIs for seamless communication between server and client.\n",
       "• Write reusable, testable, and efficient code following best practices.\n",
       "• Manage and optimize multiple databases and data storage solutions.\n",
       "• Perform unit and integration testing to ensure software reliability.\n",
       "• Participate in code reviews and maintain version control in Git.\n",
       "• Gather and analyze user requirements to provide optimal solutions\n",
       "• Contribute to project documentation and specifications.\n",
       "• Collaborate with QA engineers to troubleshoot and resolve issues.\n",
       "• Maintain quality assurance processes to ensure best practices are enforced.\n",
       "• Engage in agile development practices, participating in sprints and meetings.\n",
       "• Mentor junior developers and provide guidance as needed.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor's degree in computer science or related field.\n",
       "• 1-2 yrs of experience in Python development.\n",
       "• Strong understanding of Django or Flask web frameworks.\n",
       "• Proficient with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB).\n",
       "• Experience with version control systems, preferably Git.\n",
       "• Solid understanding of RESTful API design principles.\n",
       "• Familiarity with front-end technologies (HTML, CSS, JavaScript).\n",
       "• Experience with containerization tools such as Docker.\n",
       "• Strong communication and teamwork abilities.\n",
       "• Familiarity with cloud services (AWS, Azure) is a plus.\n",
       "• Understanding of security principles and best practices.\n",
       "• Experience with Agile/Scrum methodologies.\n",
       "• Proven ability to manage multiple tasks and meet deadlines.\n",
       "\n",
       "Skills: agile,communication,nosql,scrum,aws lambda,docker,sql,azure,html,javascript,mongodb,css,django,aws,git,postgresql,restful apis,python,flask</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIFJvbGUiLCJjb21wYW55X25hbWUiOiJQaXRhbmdlbnQgQW5hbHl0aWNzIGFuZCBUZWNobm9sb2d5IFNvbHV0aW9ucyBQdnQuIEx0ZC4iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiLUNHQ2RNdUVNQ2V4X3F2MkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>2 days ago</td><td>Python Developer</td></tr><tr><td>Python and Groovy Framework Developer</td><td>APTITA</td><td>India</td><td>Urgent Hiring!!!\n",
       "\n",
       "Role : Python and Groovy Framework Developer\n",
       "\n",
       "Mandatory Skills: Python, Appium, Groovy, Git\n",
       "\n",
       "Experience: 3 to 8 Years\n",
       "\n",
       "Location: Bengaluru\n",
       "\n",
       "Contract - 1Year\n",
       "\n",
       "Job Description:\n",
       "\n",
       "Qualifications\n",
       "\n",
       " Bachelor’s or master’s degree in Computer Science, Software Engineering, or a\n",
       "\n",
       "related field\n",
       "\n",
       " 6 to 8 years of relevant experience in quality assurance, with a proven track record in\n",
       "\n",
       "WebKit or browser engine testing, including team leadership responsibilities.\n",
       "\n",
       " Proficiency in CI/CD and test automation frameworks such as Appium, and scripting\n",
       "\n",
       "languages like Python, or Shell.\n",
       "\n",
       "Job Overview\n",
       "\n",
       "We are seeking a dynamic and experienced Lead Python & Groovy Framework developer\n",
       "\n",
       "to join our team You will be part of a fast-paced, Agile development team and work on a\n",
       "\n",
       "variety of projects, from building new tools and solutions to improving existing ones.\n",
       "\n",
       "In this role, you will have the chance to grow your skills and take your career to the next\n",
       "\n",
       "level. We offer a supportive, challenging, and exciting work environment, with\n",
       "\n",
       "opportunities for professional development, training, and advancement.\n",
       "\n",
       "If you are a Python & Groovy Framework Developer Engineer with a passion for\n",
       "\n",
       "technology and a drive to continuously improve processes, we want to hear from you!\n",
       "\n",
       "If you are passionate about browser engine technologies, performance optimization, and\n",
       "\n",
       "leadership, we encourage you to apply!\n",
       "\n",
       "Primary Skills:\n",
       "\n",
       " Strong experience in Python Framework development, with the ability to automate\n",
       "\n",
       "and optimize processes using Jenkins Pipeline script\n",
       "\n",
       " Good knowledge in Groovy scripting\n",
       "\n",
       " Expertise in CI/CD tools such as Jenkins, CircleCI, or GitLab\n",
       "\n",
       " Good understanding of Appium.\n",
       "\n",
       "Strong Problem solving and debugging skills.\n",
       "\n",
       " Excellent communication and collaboration skills, both with technical and non-\n",
       "\n",
       "technical stakeholders\n",
       "\n",
       " Version Control: Familiarity with version control systems such as Git for reviewing\n",
       "\n",
       "changes and ensuring test coverage.\n",
       "\n",
       " Communication: Strong communication and collaboration skills for working with\n",
       "\n",
       "cross-functional teams.\n",
       "\n",
       " Agile Methodologies: Experience with Agile Scrum methodologies\n",
       "\n",
       "Notice Period: Immediate- 30 Days\n",
       "\n",
       "Email to : sharmila.m@aptita.com\n",
       "\n",
       "·</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gYW5kIEdyb292eSBGcmFtZXdvcmsgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQXB0aXRhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Imx1ckt5NFVRSi0wb3JkMi1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>2 days ago</td><td>Python Developer</td></tr><tr><td>AI Python Developer</td><td>ALLIANZ INSURANCE</td><td>India</td><td>We are seeking a skilled AI Developer proficient in Python to support various AI use cases within our dynamic team. The ideal candidate will possess a strong understanding of Python programming and a basic familiarity with PyTest. This role will also provide opportunities to work with cloud computing technologies, particularly Azure, and develop APIs.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Utilize advanced Python skills to support and enhance AI use cases, ensuring the efficient and effective implementation of AI solutions.\n",
       "• Develop and execute comprehensive test scripts using PyTest to ensure the reliability and accuracy of AI models and applications.\n",
       "• Collaborate with cross-functional teams, including data scientists, engineers, and product managers, to integrate AI capabilities into existing systems and workflows.\n",
       "• Analyze and optimize AI algorithms for performance and scalability, ensuring they meet business requirements.\n",
       "• Stay updated with the latest advancements in AI and machine learning technologies and apply them to improve existing solutions.\n",
       "\n",
       "Requirements:\n",
       "\n",
       "Must-Have\n",
       "• Python: Advanced proficiency with extensive experience in writing efficient, maintainable, and scalable code. Demonstrated ability to solve complex problems using Python.\n",
       "• PyTest: Basic experience in writing and executing unit tests, with a fundamental understanding of test-driven development practices.\n",
       "\n",
       "Good-to-Have\n",
       "• Cloud Computing: Familiarity with cloud platforms and services, with hands-on experience in deploying and managing AI applications in a cloud environment.\n",
       "• Azure: Experience with Microsoft Azure, particularly in using its AI and machine learning services.\n",
       "• APIs: Knowledge of designing, developing, and consuming APIs, particularly RESTful APIs, for integrating AI solutions with other systems and applications\n",
       "\n",
       "About Allianz Technology\n",
       "\n",
       "Allianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 13,000 employees located in 22 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry. We oversee the full digitalization spectrum – from one of the industry’s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n",
       "\n",
       "D&I statement\n",
       "\n",
       "Allianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, race or ethnicity, age, nationality, religion, disability, or philosophy of life. Join us. Let´s care for tomorrow.</td><td>eyJqb2JfdGl0bGUiOiJBSSBQeXRob24gRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQWxsaWFueiBJbnN1cmFuY2UiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQkg5Z0tqUFM2TjBGeExOb0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>24 days ago</td><td>Python Developer</td></tr><tr><td>Developer- Angular, Python & Azure</td><td>THE VALUE MAXIMIZER</td><td>India</td><td>About the Role :\n",
       "\n",
       "As a Senior Developer, you will play a critical role in designing, implementing, and optimizing IoT applications, Salesforce development, and web solutions. This role is ideal for experienced developers eager to lead projects, mentor junior team members, and shape technical strategies across diverse platforms and technologies. You will be an important part of our international project teams, will work closely with team members from the headquarters and if qualified can lead international software development projects.\n",
       "\n",
       "Key Responsibilities :\n",
       "• Central role and expert for development of IoT frontend applications with Angular, Capacitor, and PrimeNG\n",
       "• Design and implement IoT backend systems using C#, .Net 8.0, and the Azure platform\n",
       "• Management and optimization of CI/CD pipelines using Azure DevOps and Bicep\n",
       "• Drive advanced Salesforce development efforts, including customizations, integrations, and automation\n",
       "• Manage and enhance the SharePoint Online intranet platform\n",
       "• Architect and implement Power Platform solutions tailored to business needs\n",
       "• Develop and maintain complex web applications using Django (Python) and PHP\n",
       "• Supervise website development and optimization with Typo3, PHP, and SolR, collaborating closely with the marketing department\n",
       "• Monitor and ensure the performance, compliance, and SEO optimization of global websites and portals\n",
       "\n",
       "Key Requirements :\n",
       "• 6-8 years of experience\n",
       "• Strong expertise in Angular, Python, and C#\n",
       "• Advanced experience with CI/CD pipelines and version control systems (e.g., Azure DevOps or Git)\n",
       "• Proficiency in web technologies such as PHP, MySQL, and modern frameworks\n",
       "• In-depth knowledge of Salesforce, Azure cloud solutions, and SharePoint Online\n",
       "• Proven ability to lead technical projects, mentor team members, and collaborate across departments\n",
       "• Excellent problem-solving skills, attention to detail, and a proactive approach to innovation\n",
       "• Proficiency in German is an advantage but not a requirement. Candidates must, however, have a minimum of B2-level English proficiency to communicate effectively within the international teams and our headquarters</td><td>eyJqb2JfdGl0bGUiOiJEZXZlbG9wZXItIEFuZ3VsYXIsIFB5dGhvbiBcdTAwMjYgQXp1cmUiLCJjb21wYW55X25hbWUiOiJUaGUgVmFsdWUgTWF4aW1pemVyIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRteXhMclAxTG5oaWFJZEVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>Python Developer</td></tr><tr><td>Freelance Python Developer</td><td>GBIM TECHNOLOGIES PVT.LTD.</td><td>Anywhere</td><td>We’re Hiring – Freelance Python Developer (Experienced)\n",
       "We are currently looking for a freelance Python developer who brings hands-on experience and can help us build and debug efficient backend systems, automate processes, and solve critical issues with speed and precision.\n",
       "Key Expertise Required:\n",
       "\n",
       "Python (Backend Development)\n",
       "\n",
       "Web Scraping & Data Extraction\n",
       "\n",
       "Web Automation\n",
       "\n",
       "Flask | Pandas | ETL\n",
       "\n",
       "AWS (Basic to Intermediate)\n",
       "\n",
       "Google / Meta / LinkedIn / Third-Party API Integration\n",
       "\n",
       "Problem-solving mindset – quick in identifying & fixing bugs/errors\n",
       "\n",
       "If you are a solution-driven individual with a solid track record in handling Python-based backend projects and automation tasks, we’d love to connect with you!\n",
       "Please DM or share your portfolio/work samples at [hr@gbim.com / sarabjeet.mann@gbim.in ].\n",
       "#PythonDeveloper #FreelanceOpportunity #WebScraping #Automation #BackendDevelopment #Flask #AWS #APIIntegration #DataExtraction #HiringNow #LinkedInJobs\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500.00 - ₹10,000.00 per hour\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "• Monday to Friday\n",
       "\n",
       "Work Location: Remote\n",
       "\n",
       "Speak with the employer\n",
       "+91-XXXXXXXXXX</td><td>eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkdCSU0gVGVjaG5vbG9naWVzIFB2dC5MdGQuIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNtMXFFOGh4Y2xUN3Z1ck9BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>3 days ago</td><td>Python Developer</td></tr><tr><td>Junior Python Developer</td><td>DEHAZELABS</td><td>Anywhere</td><td>Location: Onsite, Kokapet, Hyderabad, Telangana.\n",
       "\n",
       "Job Type: Full-Time\n",
       "\n",
       "About Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n",
       "\n",
       "Responsibilities:\n",
       "• Develop and maintain Python applications and services\n",
       "• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n",
       "• Write clean, efficient, and well-documented code\n",
       "• Troubleshoot, debug, and optimize applications for performance and scalability\n",
       "• Participate in code reviews and maintain coding standards\n",
       "• Stay up-to-date with the latest industry trends and technologies\n",
       "\n",
       "Requirements:\n",
       "• Ability to code in Python and SQL\n",
       "• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n",
       "• Knowledge of version control systems (e.g., Git)\n",
       "• Excellent problem-solving skills and attention to detail\n",
       "• Strong communication and teamwork abilities.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n",
       "• Knowledge of RESTful APIs and microservices architecture.</td><td>eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Python Developer</td></tr><tr><td>DET-Senior GIG Python Developer-GDSNF02</td><td>EY</td><td>India</td><td>At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. Senior Python Developer – Client – Merck, H and M, TIAA, MINT Job Description Bachelor’s or master’s degree with 3+ years of strong Python development experience Design, develop, and maintain high-performance web applications using Python and related frameworks. Strong understanding of Python OOPs, Data type, Data Structure and algorithm , Exception handling, Decarotor , Generator, Iterator, Automation. Strong understanding of Python Libraries (Pandas, TensorFlow, Numpy, SciPy) Experience in Cloud Azure / AWS Develop, optimize, and manage complex APIs (RESTful or GraphQL). Collaborate with cross-functional teams to define, design, and ship new features. Troubleshoot and resolve advanced technical issues in development and production environments. Conduct technical evaluations of new tools and frameworks, recommending their adoption when appropriate. Stay ahead of emerging trends in Python development, ensuring the team remains at the forefront of innovation. Advanced proficiency in Python and frameworks like Django,Flask, or FastAPI. Good understanding of Database Postgres / MySQL & ORM Library i.e. SQL Alchemy/ any ORM libraries Understanding of Code Repository tools i.e. GIT , SVN Strong understanding of DevOps principles(Docker, Kubernetes and microservices) EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.</td><td>eyJqb2JfdGl0bGUiOiJERVQtU2VuaW9yIEdJRyBQeXRob24gRGV2ZWxvcGVyLUdEU05GMDIiLCJjb21wYW55X25hbWUiOiJFWSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ4Mk5pUFZHVVJ3cjczaVE1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>18 hours ago</td><td>Python Developer</td></tr><tr><td>Informatica ETL Developer: Agile Dev Team Member IV</td><td>CAPGEMINI</td><td>Hyderabad, Telangana, India</td><td>The ideal candidate will be responsible for designing, developing, and deploying scalable ETL processes using Informatica PowerCenter to support our data warehousing and analytics initiatives. You will collaborate with business and technical stakeholders to ensure high data quality, availability, and performance. Key Responsibilities:Design, develop, and maintain ETL workflows and mappings using Informatica PowerCenter or Informatica Intelligent Cloud Services (IICS).Extract, transform, and load data from various source systems (e.g., SQL Server, Oracle, flat files, cloud APIs) into data warehouses or operational data stores.Optimize ETL performance, conduct tuning, and ensure error handling and logging.Collaborate with data architects and analysts to understand data requirements and deliver high-quality data solutions.Work with QA teams to support data validation and testing efforts.Support data integration, migration, and transformation initiatives.Document ETL processes, data flows, and job schedules.Monitor daily ETL jobs and resolve production issues in a timely manner.RequirementsBachelor’s degree in Computer Science, Information Systems, or a related field (or equivalent work experience).3+ years of experience with Informatica PowerCenter or Informatica IICS.Strong SQL skills and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).Solid understanding of data warehousing concepts and dimensional modeling.Experience in performance tuning and troubleshooting ETL processes.Hands-on experience with job scheduling tools (e.g., Autosys, Control-M, Tidal).Familiarity with version control systems and DevOps practices.Preferred Qualifications:Experience with cloud data platforms (e.g., Snowflake, AWS Redshift, Azure Synapse).Exposure to data governance and data quality tools.Knowledge of scripting languages (e.g., Shell, Python).Experience working in Agile/Scrum environments.Familiarity with BI tools (e.g., Tableau, Power BI) is a plus. BenefitsThis position comes with competitive compensation and benefits package: Competitive salary and performance-based bonuses Comprehensive benefits package Home Office model Career development and training opportunities Flexible work arrangements (remote and/or office-based) Dynamic and inclusive work culture within a globally known group Private Health Insurance Pension Plan Paid Time Off Training & Development *Note: Benefits differ based on employee level</td><td>eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyOiBBZ2lsZSBEZXYgVGVhbSBNZW1iZXIgSVYiLCJjb21wYW55X25hbWUiOiJDYXBnZW1pbmkiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InFZVlFqNTFEM2VzZ0lZSExBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 days ago</td><td>ETL Developer</td></tr><tr><td>Senior ETL and Backend Developer (Salesforce)</td><td>S&P GLOBAL</td><td>Hyderabad, Telangana, India (+1 other)</td><td>About the Role:\n",
       "\n",
       "Grade Level (for internal use):\n",
       "10\n",
       "\n",
       "Title: Senior ETL and Backend Developer (Salesforce)\n",
       "\n",
       "Job Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n",
       "\n",
       "The Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n",
       "\n",
       "The team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n",
       "\n",
       "The Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n",
       "\n",
       "Responsibilities:\n",
       "• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n",
       "• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n",
       "• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n",
       "• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n",
       "• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n",
       "• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\n",
       "Monitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n",
       "• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\n",
       "Qualifications:\n",
       "\n",
       "Basic Qualifications:\n",
       "• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n",
       "• A minimum of 8+ years of experience in software engineering & Architecture.\n",
       "• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n",
       "• A minimum of 3+ years of Salesforce development, administration/Integration.\n",
       "• Proficiency in Informatica PowerCenter and other ETL tools.\n",
       "• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n",
       "• Experience with Salesforce integration and administration.\n",
       "• Proficiency in backend development languages (e.g., Java, Python, C#).\n",
       "• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Ability to work independently and as part of a team.\n",
       "• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n",
       "• Experience with other ETL tools and data integration platforms.\n",
       "• Informatica Certified Professional\n",
       "Salesforce Certified Administrator or Developer\n",
       "• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n",
       "• Excellent problem solving, analytical and technical troubleshooting skills.\n",
       "• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n",
       "• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n",
       "\n",
       "About S&P Global Commodity Insights\n",
       "At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n",
       "\n",
       "We’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n",
       "\n",
       "S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n",
       "\n",
       "For more information, visit http://www.spglobal.com/commodity-insights.\n",
       "\n",
       "What’s In It For You?\n",
       "\n",
       "Our Purpose:\n",
       "\n",
       "Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n",
       "\n",
       "Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n",
       "\n",
       "Our People:\n",
       "\n",
       "We're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n",
       "\n",
       "From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n",
       "\n",
       "Our Values:\n",
       "\n",
       "Integrity, Discovery, Partnership\n",
       "\n",
       "At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n",
       "\n",
       "Benefits:\n",
       "\n",
       "We take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n",
       "\n",
       "Our benefits include:\n",
       "• Health & Wellness: Health care coverage designed for the mind and body.\n",
       "• Flexible Downtime: Generous time off helps keep you energized for your time on.\n",
       "• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n",
       "• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n",
       "• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n",
       "• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n",
       "\n",
       "For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n",
       "\n",
       "Global Hiring and Opportunity at S&P Global:\n",
       "\n",
       "At S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "Equal Opportunity Employer\n",
       "\n",
       "S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n",
       "\n",
       "If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n",
       "\n",
       "US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n",
       "\n",
       "Job ID: 316835\n",
       "Posted On: 2025-06-03\n",
       "Location: Hyderabad, Telangana, India</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjVGNUtpak5fRjROQXhDcERBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>19 days ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>ZENSAR TECHNOLOGIES</td><td>Madhavaram, Telangana, India</td><td>Job Description\n",
       "\n",
       "Primary Skill Set\n",
       "• ETL Informatica\n",
       "• SQL\n",
       "• Unix\n",
       "• Realtime Data Integration (CDC) using Power Exchange (Can be optional if candidate good in other skills)\n",
       "\n",
       "Good to Have\n",
       "\n",
       "Experience on working with Mainframe Databases/files\n",
       "\n",
       "ETL Batch Scheduling tools like TWS/Tidal\n",
       "\n",
       "Roles & Responsibilities\n",
       "\n",
       "Informatica PowerCenter, Unix scripting, SQL/PLSQL\n",
       "\n",
       "Knowledge of Informatica Power Exchange is preferred\n",
       "\n",
       "Experience With Mainframe Sources/targets Is Preferred\n",
       "• Bachelor’s degree in Computer Science or similar field or equivalent work experience.\n",
       "• 5-8 years of development experience with Informatica Power Center on Data Integration projects.\n",
       "• Strong analytic, problem-solving and organizational skills.\n",
       "• Excellent SQL knowledge and ability to write the complex queries keeping performance aspect in mind.\n",
       "• Experience with analysis of business requirements, designing and writing technical specifications to design.\n",
       "• Hands-on experience to process mainframe files using Informatica Power Exchange.\n",
       "• Hands-on experience with UNIX shell scripting.\n",
       "• Participate in testing and issue resolution to validate functionality and performance.\n",
       "• Hands-on experience on any job scheduling tool, TWS is preferred.\n",
       "• Good written and verbal communication skills.\n",
       "\n",
       "Location\n",
       "\n",
       "1 st Preference: Noida\n",
       "\n",
       "2 nd Preference: Hyderabad\n",
       "\n",
       "3 rd Preference: Gurgaon</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiWmVuc2FyIFRlY2hub2xvZ2llcyIsImFkZHJlc3NfY2l0eSI6Ik1hZGhhdmFyYW0sIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InJTQzlGVjVoRHhXZ1h3QURBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>11 days ago</td><td>ETL Developer</td></tr><tr><td>Insight Global</td><td>INSIGHT GLOBAL</td><td>Hyderabad, Telangana, India</td><td>We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\n",
       "Responsibilities:\n",
       "Lead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\n",
       "Collaborate with developers to ensure a smooth transition and integration of the upgraded system.\n",
       "Enhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\n",
       "Provide support and guidance to the ETL development lead.\n",
       "Engage with end-users to ensure their needs are met during and after the upgrade process.\n",
       "Utilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\n",
       "Requirements:\n",
       "Minimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\n",
       "Proven experience with SQL Server upgrades, particularly from 2012 to 2022.\n",
       "Strong SQL Server skills, including building tables, stored procedures, views, and functions.\n",
       "Experience with .NET development is highly desirable to be leveraged for other projects\n",
       "Excellent problem-solving skills and attention to detail.\n",
       "Ability to work effectively in a team environment and communicate with end-users.\n",
       "Preferred Qualifications:\n",
       "Experience with SQL Server 2016.\n",
       "Experience with Oracle\n",
       "Familiarity with the latest features and enhancements in SQL Server 2022.\n",
       "Strong understanding of database architecture and migration strategies.\n",
       "\n",
       "We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>eyJqb2JfdGl0bGUiOiJJbnNpZ2h0IEdsb2JhbCIsImNvbXBhbnlfbmFtZSI6Ikluc2lnaHQgR2xvYmFsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location</td><td>FISERV</td><td>India</td><td>Calling all innovators – find your future at Fiserv.\n",
       "\n",
       "We’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n",
       "\n",
       "Job Title\n",
       "\n",
       "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n",
       "\n",
       "Work Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n",
       "• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n",
       "• Participates in client kickoff activities including requirements and data gathering sessions.\n",
       "• Ability to analyze and migrate incoming external data into Fiserv solutions.\n",
       "• Strong working knowledge of ETL processes and best practices.\n",
       "• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n",
       "• Documents business processes and identifies opportunities for process redesign.\n",
       "• Reports system defects and identifies opportunities for system enhancements.\n",
       "• Performs as a technical consulting resource for new clients during the implementation process.\n",
       "• Supports several project managers during the pre and post go-live activities\n",
       "• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n",
       "• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n",
       "• Provides technical and analytical guidance to the project team.\n",
       "• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n",
       "• The position requires working in shifts, 2 PM to 11 PM IST\n",
       "\n",
       "What you will need to have:\n",
       "• Minimum of 5 – 10 years of experience in software development.\n",
       "• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n",
       "• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n",
       "• Self-starter who can work effectively, both independently and in a team environment.\n",
       "• Strong analytical, organizational, and problem-solving skills.\n",
       "• Excellent verbal and written communication skills.\n",
       "• Ability to maintain a professional attitude and demeanor in high pressure situations.\n",
       "• Ability to multi-task and manage multiple projects simultaneously.\n",
       "• Weekend/evening availability and support (10% - 25%).\n",
       "\n",
       "What would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n",
       "\n",
       "Thank you for considering employment with Fiserv. Please:\n",
       "• Apply using your legal name\n",
       "• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n",
       "\n",
       "What you should know about us:\n",
       "\n",
       "Fiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n",
       "\n",
       "Our commitment to Diversity and Inclusion:\n",
       "\n",
       "We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n",
       "\n",
       "Warning about fake job posts:\n",
       "\n",
       "Please be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n",
       "\n",
       "Any communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n",
       "\n",
       "If you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>EPAM - ETL Developer - SSIS/SSRS</td><td>SWATHI V</td><td>Hyderabad, Telangana, India</td><td>Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n",
       "\n",
       "Experience : 5+ Years\n",
       "\n",
       "Location : Hyderabad\n",
       "\n",
       "Job Description :\n",
       "\n",
       "We are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n",
       "\n",
       "You will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n",
       "\n",
       "Key Responsibilities :\n",
       "\n",
       "- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n",
       "\n",
       "- Develop, deploy, and maintain SSIS packages for ETL processes.\n",
       "\n",
       "- Work on data warehouse design, development, and maintenance.\n",
       "\n",
       "- Ensure high performance and reliability of data integration workflows.\n",
       "\n",
       "- Troubleshoot SQL queries and ETL issues; optimize database performance.\n",
       "\n",
       "- Collaborate with cross-functional teams to define data solutions.\n",
       "\n",
       "- Work with complex data structures including JSON and XML.\n",
       "\n",
       "- Understand and implement different data models (star, snowflake, etc.).</td><td>eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IlN3YXRoaSBWIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ2ekQ3VTVqbHNoc2EtN3luQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>Data ETL Developer / BI Engineer</td><td>AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL</td><td>India</td><td>ETL Developer\n",
       "\n",
       "Amex GBT is a place where colleagues find inspiration in travel as a force for good and – through their work – can make an impact on our industry. We’re here to help our colleagues achieve success and offer an inclusive and collaborative culture where your voice is valued.\n",
       "\n",
       "We are looking for an experienced Data ETL Developer / BI Engineer who loves solving complex problems across a full spectrum of data & technologies. You will lead the building effort of GBT's new BI platform and manage the legacy platform to seamlessly support our business function around data and analytics. You will create dashboards, databases, and other platforms that allow for the efficient collection and evaluation of BI data.\n",
       "\n",
       "What You’ll Do on a Typical Day:\n",
       "• Design, implement, and maintain systems that collect and analyze business intelligence data.\n",
       "• Design and architect an analytical data store or cluster for the enterprise and implement data pipelines that extract, transform, and load data into an information product that helps the organization reach strategic goals.\n",
       "• Create physical and logical data models to store and share data that can be easily consumed for different BI needs.\n",
       "• Develop Tableau dashboards and features.\n",
       "• Create scalable and high-performance data load and management process to make data available near real-time to support on-demand analytics and insights.\n",
       "• Translate complex technical and functional requirements into detailed designs.\n",
       "• Investigate and analyze alternative solutions to data storing, processing, etc., to ensure the most streamlined approaches are implemented.\n",
       "• Serve as a mentor to junior staff by conducting technical training sessions and reviewing project outputs\n",
       "• Design & develop, and maintain a data model implementing ETL processes.\n",
       "• Manage and maintain the database, warehouse, & cluster with other dependent infrastructure.\n",
       "• Work closely with data, products, and another team to implement data analytic solutions.\n",
       "• Support production application and Incident management.\n",
       "• Help define data governance policies and support data versioning processes\n",
       "• Maintain security and data privacy by working closely with the Data Protection Officer internally.\n",
       "• Analyze a vast number of data stores and uncover insights\n",
       "\n",
       "What We’re Looking For:\n",
       "• Degree in computer sciences or engineering\n",
       "• Overall, 3-5 years of experience in data & data warehouse, ETL, and data modeling.\n",
       "• 2+ years of experience working and managing large data stores, complex data pipelines, and BI solutions.\n",
       "• Strong experience in SQL and writing complex queries.\n",
       "• Hands-on experience with Tableau development.\n",
       "• Hands-on working experience on Redshift, data modeling, data warehouse, ETL tool, Python, and Shell scripting.\n",
       "• Understanding of data warehousing and data modeling techniques\n",
       "• Strong data engineering skills on the AWS Cloud Platform are essential.\n",
       "• Knowledge of Linux, SQL, and any scripting language\n",
       "• Good interpersonal skills and a positive attitude\n",
       "• Experience in travel data would be a plus.\n",
       "\n",
       "Location\n",
       "Gurgaon, India\n",
       "\n",
       "The #TeamGBT Experience\n",
       "\n",
       "Work and life: Find your happy medium at Amex GBT.\n",
       "• Flexible benefits are tailored to each country and start the day you do. These include health and welfare insurance plans, retirement programs, parental leave, adoption assistance, and wellbeing resources to support you and your immediate family.\n",
       "• Travel perks: get a choice of deals each week from major travel providers on everything from flights to hotels to cruises and car rentals.\n",
       "• Develop the skills you want when the time is right for you, with access to over 20,000 courses on our learning platform, leadership courses, and new job openings available to internal candidates first.\n",
       "• We strive to champion Inclusion in every aspect of our business at Amex GBT. You can connect with colleagues through our global INclusion Groups, centered around common identities or initiatives, to discuss challenges, obstacles, achievements, and drive company awareness and action.\n",
       "• And much more!\n",
       "\n",
       "All applicants will receive equal consideration for employment without regard to age, sex, gender (and characteristics related to sex and gender), pregnancy (and related medical conditions), race, color, citizenship, religion, disability, or any other class or characteristic protected by law.\n",
       "\n",
       "Click Here for Additional Disclosures in Accordance with the LA County Fair Chance Ordinance.\n",
       "\n",
       "Furthermore, we are committed to providing reasonable accommodation to qualified individuals with disabilities. Please let your recruiter know if you need an accommodation at any point during the hiring process. For details regarding how we protect your data, please consult the Amex GBT Recruitment Privacy Statement.\n",
       "\n",
       "What if I don’t meet every requirement? If you’re passionate about our mission and believe you’d be a phenomenal addition to our team, don’t worry about “checking every box;\" please apply anyway. You may be exactly the person we’re looking for!\n",
       "Experience Level\n",
       "Mid Level\n",
       "\n",
       "More about this Data ETL Developer / BI Engineer job\n",
       "\n",
       "American Express Global Business Travel is aggressively hiring for the job profile of Data ETL Developer / BI Engineer at undefined in null locality. Kindly go through the FAQs below to get all answers related to the given job.\n",
       "\n",
       "1. How much salary can I expect as a Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n",
       "\n",
       "Ans. You can expect a minimum salary of 0 INR. The salary offered will depend on your skills, experience and performance in the interview.\n",
       "\n",
       "2. What is the eligibility criteria to apply for Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n",
       "\n",
       "Ans. The candidate should have completed undefined degree and people who have 3 to 5 years are eligible to apply for this job. You can apply for more jobs in undefined to get hired quickly.\n",
       "\n",
       "3. Is there any specific skill required for this job?\n",
       "\n",
       "Ans. The candidate should have undefined skills and sound communication skills for this job.\n",
       "\n",
       "4. Who can apply for this job?\n",
       "\n",
       "Ans. Both Male and Female candidates can apply for this job.\n",
       "\n",
       "5. Is it a work from home job?\n",
       "\n",
       "Ans. No, it’s not a work from home job and can’t be done online. You can explore and apply for other work from home jobs in undefined at apna.\n",
       "\n",
       "6. Are there any charges or deposits required while applying for the role or while joining?\n",
       "\n",
       "Ans. No work-related deposit needs to be made during your employment with the company.\n",
       "\n",
       "7. How can I apply for this job?\n",
       "\n",
       "Ans. Go to the apna app and apply for this job. Click on the apply button and call HR directly to schedule your interview.\n",
       "\n",
       "8. What is the last date to apply?\n",
       "\n",
       "Ans. The last date to apply for this job is .\n",
       "\n",
       "For more details, download apna app and find Full Time jobs in undefined. Through apna, you can find jobs in 74 cities across India. Join NOW!</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVUTCBEZXZlbG9wZXIgLyBCSSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkFtZXJpY2FuIEV4cHJlc3MgR2xvYmFsIEJ1c2luZXNzIFRyYXZlbCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJnY3VjckFjcm5KVmRmdjRxQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>6 days ago</td><td>ETL Developer</td></tr><tr><td>Informatica ETL Developer - SQL/Power Center</td><td>RENOVISION AUTOMATION SERVICES PVT.LTD.</td><td>Telangana, India</td><td>Experience - 6+ yearsWork Mode - HybridJob Summary : We are seeking a skilled Informatica ETL Developer with 5+ years of experience in ETL and Business Intelligence projects. The ideal candidate will have a strong background in Informatica PowerCenter, a solid understanding of data warehousing concepts, and hands-on experience in SQL, performance tuning, and production support. This role involves designing and maintaining robust ETL pipelines to support digital transformation initiatives for clients in manufacturing, automotive, transportation, and engineering domains.Key Responsibilities : - Design, develop, and maintain ETL workflows using Informatica PowerCenter.- Troubleshoot and optimize ETL jobs for performance and reliability.- Analyze complex data sets and write advanced SQL queries for data validation and transformation.- Collaborate with data architects and business analysts to implement data warehousing solutions.- Apply SDLC methodologies throughout the ETL development lifecycle.- Support production environments by identifying and resolving data and performance issues.- Work with Unix shell scripting for job automation and scheduling.Required Skills : - 35 years of hands-on experience with Informatica PowerCenter.- Proficiency in SQL and familiarity with NoSQL platforms.- Experience in ETL performance tuning and troubleshooting.- Solid understanding of Unix/Linux environments and scripting.- Excellent verbal and written communication skills.Preferred Qualifications : - AWS Certification or experience with cloud-based data integration is a plus.- Exposure to data modeling and data governance practices. (ref: hirist.tech)</td><td>eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyIC0gU1FML1Bvd2VyIENlbnRlciIsImNvbXBhbnlfbmFtZSI6IlJlbm92aXNpb24gQXV0b21hdGlvbiBTZXJ2aWNlcyBQdnQuTHRkLiIsImFkZHJlc3NfY2l0eSI6IlRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlVqTnczajVrcnJpS21VQjdBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer- Hyderabad (2-3+ Years of Experience)</td><td>A CLIENT OF ANALYTICS VIDHYA</td><td>Hyderabad, Telangana, India</td><td>Role Summary:\n",
       "\n",
       "•ETL Developer is responsible for Design and Development of ETL Jobs which follow standards, best practices and are maintainable, modular and reusable.\n",
       "\n",
       "•ETL Developer will analyze and review complex object and data models and the metadata repository in order to structure the processes and data for better management and efficient access.\n",
       "\n",
       "•Working on multiple projects, and delegating work to Junior Analysts to deliver projects on time.\n",
       "\n",
       "•Training and mentoring Junior Analysts and building their proficiency in the ETL process.Job Duties:\n",
       "\n",
       "•Preparing mapping document to extract, transform, and load data ensuring compatibility with all tables and requirement specifications.\n",
       "\n",
       "•Experience in ETL system design and development with Talend / Pentaho PDI is essential.\n",
       "\n",
       "•Create quality rules in Talend.\n",
       "\n",
       "•Tune Talend jobs for performance optimization.\n",
       "\n",
       "•Write relational and multidimensional database queries.\n",
       "\n",
       "•Functional Knowledge of Talend Adminstration Center, Job Servers & Load balancing setup, and all its administrative fucntions.\n",
       "\n",
       "•Develop, maintain, and enhance unit test suites to verify the accuracy of ETL processes, dimensional data, OLAP cubes and various forms of BI content including reports, dashboards, and analytical models.\n",
       "\n",
       "•Exposure in Map Reduce components of Talend / Pentaho PDI.\n",
       "\n",
       "•Creating and deploying Talend / Pentaho custom components is an add-on advantage.\n",
       "\n",
       "Job Specification:\n",
       "\n",
       "•BE, B.Tech / MS Degree in Computer Science, Engineering or a related subject.\n",
       "\n",
       "•Having an experience of 2 – 3+ years.\n",
       "\n",
       "•Comprehensive understanding and working knowledge in Data Warehouse loading, tuning, and maintenance.\n",
       "\n",
       "•Proficiency with Talend or related tools like Pentaho Data Integration / Kettle.\n",
       "\n",
       "•Working knowledge of relational database theory and dimensional database models.\n",
       "\n",
       "•Ability to write complex SQL database queries.\n",
       "\n",
       "•Ability to work independently.</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyLSBIeWRlcmFiYWQgKDItMysgWWVhcnMgb2YgRXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJBIENsaWVudCBvZiBBbmFseXRpY3MgVmlkaHlhIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJYOGlLR3lLZ0p6MDQ5UkdIQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>LUXOFT</td><td>Maharashtra, India</td><td>Project Description:\n",
       "\n",
       "Our client is an EU subsidiary of a Global Financial Bank working in multiple markets and asset classes.\n",
       "\n",
       "DWH/ETL developer will work closely with the Development Lead to design, build interfaces and integrate data from a variety from internal and external data sources into the new Enterprise Data Warehouse environment. The ETL Developer will be responsible for developing ETL primarily utilizing Microsoft & Azure technologies within industry recognized ETL standards, architecture, and best practices.\n",
       "\n",
       "Responsibilities:\n",
       "• Act as a technical expert in the designing, coding, unit testing, supporting, and debugging of data warehouse software components in all aspects of SDLC\n",
       "• Apply cloud and ETL engineering skills to solve problems and design approaches\n",
       "• Troubleshoot and debug ETL pipelines and creating unit tests for ETL pipelines.\n",
       "• Assess query performance and actively contribute to optimizing the code\n",
       "• Write technical documentation and specifications\n",
       "• Support internal audit by submitting required evidence\n",
       "• Create reports and dashboards in the BI portal\n",
       "• Work with Development Lead, DWH Architect and QA Engineers to plan, implement and deliver best ETL strategies\n",
       "• Work with business analysts to understand requirements to create technical design specifications, gaining a sound understanding of business processes for related applications so that integration processes fulfill the end-user requirements\n",
       "• Communicate effectively in a collaborative, complex and high performing team environment as per Agile principles\n",
       "\n",
       "Mandatory Skills Description:\n",
       "• Proven work experience as an ETL Developer\n",
       "• Advanced knowledge of relational databases and dimensional Data Warehouse modelling concepts\n",
       "• Good understanding of physical and logical data modeling\n",
       "• Very good understanding of modern SaaS/PaaS data solutions in a cost conscious approach\n",
       "• Expert level of knowledge of Microsoft Data stack\n",
       "• Experience in developing and deploying data oriented solutions in Cloud (Azure/Synapse Analytics/Fabric)\n",
       "• Experience in designing and implementing data transformation and ETL layers using Data Factory, Notebooks\n",
       "• Experience with PowerBI for report & dashboard creation. PowerQuery and/or DAX is an advantage.\n",
       "• Experience in/understanding of Azure Data Lake Storage\n",
       "• Knowledge/use of CI/CD tools and principles, preferably Azure DevOps or Bamboo\n",
       "• Strong SQL knowledge, able to create complex SQL queries and good understanding of stored procedures, views, indexes, functions, etc.\n",
       "• Good working knowledge of at least one Scripting language. Python is an advantage.\n",
       "• Experience with GIT repositories and working with branches. GitHub, Azure DevOps or Bitbucket experience are preferable.\n",
       "• Ability to troubleshoot and solve complex technical problems\n",
       "• Good understanding of software development best practices\n",
       "• Working experience in Agile projects; preferably using JIRA\n",
       "• Experience in working in high priority projects preferably greenfield project experience\n",
       "• Able to communicate complex information clearly and concisely.\n",
       "• Able to work independently and also to collaborate across the organization\n",
       "• Highly developed problem-solving skills with minimal supervision\n",
       "• Understanding of data governance and enterprise concepts preferably in banking environment\n",
       "• Verbal and written communication skills in English are essential.\n",
       "\n",
       "Nice-to-Have Skills Description:\n",
       "• Microsoft Fabric\n",
       "• Snowflake\n",
       "• Background in SSIS/SSAS/SSRS\n",
       "• Azure DevTest Labs, ARM templates\n",
       "• Azure PurView\n",
       "• Banking/finance experience</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiTHV4b2Z0IiwiYWRkcmVzc19jaXR5IjoiTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJlZF9YUGZ2UVNVcTZXdXltQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>ETL Developer</td></tr><tr><td>Data Engineer (Hadoop, Spark, Scala, Hive)</td><td>VISA</td><td>India</td><td>Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n",
       "\n",
       "Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n",
       "\n",
       "Job Description\n",
       "\n",
       "Translate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\n",
       "\n",
       "Good to have GenAI Exposure and Agentic AI Knowledge.\n",
       "\n",
       "Work with business partners directly to seek clarity on requirements.\n",
       "\n",
       "Define solutions in terms of components, modules, and algorithms.\n",
       "\n",
       "Design, develop, document, and implement new programs and subprograms, as well as enhancements, modifications and corrections to existing software.\n",
       "\n",
       "Create technical documentation and procedures for installation and maintenance.\n",
       "\n",
       "Write Unit Tests covering known use cases using appropriate tools.\n",
       "\n",
       "Integrate test frameworks in the development process.\n",
       "\n",
       "Work with operations to get the solutions deployed.\n",
       "\n",
       "Take ownership of production deployment of code.\n",
       "\n",
       "Come up with Coding and Design best practices.\n",
       "\n",
       "Thrive in a self-motivated, internal-innovation driven environment.\n",
       "\n",
       "Adapt quickly to new application knowledge and changes.\n",
       "\n",
       "This is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Basic Qualifications\n",
       "\n",
       "-Minimum of 6 months of work experience or a Bachelor's Degree\n",
       "\n",
       "Preferred Qualifications\n",
       "\n",
       "-Bachelor degree in Computer Science.\n",
       "\n",
       "-Minimum of 1 plus years of software development experience in Hadoop using\n",
       "\n",
       "Spark, Scala, Hive.\n",
       "\n",
       "-Expertise in Object Oriented Programming Language Java, Python.\n",
       "\n",
       "-Experience using CI CD Process, version control and bug tracking tools.\n",
       "\n",
       "-Result-oriented with strong analytical and problem-solving skills.\n",
       "\n",
       "-Experience with automation of job execution, validation and comparison of data\n",
       "\n",
       "files on Hadoop Environment at the field level.\n",
       "\n",
       "-Experience in leading a small team and being a team player.\n",
       "\n",
       "-Strong communication skills with proven ability to present complex ideas and\n",
       "\n",
       "document them in a clear and concise way.\n",
       "\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChIYWRvb3AsIFNwYXJrLCBTY2FsYSwgSGl2ZSkiLCJjb21wYW55X25hbWUiOiJWaXNhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImVULTZKTlc2NHVzTUxhdS1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 days ago</td><td>Spark Engineer</td></tr><tr><td>Data Engineer - Spark/Python</td><td>ETELLIGENS TECHNOLOGIES</td><td>India</td><td>Job Description\n",
       "\n",
       "We are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n",
       "• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n",
       "• Perform data cleansing and migration from diverse sources to target systems.\n",
       "• Collaborate with stakeholders to understand requirements and convert them into technical\n",
       "\n",
       "solutions.\n",
       "• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n",
       "• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n",
       "• Support, troubleshoot, and maintain data pipelines and workflows.\n",
       "• Participate in all phases of software development lifecycle including unit testing, integration\n",
       "\n",
       "testing, and performance testing.\n",
       "• Contribute to the modernization of data platforms and analytics tools on Azure.\n",
       "• Ensure data quality and integrity across all pipelines and systems.\n",
       "\n",
       "Required Skills & Qualifications\n",
       "• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n",
       "• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n",
       "• 1+ year of experience in Python and Spark scripting.\n",
       "• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n",
       "• Good understanding of Data Warehousing (DWH) concepts.\n",
       "• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n",
       "• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n",
       "• Hands-on experience in data cleansing, transformation, and migration projects.\n",
       "• Ability to work independently and within a team environment.\n",
       "• Microsoft Certified : Azure Data Engineer Associate\n",
       "• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n",
       "\n",
       "Location : Artha SEZ, Greater Noida West\n",
       "\n",
       "(ref:hirist.tech)</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Spark Engineer</td><td>STAFFINGINE LLC</td><td>India</td><td>- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>1 day ago</td><td>Spark Engineer</td></tr><tr><td>Staff Data Engineer (Spark, Python, Hadoop)</td><td>VISA</td><td>India</td><td>Company Description\n",
       "\n",
       "Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n",
       "\n",
       "When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n",
       "\n",
       "Join Visa: A Network Working for Everyone.\n",
       "Job Description\n",
       "\n",
       "The Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n",
       "\n",
       "Are you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n",
       "\n",
       "As a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n",
       "\n",
       "Essential Functions\n",
       "• Work with manager and clients to fully understand business requirements and desired business outcomes\n",
       "• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n",
       "• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n",
       "• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n",
       "• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n",
       "• Execute data engineering projects ranging from small to large either individually or as part of a project team\n",
       "• Ensure project delivery within timelines and budget requirements\n",
       "• Provide coaching and mentoring to junior team members\n",
       "\n",
       "This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\n",
       "Qualifications\n",
       "\n",
       "• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n",
       "• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n",
       "• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n",
       "• Minimum of 4 years of hands-on expertise with Java or Scala\n",
       "• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n",
       "• Experience working in an Agile and Test Driven Development environment.\n",
       "• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n",
       "• Experience with SAS as a statistical package is preferred\n",
       "• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>null</td><td>Spark Engineer</td></tr><tr><td>Databricks Engineer - Spark / PySpark</td><td>ENKEFALOS TECHNOLOGIES LLP</td><td>Anywhere</td><td>Databricks Engineer – Spark / PySpark\n",
       "\n",
       "Location : Remote / Mysore\n",
       "\n",
       "Joining : Immediate\n",
       "\n",
       "Experience : 5+ years\n",
       "\n",
       "Responsibilities :\n",
       "\n",
       "Will implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n",
       "\n",
       "Requirements:\n",
       "• PySpark (RDDs, DataFrames, performance tuning)\n",
       "• Building gold‐layer data models for financial reporting\n",
       "• Experience with complex joins, aggregations, GL hierarchies\n",
       "• Version handling (Actuals vs Budget), currency conversions\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500,395.35 - ₹1,840,348.25 per year\n",
       "\n",
       "Benefits:\n",
       "• Flexible schedule\n",
       "• Paid sick time\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Application Question(s):\n",
       "• Have you worked on ADF/ADLS ?\n",
       "• Do you have hands-on experience of Spark / PySpark\n",
       "\n",
       "Experience:\n",
       "• Databricks Engineering: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 days ago</td><td>Spark Engineer</td></tr><tr><td>Pi Square Technologies - Spark & Scala Engineer</td><td>SANDEEP RAJA</td><td>India</td><td>Job Summary :\n",
       "\n",
       "We are seeking a skilled and motivated Spark & Scala Engineer with 46 years of hands-on experience in building scalable data processing applications. The ideal candidate will have a strong background in big data ecosystems and a solid understanding of distributed computing using Apache Spark with Scala.\n",
       "\n",
       "Roles and Responsibilities :\n",
       "\n",
       "- Design, build, and maintain efficient, reusable, and reliable Apache Spark applications.\n",
       "\n",
       "- Optimize Spark applications for maximum speed and scalability.\n",
       "\n",
       "- Implement data ingestion and ETL processes.\n",
       "\n",
       "- Collaborate with data scientists and architects to implement complex big data solutions.\n",
       "\n",
       "- Debug and resolve issues in Spark applications.\n",
       "\n",
       "- Stay up to date with the latest trends in big data technologies and Apache Spark.\n",
       "\n",
       "- Write clean, readable, and maintainable code.\n",
       "\n",
       "- Participate in code reviews and contribute to team knowledge sharing.\n",
       "\n",
       "Required Skills :\n",
       "\n",
       "- 46 years of experience working with Apache Spark (core, SQL, streaming).\n",
       "\n",
       "- Strong proficiency in Scala programming.\n",
       "\n",
       "- Experience in building and optimizing data pipelines and ETL workflows.\n",
       "\n",
       "- Proficient in working with big data tools and frameworks (e.g., Hive, HDFS, Kafka).</td><td>eyJqb2JfdGl0bGUiOiJQaSBTcXVhcmUgVGVjaG5vbG9naWVzIC0gU3BhcmsgXHUwMDI2IFNjYWxhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoic2FuZGVlcCByYWphIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlFMNDBNZDljZDlqQkxBX3BBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>27 days ago</td><td>Spark Engineer</td></tr><tr><td>Spark Developer</td><td>INFOSYS</td><td>India</td><td>• Primary skills:Technology->Big Data - Data Processing->Spark\n",
       "\n",
       "A day in the life of an Infoscion\n",
       "• As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\n",
       "• You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\n",
       "• You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\n",
       "• You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\n",
       "• You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n",
       "• Knowledge of more than one technology\n",
       "• Basics of Architecture and Design fundamentals\n",
       "• Knowledge of Testing tools\n",
       "• Knowledge of agile methodologies\n",
       "• Understanding of Project life cycle activities on development and maintenance projects\n",
       "• Understanding of one or more Estimation methodologies, Knowledge of Quality processes\n",
       "• Basics of business domain to understand the business requirements\n",
       "• Analytical abilities, Strong Technical Skills, Good communication skills\n",
       "• Good understanding of the technology and domain\n",
       "• Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\n",
       "• Awareness of latest technologies and trends\n",
       "• Excellent problem solving, analytical and debugging skills</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBEZXZlbG9wZXIiLCJjb21wYW55X25hbWUiOiJJbmZvc3lzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkZ2VmRvVC1NQnFvWHRRUmJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>16 days ago</td><td>Spark Engineer</td></tr><tr><td>Data Engineer (Snowflake, Spark, AWS) - AVP</td><td>12542 CITICORP SERVICES INDIA PRIVATE LIMITED</td><td>India</td><td>The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>null</td><td>Spark Engineer</td></tr><tr><td>SW Engineer (Java and Bigdata/Hadoop/Spark) 1yr</td><td>VISA</td><td>India</td><td>Job Description\n",
       "\n",
       "This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. They will be an integral part of the Payment Products Development team, focusing on the development of software solutions that leverage data to address business challenges. They will be extensively involved in hands-on activities including proof of concepts, development, and testing. They should demonstrate adaptability and readiness to change tasks according to the team's requirements.\n",
       "\n",
       "Key Responsibilities\n",
       "• Deliver high-quality products and features, taking a holistic view from the initial idea to final delivery. This involves design, development, testing, and accountability for release deployments as well as providing support post-deployment.\n",
       "• Understand the product in its entirety and ensure the timely delivery of your assigned module within an agile team setting.\n",
       "• Accurately scope efforts, identify risks, and clearly communicate trade-offs with team members and other stakeholders.\n",
       "• Coordinate and participate in Continuous Integration activities, testing automation frameworks, and other related tasks, in addition to contributing to core product code.\n",
       "• Effectively communicate status updates, issues, and potential risks accurately and promptly.\n",
       "• Perform other tasks related to data governance and system infrastructure as required.\n",
       "\n",
       "This is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Basic Qualifications\n",
       "\n",
       "-Bachelor's degree in Computer Science or equivalent field\n",
       "\n",
       "-Relevant working experience of up to 2 years in the industry\n",
       "\n",
       "-Proven experience in software development, particularly in data-centric\n",
       "\n",
       "projects, demonstrating adherence to standard development best practices\n",
       "\n",
       "-Strong understanding and practical experience with data structures and\n",
       "\n",
       "algorithms, with a passion for tackling complex problems\n",
       "\n",
       "-Proficiency in Java programming\n",
       "\n",
       "-Hands-on experience with Big Data technologies such as Hadoop, Spark, and\n",
       "\n",
       "Hive\n",
       "\n",
       "-Strong knowledge of Unix/Linux operating systems and Shell Scripting\n",
       "\n",
       "-Proficiency in working with RDBMS and SQL\n",
       "\n",
       "-Basic knowledge of manual and automated testing\n",
       "\n",
       "-Familiarity with version control systems, specifically Git\n",
       "\n",
       "-Awareness of and experience with software design patterns\n",
       "\n",
       "-Experience working within an Agile framework\n",
       "\n",
       "Preferred Qualifications\n",
       "\n",
       "-Proficiency in Scala & Kafka programming is a good to have\n",
       "\n",
       "-Experience with Airflow for workflow management\n",
       "\n",
       "-Familiarity with AI concepts and tools, including GitHub Copilot for code\n",
       "\n",
       "development\n",
       "\n",
       "-Exposure to AI/ML development is an added advantage\n",
       "\n",
       "-Proficiency in working with In-memory Databases like Redis\n",
       "\n",
       "-Good knowledge of API development is highly advantageous\n",
       "\n",
       "-Strong verbal and written communication skills, with a proactive and self-\n",
       "\n",
       "motivated approach to improving existing processes to enable faster\n",
       "\n",
       "iterations.\n",
       "\n",
       "-Demonstrated intellectual and analytical rigor, with a keen attention to detail\n",
       "\n",
       "-Team-oriented, energetic, and collaborative approach to work, coupled with a\n",
       "\n",
       "diplomatic and adaptable style\n",
       "\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>eyJqb2JfdGl0bGUiOiJTVyBFbmdpbmVlciAoSmF2YSBhbmQgQmlnZGF0YS9IYWRvb3AvU3BhcmspIDF5ciIsImNvbXBhbnlfbmFtZSI6IlZJU0EiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoidHV1TkJsMDF3amRIc0xnSEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>5 days ago</td><td>Spark Engineer</td></tr><tr><td>Big Data Lead/ Lead Data Engineer/Spark Tech Lead</td><td>TANISHA SYSTEMS  INC</td><td>India</td><td>Job Description:Mandatory:10+ years of experience in solution, design and development of applications using Java 8+/J2EE, Spring, Spring-Boot, Micro Services, RESTful Services and with experience in Big Data and with experience working in heavy data background needed.Develop, program, and maintain applications using the Apache Spark open source frameworkWork with different aspects of the Spark ecosystem, including Spark SQL, Data Frames, Datasets, and streaming.Spark Developer must have strong programming skills in Java, Scala, or PythonFamiliar with big data processing tools and techniquesProven experience as a Spark Developer or a related roleStrong programming skills in Java, Scala, or PythonFamiliarity with big data processing tools and techniquesExperience with the Hadoop ecosystemGood understanding of distributed systemsExperience with streaming data platformsMust have strong experience in Big Data and with experience working in heavy data background needed.Must be strong in Cloud AWS event-based architecture, Kubernetes, ELK (Elasticsearch, Logstash & Kibana)Must have excellent experience in designing and Implementing cloud-based solutions in various AWS Services (: s3, Lambda, Step Function, AMQ, SNS, SQS, CloudWatch Events, etc.)Must be well experienced in design and development of Microservice using Spring-Boot and REST API and with GraphQLMust have solid knowledge and experience in NoSQL (MongoDB)Good knowledge and experience in any Queue based implementations.Strong knowledge/experience in ORM Framework - JPA / HibernateGood knowledge in technical concepts – Security, Transaction, Monitoring, PerformanceShould we well have versed with TDD/ATDDShould have experience on Java, Python and Spark2+ years of experience in designing and Implementing cloud-based solutions in various AWS ServicesStrong experience in DevOps tool chain (Jenkins, Artifactory, Ansible/Chef/Puppet/Spinnaker, Maven/Gradle, Atlassian Tool suite)Very Good knowledge and experience in Non-Functional (Technical) Requirements like Security, Transaction, Performance, etc.Excellent analytical and problem-solving skills Nice to have:· Experience in Experience with OAuth implementation using Ping Identity· Experience in API Management (Apigee) or Service Mesh (Istio)· Good knowledge and experience in Queue/Topic (Active-MQ) based implementations.· Good knowledge and experience in Scheduler and Batch Jobs· Experience with scripting languages using Unix· Preferably certified in AWS</td><td>eyJqb2JfdGl0bGUiOiJCaWcgRGF0YSBMZWFkLyBMZWFkIERhdGEgRW5naW5lZXIvU3BhcmsgVGVjaCBMZWFkIiwiY29tcGFueV9uYW1lIjoiVGFuaXNoYSBTeXN0ZW1zICBJbmMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQmRWaDV5cnl3aTREUFNOT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 hours ago</td><td>Spark Engineer</td></tr><tr><td>Data Insights Analyst</td><td>IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED</td><td>India</td><td>Responsible for leveraging Google Analytics 4 (GA4) to extract, analyze, and interpret data to support business objectives and decision-making processes. Requires proficient understanding and translating business requirements into actionable insights and recommendations derived from GA4 data.Key Responsibilities:1.Data Extraction & Analysis: Extract and analyze GA4 data to provide meaningful insights into user behavior, website performance, and digital marketing effectiveness.2.Reporting & Dashboarding: Develop custom reports, dashboards, and data visualizations using GA4 data to monitor key metrics and performance indicators.3.Insights and Recommendations: Providing actionable insights and recommendations to optimize website performance, user experience, and marketing campaigns.4.Customization & Configurations: Configure GA4 data streams, events, and conversions to align with business goals and KPIs (key performance indicators). Utilize custom dimensions, metrics, and attribution models to capture and analyze data relevant to unique business needs.5.Pharma knowledge: Understanding of the Pharma industry landscape, including regulatory compliance, patient journey mapping, and healthcare data privacy considerationsMinimum Requirement:·2-4 Years of experience in digital analytics, hands on experience on Google Analytics 4.·Proficiency in Google Analytics and Google Tag Manager.·Strong analytical skills and ability to interpret data.·Understanding of digital marketing concepts and website optimization techniques.·Excellent communication skills to present findings and recommendations effectively·GA4 Certification is strongly preferred</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEluc2lnaHRzIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJJTjEwIChGQ1JTID0gSU4wMTApIE5vdmFydGlzIEhlYWx0aGNhcmUgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImFiMU5uYTFGN1dHbFd4eC1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 days ago</td><td>Data Analyst</td></tr><tr><td>Senior Data Management Analyst</td><td>WELLS FARGO</td><td>Hyderabad, Telangana, India</td><td>About this role:\n",
       "\n",
       "Wells Fargo is seeking a Senior Data Management Analyst\n",
       "\n",
       "In this role, you will:\n",
       "• Lead or participate in moderately complex programs and initiatives for data quality, governance, and metadata activities\n",
       "• Design and conduct moderately complex analysis to identify and remediate data quality, data integrity, process, and control gaps\n",
       "• Analyze, assess, and test data controls and data systems to ensure quality and risk compliance standards are met and adhere to data governance standards and procedures\n",
       "• Identify data quality metrics and execute data quality audits to benchmark the state of data quality\n",
       "• Develop recommendations for optimal approaches to resolve data quality issues and implement plans for assessing the quality of new data sources leveraging domain expertise and data, business, or process analysis to inform and support solution design\n",
       "• Lead project teams and mentor less experienced staff members\n",
       "• Drive planning and coordination on moderately complex remediation efforts acting as central point of contact\n",
       "• Consult with clients to assess the current state of data and metadata quality within area of assigned responsibility\n",
       "• Participate in cross-functional groups to develop companywide data governance strategies\n",
       "• Provide input into communication routines with stakeholders, business partners, and experienced leaders\n",
       "\n",
       "Required Qualifications:\n",
       "• 4+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n",
       "\n",
       "Desired Qualifications:\n",
       "• Experience in large enterprise data initiatives\n",
       "• Contact center business or technology experience\n",
       "• Experience implementing, managing, or utilizing big data environments (Hadoop, Big Query, etc.)\n",
       "• Experience using standard BI tools (Tableau, Power BI, MicroStrategy, etc.) preferably from big data environments\n",
       "\n",
       "Posting End Date:\n",
       "24 Jun 2025\n",
       "• Job posting may come down early due to volume of applicants.\n",
       "\n",
       "We Value Equal Opportunity\n",
       "\n",
       "Wells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n",
       "\n",
       "Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n",
       "\n",
       "Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n",
       "\n",
       "Applicants with Disabilities\n",
       "\n",
       "To request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n",
       "\n",
       "Drug and Alcohol Policy\n",
       "\n",
       "Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n",
       "\n",
       "Wells Fargo Recruitment and Hiring Requirements:\n",
       "\n",
       "a. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n",
       "\n",
       "b. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBNYW5hZ2VtZW50IEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJXZWxscyBGYXJnbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEFWbXpFcV9ySTNSUG1faUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Associate/Analyst - Data Analytics</td><td>D. E. SHAW INDIA</td><td>Hyderabad, Telangana, India</td><td>The Financial Research (FinRes) group provides research and analysis support to the entire investment life cycle of the firm’s global proprietary trading strategies. The group caters to almost all the quantitative and qualitative strategies of the firm and its research support covers multiple asset classes, sectors, and geographies.</td><td>eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUvQW5hbHlzdCAtIERhdGEgQW5hbHl0aWNzIiwiY29tcGFueV9uYW1lIjoiRC4gRS4gU2hhdyBJbmRpYSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiak9McEJja1dWaXhiWjJ4c0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>14 hours ago</td><td>Data Analyst</td></tr><tr><td>Senior Analyst- Data Risk Office</td><td>BRISTOL MYERS SQUIBB</td><td>Hyderabad, Telangana, India</td><td>Working with Us\n",
       "Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n",
       "\n",
       "Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n",
       "\n",
       "Roles & Responsibilities\n",
       "\n",
       "Functional and Technical\n",
       "• Execution and monitoring of data privacy office key activties.\n",
       "• Assist in the development and implementation of data privacy policies and procedures to ensure compliance with international, federal, and state regulations, including GDPR, CCPA, and HIPAA.\n",
       "• Work closely with legal, IT, HR, and other departments to address data privacy concerns and implement best practices.\n",
       "• Assist in the development and delivery of data privacy training and awareness programs across the organization. Keeps up to date with evolution of regulations impacting privacy, ethics, and data.\n",
       "• Experienced in configuring and working with various industry leading data risk and privacy tools like OneTrust, TrustArc, Microsoft Purview etc.\n",
       "• Demonstrate critical thinking around interpreting business and industry challenges and recommending best practices-based solutions to improve products, processes, systems and reduce risk.\n",
       "• Implement organizational IT controls in accordance with applicable regulations and the ability to evaluate and understand the impact of new regulations and requirements.\n",
       "\n",
       "People Management:\n",
       "• Responsible for training and mentoring junior staff to meet BMS standards.\n",
       "• Preferred experience with working in a multi-cultural, multi-location and diverse environments.\n",
       "\n",
       "Qualifications & Experience\n",
       "• B.E./B.Tech. or equivalent in computer science, engineering, life science field\n",
       "• Recognized privacy/DLP certifications and experience preferred.\n",
       "• At least years of privacy program management, compliance, or strong operations management experience (regulated or healthcare or tech preferred).\n",
       "• Knowledge of HIPAA, GDPR, CPRA, PIPL etc., and other privacy regulations is a must.\n",
       "• Ability to make decisions that impact own work and other groups/teams and works under minimal supervision.\n",
       "• Demonstrates openness to learning and developing. Takes a responsibility for their own and team’s development and growth.\n",
       "• Demonstrates an understanding of factors driving team performance and how they contribute to the team's overall success.\n",
       "• Excellent English Oral and written communication skills including the ability to deliver clear and articulate presentations.\n",
       "• Ability to use PowerPoint, Excel, Word, or other technologies to communicate complex topics to stakeholders, manage personal workload, and track projects and issues.\n",
       "\n",
       "If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n",
       "\n",
       "Uniquely Interesting Work, Life-changing Careers\n",
       "With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n",
       "\n",
       "On-site Protocol\n",
       "\n",
       "BMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n",
       "\n",
       "Site-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n",
       "\n",
       "BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n",
       "\n",
       "BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n",
       "\n",
       "BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n",
       "\n",
       "If you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n",
       "\n",
       "Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHlzdC0gRGF0YSBSaXNrIE9mZmljZSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJsWm9Rdlc1dnAyWjZyYVlMQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst II – Product Information Capabilities | Digital & Technology</td><td>GENERAL MILLS INDIA</td><td>India</td><td>India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.\n",
       "\n",
       "Position Title\n",
       "\n",
       "Software Engineer II – Product Information Capability\n",
       "\n",
       "Function/Group\n",
       "\n",
       "Digital & Technology\n",
       "\n",
       "Location\n",
       "\n",
       "Mumbai\n",
       "\n",
       "Shift Timing\n",
       "\n",
       "Regular\n",
       "\n",
       "Role Reports to\n",
       "\n",
       "D&T Manager – Product Information Capability\n",
       "\n",
       "Remote/Hybrid/in-Office\n",
       "\n",
       "Hybrid\n",
       "\n",
       "About General Mills\n",
       "\n",
       "We make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and Häagen-Dazs, we’ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\n",
       "\n",
       "How we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\n",
       "\n",
       "us into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\n",
       "\n",
       "General Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\n",
       "\n",
       "With our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital & Technology (D&T) Innovation, Technology & Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy & Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\n",
       "\n",
       "We advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\n",
       "\n",
       "Job Overview\n",
       "\n",
       "Function Overview\n",
       "\n",
       "The Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The team's expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\n",
       "\n",
       "The team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the \"Work with Heart\" philosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the following Link\n",
       "\n",
       "Purpose of the role\n",
       "\n",
       "This is an exciting time to work in General Mills' Supply Chain & ITQ Organization! We are accelerating Digital Transformation of our Product Information Capabilities (PIC) capabilities to provide a competitive advantage to our business. To this end, we are seeking a skilled and motivated STIBO Developer to join our Product Information Capabilities team. As a STIBO Developer, you will play a crucial role in designing, developing, and implementing solutions within our STIBO STEP platform to support our product information management (PIM) and master data management (MDM) processes. You will collaborate with cross-functional teams to understand business requirements, translate them into technical specifications, and deliver high-quality solutions that meet our business needs.\n",
       "\n",
       "Key Accountabilities\n",
       "• Design, develop, and maintain STIBO STEP solutions to support product information management processes.\n",
       "• Write and maintain code for business rules to ensure data quality and consistency.\n",
       "• Configure outbound and inbound integrations to exchange data with other systems.\n",
       "• Configure gateway endpoints for seamless data flow.\n",
       "• Develop and maintain data models within STIBO STEP to accurately represent product information.\n",
       "• Build web UI screens for data entry, validation, and reporting.\n",
       "• Develop solutions based on documented requirements and specifications.\n",
       "• Participate in agile project development, including sprint planning, daily stand-ups, and retrospectives.\n",
       "• Collaborate with business analysts, data architects, and other developers to ensure solutions align with business needs and technical standards.\n",
       "• Troubleshoot and resolve issues related to STIBO STEP implementations.\n",
       "• Stay up-to-date with the latest STIBO STEP features and best practices.\n",
       "• Create and maintain technical documentation for STIBO STEP solutions.\n",
       "\n",
       "Minimum Qualifications\n",
       "• Education – Full time graduation from an accredited university (Mandatory- Note: This is the minimum education criteria which cannot be altered)\n",
       "• Experience with tools and concepts related to MDM and/or Digital Product Publishing, such as data governance, data quality, data integration and data exporting\n",
       "• Exposure to Product Information Management Systems (PIM/MDM)\n",
       "• Technical expertise into Stibo platform\n",
       "• Experience with Data Syndication partners such as 1WorldSync, Syndigo and Salsify.\n",
       "• Exposure to GDSN Standards\n",
       "• Strong team player and collaborator with core focus on solution making; Effective analytical and technical skills; Ability to work in a cross functional team environment\n",
       "\n",
       "Preferred Qualifications\n",
       "• Product Information Management / Master Data Management\n",
       "• STIBO STEP certification\n",
       "• Business Analysis skills\n",
       "• SQL, Cloud GCP\n",
       "• Agile / SCRUM Delivery\n",
       "• Familiarity with Service Bus Integration\n",
       "• Preferably experience in Consumer Goods industry.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUkg4oCTIFByb2R1Y3QgSW5mb3JtYXRpb24gQ2FwYWJpbGl0aWVzIHwgRGlnaXRhbCBcdTAwMjYgVGVjaG5vbG9neSIsImNvbXBhbnlfbmFtZSI6IkdlbmVyYWwgTWlsbHMgSW5kaWEiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTWplT1ByZWI4UVZPLVpDVUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>2 days ago</td><td>Data Analyst</td></tr><tr><td>Lead Data Management Analyst</td><td>WELLS FARGO</td><td>Hyderabad, Telangana, India</td><td>About this role:\n",
       "\n",
       "Wells Fargo is seeking a Lead Data Management Analyst\n",
       "\n",
       "In this role, you will:\n",
       "• Organize and lead complex companywide initiatives to ensure that data quality is maintained so that data can effectively support business processes\n",
       "• Oversee analysis and reporting in support of regulatory requirements\n",
       "• Identify and recommend analysis of data quality or integrity issues\n",
       "• Evaluate data quality metrics and data quality audits to benchmark the state of data quality\n",
       "• Make decisions in complex and multi-faceted situations requiring solid understanding of data governance standards and procedures\n",
       "• Identify new data sources and develop recommendations for assessing the quality of new data\n",
       "• Lead project teams and mentor less experienced staff members\n",
       "• Recommend remediation of process or control gaps that align to management strategy\n",
       "• Serve as relationship manager for a line of business\n",
       "• Consult with and provide recommendations to senior management regarding assessments of the current state of data and metadata quality within area of assigned responsibility\n",
       "• Represent client in cross-functional groups to develop companywide data governance strategies\n",
       "• Strategically collaborate and consult with peers, colleagues, and mid-level to senior managers to coordinate and drive collaboration on solution design and remediation execution\n",
       "\n",
       "Required Qualifications:\n",
       "• 5+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n",
       "\n",
       "Desired Qualifications:\n",
       "• Experience in Data Management, Business Analysis, Analytics, Project Management.\n",
       "\n",
       "Posting End Date:\n",
       "24 Jun 2025\n",
       "• Job posting may come down early due to volume of applicants.\n",
       "\n",
       "We Value Equal Opportunity\n",
       "\n",
       "Wells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n",
       "\n",
       "Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n",
       "\n",
       "Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n",
       "\n",
       "Applicants with Disabilities\n",
       "\n",
       "To request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n",
       "\n",
       "Drug and Alcohol Policy\n",
       "\n",
       "Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n",
       "\n",
       "Wells Fargo Recruitment and Hiring Requirements:\n",
       "\n",
       "a. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n",
       "\n",
       "b. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgTWFuYWdlbWVudCBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiV2VsbHMgRmFyZ28iLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ik5YbnE1ZlFtc3M5WXNVQ0JBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Senior Data Analyst, Marketing Science</td><td>CRUNCHYROLL</td><td>Hyderabad, Telangana, India</td><td>About the role\n",
       "\n",
       "We are looking for a hands-on Senior Marketing Science Analyst with a passion for understanding data, tracking business trends, and storytelling to join the Center for Data and Insights team. This position will report to the Vice President, Business Intelligence & Analytics, and will collaborate with data engineers, campaign managers, and marketing leaders to investigate campaign trends, build business insights, and recommend proactive measures to enable better business decisions every day. The right person for this position is a service-minded, empathetic problem-solver who will be motivated by the opportunity to build a centralized insights service team from the ground up!\n",
       "• Partner with Branch/ Adjust MMPs, direct response, and media buying agencies to define and implement campaign measurement of integrated marketing campaigns, including TV, OOH, Digital, and Social Media across a wide range of business lines (theatrical, streaming, e-commerce, etc.)\n",
       "• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n",
       "• Evaluating media measurement leveraging MMM (Marketing Mix Modeling), MTA (Multi-Touch Attribution), and Incrementality A/B Testing\n",
       "• Lead / Assist in development of testing roadmaps, measurement plans, KPI & KLI reporting based on set marketing objectives linked to key campaigns and always-on digital marketing tactics.\n",
       "• Deliver ad hoc analysis for client stakeholders throughout marketing, working with the channel leads, planning, and client service departments.\n",
       "• Partner with paid marketing teams in building weekly insight decks for the Marketing leadership team by collaborating effectively across teams, documenting the impact of strategic initiatives on benchmarks, and tracking the progress towards company goals.\n",
       "• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n",
       "• Connect ideas into cohesive, well-grounded recommendations, using creative, structured, and analytical thinking with the help of effective data visualization\n",
       "• Work with offshore and onsite teams and lead the sprint planning/management\n",
       "• Maintain a culture of high-quality output and outstanding customer service by effectively communicating at all levels, ensuring that work gets done, and responding effectively to\n",
       "\n",
       "About You\n",
       "• 5+ years of experience with data analysis, paid campaign analytics, statistics, experimentation, and optimization.\n",
       "• 3+ years of experience in writing complex SQL queries, experience in marketing data automation.\n",
       "• 3+ years of experience with data visualization tools like Tableau, Superset, etc.\n",
       "• Understanding of install and user-level paid campaign tracking to support multi-touch attribution via UTMs and MMP data for all major marketing channels\n",
       "• Proficiency in data analysis, including defining critical metrics, statistical and predictive modeling concepts, descriptive statistics, and experimental design\n",
       "• Experience in Marketing analytics tools like Google Analytics, Adjust, Braze, Branch, etc.\n",
       "• Experience working with large data sets (Terabytes of data/ billions of records).\n",
       "• Deep expertise in measuring marketing performance against lifetime value metrics.\n",
       "• Outstanding teamwork skills: You have excellent interpersonal skills. You’re a good listener. You place the success of the company ahead of any particular idea. Proven ability to work across a globally, matrixed organization.\n",
       "• BS in Statistics, Computer Science, Information Systems, or a related field\n",
       "\n",
       "About the Team\n",
       "\n",
       "The Center for Data and Insights (CDI) is the centralized team of data engineering, BI, analytics, and data science experts, passionate about servicing the organization with timely and certified reports and insights! The mission of the group is to inspire, support, and guide our stakeholders to be data-aware as well as build out the systems of intelligence to discover insights and act on them.\n",
       "\n",
       "Why you will love working at Crunchyroll\n",
       "\n",
       "In addition to getting to work with fun, passionate and inspired colleagues, you will also enjoy the following benefits and perks:\n",
       "• Best-in class medical, dental, and vision private insurance healthcare coverage\n",
       "• Access to counseling & mental health sessions 24/7 through our Employee Assistance Program (EAP)\n",
       "• Free premium access to Crunchyroll\n",
       "• Professional Development\n",
       "• Company's Paid Parental Leave\n",
       "• up to 26 weeks for birthing parents\n",
       "• up to 12 weeks for non-birthing parents\n",
       "• Hybrid Work Schedule\n",
       "• Paid Time Off\n",
       "• Flex Time Off\n",
       "• 5 Yasumi Days\n",
       "• Half-Day Fridays during the summer\n",
       "• Winter Break\n",
       "\n",
       "#LifeAtCrunchyroll #LI-Hybrid</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBBbmFseXN0LCBNYXJrZXRpbmcgU2NpZW5jZSIsImNvbXBhbnlfbmFtZSI6IkNydW5jaHlyb2xsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiI3QU16TGktYmRTM1gybEV4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Principal Data Analyst</td><td>STORABLE</td><td>Serilingampalle (M), Hyderabad, Telangana, India</td><td>About the Role:\n",
       "We’re seeking a strategic, data-driven professional to lead our Business Analysis initiatives for our Marketplace product within the Self-Storage vertical. This is a ‘hands on keyboard’ high-impact role that bridges business strategy, analytical rigor, and data visualization —ideal for someone who thrives on translating data into actionable insights that drive performance. As the Principal Data Analyst, you will own our marketplace performance metrics, lead the development of world-class dashboards and reporting frameworks, and partner closely with Product, Marketing, Operations, and Finance teams to inform strategic decisions. You’ll drive both hands-on analysis and organizational data literacy while mentoring a team of analysts and collaborating with engineering on data infrastructure.\n",
       "Key Responsibilities:\n",
       "\n",
       "Own and continuously optimize key Marketplace KPIs (conversion, revenue yield, inventory, pricing, etc.).\n",
       "Serve as the steward of metric definitions and business logic—ensuring consistency, accuracy, and alignment across stakeholders.\n",
       "Have a deep understanding of how to run a BI environment. Proactive, insightful, curious.\n",
       "Build and scale world class self-service dashboards and Tableau reports for internal and executive stakeholders.\n",
       "Lead cross-functional analysis projects to uncover trends, diagnose problems, and identify growth and efficiency opportunities.\n",
       "Translate complex data into clear, actionable insights and concise narratives for business and executive audiences.\n",
       "Drive rigorous SQL-based analysis from raw datasets, ensuring high-quality, reproducible outputs.\n",
       "Guide and collaborate with junior analysts in other groups and collaborate with engineering to shape data pipelines and definitions.\n",
       "Collaborate with cross functional partners and teams (Product,Marketing, Finance, Business Insights) to test hypotheses, evaluate feature/campaign performance, and measure outcomes.\n",
       "Identify data gaps or integrity risks and work with stakeholders to improve reliability and accuracy.\n",
       "Champion a culture of curiosity, experimentation, and evidence-based decision-making.\n",
       "Proactively keep other teams in the loop on new data capabilities unlocked and changes being made.\n",
       "\n",
       "Requirements:\n",
       "\n",
       "5-7 years of experience in business analysis, data analytics, or related roles, ideally with at least 2+ years in a leadership capacity.\n",
       "Advanced proficiency in SQL with experience working with complex data models and the ability to query complex relational datasets.\n",
       "Other data engineering experience is a significant plus to facilitate sourcing/formating of data.\n",
       "Deep understanding of performance metrics and business levers in a supply/demand or marketplace environment.\n",
       "Experience leading analytics efforts in partnership with cross-functional teams including Product, Marketing, Finance, and Engineering.\n",
       "Proven track record of developing intuitive, scalable dashboards using Tableau (or similar BI platforms).\n",
       "Strong business acumen with the ability to connect the dots between business objectives and analytical frameworks.\n",
       "Exceptional communication skills with the ability to distill technical findings for non-technical audiences.\n",
       "Capable of influencing and informing executive stakeholders with clear, concise insights.\n",
       "Demonstrated experience owning KPIs, building data products, and drive insights into action in a fast paced environment.\n",
       "Ability to navigate ambiguity, manage and prioritize competing needs, and work cross-functionally.\n",
       "\n",
       "Preferred Qualifications:\n",
       "\n",
       "Experience in the storage, real estate, or marketplace industries strongly preferred\n",
       "Familiarity with modern data stack tools such as Snowflake, dbt, or similar Experience analyzing marketplace dynamics or supply/demand business models\n",
       "Exposure to experimentation frameworks, A/B testing, or uplift modeling\n",
       "Prior exposure to high-growth SaaS or Marketplace operations\n",
       "Data engineering capabilities—especially in sourcing, transforming, or centralizing disparate datasets.\n",
       "\n",
       "About Us:\n",
       "At Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\n",
       "We leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.\n",
       "Important Notice: Protect Yourself from Fraudulent Activities Targeting Job Seekers\n",
       "We’ve been made aware of fraudulent activities where an individual or group is pretending to represent Storable, attempting to deceive job seekers by using our company’s good name and stellar reputation.\n",
       "To protect yourself, please consider the following guidelines:\n",
       "– Official Communication: All genuine communication from Storable will come from official email addresses, ending in “@storable.com.” Be cautious of any communication that doesn’t match this criteria.– No Unsolicited Offers: We do not extend job offers without a formal interview process. If you receive an unsolicited job offer claiming to be from Storable or any of its representatives, it’s a red flag.– Verification: If you’re uncertain about the legitimacy of any job offer or communication claiming to be from Storable, please directly contact our HR department directly at POps@storable.com for verification.\n",
       "Your security and trust are paramount to us. If you suspect you’ve been contacted by someone falsely claiming to be from Storable or using our company’s name for any dubious purpose, please immediately report the incident to POps@storable.com\n",
       "We’re committed to ensuring a transparent and secure hiring process.\n",
       "Thank you for your vigilance and interest in joining our team.</td><td>eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiU3RvcmFibGUiLCJhZGRyZXNzX2NpdHkiOiJTZXJpbGluZ2FtcGFsbGUgKE0pLCBIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InljaTdhWW0wWnR2MEMzbkZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>18 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst 1</td><td>UNITEDHEALTH GROUP</td><td>India</td><td>At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.\n",
       "\n",
       "Primary Responsibilities:\n",
       "• Validate data with administrative source systems (source of truth)\n",
       "• Analyze complex datasets\n",
       "• Generate actionable insights and recommendations based on data analysis\n",
       "• Database Management:\n",
       "• Develop and maintain data models, data dictionaries, and other documentation\n",
       "• Troubleshoot and resolve database-related issues\n",
       "• Data Extraction and Transformation:\n",
       "• Support capital project that will, in part, import and transform data from various sources for broker bonus calculations\n",
       "• Ensure data integrity and quality through rigorous validation and testing\n",
       "• Data Visualization and Reporting:\n",
       "• Create visually appealing and informative dashboards and reports\n",
       "• Present findings and insights to both technical and non-technical stakeholders in a clear and concise manner\n",
       "• Continuous Learning and Improvement:\n",
       "• Stay up to date with the latest data analysis techniques and tools\n",
       "• Identify opportunities to improve data analysis processes and methodologies\n",
       "• Actively participate in knowledge sharing and mentoring within the team\n",
       "• Comply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n",
       "\n",
       "Required Qualifications:\n",
       "• Undergraduate degree or equivalent experience\n",
       "• 4+ years Experience as SAS Data Analyst\n",
       "• Experience with relational database management systems (e.g., MySQL, Oracle, SQL Server)\n",
       "• Experience with statistical analysis\n",
       "• Familiarity with data visualization tools (e.g., Tableau, Power BI)\n",
       "• Proven excellent problem-solving and critical thinking skills\n",
       "• Proven solid communication and presentation skills to effectively convey complex data analysis findings to both technical and non-technical stakeholders\n",
       "• Proven ability to work independently and collaboratively in a fast-paced, deadline-driven environment\n",
       "• Proven detail-oriented with a focus on accuracy and data integrity\n",
       "\n",
       "At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone - of every race, gender, sexuality, age, location and income - deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.\n",
       "\n",
       "#NTRQ</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IlVuaXRlZEhlYWx0aCBHcm91cCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJhdHFPMEswckdadGwwUy1xQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>3 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst – Competitive Benchmarking & Reporting</td><td>REPUTATION</td><td>Hyderabad, Telangana, India</td><td>Why Work at Reputation?\n",
       "• Reputation reached over $100m in Annual Recurring Revenue (ARR) in 2022 and continues to grow worldwide.\n",
       "• We've raised over $200 million in funding from A-list venture capital firms such as Bessemer Venture Partner and Kleiner Perkins, including $150 million in equity financing from Marlin Equity Partners in January 2022.\n",
       "• Reputation is trusted by more than 250 partners, including Google, Facebook, Salesforce, J.D. Power, Amazon and Web.com.\n",
       "• Our industry leading platform has been recognized by Forrester and Gartner as a vendor of choice in Voice of the Customer, Customer Feedback Management, and Social Suites research reports.\n",
       "• The platform is used by 10+ major automotive OEMs and 16,000 auto dealerships, more than 250 healthcare systems, and over 100 leading property management firms.\n",
       "• Our executive management team is committed to building a performance-based culture where excellence is rewarded and careers are developed.\n",
       "• Who thrives at Reputation? Managers who embody a player-coach mentality. Employees who value teamwork and cross-functional collaboration. People who emphasize perseverance and hustle over quick wins and luck.\n",
       "• Our Mission: We exist to forge relationships between companies and communities.\n",
       "\n",
       "We are seeking a Data Analyst with expertise in data stewardship and analysis to manage our competitive benchmark data tenants and produce customer-facing reports for marketing and sales. The ideal candidate is proficient in BigQuery (SQL), detail-oriented, and capable of working independently on complex data projects.\n",
       "\n",
       "Responsibilities:\n",
       "• Data Stewardship & Governance: Ensure data accuracy, integrity, and accessibility by managing competitive benchmark data tenants, implementing verification processes, and maintaining governance best practices.\n",
       "• Data Pipeline Management: Develop and maintain data ingestion pipelines, including API integrations and web scraping, ensuring timely updates and scalability.\n",
       "• Data Validation & Quality Control: Oversee large-scale data input and validation, balancing hands-on data manipulation with automation strategies. Collaborate with data owners to resolve quality issues.\n",
       "• Advanced Analytics & Reporting: Conduct in-depth data analysis to uncover trends, patterns, and insights that drive business decisions. Scope, design, and execute analysis projects with minimal supervision.\n",
       "• Industry & Market Insights: Develop data-driven industry reports for marketing and sales, leveraging analytics to provide insights on market trends and competitive landscapes.\n",
       "• Cross-Functional Collaboration: Work closely with product managers, engineers, marketing, and leadership to resolve data issues, provide training, and ensure alignment on data best practices.\n",
       "• Tool & Process Optimization: Design scalable internal tools and reporting solutions to enhance data accessibility, automation, and usability across teams.\n",
       "• Stakeholder Communication: Translate complex data insights into actionable recommendations for key stakeholders, including non-technical audiences.\n",
       "\n",
       "Qualifications:\n",
       "• 5+ years of proven experience in data analysis, data stewardship, and managing large-scale datasets, with a strong focus on ensuring data accuracy, integrity, and accessibility.\n",
       "• Must be proficient with SQL and NoSQL (BigQuery), as well as have the ability to thrive in an environment that often requires patience and creativity to query, extract and validate data in which there are high levels of uncertainty.\n",
       "• Strong prior professional experience managing databases and using applicable tools is required.\n",
       "• Experience with and knowledge of ETL processes and data migration.\n",
       "• Understanding of and prior experience with General Data Protection Regulation.\n",
       "• Demonstrated experience and proficiency in using Python in real-world, business applications are substantial advantages to candidates.\n",
       "• Highly articulate with outstanding verbal and written communication skills; you are able to explain data-driven insights and analyses intuitively to clients, executives, and technical and non-technical coworkers in both large and small groups.\n",
       "• You should be comfortable taking feedback from everyone – Product Managers, Engineers, customers, the CEO, etc. – to help understand business objectives and develop the most powerful analyses and tools possible.\n",
       "• Proven ability to operate in a fast-paced, data-driven environment.\n",
       "\n",
       "When you join Reputation, you can expect:\n",
       "• Flexible working arrangements.\n",
       "• Career growth with paid training tuition opportunities.\n",
       "• Active Employee Resource Groups (ERGs) to engage with.\n",
       "• An equitable work environment.\n",
       "\n",
       "Our employees say it best:\n",
       "\n",
       "According to Glassdoor, 94% of our employees approve of our CEO, Joe Burton.\n",
       "\n",
       "Our employees highlight our:\n",
       "• Ample Opportunities- “There are many opportunities to learn and grow. Many open roles are replaced with internal promotions.”\n",
       "• Positive Culture- \"Great opportunity and exceptional culture.\" “You will never have a better culture anywhere else. Period.”\n",
       "• Training and Tools- “All managers truly want you to succeed, and you are given great tools and training to be successful in your role.”\n",
       "• Balance- “Great work life balance and awesome team environment!”\n",
       "\n",
       "Diversity Programs & Initiatives:\n",
       "\n",
       "Our Reputation Nation spans around the world. This global perspective allows us to intentionally unlock the magic that comes from diversity of experience to contribute to our success.\n",
       "\n",
       "At Reputation, we believe in:\n",
       "• Diversity: Embracing a culture that values uniqueness.\n",
       "• Inclusion: Inviting diverse groups to take part in company life.\n",
       "• Belonging: Helping each individual feel accepted for who they are.\n",
       "\n",
       "\"At Reputation, we see diversity and inclusion as the foundation for an equitable workplace. Our goal is to empower all of our employees, regardless of their background, to make an impact in their work each and every day.\" - Joe Burton, CEO, Reputation\n",
       "\n",
       "Additionally, we offer a variety of benefits and perks, such as:\n",
       "• Health Insurance & Wellness Benefits: Group Health Insurance: Medical Insurance with floater policy of up to 10,00,000 for employee + spouse + 2 dependent children + 2 parents / parent-in-laws\n",
       "• Maternity Benefits: Medical insurance up to 75,000 INR, 26 weeks of leave for birth, adoption or surrogacy\n",
       "• Life Insurance: Insurance at 3x annual cost to the company (Term Insurance, GPA)\n",
       "• Accident/Disability Insurance: Insured at 3x base salary for permanent total disability, permanent partial disability and temporary total disability (GPA)\n",
       "• OPD: of 7500 per annum per employee\n",
       "\n",
       "Leaves\n",
       "• 10 Company observed holidays a year (Refer to the Holiday Calendar for the Year)\n",
       "• 12 Casual/Sick leaves (Pro-rata calculated)\n",
       "• 02 Earned Leaves per Month (Pro-rata calculated)\n",
       "• 04 Employee Recharge days (aka company holiday/office closed)\n",
       "• Maternity & Paternity (6 months)\n",
       "• Bereavement Leave (10 Days)\n",
       "\n",
       "Car Lease:\n",
       "Reputation offers Car Lease Program that allows employees to lease a car with no upfront cost or down payment. They benefit from a fixed monthly lease rental and 20-30% tax savings.\n",
       "\n",
       "We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n",
       "\n",
       "To learn more about how we handle the personal data of applicants, visit our Candidate Privacy Notice.\n",
       "\n",
       "Applicants only - No 3rd party agency candidates.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3Qg4oCTIENvbXBldGl0aXZlIEJlbmNobWFya2luZyBcdTAwMjYgUmVwb3J0aW5nIiwiY29tcGFueV9uYW1lIjoiUmVwdXRhdGlvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiT0NPR3UtU1dxWjh4X0gzS0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 days ago</td><td>Data Analyst</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Lead Consultant - Technical Lead - Fullstack Data Engineer",
         "ASTRAZENECA",
         "Chennai, Tamil Nadu, India",
         "Job Title: Lead Consultant - Technical Lead - Data, Analytics & AI\nCareer Level: E\n\nIntroduction to role: Are you ready to redefine an industry and change lives? Join our Operations IT organization, where your work will have a direct impact on patients by redefining our ability to develop life-changing medicines. We empower the business to perform at its peak, combining innovative science with leading digital technology platforms and data. As part of the Data Analytics and AI (DA&AI) group, you'll deliver innovative solutions that drive the transformation of medicine development. As the Technical Lead, you'll oversee the technical delivery of products and projects aligned with business objectives, tapping into your expertise in Data and Cloud Engineering, Software Engineering, AI, and more. Collaborate with stakeholders to ensure seamless delivery and maximum business impact!\n\nAccountabilities:\n• Bridge business needs with technical solutions by leading IT application design and implementation.\n• Collaborate with stakeholders to define and deliver requirements, translating them into detailed specifications.\n• Own the technical vision of products and compile detailed technical designs, refining user Epics/stories.\n• Optimize performance, mitigate risks, and ensure alignment to timelines and resource allocations.\n• Advise on industry trends and standard methodologies to enhance performance and business outcomes.\n• Provide technical direction and guidance to IT teams and business units.\n• Contribute to Data & Software Engineering standards and best practices.\n• Research new technologies to boost system performance and scalability.\n• Lead and mentor technical teams (Data Engineering, Cloud Engineering, Software Engineering) and work with AI/GenAI leads to foster collaboration and innovation.\n• Ensure platform stability, scalability, and simplicity while adhering to regulatory requirements and data security standards.\n• Foster continuous improvement and innovation.\n• Supervise technical leadership throughout the software development lifecycle and provide hands-on support in development, technical delivery, review, and testing while collaborating with global teams.\n• Apply central Enterprise Data Platforms and guide DevOps, DataOps, and MLOps teams to ensure standard methodologies.\n• Ensure data solutions align with FAIR principles and support end-to-end data science and machine learning.\n\nEssential Skills/Experience:\n• Minimum 10 years of experience in the design, development, and delivery of software and data engineering solutions.\n• Extensive technical expertise in Data Engineering, Software Engineering, and Cloud Engineering.\n• Strong foundational knowledge of AI Engineering principles and practices.\n• Deep understanding of DevOps, MLOps, and DataOps methodologies with practical implementation experience.\n• Demonstrated success in product development and/or product management, delivering complex solutions end-to-end.\n• Demonstrable ability to provide technical thought leadership across Data, Analytics, and AI domains.\n• Exceptional communication, customer management, and multi-functional collaboration skills.\n• Robust analytical and problem-solving abilities with a collaborative, team-oriented approach.\n• Hands-on experience driving innovation throughout the full product development lifecycle.\n• Solid understanding of Data Mesh and Data Product concepts and architectures.\n• Proficiency in Agile methodologies and facilitating iterative, multi-functional team delivery.\n• Hands-on experience designing, implementing, and optimizing data pipelines using leading ETL tools.\n• Skilled in architecting, deploying, and managing scalable, secure AWS cloud environments.\n• Proficient in workflow orchestration tools such as Apache Airflow.\n• Practical experience implementing DataOps practices with tools like DataOps.Live.\n• Strong expertise in data storage and analytics platforms such as Snowflake.\n• Ability to deliver actionable insights through business intelligence tools, including Power BI.\n• Extensive full-stack development experience, including backend proficiency with Node.js and Python and frontend expertise with ReactJS or NextJS.\n• Experience designing and deploying Generative AI solutions.\n• Hands-on implementation of AI/ML models using platforms such as Amazon SageMaker.\n• Advanced programming skills, especially in Python.\n• Solid knowledge of both SQL and NoSQL database technologies.\n• Familiarity with agile ways of working and iterative development environments.\n• Experience working in large, multinational organizations or pharmaceutical environments is highly desirable.\n• Demonstrated leadership and mentoring skills, with a demonstrable ability to develop high-performing technical teams.\n\nDesirable Skills/Experience:\n• Bachelor's or master's degree in health sciences, Life Sciences, Data Management, IT, or a related field.\n• Experience in the pharmaceutical industry or a similar multinational environment.\n• AWS Cloud or relevant data/software engineering certifications.\n• Awareness of use case specific GenAI tools available in the market and their application in day-to-day work scenarios.\n• Possess working knowledge of basic prompting techniques and continuously improve these skills.\n• Stay up to date with developments in AI and GenAI, applying new insights to work-related situations.\n\nWhen we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That's why we work, on average, a minimum of three days per week from the office. But that doesn't mean we're not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\n\nAt AstraZeneca, we are at a crucial stage of our journey to become a digital and data-led enterprise. Our commitment to innovation empowers us to make the impossible possible by building partnerships and ecosystems that drive scale and speed for exponential growth. With investment backing us all the way, we are focused on disrupting the industry while making a meaningful impact through our work. Here you'll find countless opportunities to learn, grow, and contribute to developing life-changing medicines.\n\nReady to make a difference? Apply now to join our team!\n\nDate Posted\n30-Jun-2025\n\nClosing Date\n\nAstraZeneca embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills. We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements.",
         "eyJqb2JfdGl0bGUiOiJMZWFkIENvbnN1bHRhbnQgLSBUZWNobmljYWwgTGVhZCAtIEZ1bGxzdGFjayBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiQXN0cmFaZW5lY2EiLCJhZGRyZXNzX2NpdHkiOiJDaGVubmFpLCBUYW1pbCBOYWR1LCBJbmRpYSIsImh0aWRvY2lkIjoiRGVXa2pFZDgxNXcwSUpJY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "3 days ago",
         "Data Engineer"
        ],
        [
         "AWS Data Engineer",
         "COGNIZANT",
         "Hyderabad, Telangana, India (+1 other)",
         "Job Summary:\n\nExperience : 4 - 8 years\n\nLocation : Bangalore\n\nThe Data Engineer will contribute to building state-of-the-art data Lakehouse platforms in AWS, leveraging Python and Spark. You will be part of a dynamic team, building innovative and scalable data solutions in a supportive and hybrid work environment. You will design, implement, and optimize workflows using Python and Spark, contributing to our robust data Lakehouse architecture on AWS. Success in this role requires previous experience of building data products using AWS services, familiarity with Python and Spark, problem-solving skills, and the ability to collaborate effectively within an agile team.\n\nMust Have Tech Skills:\n\n· Demonstrable previous experience as a data engineer.\n• Technical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\n\n· Proficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nNice To Have Tech Skills:\n\n· Familiar with data services in a Lakehouse architecture.\n\n· Familiar with technical design practices, allowing for the creation of scalable, reliable data products that meet both technical and business requirements\n\n· A master’s degree or relevant certifications (e.g., AWS Certified Solutions Architect, Certified Data Analytics) is advantageous\n\nKey Accountabilities:\n• Writes high quality code, ensuring solutions meet business requirements and technical standards.\n• Works with architects, Product Owners, and Development leads to decompose solutions into Epics, assisting the design and planning of these components.\n• Creates clear, comprehensive technical documentation that supports knowledge sharing and compliance. Experience in decomposing solutions into components (Epics, stories) to streamline development.\n• Actively contributes to technical discussions, supporting a culture of continuous learning and innovation.\n\nKey Skills:\n• Proficient in Python and familiar with a variety of development technologies.\n• Previous experience of implementing data pipelines, including use of ETL tools to streamline data ingestion, transformation, and loading.\n• Solid understanding of AWS services and cloud solutions, particularly as they pertain to data engineering practices. Familiar with AWS solutions including IAM, Step Functions, Glue, Lambda, RDS, SQS, API Gateway, Athena.\n• Proficient in quality assurance practices, including code reviews, automated testing, and best practices for data validation.\n• Experienced in Agile development, including sprint planning, reviews, and retrospectives\n\nEducational Background:\n• Bachelor’s degree in computer science, Software Engineering, or related essential.\n\nBonus Skills:\n• Financial Services expertise preferred, working with Equity and Fixed Income asset classes and a working knowledge of Indices.\n• Familiar with implementing and optimizing CI/CD pipelines. Understands the processes that enable rapid, reliable releases, minimizing manual effort and supporting agile development cycles.",
         "eyJqb2JfdGl0bGUiOiJBV1MgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkNvZ25pemFudCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiWjYyVVNZZkhtUHk0dHdwNkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 days ago",
         "Data Engineer"
        ],
        [
         "Engineer III Consultant-Data Engineering",
         "VERIZON",
         "Hyderabad, Telangana, India (+2 others)",
         "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat you’ll be doing…\n\nWe are looking for data engineers who can work with world class team members to help drive telecom business to its full potential. We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n\nAs a data engineer you will be collaborating with business product owners, coaches, industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n• Understanding the business requirements and converting them to technical design.\n• Working on Data Ingestion, Preparation and Transformation.\n• Developing data streaming applications.\n• Debugging the production failures and identifying the solution.\n• Working on ETL/ELT development.\n• Understanding devops process and contributing for devops pipelines\n\nWhat we’re looking for...\n\nYou’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n\nYou’ll need to have…\n• Bachelor’s degree or four or more years of work experience.\n• Four or more years of relevant work experience.\n• Experience with Data Warehouse concepts and Data Management life cycle.\n• Experience in Big Data technologies - GCP/Hadoop/Spark/Composer/DataFlow/Bigquery.\n• Experience in complex SQL.\n• Experience working on Streaming ETL pipelines\n• Expertise in Java\n• Experience with MemoryStore / Redis / Spanner\n• Experience in troubleshooting the data issues.\n• Experience with data pipeline and workflow management & Governance tools.\n• Knowledge of Information Systems and their applications to data management processes.\n\nEven better if you have one or more of the following…\n• Three or more years of relevant experience.\n• Any relevant Certification on ETL/ELT developer.\n• Certification in GCP-Data Engineer.\n• Accuracy and attention to detail.\n• Good problem solving, analytical, and research capabilities.\n• Good verbal and written communication.\n• Experience presenting to and influence stakeholders.\n• Experience in driving a small team of 2 or more members for technical delivery\n\n#AI&D\n\nWhere you’ll be working\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.",
         "eyJqb2JfdGl0bGUiOiJFbmdpbmVlciBJSUkgQ29uc3VsdGFudC1EYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiVmVyaXpvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEhtc2E1cThWc3RlSGhKaUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Data Engineer"
        ],
        [
         "Data Engineer (Python, Pyspark, and Azure Databricks) || (4 - 6 Years)",
         "EMIDS",
         "Bengaluru, Karnataka, India",
         "Hi All,\n\nGreetings for the day!!\n\nWe are currently hiring for Data Engineer (Python, Pyspark, and Azure Databricks) for Emids(MNC) at Bangalore location.\n\nRole: Data Engineer\n\nExp: 5 to 8 Years\n\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\n\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\nNote: Candidate Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.\n\nRole Overview:\n\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\n• Lead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\n• Architect scalable data streaming and processing solutions to support healthcare data workflows.\n• Develop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\n• Ensure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\n• Collaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\n• Troubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\n• Mentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\n• Stay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\nRequired Skills & Qualifications:\n• 4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\n• Expertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\n• Experience with Azure Databricks (or willingness to learn and adopt it quickly).\n• Hands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\n• Proficiency in SQL, NoSQL databases, and data modeling for big data processing.\n• Knowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\n• Experience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\n• Strong analytical skills, problem-solving mindset, and ability to lead complex data projects.\n• Excellent communication and stakeholder management skills.\n\nNote: This is not a contract position, this will be a permanent position with Emids.\n\nInterested candidates Can Share Your Updated Profile with details for below Email.\n\nNAME:\n\nCCTC:\n\nECTC:\n\nNotice Period:\n\nOffers in Hand :\n\nEmail ID: Ravi.chekka@emids.com",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChQeXRob24sIFB5c3BhcmssIGFuZCBBenVyZSBEYXRhYnJpY2tzKSB8fCAoNCAtIDYgWWVhcnMpIiwiY29tcGFueV9uYW1lIjoiRW1pZHMiLCJhZGRyZXNzX2NpdHkiOiJCZW5nYWx1cnUsIEthcm5hdGFrYSwgSW5kaWEiLCJodGlkb2NpZCI6Im9TMEEyREhLeU9wd0lVdnVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "8 hours ago",
         "Data Engineer"
        ],
        [
         "Senior Data Engineer",
         "MASTERCARD",
         "Pune, Maharashtra, India",
         "Job Title:\n\nSenior Data Engineer\n\nOverview:\n\nPosition Overview:\n\nThe Senior Data Engineer, MyMPA will be part of GBSC’s Automation & Engineering Team, responsible for implementing enhancements and periodic refreshes of an enterprise-wide data platform.\n\nThis role will also work closely with the VP of Analytics & Metrics and Director of FP&A and gather requirements for changes and enhancements to the application and contribute to the technology platform's evolution as it grows to support the rapidly expanding Mastercard business.\n\nThe ideal candidate will have hands-on development skills combined with an ability to analyze and understand end user requirements that are critical success factors within this role. This role requires the skills and desire to work as an individual contributor as well as, collaborate cross functionally with various business constituents.\n\n1. Have you ever worked on an enterprise-wide reporting solution that relied heavily on your own knowledge and abilities to build and maintain the solution?\n2. Are you constantly hungry to learn? Do you have the “growth mindset” as opposed to the “fixed mindset”?\n3. Do you love working with people, helping them, and turning their requirements into something that can make a difference?\n\nRole:\n\n• Skilled at writing performant and reliable SQL queries to source data from Centralized data repositories such as a Data Warehouse.\n• Proficient at designing and developing Relational and Multi-dimensional Databases to host the sourced data and work with Tableau and Power BI developers within and outside the team to support reporting and dashboarding needs of the organization.\n• Develop ETL workflows and macros in Alteryx to source data for projects and build high-level and detailed data validations to ensure accuracy of the sourced data\n• Contribute to initiatives aimed at automating the data extraction, application of data quality checks to ensure datasets are released on time and with 100% accuracy.\n• Develop technical components to ensure department’s compliance with audit requirements such as SOX and other statutory/audit requirements as applicable from time to time.\n• Organize and lead discussions with customers to brainstorm on data quality issues and contribute to discussions to devise business rules to address data quality issues\n• Use MS-Excel and MS-PPT to capture findings and present to customers in an easy-to-understand and impactful manner\n• Liaison with the internal groups in MasterCard Operations and Technology to ensure our solutions remain in compliance with MasterCard technical standards. Navigate O&T requirements around change management and new development.\n\nAll About You:\n\n• Strong understanding of Windows and Linux server.\n• Good understanding of SQL Server or Oracle DB.\n• Solid understanding of Essbase technology – understand how this technology works, for both BSO\nand ASO cubes.\n• Develop BSO and ASO cubes with a strong eye for performance.\n• Strong commitment to quality and error testing code you develop. Strong ability to step in and analyze the code of others on the team.\n• Be able to work within an Agile environment that is highly responsive to the business. Our team is part of the Finance organization – you must be comfortable with working as part of the business with a strong “roll up your sleeves” mentality.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6Ik1hc3RlcmNhcmQiLCJhZGRyZXNzX2NpdHkiOiJQdW5lLCBNYWhhcmFzaHRyYSwgSW5kaWEiLCJodGlkb2NpZCI6IlV1SE5zREk1eXZ2ckFfdVBBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "Data Engineer"
        ],
        [
         "Architect - Data Engineer",
         "PEPSICO",
         "Hyderabad, Telangana, India",
         "Overview\n\nProvide the job title you would like to be displayed on the job posting:\n\nData Platform Engineer – Transformation & Modernization\n\nJob Overview: A Data Platform Engineer to be a key player in our transformation and modernization programs, leading the migration of applications from legacy systems to Azure-based architectures. This role involves designing, implementing, and optimizing scalable, cloud-native data solutions using Databricks, Azure DevOps (ADO), and Agile development methodologies.\n\nAs an active contributor to code development, you will help drive automation, operational excellence, and data quality across our platforms. You will collaborate with data science and product teams to create solutions that enhance our data-driven decision-making capabilities.\n\nResponsibilities\n\nResponsibilities: • Lead the migration and modernization of data platforms, moving applications and pipelines to Azure-based solutions.\n• Actively contribute to code development in projects and services.\n• Manage and scale data pipelines from internal and external data sources to support new product launches and ensure high data quality.\n• Develop automation and monitoring frameworks to capture key metrics and operational KPIs for pipeline performance.\n• Implement best practices around systems integration, security, performance, and data management.\n• Collaborate with internal teams, including data science and product teams, to drive solutioning and proof-of-concept (PoC) discussions.\n• Develop and optimize procedures to transition data into production.\n• Define and manage SLAs for data products and operational processes.\n• Prototype and build scalable solutions for data engineering and analytics.\n• Research and apply state-of-the-art methodologies in data and Platform engineering.\n• Create and maintain technical documentation for knowledge sharing.\n• Develop reusable packages and libraries to enhance development efficiency.\n\nQualifications\n\nQualifications: • Bachelor’s degree in Computer Science, MIS, Business Management, or related field\n• 10 + years’ experience in Information Technology\n• 4 + years of Azure, AWS and Cloud technologies\n• Experience in data platform engineering, with a focus on cloud transformation and modernization.\n• Strong knowledge of Azure services, including Databricks, Azure Data Factory, Synapse Analytics, and Azure DevOps (ADO).\n• Proficiency in SQL, Python, and Spark for data engineering tasks.\n• Hands-on experience building and scaling data pipelines in cloud environments.\n• Experience with CI/CD pipeline management in Azure DevOps (ADO).\n• Understanding of data governance, security, and compliance best practices.\n• Experience working in an Agile development environment.\n• Prior experience in migrating applications from legacy platforms to the cloud.\n• Knowledge of Terraform or Infrastructure-as-Code (IaC) for cloud resource management.\n• Familiarity with Kafka, Event Hubs, or other real-time data streaming solutions.\n• Experience with lagacy RDBMS (Oracl, DB2, Teradata)\n• Background in supporting data science models in production.\n\nDoes the person hired for this job need to be based in a PepsiCo office, or can they be remote?: Employee must be based in a Pepsico office\n\nPrimary Work Location: Hyderabad HUB-IND\n\nIs this role approved for relocation?: No\n\nWould you like to initially post this job internally-only or both internally and externally?: Post both internally and externally",
         "eyJqb2JfdGl0bGUiOiJBcmNoaXRlY3QgLSBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiUGVwc2lDbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNzVXaWFLWW1Xa1V2Vk5VY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Data Engineer"
        ],
        [
         "Senior Analytics Data Engineer",
         "OKTA, INC.",
         "Bengaluru, Karnataka, India",
         "Get to know OktaOkta is The World’s Identity Company. We free everyone to safely use any technology, anywhere, on any device or app. Our flexible and neutral products, Okta Platform and Auth0 Platform, provide secure access, authentication, and automation, placing identity at the core of business security and growth.At Okta, we celebrate a variety of perspectives and experiences. We are not looking for someone who checks every single box - we’re looking for lifelong learners and people who can make us better with their unique experiences. Join our team! We’re building a world where Identity belongs to you.Senior Analytics Engineer\nWe are looking for an experienced Analytics Engineer to join Okta’s enterprise data team. This analyst will have strong background in SaaS subscription and product analytics, a passion for providing customer usage insights to internal stakeholders, and experience organizing complex data into consumable data assets. In this.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHl0aWNzIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJPa3RhLCBJbmMuIiwiYWRkcmVzc19jaXR5IjoiQmVuZ2FsdXJ1LCBLYXJuYXRha2EsIEluZGlhIiwiaHRpZG9jaWQiOiJhZHlINU5lRGFEQWwyUW1aQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "Data Engineer"
        ],
        [
         "Lead Data Engineer - Data Engineering",
         "CENCORA",
         "India",
         "Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\n\nJob Details\n\nPRIMARY DUTIES AND RESPONSIBILITIES:\n• Leads design, development, and automation of scalable Analytic data engineering products leveraging Machine Learning, cloud infrastructure\n• Leads the design, build and operationalization of monitoring and tracking of data quality and data flow dynamics\n• Optimizes existing data processes and implements best-in-class data transformation capabilities\n• Leverages advanced statistical and computational methodologies to deliver insights and identify strategic opportunities\n• Implements Data Ingestion Framework and other re-usable components leveraged for delivering data pipeline and supporting ad-hoc data ingestions\n• Assists with development and storage of analytics-ready data for development of analytic deliverables\n• Recommends data products to solve business problems meeting multiple stakeholder requirements\n• Drives project planning processes, delegates non-complex tasks to junior team members\n• Mentors other team members and assists them with priority setting and issue resolution\n• Maintains data and information environment that provides consistent and accurate reporting of the client’s data\n• Leverages Machine Learning to enhance the developed solution\n• Collaborates with Business and Client representatives to target strategic analytics opportunities and define deliverable scope\n• Facilitates the resolution of issues regarding projects being worked on to include proper documentation of recommendations for those issues\n• Manages multiple initiatives in parallel partnering with other Analytics team members and conducts review meetings with stakeholders to drive prioritization effort\n• Analyzes model errors and design strategies to overcome them\n• Implements best practices for data engineering to ensure quality delivery of enterprise solutions\n• Brings expert knowledge of data visualization tools and techniques to drive business analytics and semantic data access requirements.\n• Work closely with business users, vendors, and delivery teams to understand the business requirements that drive the analysis and design of business analytics and reporting solutions.\n• Conceptualize, design, and develop data visualization solutions that synthesize data concepts into clear communications for key business stakeholders.\n• Drives business stakeholder adoption of insights-driven decision making and/or business process innovation.\n• Leads knowledge transfer around using data visualizations to business stakeholders.\n• Assist in developing best practices for data presentation and sharing across the organization.\n• Ensures data visualization standards are maintained and implemented.\n• Demonstrate an ability to reduce data to the bare minimum of what is needed to optimally communicate a message.\n• Provides technical leadership, coaching and mentoring to team members and business users.\n• Participates in POC projects and provides business analytics solutions recommendations.\n• Evaluates new visualization tools and performs research on best practices.\n• Contributes to strategic planning meetings and provides guidance and expertise on system options, risk, cost vs. benefits, and impacts on business processes and goals.\n• Has expertise in multiple technical environments and possesses business knowledge that spans multiple business areas.\n• Responsible for BI Tool administration & security functions as designated\n\n.\n\nEDUCATIONAL QUALIFICATIONS:\n\nBachelor’s Degree in Statistics, Computer Science, Information Technology or any other related discipline or equivalent related experience.\n\nPreferred Certifications:\n• Advanced Data Analytics Certifications\n• AI and ML Certifications\n• SAS Statistical Business Analyst Professional Certification\n\nWORK EXPERIENCE:\n6+ years of directly-related or relevant experience, preferably in healthcare data analytics or data engineering.\n\nWorking Hours:\n\n7PM IST to 2AM IST; Hybrid Working Model\n\nSKILLS & KNOWLEDGE:\n\nBehavioral Skills:\n• Conflict Resolution\n• Creativity & Innovation\n• Decision Making\n• Planning\n• Presentation Skills\n• Risk-taking\n\nTechnical Skills:\n• Advanced Data Visualization Techniques\n• Advanced Statistical Analysis\n• Big Data Analysis Tools and Techniques\n• Data Governance\n• Data Management\n• Data Modelling\n• Data Quality Assurance\n• Machine Learning and AI Fundamentals\n• Programming languages like SQL, R, Python\n\nTools Knowledge:\n• Business Intelligence Software like Tableau, Power BI, Alteryx, QlikSense\n• Data Visualization Tools\n• Microsoft Office Suite\n• Statistical Analytics tools (SAS, SPSS3)\n\nWhat Cencora offers\n\n​Benefit offerings outside the US may vary by country and will be aligned to local market practice. The eligibility and effective date may differ for some benefits and for team members covered under collective bargaining agreements.\n\nFull time\n\nAffiliated Companies\nAffiliated Companies: CENCORA INDIA TECHNOLOGY SERVICES PRIVATE LIMITED\n\nEqual Employment Opportunity\n\nCencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\n\nThe company’s continued success depends on the full and effective utilization of qualified individuals. Therefore, harassment is prohibited and all matters related to recruiting, training, compensation, benefits, promotions and transfers comply with equal opportunity principles and are non-discriminatory.\n\nCencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com. We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned",
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgLSBEYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiQ2VuY29yYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJHdzRzcW9FbWlPdEVnTDAwQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "3 days ago",
         "Data Engineer"
        ],
        [
         "Azure Data Engineer – Azure Databricks",
         "AIPRUS SOFTWARE PRIVATE LIMITED",
         "Bengaluru, Karnataka, India",
         "Job Title: Azure Data Engineer – Azure Databricks\n\nLocation: Bangalore, India\n\nExperience: 5 to 10 Years\n\nJob Summary:\n\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n\nKey Responsibilities:\n• Design and develop scalable data pipelines using Azure Databricks and PySpark.\n• Transform raw data into actionable insights through advanced data engineering techniques.\n• Build, deploy, and maintain machine learning models using MLlib, TensorFlow, and MLflow.\n• Optimize data integration workflows from Azure Blob Storage, Data Lake, and SQL/NoSQL sources.\n• Execute large-scale data processing using Spark Pools, fine-tuning configurations for performance and cost-efficiency.\n• Collaborate with data scientists, analysts, and business stakeholders to deliver robust data solutions.\n• Maintain and enhance Databricks notebooks and Delta Lake architectures.\n\nRequired Skills & Qualifications:\n• Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field.\n• 7+ years of experience in data engineering, with at least 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n• Strong proficiency in:\n• Python, PySpark, Pandas, NumPy, SciPy\n• Spark SQL, DataFrames, RDDs\n• Delta Lake, Databricks Notebooks, MLflow\n• Hands-on experience with:\n• Azure Data Lake, Blob Storage, Synapse Analytics\n• Excellent problem-solving and communication skills.\n• Ability to work independently and in a collaborative team environment.\n\nPreferred Qualifications:\n• Experience with CI/CD pipelines for data workflows.\n• Familiarity with data governance and security best practices in Azure.\n• Knowledge of real-time data processing and streaming technologies.",
         "eyJqb2JfdGl0bGUiOiJBenVyZSBEYXRhIEVuZ2luZWVyIOKAkyBBenVyZSBEYXRhYnJpY2tzIiwiY29tcGFueV9uYW1lIjoiQWlwcnVzIFNvZnR3YXJlIFByaXZhdGUgTGltaXRlZCIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiek5tRUxOMG5Iam5zazl6TkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 hours ago",
         "Data Engineer"
        ],
        [
         "Principle Software Engineer for Data Platform - 31866",
         "SPLUNK",
         "Bengaluru, Karnataka, India",
         "Splunk, a Cisco company, is building a safer and more resilient digital world with an end-to-end full stack platform made for a hybrid, multi-cloud world. Leading enterprises use our unified security and observability platform to keep their digital systems secure and reliable. Our customers love our technology, but it's our caring employees that make Splunk stand out as an amazing career destination. No matter where in the world or what level of the organization, we approach our work with kindness. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Come help organizations be their best, while you reach new heights with a team that has your back.Role SummarySplunk AppDynamics, a leader in observability solutions for both cloud and on-premises environments, empowers customers with end-to-end visibility across their entire application ecosystems, cloud, on-premises, or hybrid. Our team is dedicated to developing, maintaining, and supporting SaaS and on-premises offerings within the observability product suite.We're looking for a Senior Software Engineer to build upon our application and data platform as we continue to innovate on application observability. We move fast and iterate quickly. We are passionate about solving customers’ problems. We have ambitious goals to build best-of-its-kind products. You will help the team win in a fast-growing market. If you are passionate about innovation and embrace the challenge of working on highly scalable systems that handle large volumes of data, this position is for you.Meet the Team (Optional)Data platform is the heart of the AppDynamics architecture. We process more than 100 billion data packets per day, more than 5 peta byte of data flows through our platform. We are in 8 regions having 40 clusters of 7000 nodes. We ingest, process and store all the data that power the dashboards that our customers rely on for observing their infrastructure and their applications.What you'll get to do\n• Design and build highly scalable solutions\n• Work with a team of exceptionally capable and dedicated peers, all the way from engineering to product management and customer support\n• Work in an open environment, work together to get things done and adapt to the team's changing needs\n• Leverage technologies including Kafka, ElasticSearch, Docker, and Kubernetes across different cloud environments like AWS and Azure\n• lead critical initiatives for the organisation\nMust-have Qualifications\n• 15+ years of full-stack developer experience in designing and developing highly scalable, distributed applications, products, and services.\n• Expertise in Java programming language.\n• Strong proficiency in data structures, algorithms, threads, concurrent programming\n• Extensive knowledge of SQL and at least one relational database engine: MySQL. Hands on experience in RDS or NoSQL (Dynamo, MongoDB) is a big plus\n• Experience of building applications using microservices architecture with expertise in Dockers and Kubernetes.\n• Strong communication skills, both verbal and written. Ability to multi-task and adapt quickly to changing requirements, scope, and priorities.\n• Optimize data pipelines, storage systems, and query engines for performance and efficiency.\n• Work closely with product teams, data engineers, software developers, and product managers to deliver on business goals.\n• Mentor team members in architecture principles, coding best practices, and system design.\n• Ensure robust monitoring, logging, and alerting systems for proactive issue detection and resolution.\n• Support CI/CD processes and automate testing for data systems\n• Stay abreast of emerging trends in big data, machine learning, and distributed systems to recommend innovative solutions.\nNice-to-have QualificationsWe’ve taken special care to separate the must-have qualifications from the nice-to-haves. “Nice-to-have” means just that: Nice. To. Have. So, don’t worry if you can’t check off every box. We’re not hiring a list of bullet points–we’re interested in the whole you.\n• Added advantage of having an experience in working on Cloud Observability Space.\n• experience of other languages like python, etc\n• experience of front-end technologies\nSplunk is an Equal Opportunity EmployerSplunk, a Cisco company, is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.\n\nNote:",
         "eyJqb2JfdGl0bGUiOiJQcmluY2lwbGUgU29mdHdhcmUgRW5naW5lZXIgZm9yIERhdGEgUGxhdGZvcm0gLSAzMTg2NiIsImNvbXBhbnlfbmFtZSI6IlNwbHVuayIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiY2k4emNxSnNNanNSemNYUEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "9 days ago",
         "Data Engineer"
        ],
        [
         "Software Developer- Python",
         "BNP PARIBAS INDIA SOLUTIONS",
         "India",
         "About BNP Paribas India Solutions:\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n\nAbout BNP Paribas Group:\n\nBNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n\nCommitment to Diversity and Inclusion\n\nAt BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n\nAbout Business line/Function:\n\nThe Intermediate Holding Company (“IHC”) program structured at the U.S. level across poles of activities of BNP Paribas provides guidance, supports the analysis, impact assessment and drives adjustments of the U.S. platform’s operating model due to the drastic changes introduced by the Enhanced Prudential Standards (“EPS”) for Foreign Banking Organizations (“FBOs”) finalized by the Federal Reserve in February 2014, implementing Section 165 of U.S. Dodd-Frank Act.\n\nThe IT Transversal Team is part of the Information Technology Group which works simultaneously on a wide range of projects arising from business, strategic initiatives, and regulatory changes and reengineering of existing applications to improve functionality and efficiency.\n\nJob Title:\n\nPython Developer\n\nDate:\n\nJune-25\n\nDepartment:\n\nITG- Fresh\n\nLocation:\n\nChennai, Mumbai\n\nBusiness Line / Function:\n\nFinance Dedicated Solutions\n\nReports to:\n\n(Direct)\n\nGrade:\n\n(if applicable)\n\n(Functional)\n\nNumber of Direct Reports:\n\nNA\n\nDirectorship / Registration:\n\nNA\nPosition Purpose\n\nThe Python Developer will play a critical role in building and maintaining financial applications and tools that support data processing, analysis, and reporting within a fast-paced financial services environment. This position involves developing scalable and secure systems. The developer will collaborate with business analysts, finance users/or finance BA to translate complex business requirements into efficient, high-quality software solutions.\n\nA strong understanding of financial concepts, data integrity, and regulatory compliance is essential. The detailed responsibilities are mentioned below.\n\nResponsibilities\n\nDirect Responsibilities\n\n- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n\n- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n\n- Expertise in PySpark for large-scale data processing and loading into databases.\n\n- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n\n- Strong communication skills to effectively collaborate with team members and stakeholders.\n\n- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\nTechnical & Behavioral Competencies\n\n- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n\n- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n\n- Expertise in PySpark for large-scale data processing and loading into databases.\n\n- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n\n- Strong communication skills to effectively collaborate with team members and stakeholders.\n\n- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n\n- Good analytical, problem solving, & communication skills\n\n- Engage in technical discussions and to help in improving the system, process etc\n\nNice to Have\n\n- Familiarity with Plotly and Matplotlib for data visualization of large datasets.\n\n- Skilled in API programming, handling JSON, CSV, and other unstructured data from various systems.\n\n- Familiarity with JavaScript, CSS, and HTML.\n\n- Experience with cloud architecture applications such as Dataiku or Databricks; competency with ETL tools.\n\n- Knowledge of regulatory frameworks, RISK, CCAR, and GDPR.\nSpecific Qualifications (if required)\n\nSkills Referential\n\nBehavioural Skills: (Please select up to 4 skills)\n\nAbility to collaborate / Teamwork\n\nCritical thinking\n\nAbility to deliver / Results driven\n\nCommunication skills - oral & written\n\nTransversal Skills: (Please select up to 5 skills)\n\nAnalytical Ability\n\nAbility to develop and adapt a process\n\nAbility to understand, explain and support change\n\nAbility to develop others & improve their skills\n\nChoose an item.\n\nEducation Level:\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 5 years\n\nOther/Specific Qualifications (if required)",
         "eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBEZXZlbG9wZXItIFB5dGhvbiIsImNvbXBhbnlfbmFtZSI6IkJOUCBQYXJpYmFzIEluZGlhIFNvbHV0aW9ucyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ6VTdCX1R2emRzczk0M0ZoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "7 days ago",
         "Python Developer"
        ],
        [
         "Freelance Python Developer",
         "TEQLAWN",
         "Anywhere",
         "We’re looking for an experienced Python Developer (4–7 years) to build scalable web and application solutions, with opportunities to integrate AI capabilities. Experience in AI integration is a must.\n\nResponsibilities:\n• Develop scalable web and application solutions using Python, with integration of AI/ML components\n• Collaborate with clients to understand project goals and technical requirements\n• Write clean, maintainable, and well-documented code\n• Troubleshoot, debug, and optimize applications for performance and reliability\n• Ensure timely and efficient delivery of milestones and final deliverables\n• Participate in code reviews and contribute to maintaining coding standards and best practices\n• Work with relevant frameworks and libraries such as Django, Flask, FastAPI, NumPy, pandas, and scikit-learn\n\nNote: Please share the link to your portfolio along with your application.\n\nJob Types: Full-time, Contractual / Temporary, Freelance\nContract length: 2 months\n\nPay: ₹50,000.00 - ₹80,000.00 per month\n\nBenefits:\n• Work from home\n\nLocation Type:\n• Remote\n\nSchedule:\n• Day shift\n• Monday to Friday\n\nExperience:\n• Python Development: 4 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlRlcWxhd24iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiY20xcUU4aHhjbFQ3dnVyT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "1 day ago",
         "Python Developer"
        ],
        [
         "Python Developer — Full-Time | 1-2 years Exp | In-Office | Bangalore",
         "SERP HAWK",
         "India",
         "\uD83D\uDE80 We’re Hiring: Python Developer\n\nSERP Hawk is looking for a talented Python Developer to join our dynamic team in Bangalore!\n\n\uD83C\uDF1F About Us\n\nSERP Hawk is a leading digital marketing and technology solutions company based in Bangalore. We specialize in building robust backend systems, scalable applications, and AI-powered solutions for clients across various industries.\n\n\uD83D\uDCCD Location:B-1, Bannerghatta Slip Road, KEB Colony, New Gurappana Palya, 1st Stage, BTM Layout 1, Bengaluru, Karnataka 560029\n\n\uD83C\uDF10 Website: www.serphawk.com\n\n\uD83D\uDCBC What You’ll Do\n• Design and develop scalable backend architectures.\n• Write clean, efficient Python code.\n• Integrate APIs and databases.\n• Implement CI/CD pipelines and automated tests.\n• Ensure high performance, security, and reliability.\n\n✅ What We’re Looking For\n\n✔️ 1–2 years of experience in Python development.\n\n✔️ Proficiency in frameworks like Django, Flask, or FastAPI.\n\n✔️ Strong understanding of APIs and databases.\n\n✔️ Experience with CI/CD tools and best practices.\n\n✔️ Excellent problem-solving skills and a collaborative mindset.\n\n\uD83D\uDCA1 Nice to Have\n\n⭐ Experience with AI/chatbots.\n\n⭐ Knowledge of cloud services and containerization.\n\n\uD83D\uDCB0 Salary\n• ₹20,000 – ₹25,000 per month (based on skills and experience).\n\n\uD83D\uDCCC Additional Details\n\n\uD83D\uDCBC This is a full-time, in-office role in Bangalore.\n\n\uD83C\uDFE2 Candidates must report to the office daily.\n\n\uD83C\uDF10 Should be flexible to work and attend meetings or calls as per client time zones.\n\n✨ Apply now and grow with us!",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAlCBGdWxsLVRpbWUgfCAxLTIgeWVhcnMgRXhwIHwgSW4tT2ZmaWNlIHwgQmFuZ2Fsb3JlIiwiY29tcGFueV9uYW1lIjoiU0VSUCBIYXdrIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjNBMWtiUmowZFhsRVZsWnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 hours ago",
         "Python Developer"
        ],
        [
         "SQL + Python",
         "WISSEN TECHNOLOGY",
         "India",
         "Wissen Technology is Hiring for SQL With Python\n\nAbout Wissen Technology:\n\nWissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges.\n\nRole Overview:\n\nWe are looking for a skilled and detail-oriented candidate with a strong foundation in SQL, Python, and data processing techniques. The ideal candidate is passionate about transforming raw data into meaningful insights and has hands-on experience across the data pipeline—from data wrangling to visualization.\n\nExperience: 3-7 Years\n\nLocation: Bengaluru\n\nRequired Skills:\n• Strong experience with SQL (e.g., joins, subqueries, CTEs, window functions).\n• Proficiency in Python for data manipulation (e.g., pandas, NumPy).\n• Experience working with relational databases like MySQL, PostgreSQL, SQL Server, or Oracle.\n• Hands-on experience in data wrangling, cleaning, and feature engineering.\n• Understanding of ETL processes and tools.\n• Familiarity with version control systems like Git.\n• Knowledge of data visualization techniques and tools.\n• Strong problem-solving and analytical skills.\n\nThe Wissen Group was founded in the year 2000. Wissen Technology, a part of Wissen Group, was established in the year 2015. Wissen Technology is a specialized technology company that delivers high-end consulting for organizations in the Banking & Finance, Telecom, and Healthcare domains. We help clients build world class products.\n\nWe offer an array of services including Core Business Application Development, Artificial Intelligence & Machine Learning, Big Data & Analytics, Visualization & Business Intelligence, Robotic Process Automation, Cloud Adoption, Mobility, Digital Adoption, Agile & DevOps, Quality Assurance & Test Automation.\n\nOver the years, Wissen Group has successfully delivered $1 billion worth of projects for more than 20 of the Fortune 500 companies. Wissen Technology provides exceptional value in mission critical projects for its clients, through thought leadership, ownership, and assured on-time deliveries that are always ‘first time right’.\n\nThe technology and thought leadership that the company commands in the industry is the direct result of the kind of people Wissen has been able to attract. Wissen is committed to providing them with the best possible opportunities and careers, which extends to providing the best possible experience and value to our clients.\n\nWe have been certified as a Great Place to Work® company for two consecutive years (2020-2022) and voted as the Top 20 AI/ML vendor by CIO Insider. Great Place to Work® Certification is recognized world over by employees and employers alike and is considered the ‘Gold Standard’. Wissen Technology has created a Great Place to Work by excelling in all dimensions - High-Trust, High-Performance Culture, Credibility, Respect, Fairness, Pride and Camaraderie.\n\nWebsite: www.wissen.com\n\nLinkedIn: https://www.linkedin.com/company/wissen-technology\n\nWissen Leadership: https://www.wissen.com/company/leadership-team/\n\nWissen Live: https://www.linkedin.com/company/wissen-technology/posts/feedView=All\n\nWissen Thought Leadership: https://www.wissen.com/articles/\n\nEmployee Speak:\n\nhttps://www.ambitionbox.com/overview/wissen-technology-overview\n\nhttps://www.glassdoor.com/Reviews/Wissen-Infotech-Reviews-E287365.htm\n\nGreat Place to Work:\n\nhttps://www.wissen.com/blog/wissen-is-a-great-place-to-work-says-the-great-place-to-work-institute-india/\n\nhttps://www.linkedin.com/posts/wissen-infotech_wissen-leadership-wissenites-activity-6935459546131763200-xF2k\n\nAbout Wissen Interview Process:\n\nhttps://www.wissen.com/blog/we-work-on-highly-complex-technology-projects-here-is-how-it-changes-whom-we-hire/\n\nLatest in Wissen in CIO Insider:\n\nhttps://www.cioinsiderindia.com/vendor/wissen-technology-setting-new-benchmarks-in-technology-consulting-cid-1064.html",
         "eyJqb2JfdGl0bGUiOiJTUUwgKyBQeXRob24iLCJjb21wYW55X25hbWUiOiJXaXNzZW4gVGVjaG5vbG9neSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJITW5vZUwxWlF1QWVma0hlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "7 hours ago",
         "Python Developer"
        ],
        [
         "Python Developer Full time",
         "VARIANCE TECHNOLOGIES PRIVATE LIMITED",
         "Anywhere",
         "Job Opportunity: Python Developer at Variance Technologies Private Limited!\n\nRole: Python Developer\n\nDuration: 1 Months\n\nLocation: Hybrid / Remote\n\nResponsibilities:\n\nCollaborate with our development team to develop and maintain Python-based applications tailored for finance-related tasks\n\nImplement object-oriented programming principles to ensure the scalability and maintainability of codebase\n\nGain hands-on experience with Pandas for data manipulation and analysis within finance contexts\n\nSupport integration of REST and WebSocket APIs for seamless communication with financial data providers and platforms\n\nAssist in automating financial modeling and analysis workflows using Python, including Excel automation for enhanced efficiency\n\nRequirements:\n\nCurrently pursuing or recently completed a degree in Computer Science, Finance, or a related field\n\nBasic proficiency in Python programming language, with a strong willingness to learn and grow\n\nExceptional attention to detail and proactive attitude towards problem-solving\n\nGenuine interest in the intersection of finance and technology\n\nBonus Skills:\n\nFamiliarity with fundamental financial concepts and markets\n\nExposure to Python libraries such as Pandas, NumPy, or SciPy\n\nDemonstrated interest in financial data analysis and visualization techniques\n\nBasic understanding of REST and WebSocket APIs\n\nPerks:\n\nHands-on experience working on real-world projects at the forefront of finance and technology\n\nMentorship and guidance from seasoned professionals in the field\n\nNetworking opportunities with industry experts to expand your professional connections\n\nFlexible scheduling to accommodate academic commitments\n\nPotential for transition to a full-time position based on exceptional performance and availability\n\nReady to kickstart your career with a prestigious 1-month internship at Variance Technologies Private Limited? Apply now by sending your resume and a tailored cover letter expressing your interest and qualifications to careers@variancefintech.com. Join our team and embark on a journey towards shaping the future of finance through technology!\n\nVariance Technologies Private Limited is committed to fostering a diverse and inclusive workplace where all individuals are empowered to thrive. We welcome applicants from all backgrounds and identities to apply.\n\nJob Type: Full-time\n\nPay: From ₹35,000.00 per month\n\nBenefits:\n• Work from home\n\nSchedule:\n• Monday to Friday\n\nEducation:\n• Bachelor's (Preferred)\n\nExperience:\n• Python: 1 year (Preferred)\n• total work: 1 year (Preferred)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIEZ1bGwgdGltZSIsImNvbXBhbnlfbmFtZSI6IlZhcmlhbmNlIFRlY2hub2xvZ2llcyBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiaXZCN3JUQTN5YnkxRzV2MUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "5 days ago",
         "Python Developer"
        ],
        [
         "Python Developer – AI & LLM Integrations",
         "DISCOVER WEBTECH PRIVATE LIMITED",
         "India",
         "We are hiring a skilled Python Developer with a minimum of 3 years of experience, who has hands-on expertise in developing AI modules, building intelligent agents using LLMs, and working with cutting-edge frameworks such as LangChain, LangGraph, and LangSmith.\n\nThe ideal candidate should be capable of designing, developing, and deploying backend services as well as intelligent AI-driven tools and systems.\n\nKey Responsibilities\n• Design and implement intelligent agents using LLM-based frameworks like LangChain, LangGraph, and LangSmith.\n• Build backend systems using Python (Django, FastAPI, or Flask).\n• Develop and integrate APIs, third-party tools, and cloud services.\n• Create AI modules that interact with knowledge bases, APIs, and perform multi-step reasoning.\n• Implement prompt engineering, memory chains, and agent behavior logic.\n• Collaborate with cross-functional teams to deliver robust AI features.\n• Optimize code for scalability, performance, and reliability.\n\nRequired Skills and Qualifications\n• 3+ years of hands-on experience with Python.\n• Proficiency in LangChain, LangGraph, or LangSmith.\n• Strong experience in working with LLMs like OpenAI, Cohere, or Anthropic.\n• Deep understanding of prompt engineering and agent orchestration.\n• Experience with APIs, JSON, and external integrations.\n• Knowledge of data storage systems (PostgreSQL, MongoDB).\n• Familiarity with Docker, Git, and CI/CD tools.\n• Excellent problem-solving and debugging skills.\n\nPreferred Qualifications\n• Knowledge of vector databases (e.g., Pinecone, Weaviate, FAISS).\n• Experience with microservices, asynchronous programming, and message queues (Celery, RabbitMQ).\n• Familiarity with frontend technologies (React.js or Vue.js) is a plus.\n• Exposure to cloud platforms such as AWS, GCP, or Azure.\n\nJob Types: Full-time, Permanent\n\nPay: ₹30,000.00 - ₹70,000.00 per month\n\nBenefits:\n• Health insurance\n\nSchedule:\n• Day shift\n\nWork Location: In person",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBBSSBcdTAwMjYgTExNIEludGVncmF0aW9ucyIsImNvbXBhbnlfbmFtZSI6IkRpc2NvdmVyIFdlYlRlY2ggUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Im1JUlJqRWJSV0pCSkdQR0VBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "3 days ago",
         "Python Developer"
        ],
        [
         "Full Stack Developer (Python / React JS)",
         "HITACHI CAREERS",
         "India",
         "Our Company\n\nWe're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n\nOur group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n\nImagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n\nThe team\n\nWe are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n\nThe role: Full Stack Developer/Specialist\n\nResponsibilities:\n• Design, develop, and maintain applications.\n• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n• Optimise applications for performance, scalability, and user experience.\n• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n• Provide training and support to end-users and IT staff on functionalities and best practices.\n• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n• Participate in project planning, execution, and post-implementation support.\n• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n• What you'll bring\nQualifications:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n• Expertise in development and customisation.\n• Proficiency in Python and React JS\n• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n• Experience with RESTful APIs and web services.\n• Excellent problem-solving skills and attention to detail.\n• Strong communication and collaboration skills.\n• Ability to work independently and as part of a team in a fast-paced environment.\n• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n• Prior experience in building AI applications is a plus.\n• Prior experience with automation tools like UIPath is a plus.\n• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\nPreferred Skills\n• Certification in\n• Experience with other low-code/no-code platforms.\n• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n• Experience with Agile development methodologies.\nAbout us\n\nWe're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n\n#LI-MS3\n\nChampioning diversity, equity, and inclusion\n\nDiversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n\nHow we look after you\n\nWe help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\nWe're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",
         "eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "14 days ago",
         "Python Developer"
        ],
        [
         "Python Back-End Developer",
         "GOODYEAR",
         "India",
         "Location: IN - Hyderabad Telangana\n\nGoodyear Talent Acquisition Representative: M Bhavya Sree\n\nSponsorship Available: No\n\nRelocation Assistance Available: No\n\nDuties and Responsibilities:\n\n• Develop and support Data Driven applications\n\n• Help design and develop back-end services and APIs for data-driven applications and simulations.\n\n• Work with our technical partners to collaborate on system requirements and data integration needs for our new applications.\n\n• Support the deployment and scaling of new back-end technologies and cloud-native architectures within the organization.\n\n• Work closely with our data scientists to support model deployment into production environments.\n\n• Develop and maintain server-side components for digital tools and products using Python or other modern back-end technologies and frameworks. Build scalable, secure, and efficient services that support a seamless experience across multiple platforms.\n\n• Design, implement, and maintain robust database systems (SQL and NoSQL), ensuring high availability and performance for critical applications.\n\n• Contribute to DevOps practices including CI/CD pipelines, infrastructure as code, containerization (Docker), and orchestration (Kubernetes).\n\n• Learn about the tire industry and tire manufacturing processes from subject matter experts.\n\n• Be a part of cross-functional teams working together to deliver impactful results.\n\nSkills Required:\n\n• Significant experience in server-side development using Python\n\n• Strong understanding of RESTful API design, microservices architecture, and service-oriented design\n\n• Experience with relational and non-relational databases such as PostgreSQL, MySQL, MongoDB, or DynamoDB\n\n• Application of software design skills and methodologies (algorithms, data structures, design patterns, software architecture and testing)\n\n• Hands-on experience working with cloud platforms such as AWS, Microsoft Azure, or Google Cloud Platform\n\n• Good teamwork skills - ability to work in a team environment and deliver results on time.\n\n• Strong communication skills - capable of conveying information concisely to diverse audiences.\n\n• Exposure to DevOps practices including CI/CD pipelines (e.g., GitHub Actions, Jenkins), containerization (e.g., Docker), and orchestration tools (e.g., Kubernetes)\n\n• Familiarity with front-end technologies like React, HTML, CSS, and JavaScript for API integration purposes\n\nGoodyear is an Equal Employment Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to that individual's race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender identity, age, physical or mental disability, ethnicity, citizenship, or any other characteristic protected by law.\n\nGoodyear is one of the world’s largest tire companies. It employs about 68,000 people and manufactures its products in 53 facilities in 20 countries around the world. Its two Innovation Centers in Akron, Ohio and Colmar-Berg, Luxembourg strive to develop state-of-the-art products and services that set the technology and performance standard for the industry. For more information about Goodyear and its products, go to www.goodyear.com/corporate\n\n#Li-Hybrid",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gQmFjay1FbmQgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiR29vZHllYXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoid0VrdmdmMVREWEhlTFhuQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Python Developer"
        ],
        [
         "Generative AI & Backend Developer(python)",
         "INTELLYPOD",
         "Anywhere",
         "Job Description (JD) For Gen Ai with Python:\n\nWe're Hiring: GenAI & Backend Developer (Python)\n\nWork Location: Remote (Work From Home)\n\nExperience: 2+ Years\n\nImmediate Joiners Preferred\n\nCompany: IntellyPod\n\nApply at: hrd@intellypod.com | hr@intellypod.com\n\nAbout the Role:\n\nIntellyPod is looking for a passionate Backend Developer with hands-on experience in GenAI, LLMs, and Python. If you’re excited about building scalable APIs, integrating AI models, and working with the latest in backend and GenAI technologies — we’d love to connect!\n\nKey Responsibilities:\n\n· Develop and maintain Python-based backend services.\n\n· Design and implement RESTful APIs.\n\n· Integrate GenAI/LLM solutions into applications.\n\n· Manage and optimize SQL/NoSQL databases.\n\n· Collaborate with cross-functional tech teams.\n\nMust-Have Skills:\n\n· 2+ years of experience in backend development (Python).\n\n· Experience with GenAI and Large Language Models (e.g., GPT, LLaMA).\n\n· Strong knowledge of REST APIs and database design.\n\n· Familiarity with Git and backend architecture best practices.\n\nNeed to Have:\n\n· Experience with AWS/GCP/Azure.\n\n· Docker, Kubernetes, or CI/CD exposure.\n\n· Familiarity with vector databases (e.g., Pinecone, FAISS).\n\n· Prompt engineering or LLM fine-tuning knowledge.\n\nWhy Join Us?\n\n· 100% Remote – Flexible work setup\n\n· Work on next-gen AI products\n\n· Fast-growing, collaborative tech team\n\n· Opportunity to innovate with emerging AI tools\n\nReady to build the future with us? Send your resume to: hrd@intellypod.com | hr@intellypod.com #GenAI #BackendDeveloper #PythonJobs #LLM #RemoteJob #HiringNow #IntellyPod\n\nJob Type: Full-time\n\nPay: Up to ₹70,000.00 per month\n\nLocation Type:\n• Remote\n\nApplication Question(s):\n• Are an immediate joiner -\n\nAre on notice period if yes [Then how many days]\n• Write YES or NO\n\n1) Need to ask have you worked on LLM based project -\n\n2) Have you worked on chatbot types apps -\n\n3) Have you strong knowleged of OOps and Python basic -\n\n4) Have you knowledge of Rest APi development -\n\nExperience:\n• 5G: 3 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJHZW5lcmF0aXZlIEFJIFx1MDAyNiBCYWNrZW5kIERldmVsb3BlcihweXRob24pIiwiY29tcGFueV9uYW1lIjoiSW50ZWxseXBvZCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiI4ekp1Sk9ZUFlTbWtOcFRuQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "5 hours ago",
         "Python Developer"
        ],
        [
         "Junior Python Developer",
         "DEHAZELABS",
         "Anywhere",
         "Location: Onsite, Kokapet, Hyderabad, Telangana.\n\nJob Type: Full-Time\n\nAbout Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n\nResponsibilities:\n• Develop and maintain Python applications and services\n• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n• Write clean, efficient, and well-documented code\n• Troubleshoot, debug, and optimize applications for performance and scalability\n• Participate in code reviews and maintain coding standards\n• Stay up-to-date with the latest industry trends and technologies\n\nRequirements:\n• Ability to code in Python and SQL\n• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n• Knowledge of version control systems (e.g., Git)\n• Excellent problem-solving skills and attention to detail\n• Strong communication and teamwork abilities.\n\nPreferred Qualifications:\n• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n• Knowledge of RESTful APIs and microservices architecture.",
         "eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Python Developer"
        ],
        [
         "Etl Developer",
         "VIVID RESOURCING",
         "Bilaspur, Chhattisgarh, India",
         "Job Title:\nData Engineer / ETL Developer\n\nLocation:\nUS, remote from India\n\nDepartment:\nIT / Data & Analytics\n\nReports To:\nHead of Data & Analytics\n\nEmployment Type:\nContract, 12 months\n\nPay:\n$28-35 per hour payrolled or self-employed\n\nAbout the Role\n\nWe are seeking a Data Engineer / ETL Developer to help design, build, and maintain data pipelines that support our internal reporting and analytics needs. This role will focus on integrating data from systems such as ERP, MES , and production databases into a centralized data platform to enable reliable and timely insights across the business. You will work alongside BI developers, IT staff, and business users to ensure the smooth flow of data and contribute to the foundation of a modern business intelligence environment.\n\nKey Responsibilities\n\nETL & Data Pipeline Development\n• Develop and maintain ETL/ELT pipelines to move data from source systems (ERP, MES, SQL databases, flat files) into our centralized platform.\n• Use tools such as Azure Data Factory , SSIS , or similar to orchestrate and automate data workflows.\n\nData Modeling & Integration\n• Assist in designing and building data models and data marts optimized for Power BI dashboards.\n• Support the creation and maintenance of dataflows and datasets in Power BI Service.\n\nData Quality & Documentation\n• Implement data validation, transformation, and cleansing logic to ensure high-quality, reliable data.\n• Document data processes, business rules, and data mappings to support knowledge sharing and governance.\n\nCross-Functional Collaboration\n• Work with internal teams (production, quality, operations, finance) to understand data requirements and reporting needs.\n• Collaborate with BI developers to ensure the data pipeline supports efficient and user-friendly reporting.\n\nRequired Qualifications\n• Bachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\n• 2+ years of experience in data engineering or ETL development roles.\n• Solid experience with SQL and ETL tools (e.G., Azure Data Factory , SSIS , Informatica , etc.).\n• Familiarity with Power BI , including working with datasets, dataflows, or basic DAX.\n• Understanding of data integration, transformation, and warehousing concepts.\n• Experience working with or integrating manufacturing systems (ERP, MES) is a strong plus.\n\nPreferred Skills\n• Experience with cloud data platforms (especially Microsoft Azure ).\n• Exposure to Python or scripting for automation.\n• Familiarity with data governance and documentation practices.\n• Experience with manufacturing environments or industrial data is beneficial.\n\nSoft Skills\n• Strong attention to detail and a logical, structured approach to problem-solving.\n• Willingness to learn and grow in a fast-paced environment.\n• Good communication and collaboration skills across technical and non-technical teams.\n• Proactive and solutions-oriented mindset.",
         "eyJqb2JfdGl0bGUiOiJFdGwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiVml2aWQgUmVzb3VyY2luZyIsImFkZHJlc3NfY2l0eSI6IkJpbGFzcHVyLCBDaGhhdHRpc2dhcmgsIEluZGlhIiwiaHRpZG9jaWQiOiJueDl3cW1oMXpfcnBSX2dFQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "14 hours ago",
         "ETL Developer"
        ],
        [
         "Senior ETL and Backend Developer (Salesforce)",
         "S&P GLOBAL",
         "Hyderabad, Telangana, India (+1 other)",
         "About the Role:\n\nGrade Level (for internal use):\n10\n\nTitle: Senior ETL and Backend Developer (Salesforce)\n\nJob Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n\nThe Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n\nThe team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n\nThe Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n\nResponsibilities:\n• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\nMonitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\nQualifications:\n\nBasic Qualifications:\n• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n• A minimum of 8+ years of experience in software engineering & Architecture.\n• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n• A minimum of 3+ years of Salesforce development, administration/Integration.\n• Proficiency in Informatica PowerCenter and other ETL tools.\n• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n• Experience with Salesforce integration and administration.\n• Proficiency in backend development languages (e.g., Java, Python, C#).\n• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n• Excellent problem-solving skills and attention to detail.\n• Ability to work independently and as part of a team.\n• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n\nPreferred Qualifications:\n• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n• Experience with other ETL tools and data integration platforms.\n• Informatica Certified Professional\nSalesforce Certified Administrator or Developer\n• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n• Excellent problem solving, analytical and technical troubleshooting skills.\n• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat’s In It For You?\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nWe're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n\nOur benefits include:\n• Health & Wellness: Health care coverage designed for the mind and body.\n• Flexible Downtime: Generous time off helps keep you energized for your time on.\n• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring and Opportunity at S&P Global:\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\n-----------------------------------------------------------\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n-----------------------------------------------------------\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n\nJob ID: 316835\nPosted On: 2025-06-03\nLocation: Hyderabad, Telangana, India",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjJlUWpKemNOclM4MXR6QzhBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "30 days ago",
         "ETL Developer"
        ],
        [
         "Senior Etl Developer",
         "VIVID RESOURCING",
         "Bilaspur, Chhattisgarh, India",
         "Job Title:\nSenior Data Engineer / ETL Developer\n\nLocation:\nUS, from India\n\nDepartment:\nIT / Data & Analytics\n\nReports To:\nDirector of Data & Analytics\n\nEmployment Type:\nContract, 12 months\n\nPay:\n$30-38 per hour payrolled or self-employed\n\nAbout the Role\n\nWe are seeking a skilled and motivated Senior Data Engineer / ETL Developer to lead the design and implementation of end-to-end data integration solutions powering this new platform. You will work closely with IT, business stakeholders, and BI developers to build scalable data pipelines, robust data models, and a high-performance reporting environment centered around Power BI.\n\nThis role is a key contributor in shaping our data infrastructure and delivering timely, accurate insights across the organization—from the shop floor to the boardroom.\n\nKey Responsibilities\n\nData Engineering & Integration\n• Design, build, and maintain scalable ETL/ELT pipelines to extract, transform, and load data from diverse manufacturing systems (ERP, MES, IoT sensors, SQL databases, flat files, APIs).\n• Develop and manage data workflows using tools such as Azure Data Factory, SQL Server Integration Services (SSIS), or other modern data orchestration platforms.\n\nPlatform & Architecture Support\n• Contribute to the design and architecture of the new internal analytics platform, ensuring flexibility, scalability, and cost-efficiency.\n• Design and optimize data lake and data warehouse solutions using cloud-native tools (preferably Microsoft Azure).\n\nPower BI Enablement\n• Build and optimize semantic data models, dataflows, and datasets for use in Power BI.\n• Collaborate with Power BI developers and business analysts to ensure data models meet reporting requirements, including DAX performance tuning and efficient data structure design.\n\nData Governance & Quality\n• Implement data validation, cleansing, and monitoring processes to ensure data quality, reliability, and accuracy.\n• Contribute to data governance policies, including documentation, lineage tracking, and security controls.\n\nCollaboration & Mentorship\n• Work closely with cross-functional teams (IT, production, quality, finance, supply chain) to gather data requirements and deliver meaningful solutions.\n• Provide technical leadership and mentor junior developers or analysts in best practices for data engineering and analytics.\n\nRequired Qualifications\n• Bachelor’s or Master’s degree in Computer Science, Information Systems, Engineering, or a related field.\n• 5+ years of experience in data engineering, ETL development, or database architecture roles.\n• Proven experience with ETL tools such as Azure Data Factory, SSIS, or Informatica.\n• Advanced SQL skills, including performance tuning, stored procedures, and query optimization.\n• Strong experience with Power BI, including DAX, data modeling, and dataset optimization.\n• Experience with data warehousing (e.G., Azure Synapse, SQL Server, Snowflake) and cloud platforms (preferably Azure).\n• Understanding of manufacturing systems and data (e.G., ERP, MES, shop floor data, SCADA, historians).\n\nPreferred Skills\n• Experience with Python or other scripting languages for automation and data manipulation.\n• Familiarity with time-series data and integration from IoT or edge devices.\n• Knowledge of DevOps practices, CI/CD for data pipelines, and version control using Git.\n• Exposure to data governance frameworks and tools like Purview, Alation, or Collibra.\n• Power BI Service administration experience and integration with Power Platform (e.G., Power Automate, Power Apps)\n\nKey Competencies\n• Strong analytical and problem-solving skills with attention to detail.\n• Excellent communication skills, with the ability to bridge technical and business domains.\n• Self-starter with the ability to lead initiatives and work independently or collaboratively.\n• A passion for continuous improvement and innovation in a manufacturing setting.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRXRsIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlZpdmlkIFJlc291cmNpbmciLCJhZGRyZXNzX2NpdHkiOiJCaWxhc3B1ciwgQ2hoYXR0aXNnYXJoLCBJbmRpYSIsImh0aWRvY2lkIjoiMnQxYXo2bnUzRU1abUlLLUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "14 hours ago",
         "ETL Developer"
        ],
        [
         "ETL Developer – IBM DataStage",
         "TATA CONSULTANCY SERVICES",
         "Hyderabad, Telangana, India",
         "Job Title: ETL Developer – IBM DataStage\n\nExperience: 5 to 10 years\n\nLocation: Hyderabad, Chennai, Mumbai, Bangalore, Ahmedabad, Indore\n\nEmployment Type: Full-time\n\nJob Summary:\n\nWe are seeking a skilled ETL Developer with hands-on experience in IBM DataStage to join our dynamic data engineering team. The ideal candidate will have a strong background in ETL development, data warehousing concepts, and performance optimization. This role involves designing and implementing robust ETL solutions, collaborating with cross-functional teams, and ensuring high data quality and integrity.\n\nKey Responsibilities:\n• Design, develop, and implement ETL processes using IBM DataStage.\n• Work across the full project lifecycle including low-level design, development, testing, and deployment.\n• Develop parallel jobs using various DataStage stages such as Copy, Join, Merge, Lookup, Funnel, Filter, Sort, Remove Duplicates, Aggregator, Change Capture, and Transformer.\n• Integrate data from heterogeneous sources including Oracle, SQL Server, and flat files.\n• Optimize and troubleshoot existing ETL processes for performance improvements.\n• Collaborate with data analysts and architects to understand business requirements and translate them into technical solutions.\n• Ensure data quality and integrity across multiple data sources.\n• Create and maintain technical documentation for ETL processes.\n• Participate in code reviews and adhere to ETL best practices.\n• Work in Agile environments and use tools like JIRA for tracking tasks and issues.\n• Demonstrate strong problem-solving skills and the ability to troubleshoot technical issues effectively.\n• Understand and support operational requirements as part of business delivery.\n\nRequired Skills:\n• Strong experience with IBM DataStage for ETL development and migration.\n• Solid understanding of database and data warehousing concepts.\n• Proficiency in SQL and UNIX.\n• Experience working with large datasets and complex data transformations.\n• Familiarity with Agile methodologies and tools like JIRA.\n• Excellent communication and collaboration skills.",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIOKAkyBJQk0gRGF0YVN0YWdlIiwiY29tcGFueV9uYW1lIjoiVGF0YSBDb25zdWx0YW5jeSBTZXJ2aWNlcyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoibkdQWmp0dFNpdlVKUWVEYkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "7 days ago",
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "INSIGHT GLOBAL",
         "Hyderabad, Telangana, India",
         "We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\nResponsibilities:\nLead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\nCollaborate with developers to ensure a smooth transition and integration of the upgraded system.\nEnhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\nProvide support and guidance to the ETL development lead.\nEngage with end-users to ensure their needs are met during and after the upgrade process.\nUtilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\nRequirements:\nMinimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\nProven experience with SQL Server upgrades, particularly from 2012 to 2022.\nStrong SQL Server skills, including building tables, stored procedures, views, and functions.\nExperience with .NET development is highly desirable to be leveraged for other projects\nExcellent problem-solving skills and attention to detail.\nAbility to work effectively in a team environment and communicate with end-users.\nPreferred Qualifications:\nExperience with SQL Server 2016.\nExperience with Oracle\nFamiliarity with the latest features and enhancements in SQL Server 2022.\nStrong understanding of database architecture and migration strategies.\n\nWe are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ikw2UEZlVW5YOU90VExsNnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "SONATAONE",
         "Hyderabad, Telangana, India",
         "Real-time data Ingestion, Streaming data, Kafka, AWS Cloud streaming tools, ETL, Semi-structured data formats like JSON, XML\n\nTools: Talend, Kafka, AWS Event Bridge, Lamda and and Strong SQL & Python",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoic29uYXRhT25lIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "EPAM - ETL Developer - SSIS/SSRS",
         "EPAM SYSTEMS INDIA PRIVATE LIMITED",
         "Hyderabad, Telangana, India",
         "Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n\nExperience : 5+ Years\n\nLocation : Hyderabad\n\nJob Description :\n\nWe are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n\nYou will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n\nKey Responsibilities :\n\n- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n\n- Develop, deploy, and maintain SSIS packages for ETL processes.\n\n- Work on data warehouse design, development, and maintenance.\n\n- Ensure high performance and reliability of data integration workflows.\n\n- Troubleshoot SQL queries and ETL issues; optimize database performance.\n\n- Collaborate with cross-functional teams to define data solutions.\n\n- Work with complex data structures including JSON and XML.\n\n- Understand and implement different data models (star, snowflake, etc.).",
         "eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IkVQQU0gU3lzdGVtcyBJbmRpYSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InZ6RDdVNWpsc2hzYS03eW5BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "ETL Developer"
        ],
        [
         "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location",
         "FISERV",
         "India",
         "Calling all innovators – find your future at Fiserv.\n\nWe’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n\nJob Title\n\nETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n\nWork Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n• Participates in client kickoff activities including requirements and data gathering sessions.\n• Ability to analyze and migrate incoming external data into Fiserv solutions.\n• Strong working knowledge of ETL processes and best practices.\n• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n• Documents business processes and identifies opportunities for process redesign.\n• Reports system defects and identifies opportunities for system enhancements.\n• Performs as a technical consulting resource for new clients during the implementation process.\n• Supports several project managers during the pre and post go-live activities\n• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n• Provides technical and analytical guidance to the project team.\n• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n• The position requires working in shifts, 2 PM to 11 PM IST\n\nWhat you will need to have:\n• Minimum of 5 – 10 years of experience in software development.\n• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n• Self-starter who can work effectively, both independently and in a team environment.\n• Strong analytical, organizational, and problem-solving skills.\n• Excellent verbal and written communication skills.\n• Ability to maintain a professional attitude and demeanor in high pressure situations.\n• Ability to multi-task and manage multiple projects simultaneously.\n• Weekend/evening availability and support (10% - 25%).\n\nWhat would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n\nThank you for considering employment with Fiserv. Please:\n• Apply using your legal name\n• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n\nWhat you should know about us:\n\nFiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n\nOur commitment to Diversity and Inclusion:\n\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n\nWarning about fake job posts:\n\nPlease be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n\nAny communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n\nIf you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "ETL Developer/Senior Consultant Specialist",
         "HSBC",
         "India",
         "Some careers shine brighter than others.If you're looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.HSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.We are currently seeking an experienced professional to join our team in the role of Senior Consultant Specialist.In this role, you will:- Communicating effectively with senior stakeholders.- Work with a team of technologists, ensuring prioritization of tasks and supporting removal of blockers.- Working on burning the product backlog and oversee product demo's- Understanding complex business requirements and new technologies and understand how to influence teams to adopt new practice and embrace automation.- Managing of risks and issues\n\nRequirements\n• name : HSBC\n• location : India, IN",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyL1NlbmlvciBDb25zdWx0YW50IFNwZWNpYWxpc3QiLCJjb21wYW55X25hbWUiOiJIU0JDIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRIYmhtVUVOWmF3OGh3Q0lBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "ETL Developer"
        ],
        [
         "Senior Informatica Developer",
         "EVERESTDX INC",
         "Hyderabad, Telangana, India",
         "About the Company:\n\nEverest DX – We are a Digital Platform Services company, headquartered in Stamford. Our Platform/Solution includes Orchestration, Intelligent operations with BOTs’, AI-powered analytics for Enterprise IT. Our vision is to enable Digital Transformation for enterprises to deliver seamless customer experience, business efficiency and actionable insights through an integrated set of futuristic digital technologies.\n\nDigital Transformation Services - Specialized in Design, Build, Develop, Integrate, and Manage cloud solutions and modernize Data centers, build a Cloud-native application and migrate existing applications into secure, multi-cloud environments to support digital transformation. Our Digital\n\nPlatform Services enable organizations to reduce IT resource requirements and improve productivity, in addition to lowering costs and speeding digital transformation.\n\nDigital Platform - Cloud Intelligent Management (CiM) - An Autonomous Hybrid Cloud Management Platform that works across multi-cloud environments. helps enterprise Digital Transformation get most out of the cloud strategy while reducing Cost, Risk and Speed.\n\nTo know more please visit: http://www.everestdx.com\n\nResponsibilities:\n• Candidate should hands-on experience on ETL and SQL.\n• Design, develop, and optimize ETL workflows using Informatica PowerCenter.\n• Implement cloud-based ETL solutions using Informatica IDMC and IICS.\n• Should have expertise on all transformations in Power Center and IDMC/IICS.\n• Should have experience or knowledge on the PC to IICS migration using CDI PC tool or some other tool.\n• Lead data migration projects, transitioning data from on-premise to cloud environments.\n• Write complex SQL queries and perform data validation and transformation.\n• Conduct detailed data analysis to ensure accuracy and integrity of migrated data.\n• Troubleshoot and optimize ETL processes for performance and error handling.\n• Collaborate with cross-functional teams to gather requirements and design solutions.\n• Create and maintain documentation for ETL processes and system configurations.\n• Implement industry best practices for data integration and performance tuning.\n\nRequired Skills:\n• Hands-on experience with Informatica Power Center, IDMC and IICS.\n• Strong expertise in writing complex SQL queries and database management.\n• Experience in data migration projects (on-premise to cloud).\n• Strong data analysis skills for large datasets and ensuring accuracy.\n• Solid understanding of ETL design & development concepts.\n• Familiarity with cloud platforms (AWS, Azure).\n• Experience with version control tools (e.g., Git) and deployment processes.\n\nPreferred Skills:\n• Experience with data lakes, data warehousing, or big data platforms.\n• Familiarity with Agile methodologies.\n• Knowledge of other ETL tools",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgSW5mb3JtYXRpY2EgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiRXZlcmVzdERYIEluYyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNDdnQ200aEh5aHpSWlhDOEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "2 days ago",
         "ETL Developer"
        ],
        [
         "Spark Engineer",
         "STAFFINGINE LLC",
         "India",
         "- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred",
         "eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "Spark Engineer"
        ],
        [
         "Data Engineer - Spark/Python",
         "ETELLIGENS TECHNOLOGIES",
         "India",
         "Job Description\n\nWe are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n\nKey Responsibilities\n• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n• Perform data cleansing and migration from diverse sources to target systems.\n• Collaborate with stakeholders to understand requirements and convert them into technical\n\nsolutions.\n• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n• Support, troubleshoot, and maintain data pipelines and workflows.\n• Participate in all phases of software development lifecycle including unit testing, integration\n\ntesting, and performance testing.\n• Contribute to the modernization of data platforms and analytics tools on Azure.\n• Ensure data quality and integrity across all pipelines and systems.\n\nRequired Skills & Qualifications\n• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n• 1+ year of experience in Python and Spark scripting.\n• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n• Good understanding of Data Warehousing (DWH) concepts.\n• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n• Hands-on experience in data cleansing, transformation, and migration projects.\n• Ability to work independently and within a team environment.\n• Microsoft Certified : Azure Data Engineer Associate\n• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n\nLocation : Artha SEZ, Greater Noida West\n\n(ref:hirist.tech)",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Spark Engineer"
        ],
        [
         "Staff Data Engineer (Spark, Python, Hadoop)",
         "VISA",
         "India",
         "Company Description\n\nVisa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n\nWhen you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n\nJoin Visa: A Network Working for Everyone.\nJob Description\n\nThe Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n\nAre you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n\nAs a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n\nEssential Functions\n• Work with manager and clients to fully understand business requirements and desired business outcomes\n• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n• Execute data engineering projects ranging from small to large either individually or as part of a project team\n• Ensure project delivery within timelines and budget requirements\n• Provide coaching and mentoring to junior team members\n\nThis is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\nQualifications\n\n• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n• Minimum of 4 years of hands-on expertise with Java or Scala\n• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n• Experience working in an Agile and Test Driven Development environment.\n• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n• Experience with SAS as a statistical package is preferred\n• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "Spark Engineer"
        ],
        [
         "Databricks Engineer - Spark / PySpark",
         "ENKEFALOS TECHNOLOGIES LLP",
         "Anywhere",
         "Databricks Engineer – Spark / PySpark\n\nLocation : Remote / Mysore\n\nJoining : Immediate\n\nExperience : 5+ years\n\nResponsibilities :\n\nWill implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n\nRequirements:\n• PySpark (RDDs, DataFrames, performance tuning)\n• Building gold‐layer data models for financial reporting\n• Experience with complex joins, aggregations, GL hierarchies\n• Version handling (Actuals vs Budget), currency conversions\n\nJob Type: Full-time\n\nPay: ₹500,395.35 - ₹1,840,348.25 per year\n\nBenefits:\n• Flexible schedule\n• Paid sick time\n• Provident Fund\n• Work from home\n\nApplication Question(s):\n• Have you worked on ADF/ADLS ?\n• Do you have hands-on experience of Spark / PySpark\n\nExperience:\n• Databricks Engineering: 4 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "15 days ago",
         "Spark Engineer"
        ],
        [
         "Senior PySpark Data Engineer (Big Data, Cloud Data Solutions, & Python)",
         "SYNECHRON",
         "India",
         "Job Summary\n\nSynechron is seeking a skilled PySpark Data Engineer to design, develop, and optimize data processing solutions leveraging modern big data technologies. In this role, you will lead efforts to build scalable data pipelines, support data integration initiatives, and work closely with cross-functional teams to enable data-driven decision-making. Your expertise will contribute to enhancing business insights and operational efficiency, positioning Synechron as a pioneer in adopting emerging data technologies.\n\nSoftware Requirements\n\nRequired Software Skills:\n• PySpark (Apache Spark with Python) – experience in developing data pipelines\n• Apache Spark ecosystem knowledge\n• Python programming (versions 3.7 or higher)\n• SQL and relational database management systems (e.g., PostgreSQL, MySQL)\n• Cloud platforms (preferably AWS or Azure)\n• Version control: GIT\n• Data workflow orchestration tools like Apache Airflow\n• Data management tools: SQL Developer or equivalent\n\nPreferred Software Skills:\n• Experience with Hadoop ecosystem components\n• Knowledge of containerization (Docker, Kubernetes)\n• Familiarity with data lake and data warehouse solutions (e.g., AWS S3, Redshift, Snowflake)\n• Monitoring and logging tools (e.g., Prometheus, Grafana)\n\nOverall Responsibilities\n• Lead the design and implementation of large-scale data processing solutions using PySpark and related technologies\n• Collaborate with data scientists, analysts, and business teams to understand data requirements and deliver scalable pipelines\n• Mentor junior team members on best practices in data engineering and emerging technologies\n• Evaluate new tools and methodologies to optimize data workflows and improve data quality\n• Ensure data solutions are robust, scalable, and aligned with organizational data governance policies\n• Stay informed on industry trends and technological advancements in big data and analytics\n• Support production environment stability and performance tuning of data pipelines\n• Drive innovative approaches to extract value from large and complex datasets\n\nTechnical Skills (By Category)\n\nProgramming Languages:\n• Required: Python (PySpark experience minimum 2 years)\n• Preferred: Scala (for Spark), SQL, Bash scripting\n\nDatabases/Data Management:\n• Relational databases (PostgreSQL, MySQL)\n• Distributed storage solutions (HDFS, cloud object storage like S3 or Azure Blob Storage)\n• Data warehousing platforms (Snowflake, Redshift – preferred)\n\nCloud Technologies:\n• Required: Experience deploying and managing data solutions on AWS or Azure\n• Preferred: Knowledge of cloud-native services like EMR, Data Factory, or Azure Data Lake\n\nFrameworks and Libraries:\n• Apache Spark (PySpark)\n• Airflow or similar orchestration tools\n• Data processing frameworks (Kafka, Spark Streaming – preferred)\n\nDevelopment Tools and Methodologies:\n• Version control with GIT\n• Agile management tools: Jira, Confluence\n• Continuous integration/deployment pipelines (Jenkins, GitLab CI)\n\nSecurity Protocols:\n• Understanding of data security, access controls, and GDPR compliance in cloud environments\n\nExperience Requirements\n• Minimum of 5+ years in data engineering, with hands-on PySpark experience\n• Proven track record of developing, deploying, and maintaining scalable data pipelines\n• Experience working with data lakes, data warehouses, and cloud data services\n• Demonstrated leadership in projects involving big data technologies\n• Experience mentoring junior team members and collaborating across teams\n• Prior experience in financial, healthcare, or retail sectors is beneficial but not mandatory\n\nDay-to-Day Activities\n• Develop, optimize, and deploy big data pipelines using PySpark and related tools\n• Collaborate with data analysts, data scientists, and business teams to define data requirements\n• Conduct code reviews, troubleshoot pipeline issues, and optimize performance\n• Mentor junior team members on best practices and emerging technologies\n• Design solutions for data ingestion, transformation, and storage\n• Evaluate new tools and frameworks for continuous improvement\n• Maintain documentation, monitor system health, and ensure security compliance\n• Participate in sprint planning, daily stand-ups, and project retrospectives to align priorities\n\nQualifications\n• Bachelor’s or Master’s degree in Computer Science, Information Technology, or related discipline\n• Relevant industry certifications (e.g., AWS Data Analytics, GCP Professional Data Engineer) preferred\n• Proven experience working with PySpark and big data ecosystems\n• Strong understanding of software development lifecycle and data governance standards\n• Commitment to continuous learning and professional development in data engineering technologies\n\nProfessional Competencies\n• Analytical mindset and problem-solving acumen for complex data challenges\n• Effective leadership and team management skills\n• Excellent communication skills tailored to technical and non-technical audiences\n• Adaptability in fast-evolving technological landscapes\n• Strong organizational skills to prioritize tasks and manage multiple projects\n• Innovation-driven with a passion for leveraging emerging data technologies\n\nS YNECHRON’S DIVERSITY & INCLUSION STATEMENT\n\nDiversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.\n\nAll employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.\n\nCandidate Application Notice",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgUHlTcGFyayBEYXRhIEVuZ2luZWVyIChCaWcgRGF0YSwgQ2xvdWQgRGF0YSBTb2x1dGlvbnMsIFx1MDAyNiBQeXRob24pIiwiY29tcGFueV9uYW1lIjoiU3luZWNocm9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjJYOEpqZUhEQmxiRndHQklBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "1 day ago",
         "Spark Engineer"
        ],
        [
         "Senior Data Engineer (Delta Lake, Spark & Unity Catalog)",
         "306 - GOTO TECHNOLOGIES INDIA PRIVATE LIMITED",
         "India",
         "Job Description Where you’ll work: India (Remote) Engineering at GoTo We’re the trailblazers of remote work technology. We build powerful, flexible work software that empowers everyone to live their best life, at work and beyond. And blaze even more trails along the way. There’s ample room for growth – so you can blaze your own trail here too. When you join a GoTo product team, you’ll take on a key role in this process and see your work be used by millions of users worldwide. Your Day to Day As a Senior Data Engineer, you would be: Design, develop, and maintain robust, scalable, and efficient ETL/ELT data pipelines to process structured and unstructured data from diverse sources. Architect and implement cloud-native data solutions using AWS services including S3, EMR, Lambda, and EKS. Build and optimize large-scale data processing workflows using Apache Spark and Databricks, with hands-on experience implementing and managing Delta Lake tables for high-volume batch and streaming use cases. Manage and govern data using Unity Catalog, maintaining strong data lineage, access controls, and metadata management. Develop and maintain data models (relational and dimensional) to support analytics, reporting, and machine learning use cases. Schedule, monitor, and orchestrate workflows using Apache Airflow or similar orchestration tools. Implement data quality checks, logging, monitoring, and alerting to ensure reliability and visibility of data pipelines. Collaborate with analysts, data scientists, and business stakeholders to deliver high-quality, trusted data for downstream applications and enable self-service BI tooling. Ensure adherence to best practices in data governance, security, and compliance. Mentor junior engineers and contribute to engineering standards, including CI/CD, automated testing, and documentation What We’re Looking For As an Senior Data Engineer, your background will look like: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. 5+ years of relevant experience in data engineering or software development, with a proven track record designing and maintaining production-grade data pipelines. Proficiency in Python programming and SQL for data transformation and analytics. Strong hands-on experience with Apache Spark and Databricks, including direct exposure to Delta Lake for data lake management, ACID transactions, schema enforcement and evolution, and time travel functionalities. Experience managing and organizing data access with Unity Catalog. In-depth experience with AWS services—specifically S3, EMR, Lambda, and EKS—and an excellent understanding of cloud-based data architecture and security best practices. Strong data modeling skills (dimensional, normalized) and understanding of data warehousing and lakehouse paradigms. Hands-on experience orchestrating workflows using Apache Airflow or similar orchestration frameworks. Familiarity with BI and data visualization tools (e.g., Tableau, Power BI). Experience establishing data quality processes, monitoring, and observability for pipelines. Exceptional communication, collaboration, and problem-solving skills, with the ability to thrive in an agile, multicultural team environment. At GoTo, authenticity and inclusive culture are key to our thriving workplace, where diverse perspectives drive innovation and growth. Our team of GoGetters is passionate about learning, exploring, and working together to achieve success while staying committed to delivering exceptional experiences for our customers. We take pride in supporting our employees with comprehensive benefits, wellness programs, and global opportunities for professional and personal development. By maintaining an inclusive environment, we empower our teams to do their best work, make a meaningful impact, and grow their career. Learn more. At GoTo, we’re bold enough to imagine a world of work without limits—where curiosity and AI-driven innovation fuel our constant growth. As the leader in cloud communications and IT, we solve real-world challenges through practical, cutting-edge solutions and an unwavering customer-first mindset. Our culture is rooted in inclusion, ownership, and transparency, fueling an environment where every voice contributes to both personal and collective achievement. Here, collaboration sparks bold ideas, and authenticity is celebrated—empowering you to adapt, evolve, and make a real impact. Join GoTo, and help shape the future of work while accelerating your own growth alongside exceptional people who are redefining what’s possible.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAoRGVsdGEgTGFrZSwgU3BhcmsgXHUwMDI2IFVuaXR5IENhdGFsb2cpIiwiY29tcGFueV9uYW1lIjoiMzA2IC0gR29UbyBUZWNobm9sb2dpZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlJxSFNWX2s0aVZkRWhBNGRBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "10 days ago",
         "Spark Engineer"
        ],
        [
         "Cloud Data Engineer- Spark & Databricks",
         "BRIGHTTIER",
         "Anywhere",
         "This a Full Remote job, the offer is available from: India\n\nJob Title: Cloud Engineer – Spark/Databricks Specialist\nLocation: Remote\nJob Type: Contract\nIndustry: IT/Cloud Engineering\nJob Summary:\nWe are looking for a highly skilled Cloud Engineer with a specialization in Apache Spark and Databricks to join our dynamic team. The ideal candidate will have extensive experience working with cloud platforms such as AWS, Azure, and GCP, and a deep understanding of data engineering, ETL processes, and cloud-native tools. Your primary responsibility will be to design, develop, and maintain scalable data pipelines using Spark and Databricks, while optimizing performance and ensuring data integrity across diverse environments.\nKey Responsibilities:\nDesign and Development:\n• Architect, develop, and maintain scalable ETL pipelines using Databricks, Apache Spark (Scala, Python), and other cloud-native tools such as AWS Glue, Azure Data Factory, and GCP Dataflow.\n• Design and build data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n• Implement efficient data ingestion, transformation, and processing workflows with Spark and Databricks.\n• Optimize the performance of ETL processes for faster data processing and lower costs.\n• Develop and manage data pipelines using other ETL tools such as Informatica, SAP Data Intelligence, and others as needed.\nData Integration and Management:\n• Integrate structured and unstructured data sources (relational databases, APIs, ERP systems) into the cloud data infrastructure.\n• Ensure data quality, validation, and integrity through rigorous testing.\n• Perform data extraction and integration from SAP or ERP systems, ensuring seamless data flow.\nPerformance Optimization:\n• Monitor, troubleshoot, and enhance the performance of Spark/Databricks pipelines.\n• Implement best practices for data governance, security, and compliance across data workflows.\nCollaboration and Communication:\n• Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders, to define data requirements and deliver scalable solutions.\n• Provide technical guidance and recommendations on cloud data engineering processes and tools.\nDocumentation and Maintenance:\n• Document data engineering solutions, ETL pipelines, and workflows.\n• Maintain and support existing data pipelines, ensuring they operate effectively and align with business goals.\nQualifications:\nEducation:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\nExperience:\n• 7+ years of experience in cloud data engineering or similar roles.\n• Expertise in Apache Spark and Databricks for data processing.\n• Proven experience with cloud platforms like AWS, Azure, and GCP.\n• Experience with cloud-native ETL tools such as AWS Glue, Azure Data Factory, Kafka, GCP Dataflow, etc.\n• Hands-on experience with data platforms like Redshift, Snowflake, Azure Synapse, and BigQuery.\n• Experience in extracting data from SAP or ERP systems is preferred.\n• Strong programming skills in Python, Scala, or Java.\n• Proficient in SQL and query optimization techniques.\nSkills:\n• In-depth knowledge of Spark/Scala for high-performance data processing.\n• Strong understanding of data modeling, ETL/ELT processes, and data warehousing concepts.\n• Familiarity with data governance, security, and compliance best practices.\n• Excellent problem-solving, communication, and collaboration skills.\nPreferred Qualifications:\n• Certifications in cloud platforms (e.g., AWS Certified Data Analytics, Google Professional Data Engineer, Azure Data Engineer Associate).\n• Experience with CI/CD pipelines and DevOps practices for data engineering.\n• Exposure to Apache Hadoop, Kafka, or other data frameworks is a plus.\n\nThis offer from \"Brighttier\" has been enriched by Jobgether.com and got a 74% flex score.",
         "eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyLSBTcGFyayBcdTAwMjYgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJyaWdodHRpZXIiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTzdzcjNWUGpVa2l1R0VpZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Spark Engineer"
        ],
        [
         "Data Engineer (Snowflake, Spark, AWS) - AVP",
         "12542 CITICORP SERVICES INDIA PRIVATE LIMITED",
         "India",
         "The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "Spark Engineer"
        ],
        [
         "Data Engineer-Python,PySpark,SQL ,Spark Architecture,Azure Databricks",
         "SIEMENS HEALTHINEERS",
         "India",
         "jobid\n• 460574\n\njobfamily\n• Research & Development\n\ncompany\n• Siemens Healthcare Private Limited\n\norganization\n• Siemens Healthineers\n\njobType\n• Full-time\n\nexperienceLevel\n• Experienced Professional\n\ncontractType\n• Permanent\n\nAs a Data Engineer , you are required to:\n\nDesign, build, and maintain data pipelines that efficiently process and transport data from various sources to storage systems or processing environments while ensuring data integrity, consistency, and accuracy across the entire data pipeline.\n\nIntegrate data from different systems, often involving data cleaning, transformation (ETL), and validation. Design the structure of databases and data storage systems, including the design of schemas, tables, and relationships between datasets to enable efficient querying. Work closely with data scientists, analysts, and other stakeholders to understand their data needs and ensure that the data is structured in a way that makes it accessible and usable.\n\nStay up-to-date with the latest trends and technologies in the data engineering space, such as new data storage solutions, processing frameworks, and cloud technologies. Evaluate and implement new tools to improve data engineering processes.\n\nQualification : Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Science, Engineering is desirable.\n\nExperience level : At least 3 - 5 years hands-on experience in Data Engineering\n\nDesired Knowledge & Experience :\n• Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\n• Knowing Spark internals: Catalyst/Tungsten/Photon\n• Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\n• IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\n• Test: pytest, Great Expectations\n• CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\n• Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\n• Languages: Python/Functional Programming (FP)\n• SQL : TSQL/Spark SQL/HiveQL\n• Storage : Data Lake and Big Data Storage Design\n\nadditionally it is helpful to know basics of:\n• Data Pipelines : ADF/Synapse Pipelines/Oozie/Airflow\n• Languages: Scala, Java\n• NoSQL : Cosmos, Mongo, Cassandra\n• Cubes : SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\n• SQL Server : TSQL, Stored Procedures\n• Hadoop : HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\n• Data Catalog : Azure Purview, Apache Atlas, Informatica\n\nRequired Soft skills & Other Capabilities :\n\nGreat attention to detail and good analytical abilities.\n\nGood planning and organizational skills\n\nCollaborative approach to sharing ideas and finding solutions\n\nAbility to work independently and also in a global team environment.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVB5dGhvbixQeVNwYXJrLFNRTCAsU3BhcmsgQXJjaGl0ZWN0dXJlLEF6dXJlIERhdGFicmlja3MiLCJjb21wYW55X25hbWUiOiJTaWVtZW5zIEhlYWx0aGluZWVycyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJXVkxrZFF1QU4tSGFXQjIzQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "8 days ago",
         "Spark Engineer"
        ],
        [
         "Cloud Data Engineer (Spark/Databricks)",
         "ANTAL JOB BOARD",
         "Nagpur, Maharashtra, India",
         "Vacancy No\nVN1228\n\nBusiness Unit\nEMEA\n\nJob Location\nIndia\n\nEmployment Type\nFull Time\n\nJob Details and Responsibilities\nWe are seeking an experienced Cloud Data Engineer with a strong background in AWS, Azure, and GCP. The ideal candidate will have extensive experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, and other ETL tools like Informatica, SAP Data Intelligence, etc. You will be responsible for designing, implementing, and maintaining robust data pipelines and building scalable data lakes. Experience with various data platforms like Redshift, Snowflake, Databricks, Synapse, Snowflake and others is essential. Familiarity with data extraction from SAP or ERP systems is a plus.\n\nKey Responsibilities:\n\nDesign and Development:\n• Design, develop, and maintain scalable ETL pipelines using cloud-native tools (AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.).\n• Architect and implement data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n• Develop and optimize data ingestion, transformation, and loading processes using Databricks, Snowflake, Redshift, BigQuery and Azure Synapse.\n• Implement ETL processes using tools like Informatica, SAP Data Intelligence, and others.\n• Develop and optimize data processing jobs using Spark Scala.\nData Integration and Management:\n• Integrate various data sources, including relational databases, APIs, unstructured data, and ERP systems into the data lake.\n• Ensure data quality and integrity through rigorous testing and validation.\n• Perform data extraction from SAP or ERP systems when necessary.\nPerformance Optimization:\n• Monitor and optimize the performance of data pipelines and ETL processes.\n• Implement best practices for data management, including data governance, security, and compliance.\nCollaboration and Communication:\n• Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\n• Collaborate with cross-functional teams to design and implement data solutions that meet business needs.\nDocumentation and Maintenance:\n• Document technical solutions, processes, and workflows.\n• Maintain and troubleshoot existing ETL pipelines and data integrations.\n\nQualifications\n\nEducation:\n\nBachelor’s degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n\nExperience:\n• 7+ years of experience as a Data Engineer or in a similar role.\n• Proven experience with cloud platforms: AWS, Azure, and GCP.\n• Hands-on experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.\n• Experience with other ETL tools like Informatica, SAP Data Intelligence, etc.\n• Experience in building and managing data lakes and data warehouses.\n• Proficiency with data platforms like Redshift, Snowflake, BigQuery, Databricks, and Azure Synapse.\n• Experience with data extraction from SAP or ERP systems is a plus.\n• Strong experience with Spark and Scala for data processing.\n\nSkills:\n• Strong programming skills in Python, Java, or Scala.\n• Proficient in SQL and query optimization techniques.\n• Familiarity with data modeling, ETL/ELT processes, and data warehousing concepts.\n• Knowledge of data governance, security, and compliance best practices.\n• Excellent problem-solving and analytical skills.\n• Strong communication and collaboration skills.\n\nPreferred Qualifications:\n• Experience with other data tools and technologies such as Apache Spark, or Hadoop.\n• Certifications in cloud platforms (AWS Certified Data Analytics – Specialty, Google Professional Data Engineer, Microsoft Certified: Azure Data Engineer Associate).\n• Experience with CI/CD pipelines and DevOps practices for data engineering\n• Selected applicant will be subject to a background investigation, which will be conducted and the results of which will be used in compliance with applicable law.\n\nWhat we offer in return:\n• Remote Working: Lemongrass always has been and always will offer 100% remote work\n• Flexibility: Work where and when you like most of the time\n• Training: A subscription to A Cloud Guru and generous budget for taking certifications and other resources you’ll find helpful\n• State of the art tech: An opportunity to learn and run the latest industry standard tools\n• Team: Colleagues who will challenge you giving the chance to learn from them and them from you\n\nLemongrass Consulting is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate on the basis of race, religion, color, national origin, religious creed, gender, sexual orientation, gender identity, gender expression, age, genetic information, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics\n\nAbout Lemongrass\nLemongrass (lemongrasscloud.com) is a global leader in SAP consulting, focused on helping organizations transform their business processes through innovative solutions and technologies. With a strong commitment to customer success, Lemongrass partners with companies to drive their digital transformation journeys, enabling them to unlock the full potential of their SAP investments.\n\nWe do this with our continuous innovation, automation, migration and operation, delivered on the world's most comprehensive cloud platforms – AWS, Azure and GCP and SAP Cloud ERP. We have been working with AWS and SAP since 2010 and we are a Premier Amazon Partner Network (APN) Consulting Partner. We are also a Microsoft Gold Partner, a Google Cloud Partner and an SAP Certified Silver Partner.\n\nOur team is what makes Lemongrass exceptional and why we have the excellent reputation in the market that we enjoy today. At Lemongrass, you will work with the smartest and most motivated people in the business. We take pride in our culture of innovation and collaboration that drives us to deliver exceptional benefits to our clients every day.",
         "eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyIChTcGFyay9EYXRhYnJpY2tzKSIsImNvbXBhbnlfbmFtZSI6IkFudGFsIEpvYiBCb2FyZCIsImFkZHJlc3NfY2l0eSI6Ik5hZ3B1ciwgTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJCQXpseTdfU0lsQjByTmF2QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "28 days ago",
         "Spark Engineer"
        ],
        [
         "Data Analyst III",
         "BRISTOL MYERS SQUIBB",
         "Hyderabad, Telangana, India",
         "Working with Us\nChallenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n\nThe US Commercial Analytics, Market Access Data Analyst will play a crucial role in supporting data-driven decision-making processes, generating insights, and providing strategic guidance to optimize our Market Access & Pricing activities in the US. This position requires a deep expertise in secondary analytics and statistical analytic techniques, along with a firm grasp of the US healthcare landscape, especially market access and pricing dynamics. The Business Analyst III will typically be responsible for leading support for multiple therapeutic areas in Market Access and will collaborate with cross-functional teams to enhance understanding of business performance and drive growth.\n\nRoles and Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Perform statistical analyses, data mining and predictive modelling to uncover trends and drivers of performance\n• Collaborate with stakeholders (US based Market Access Business Insights & Analytics team and US Market Access team members) to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects including payer, PBM and patient level subnational & claims data analyses, formulary and policy level analysis, and ad-hoc analytics based on secondary data to support US Market Access strategy\n• Prepare reports, dashboards and presentations to communicate findings, insights and recommendations to stakeholders\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up-to-date with industry trends (US access and health care landscape), best practices, and emerging technologies\n• Partner with US based Market Access BIA/Commercial team members (for a given project scope)\n• Provide overall project oversight and training/mentorship to junior analysts and team members as required\n• Supports culture of fact-based decision making through application of best-in-class, innovative, and appropriate methodologies to address key business questions\n\nSkills & Competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions, around US pricing and reimbursement strategies\n• Proficiency in statistical analysis techniques and applications of those to US commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and strategic thinking, with the ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome Candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• 5-8 years prior Pharmaceutical industry experience with 1-2 years within US Market Access & Pricing Analytics\n• Experience working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims, medical policy and formulary data), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with reimbursement and regulatory requirements and compliance in the US biopharma industry\n• Certification or training in relevant analytics or business intelligence tools is a plus\n\nIf you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n\nUniquely Interesting Work, Life-changing Careers\nWith a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n\nOn-site Protocol\n\nBMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n\nSite-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n\nBMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n\nIf you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUlJIiwiY29tcGFueV9uYW1lIjoiQnJpc3RvbCBNeWVycyBTcXVpYmIiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZHU3JGRGo4OTcybHdnQWpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "3 days ago",
         "Data Analyst"
        ],
        [
         "Data Science Analyst",
         "IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED",
         "India",
         "About the Role:Within Insights and Commercial Solutions, below are the key service groups:  Marketing Effectiveness (Business Analytics, Forecasting, Competitive Intelligence)  Field Force Operations  Commercial Consulting  Brand Insights  Patient access services (Pricing and market access, etc.)  Primary Market Research Provide analytics support to Novartis internal customers (CPOs & Regional marketing and sales teams) on various projects • Support and facilitate data enabled decision making for Novartis internal customers using data analysis and data science techniques/methods on internal and external (3rd party) data to solve business problems • Support I&CS business in building capabilities by involving in various initiatives like knowledge sharing, on-boarding and training support, support in all business-related tasks/activities, building process documentation and knowledge repositories.Analyst, Data ScienceLocation – Hyderabad #LI Hybrid Key Responsibilities:Delivering projects and managing internal customer expectations across multi-channel marketing analytics, Portfolio Analytics, Targeting and Segmentation, Predictive Analytics, Resource Allocation and Optimization, Other ad-hoc requests.Create and deliver customer requirements as agreed SLAs (timeliness, accuracy, quality, etc.) and drive excellent customer satisfaction. Deliver advanced analytical and statistical solutions for various projects related to promotion evaluation, multi-channel marketing (MCM) campaign design, return on investment (ROI) analysis, resource allocation, segmentation, targeting, and other ad-hoc business questionsSupport exploratory research to identify new areas of application of advanced analytics/data science in providing enhanced decision-making support. Deliver services through a structured project management approach with appropriate documentation and communication throughout the delivery of servicesAutomation of project codes and development of front-end delivery solutions. Support in creation and maintenance of standard operating procedures (SOPs), quality checklists that will enable excellent quality outputs within the function.Support in developing and maintaining knowledge repositories that captures qualitative and quantitative reports of brands, disease areas, macroeconomic trends of Novartis operating markets, etc.Comply with all internal functional operating procedures like time tracking, KPI tracking and reporting, and other internal systems and processes. Comply to all Novartis operating procedures as per legal/IT/HR requirementsEssential Requirements:Technical Skills: Statistical Modeling Experience (Covering but not limited to, Regression, Classification, Supervised and Unsupervised Learning Methods, Optimization techniques, Predictive Models, Deep learning Models, e.g. Neural Networks, Natural Language Processing, Text Mining etc.)Programming languages/tools – R, SAS, Python, SQLVisualization tools – Qlikview, Qliksense, TableauWorking knowledge of MS-Office (MS Excel, VBA, PowerPoint, Access)Pharmaceutical industry domain/datasets knowledge – desirableEducation:Graduation/Post Graduation in Business Administration, Mathematics, Economics, Statistics, Engineering or Quantitative streamsExperience: Experience (Minimum 2years) in analytics in marketing analytics, experience in pharma industry is preferableShould have strong quantitative and systems backgroundStrong analytical thinking with problem solving approachShould have good ability to understand new data sources in short timeframe and embed them into standard analysesA good understanding of pharmaceutical domain and data would be ideal. Should have worked in an international company with exposure to working in a cross-cultural environmentStrong and proactive business results focus, and proven ability to provide insights that increase productivityWhy Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook. https://www.novartis.com/careers/benefits-rewards Commitment to Diversity and Inclusion: Novartis is committed to building an outstanding, inclusive work environment and diverse teams' representative of the patients and communities we serve. Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVuY2UgQW5hbHlzdCIsImNvbXBhbnlfbmFtZSI6IklOMTAgKEZDUlMgPSBJTjAxMCkgTm92YXJ0aXMgSGVhbHRoY2FyZSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiejA4Y3NEMUZUdmdvZExfWkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "7 days ago",
         "Data Analyst"
        ],
        [
         "Data Sr.Modeler/Data Analyst( Immediate Joiner)",
         "THE TALENT QUEST",
         "Hyderabad, Telangana, India",
         "Vacancy with a company focused on digital transformation, specializing in intelligent automation, digitalization, data science & analytics, and mobile enablement. They help businesses improve cost efficiency, productivity, and agility by reducing turnaround time and errors. The company provides services and solutions including operations digital transformation consulting, next-gen shared services setup consulting, cognitive RPA deployment, and AI-enabled CX enhancement. Founded in 2020 ;with HQ in Gurugram, India; the Company is now operating from Noida, Mumbai, Hyderabad, and Bengaluru as well.\n\nJob Role:We are seeking a highly skilled and detail-oriented Data Modeler to join our Data\n\nManagement team. The ideal candidate will be responsible for designing and\n\nimplementing logical and physical data models to support enterprise data\n\ninitiatives. This role requires close collaboration with business stakeholders, data\n\narchitects, and engineers to ensure data is structured and accessible for analytics,\n\nreporting, and operational needs.\n\nThe successful candidate will:\n\nProvides technical expertise in needs identification, data modelling, data\n\nmovement and transformation mapping (source to target), automation and testing\n\nstrategies, translating business needs into technical solutions with adherence to\n\nestablished data guidelines and approaches from a business unit or project\n\nperspective.\n\n7-10 Years industry implementation experience with one or more data\n\nmodelling tools such as Erwin, ERStudio, PowerDesigner etc.\n\n Minimum of 8 years of data architecture, data modelling or similar\n\nexperience\n\n 5-7 years of management experience required\n\n 5-7 years consulting experience preferred\n\n Experience working with dimensionally modelled data\n\n Bachelor’s degree or equivalent experience, Master’s Degree Preferred\n\n Understanding of cloud (Azure, AWS, GCP, Snowflake preferred) and on\n\npremises architectures\n\nJob Types: Full-time, Permanent\n\nPay: Up to ₹3,000,000.00 per year\n\nBenefits:\n• Cell phone reimbursement\n• Internet reimbursement\n• Life insurance\n• Paid sick time\n• Paid time off\n• Work from home\n\nWork Location: In person",
         "eyJqb2JfdGl0bGUiOiJEYXRhIFNyLk1vZGVsZXIvRGF0YSBBbmFseXN0KCBJbW1lZGlhdGUgSm9pbmVyKSIsImNvbXBhbnlfbmFtZSI6IlRoZSBUYWxlbnQgUXVlc3QiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ino0YkdrY3RHOUdZdmFTRVVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "2 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst - INTL - Mexico or India",
         "INSIGHT GLOBAL",
         "Hyderabad, Telangana, India",
         "Project Background:\nMosaic is our financial planning and reporting transformation program, bringing an aligned business process and toolset to all key business units enabling us to get consistent and standardized financial metrics across geographies. Started in 2019 the program is already rolled out to Europe and most of North America and continues across the world.\nThere are three key components to the program:\n1. A standardized planning tool IBM Cognos TM1\n2. A global data warehouse based on Azure Cloud know as SPOT (single point of truth). The SPOT team is made up of functional data analysts & data engineers\n3. An extensive suite of dashboards known as Cockpit using Tableau. The Cockpit team are generally finance analysts with Tableau skills\nRole Background:\nWe are looking for an additional functional/data analyst to join the existing team of functional/data analysts working on the SPOT data warehouse.\nThe role of the functional/data analyst is to be the bridge between the Cockpit team, Data Engineering team, and Cognos TM1 team. The analyst understands the business data requirements and translates these into data definition and data transformation documents that are then build by the data engineers. The analyst can also be involved in testing, data validation and any sustain activity related to the flow and accuracy of the data in SPOT & Cockpit.\nTypically, the functional data analyst receives requirements for new and/or amended data pipelines from the Cockpit team or works with the Cognos TM1 team on new data sets that need to flow to SPOT & Cockpit.\nThe current functional data analysts come from a range of backgrounds from data engineers to business analysts. What unites the team is a passion for data, attention to detail, ability to navigate and understand complex data flows, and a customer-centric mindset. This is a high profile and at times high pressure transformation program and everyone on the team must be responsive to our stakeholders needs.\nKey Accountabilities:\nThis role will primarily work with the North America Cockpit team primarily based in Mexico City, but as part of the team they will be expected to build a good knowledge of all data pipelines and work on any area as needed. The key elements of the role are:\nDevelop and maintain SPOT solution design & data architecture:\no Ensure SPOT solution design & data model is up to date with latest business requirements\no Contribute to the delivery of current technical roadmap that can support the predicted growth in data volumes\nTranslate and communicate business requirements across all IT delivery teams and/or partners:\no Collaborate closely with SPOT Cockpit teams to ensure business requirements received from Global & Regional Planning teams are well-understood and translated into the SPOT solution design (functional & data model)\nAct as an owner in the diagnosis and resolution of any data issues raised with the team, partnering with other teams as needed.\n\nWe are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBJTlRMIC0gTWV4aWNvIG9yIEluZGlhIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Il96ZU5oSUE0Ujg3ak1KZHNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "6 days ago",
         "Data Analyst"
        ],
        [
         "IN Specialist 3- Data Analyst - TRAS -Assurance– Bangalore",
         "PWC",
         "India",
         "Line of ServiceAssuranceIndustry/SectorNot ApplicableSpecialismAssuranceManagement LevelSpecialistJob Description & SummaryAt PwC, our people in audit and assurance focus on providing independent and objective assessments of financial statements, internal controls, and other assurable information enhancing the credibility and reliability of this information with a variety of stakeholders. They evaluate compliance with regulations including assessing governance and risk management processes and related controls.In digital assurance at PwC, you will focus on providing assurance services over clients' digital environment, including processes and controls, cyber security measures, data and AI systems, and their associated governance, to help organisations and their stakeholders build trust in their technology while complying with relevant regulations.*Why PWCAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm’s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations. \"Job Description & SummaryABOUT TECHNOLOGY RISK ASSURANCE SERVICES:The Technology Risk Assurance Services practice in the firm, provides both audit and non-audit services related to controls around the financial reporting process, including financial business process and IT management controls.SPA provides:· Financial and operation applications/business process controls reviews· Database security controls reviews· IT general controls reviews· Infrastructure security reviews· Third party assurance and opinion services· Sarbanes-Oxley readiness, process improvement and sustainability services· Due diligence on systems and controls· Pre- and post-implementation systems reviews· Project assurance services· Data services (e.g., CAATs, data quality reviews)· Computer security reviewsResponsibilities (essential functions and responsibilities of the job):The tasks, roles and responsibilities concerned with the job will include the following:Executing various audit engagements as a support to field in-charge andApplying The Firm’s methodology while executing various job rolesApply current knowledge of IT trends and systems to identify security and risk management issues, and other opportunities for improvementAssist in the planning and execution of business process control reviews, information systems audits, and other Assurance offerings across a variety of industriesA good understanding of clients Business Risks, Audit Risks, IT Risks and the mitigating controls in addressing these risksGain strong comprehension of client operations, processes, and business objectives and utilize that knowledge on engagements.Evaluate and test business processes and controls and identify areas of risks.Perform general computer and application controls reviews including ERPs like SAP, Oracle Application, JD Edwards, MFG Pro, etc.Comply with requirements of ICAI Auditing Standards like AAS 29Data Analysis using CAATs tool like Excel, ACL, SQL etc*Mandatory skill sets - Data Analyst*Preferred Skill Sets – Business Analyst*Years of experience required – 0-1 Years*Education Qualification – Post Graduation in Data ScienceEducation (if blank, degree and/or field of study not specified)Degrees/Field of Study required: Postgraduate (Diploma)Degrees/Field of Study preferred:Certifications (if blank, certifications not specified)Required SkillsData AnalyticsOptional SkillsAccepting Feedback, Accepting Feedback, Active Listening, Artificial Intelligence (AI) Platform, Auditing Methodologies, Cloud Engineering, Communication, Compliance and Governance, Compliance and Standards, Compliance Auditing, Corporate Governance, Cybersecurity Governance, Cybersecurity Risk Management, Data Quality, Data Quality Assessment, Data Quality Assurance Testing, Data Validation, Emotional Regulation, Empathy, Governance Framework, Inclusion, Information Assurance, Information Security Governance, Intellectual Curiosity, Internal Controls {+ 11 more}Desired Languages (If blank, desired languages not specified)Travel RequirementsNot SpecifiedAvailable for Work Visa Sponsorship?NoGovernment Clearance Required?NoJob Posting End Date",
         "eyJqb2JfdGl0bGUiOiJJTiBTcGVjaWFsaXN0IDMtIERhdGEgQW5hbHlzdCAtIFRSQVMgLUFzc3VyYW5jZeKAkyBCYW5nYWxvcmUiLCJjb21wYW55X25hbWUiOiJQd0MiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiZTlNdVBjTnNJWHlmb3ZrSkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "1 day ago",
         "Data Analyst"
        ],
        [
         "Data Analyst 1",
         "BRISTOL MYERS SQUIBB",
         "Hyderabad, Telangana, India",
         "The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n\nRoles & Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up to date with industry trends, best practices, and emerging technologies\n\nSkills and competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n• commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• Proven experience (1-3 years) in a similar business analyst role\n• Prior Pharmaceutical industry and/or healthcare consulting experience required\n• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with regulatory requirements and compliance in the US biopharma industry\n\nThe US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n\nRoles & Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up to date with industry trends, best practices, and emerging technologies\n\nSkills and competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n• commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• Proven experience (1-3 years) in a similar business analyst role\n• Prior Pharmaceutical industry and/or healthcare consulting experience required\n• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with regulatory requirements and compliance in the US biopharma industry",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ1ZUlVZURnaVJaU1ludkdlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "16 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst Finance (VBA, Power Query, Power BI, Python) 4+ years of experience)",
         "DUPONT",
         "Hyderabad, Telangana, India",
         "At DuPont, our purpose is to empower the world with essential innovations to thrive. We work on things that matter. Whether it’s providing clean water to more than a billion people on the planet, producing materials that are essential in everyday technology devices from smartphones to electric vehicles, or protecting workers around the world. Discover the many reasons the world’s most talented people are choosing to work at DuPont. Why Join Us | DuPont Careers\n\nJob Summary:\n\nThe Data Analyst/Developer will play a crucial role in developing automated data solutions, creating comprehensive dashboards, and utilizing advanced Excel functionalities. This position will require collaboration with various teams to translate data needs into actionable intelligence.\n\nKey Areas of Expertise and Responsibilities:\n\n1. Visual Basic for Applications (VBA)\n• Responsibilities:\n• Develop and maintain complex VBA applications to automate repetitive tasks.\n• Incorporate SAP Scripting within VBA to optimize business processes.\n• Troubleshoot and debug existing VBA code to ensure smooth functionality.\n• Criteria:\n• Advanced proficiency in VBA programming.\n• Demonstrated experience with SAP interfaces and scripting.\n• Ability to write modular, efficient, and maintainable code.\n• Knowledge of Excel object model and its functionalities.\n\n2. Power Query\n• Responsibilities:\n• Utilize Power Query for data extraction, transformation, and loading (ETL) processes.\n• Develop and maintain data models in Excel to streamline data preparation.\n• Create and optimize Power Query scripts for efficient data processing.\n• Criteria:\n• Intermediate experience with Power Query including M language for data transformation.\n• Familiarity with connecting Power Query to various data sources (e.g., databases, APIs).\n• Ability to perform data cleansing and manipulation through Power Query.\n\n3. Power BI\n• Responsibilities:\n• Create interactive, user-friendly dashboards and reports using Power BI.\n• Collaborate with stakeholders to gather reporting requirements and deliver insights.\n• Optimize Power BI reports for performance and usability.\n• Criteria:\n• Intermediate knowledge of Power BI Desktop and Power BI Service.\n• Ability to create DAX measures and calculated columns for enhanced analytics.\n• Familiarity with data visualization best practices and techniques.\n\n4. Python\n• Responsibilities:\n• Develop Python scripts to automate data manipulation and Excel-related tasks.\n• Utilize libraries such as Pandas and NumPy for data analysis and automation.\n• Collaborate with the data team to integrate Python solutions with existing tools.\n• Criteria:\n• Intermediate proficiency in Python, especially in data manipulation and automation.\n• Experience with libraries like Pandas, NumPy, and openpyxl for Excel automation.\n• Understanding of APIs and ability to retrieve data programmatically.\n\nQualifications:\n• Bachelor’s degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n• A minimum of 4 of experience in a data-related role, focusing on the technologies mentioned.\n• Strong analytical and problem-solving skills with attention to detail.\n• Excellent communication skills and the ability to work collaboratively with diverse teams.\n\nPreferred Skills:\n• Experience with SQL and relational databases for data querying and data management.\n• Familiarity with other business intelligence tools (like Tableau or QlikView) is a plus.\n• Knowledge of machine learning principles is an advantage.\n• Understanding of data warehousing concepts and methodologies.\n\nJoin our Talent Community to stay connected with us!\n\nOn May 22, 2024, we announced a plan to separate our Electronics and Water businesses in a tax-free manner to its shareholders. On January 15, 2025, we announced that we are targeting November 1, 2025, for the completion of the intended separation of the Electronics business (the “Intended Electronics Separation”)*. We also announced that we would retain the Water business. We are committed to ensuring a smooth and successful separation process for the Future Electronics business. We look forward to welcoming new talent interested in contributing to the continued success and growth of our evolving organization.\n\n(1)The separation transactions are subject to satisfaction of customary conditions, including final approval by DuPont's Board of Directors, receipt of tax opinion from counsel, the filing and effectiveness of Form 10 registration statements with the U.S. Securities and Exchange Commission, applicable regulatory approvals, and satisfactory completion of financing.  For further discussion of risks, uncertainties and assumptions that could impact the achievement, expected timing and intended benefits of the separation transactions, see DuPont’s announcement.\n\nDuPont is an equal opportunity employer. Qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability or any other protected class. If you need a reasonable accommodation to search or apply for a position, please visit our Accessibility Page for Contact Information.\n\nDuPont offers a comprehensive pay and benefits package. To learn more visit the Compensation and Benefits page.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgRmluYW5jZSAoVkJBLCBQb3dlciBRdWVyeSwgUG93ZXIgQkksIFB5dGhvbikgNCsgeWVhcnMgb2YgZXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJEdXBvbnQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImNGeEdTMkhJQXpJRW5kb2ZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "2 days ago",
         "Data Analyst"
        ],
        [
         "Sustainability Data Analyst",
         "CARRIER",
         "Hyderabad, Telangana, India",
         "Role: Sustainability Data Analyst\n\nLocation: Hyderabad, India\n\nFull/ Part-time: Full time\n\nBuild a career with confidence\n\nCarrier Global Corporation, global leader in intelligent climate and energy solutions is committed to creating solutions that matter for people and our planet for generations to come. From the beginning, we've led in inventing new technologies and entirely new industries. Today, we continue to lead because we have a world-class, diverse workforce that puts the customer at the center of everything we do.\n\nAbout the role\n\nWe are seeking a results-driven Sustainability Data Analyst to join Carrier’s product sustainability analytics team. The ideal candidate will have a strong background in sustainability practices and data analytics, with a focus on analyzing environmental impact and identifying opportunities for improvement. This role involves conducting comprehensive sustainability/energy audits, analyzing data, and developing strategies to enhance sustainability performance.\n\nKey responsibilities:\n• Perform detailed sustainability audits and analyze data to identify trends and areas for improvement.\n• Develop and implement strategies to enhance sustainability and reduce environmental impact.\n• Monitor and evaluate the performance of sustainability initiatives using data analytics.\n• Collaborate with cross-functional teams to optimize sustainability practices.\n• Prepare reports and presentations on sustainability metrics and audit findings.\n• Stay updated on industry trends and best practices in sustainability and data analytics.\n\nMinimum Requirements:\n\nEducation: Bachelor’s or Master’s degree in Mechanical, Energy Engineering, Environmental Science, Sustainability, Data Analytics, or a related field.\n\nExperience: atleast 6 years in sustainability, data analytics, or BI development roles to be able to drive himself towards business goals independently.\n\nKey Skills:\n• Strong analytical skills, attention to detail and ability to think from first principles.\n• Excellent communication and teamwork abilities.\n• Proficiency in data analytics software and tools. Knowledge of python, SQL, Power-BI, would be added advantage.\n• Knowledge of relevant regulations and standards in sustainability.\n• Familiarity with data visualization tools and techniques.\n• Willingness to be flexible, learn new tools, techniques and deliver.\n\nBenefits\n\nWe are committed to offering competitive benefits programs for all of our employees, and enhancing our programs when necessary.\n• Enjoy your best years with our retirement savings plan\n• Have peace of mind and body with our health insurance\n• Make yourself a priority with flexible schedules, parental leave and our holiday purchase scheme\n• Drive forward your career through professional development opportunities\n• Achieve your personal goals with our Employee Assistance Programme.\n\nOur commitment to you\n\nOur greatest assets are the expertise, creativity and passion of our employees. We strive to provide a great place to work that attracts, develops and retains the best talent, promotes employee engagement, fosters teamwork and ultimately drives innovation for the benefit of our customers. We strive to create an environment where you feel that you belong, with diversity and inclusion as the engine to growth and innovation. We develop and deploy best-in-class programs and practices, providing enriching career opportunities, listening to employee feedback and always challenging ourselves to do better. This is The Carrier Way.\n\nJoin us and make a difference.\n\nApply Now!\n\nCarrier is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.\n\nJob Applicant's Privacy Notice:\n\nClick on this link to read the Job Applicant's Privacy Notice",
         "eyJqb2JfdGl0bGUiOiJTdXN0YWluYWJpbGl0eSBEYXRhIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJDYXJyaWVyIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJPMkpEMHBVcG0xc3dxYnc1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "9 days ago",
         "Data Analyst"
        ],
        [
         "Sr. Data Analyst",
         "ICIMS TALENT ACQUISITION",
         "Rai Durg, Telangana, India",
         "Job Overview\n\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.\n\nAbout Us\n\nWhen you join iCIMS, you join the team helping global companies transform business and the world through the power of talent. Our customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile. As the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent. We’re passionate about helping companies build a diverse, winning workforce and about building our home team. We're dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\n\nResponsibilities\n• Perform various data analysis functions to analyze data from a variety of sources including external labor market data and research and internal data sets from our platforms \n• Incorporate information from a variety of systems to produce comprehensive and compelling narratives for thought-leadership initiatives and customer engagements\n• Demonstrate critical thinking - identify the story in context using multiple data sets, and present results. A strong proficiency in data storytelling will be critical to success in this role.\n• Understand principles of quality data visualization and apply them in Tableau to create and maintain custom dashboards for consumption by other employees\n• Find and investigate data quality issues, root causes and recommend remedies to be implemented by the data scientists and engineers  \n• Liaise with teams around our business to understand their problems, determine how our team can help, then use our database to produce the content they need\n• Identify data mapping and enrichment requirements. Familiarity with SQL, especially the logic behind different types of data joins and writing efficient queries, will be necessary\n• Consistently ensure that business is always conducted with integrity and that behavior aligns with iCIMS policies, procedures, and core competencies\n\nAdditional Job Responsibilities: \n• Produce and adapt data visualizations in response to business requests for internal and external use\n• Shows good judgement in prioritizing their own commitments and those of the larger team, while demonstrating initiative and appropriate urgency when needed\n• Mentor junior team members in best practices for analytics, data visualization, and data storytelling. Exemplify these standards and guide teammates in following them.\n• Think creatively to produce unique, actionable insights from complex datasets, which can deliver value to our business and to our customers.\n\nQualifications\n• 5-10 years professional experience working in an analytics capacity\n• Excellent communication skills, especially with regards to data storytelling – finding insights from complex datasets and sharing those findings with key stakeholders\n• Strong data analytics and visualization skills\n• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n• Advanced knowledge of Excel (Pivot tables, VLOOKUPs, IF statements)\n• Familiarity with data guardrails to ensure compliance with applicable data governance regulations and privacy laws (i.e., GDPR)\n\nPreferred\n• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n\nEEO Statement\n\niCIMS is a place where everyone belongs. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at iCIMS.\n\nWe are proud to be an equal opportunity and affirmative action employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at careers@icims.com.\n\nCompensation and Benefits\n\nCompetitive health and wellness benefits include medical insurance (employee and dependent family members), personal accident and group term life insurance, bonding and parental leave, lifestyle spending account reimbursements, wellness services offerings, sick and casual/emergency days, paid holidays, tuition reimbursement, retirals (PF - employer contribution) and gratuity. Benefits and eligibility may vary by location, role, and tenure. Learn more here: https://careers.icims.com/benefits",
         "eyJqb2JfdGl0bGUiOiJTci4gRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiaUNJTVMgVGFsZW50IEFjcXVpc2l0aW9uIiwiYWRkcmVzc19jaXR5IjoiUmFpIER1cmcsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImZMUlRBQjlESVFWdFdOeGpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "21 days ago",
         "Data Analyst"
        ],
        [
         "Master Data Analyst",
         "C32511 ALFA LAVAL INDIA PRIVATE LIMITED",
         "India",
         "Purpose of the Job: To provide direct support to the Sales and Order Handling teams by managing customer master data and administering the Order Management System. This role ensures data integrity, system readiness, and compliance with internal controls during order booking and execution processes. Key Responsibilities: Customer Master Data Management Manage the full lifecycle of customer codes, including creation, amendment, and deactivation. Conduct background checks using the Global World Check system and coordinate with the Export Control Super User for compliance validation. Ensure customer data is configured correctly in the system for seamless order booking and execution. Maintain data accuracy and cleanliness in line with Alfa Laval’s global data standards. System Administration Act as the system administrator for the Customer Order Management system. Maintain system master data and troubleshoot operational issues. Ensure the system is up-to-date and aligned with business requirements. Order Booking Compliance Perform basic compliance checks during order booking. Verify completeness and correctness of mandatory information. Ensure all required approvals are in place as per the authorization matrix. Process Activation Initiate internal processes required for order execution. Collaborate with cross-functional teams to ensure timely and accurate order processing. Key Competencies: Strong attention to detail and data accuracy Knowledge of compliance and export control processes Proficiency in ERP and order management systems Analytical and problem-solving skills Effective communication and collaboration across teams Qualifications & Experience: Bachelor’s degree in Business Administration, Information Systems, or related field 3+ years of experience in data management or order processing roles Familiarity with data governance and compliance frameworks Experience with customer master data and ERP systems (e.g., SAP) \"We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\" Every day, we get opportunities to make a positive impact – on our colleagues, partners, customers and society. Together, we’re pioneering the solutions of the future and unlocking the full potential of precious resources. Trusted to act on initiative, we challenge conventional thinking to develop world-leading technologies that inspire progress in vital areas, including energy, food, water and shipping. As we push forward, the innovative, open spirit that fuels our 140-year-old start-up culture and rapid growth also drives our personal growth. So, as we shape a more resourceful, less wasteful world, we build our careers too. Meet our dedicated people who always go the extra mile Alfa Laval Career Site Or sign up for Job Alerts to stay in touch Sign up for Job Alerts",
         "eyJqb2JfdGl0bGUiOiJNYXN0ZXIgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiQzMyNTExIEFsZmEgTGF2YWwgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImhJMnc5MHYxS1E3QXVQTkNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "6 days ago",
         "Data Analyst"
        ],
        [
         "Engr II-Data Engineering",
         "VERIZON",
         "India",
         "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat You’ll Be Doing...\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform and analytics teams, you will understand and enable the required data sets from different sources. This includes both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n• Understanding the business requirements.\n• Transforming technical design.\n• Working on data ingestion, preparation and transformation.\n• Developing the scripts for data sourcing and parsing.\n• Developing data streaming applications.\n• Debugging the production failures and identifying the solution.\n• Working on ETL/ELT development.\n\nWhat We’re Looking For...\n\nYou’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems.\n\nYou'll Need To Have\n• Bachelor’s degree or one or more years of relevant experience required, demonstrated through work experience and/or military experience.\n• Experience with Data Warehouse concepts and Data Management life cycle.\n\nEven better if you have one or more of the following:\n• Any related Certification on ETL/ELT developer.\n• Accuracy and attention to detail.\n• Good problem solving, analytical, and research capabilities.\n• Good verbal and written communication.\n• Experience presenting to and influencing partners.\n\nWhy Verizon?\n\nVerizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.\n• We are a ‘pay for performance’ company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.\n• Your benefits are market competitive and delivered by some of the best providers.\n• You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.\n• We offer generous paid time off benefits to help you manage your work life balance and opportunities for flexible working arrangements*.\n• Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.\n• You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.\n\nYour benefits package will vary depending on the country in which you work.\n• subject to business approval\n\nIf Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.\n\nWhere you’ll be working\n\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.",
         "eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkpaY3lnNzJpTUJGcHpfUTJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "1 day ago",
         "Data Engineer"
        ],
        [
         "Associate Analyst - Data Engineer",
         "PEPSICO",
         "Hyderabad, Telangana, India",
         "Overview\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT.  The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics, and new product development.  PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\n\nWhat PepsiCo Data Management and Operations does:\n\nMaintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\n\nResponsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\n\nWork cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\n\nIncrease awareness about available data and democratize access to it across the company.\n\n \n\n               As a data engineer, you will be the key technical expert building PepsiCo's data products to drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help developing very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n\nResponsibilities\n• Act as a subject matter expert across different digital projects.\n• Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\n• Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\n• Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\n• Responsible for implementing best practices around systems integration, security, performance, and data management.\n• Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\n• Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\n• Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\n• Develop and optimize procedures to “productionalize” data science models.\n• Define and manage SLA’s for data products and processes running in production.\n• Support large-scale experimentation done by data scientists.\n• Prototype new approaches and build solutions at scale.\n• Research in state-of-the-art methodologies.\n• Create documentation for learnings and knowledge transfer.\n• Create and audit reusable packages or libraries.\n\nQualifications\n• 4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n• 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n• 3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n• 2+ years in cloud data engineering experience in Azure.\n• Fluent with Azure cloud services. Azure Certification is a plus.\n• Experience in Azure Log Analytics\n• Experience with integration of multi cloud services with on-premises technologies.\n• Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\n• Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\n• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n• Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\n• Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\n• Experience with Statistical/ML techniques is a plus.\n• Experience with building solutions in the retail or in the supply chain space is a plus.\n• Experience with version control systems like Github and deployment & CI tools.\n• Working knowledge of agile development, including DevOps and DataOps concepts.\n• B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.\n\n Skills, Abilities, Knowledge:\n• Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\n• Strong change manager. Comfortable with change, especially that which arises through company growth.\n• Ability to understand and translate business requirements into data and technical requirements.\n• High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\n• Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\n• Strong organizational and interpersonal skills; comfortable managing trade-offs.",
         "eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUgQW5hbHlzdCAtIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJQZXBzaUNvIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJkdUI4NkhSU1JwRWNPdlBCQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "1 month ago",
         "Data Engineer"
        ],
        [
         "Senior Big Data Engineer",
         "QUALCOMM",
         "Hyderabad, Telangana, India",
         "Company:\nQualcomm India Private Limited\n\nJob Area:\nEngineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\n• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n• 2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary:\n\nPreferred Qualifications\n• 3+ years of experience as a Data Engineer or in a similar role\n• Experience with data modeling, data warehousing, and building ETL pipelines\n• Solid working experience with Python, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n• Experience with Big Data tools, platforms and architecture with solid working experience with SQL\n• Experience working in a very large data warehousing environment, Distributed System.\n• Solid understanding on various data exchange formats and complexities\n• Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n• Strong data visualization skills\n• Basic understanding of Machine Learning; Prior experience in ML Engineering a plus\n• Ability to manage on-premises data and make it inter-operate with AWS based pipelines\n• Ability to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\nEducation\n• Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n• Preferred Qualifications: Masters in CS/ECE with a Data Science / ML Specialization\n\nMinimum Qualifications:\n• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n• 3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n• Completes assigned coding tasks to specifications on time without significant errors or bugs.\n• Adapts to changes and setbacks in order to manage pressure and meet deadlines.\n• Collaborates with others inside project team to accomplish project objectives.\n• Communicates with project lead to provide status and information about impending obstacles.\n• Quickly resolves complex software issues and bugs.\n• Gathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n• Seeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n• Participates in technical conversations with tech leads/managers.\n• Anticipates and communicates issues with project team to maintain open communication.\n• Makes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n• Prioritizes project deadlines and deliverables with minimal supervision.\n• Resolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n• Writes readable code for large features or significant bug fixes to support collaboration with other engineers.\n• Determines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n• Unit tests own code to verify the stability and functionality of a feature.\n\nApplicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n\nQualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n\nTo all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n\nIf you would like more information about this role, please contact Qualcomm Careers.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQmlnIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJRdWFsY29tbSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieVRVTHdrQjB2Ujk5bmJQUUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 days ago",
         "Data Engineer"
        ],
        [
         "Manager Data Engineer - AWS Databricks",
         "BLEND360",
         "Hyderabad, Telangana, India",
         "Company Description\n\nBlend is a premier AI services provider, committed to co-creating meaningful impact for its clients through the power of data science, AI, technology, and people. With a mission to fuel bold visions, Blend tackles significant challenges by seamlessly aligning human expertise with artificial intelligence. The company is dedicated to unlocking value and fostering innovation for its clients by harnessing world-class people and data-driven strategy. We believe that the power of people and AI can have a meaningful impact on your world, creating more fulfilling work and projects for our people and clients. For more information, visit www.blend360.com\n\nJob Description\n\nWe are seeking a seasoned Data Engineering Manager with 8+ years of experience to lead and grow our data engineering capabilities. This role demands strong hands-on expertise in Python, SQL, Spark, and advanced proficiency in AWS and Databricks. As a technical leader, you will be responsible for architecting and optimizing scalable data solutions that enable analytics, data science, and business intelligence across the organization.\n\nKey Responsibilities:\n• Lead the design, development, and optimization of scalable and secure data pipelines using AWS services such as Glue, S3, Lambda, EMR, and Databricks Notebooks, Jobs, and Workflows.\n• Oversee the development and maintenance of data lakes on AWS Databricks, ensuring performance and scalability.\n• Build and manage robust ETL/ELT workflows using Python and SQL, handling both structured and semi-structured data.\n• Implement distributed data processing solutions using Apache Spark/PySpark for large-scale data transformation.\n• Collaborate with cross-functional teams including data scientists, analysts, and product managers to ensure data is accurate, accessible, and well-structured.\n• Enforce best practices for data quality, governance, security, and compliance across the entire data ecosystem.\n• Monitor system performance, troubleshoot issues, and drive continuous improvements in data infrastructure.\n• Conduct code reviews, define coding standards, and promote engineering excellence across the team.\n• Mentor and guide junior data engineers, fostering a culture of technical growth and innovation.\n\nQualifications\n\nRequirements\n• 8+ years of experience in data engineering with proven leadership in managing data projects and teams.\n• Expertise in Python, SQL, Spark (PySpark), and experience with AWS and Databricks in production environments.\n• Strong understanding of modern data architecture, distributed systems, and cloud-native solutions.\n• Excellent problem-solving, communication, and collaboration skills.\n• Prior experience mentoring team members and contributing to strategic technical decisions is highly desirable.",
         "eyJqb2JfdGl0bGUiOiJNYW5hZ2VyIERhdGEgRW5naW5lZXIgLSBBV1MgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJsZW5kMzYwIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJxelhlVGQtWWc2ekE5U3R4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "3 days ago",
         "Data Engineer"
        ],
        [
         "Data Engineer INTL India - EOR 6fb570f8",
         "INSIGHT GLOBAL",
         "Hyderabad, Telangana, India",
         "- In this role, you will be building data pipeline solutions by designing, adopting, and applying big data strategies and architectures. You must have knowledge of scalable system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration, data transformation, data modeling, and data cleansing.\n- The Sr. Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for business stakeholders.\n- Design and develop a logical data model for a large multi-tenant application with intent to facilitate consistency and efficiency.\n- Collaborate with analysts and internal clients to understand the goals of the overall system data architecture, and further develop on business needs/issues.\n- Test/troubleshoot problems and conduct root cause analysis.\n- Own communications with technical contacts at software vendors to escalate software issues, recommend enhancements, and find sustainable workarounds to issues.\n- Work in tandem with analysts and other stakeholders to develop and execute necessary processes and controls around the flow of data to meet data governance standards.\n- Verify accuracy of table changes and data transformation processes\n- Deliver fully tested code prior to prod-deployment when appropriate.\n- Recommend and implement enhancements that address system performance requirements, streamline processes and improve data integrity.\n- Create sound technical documentation and train peer developers on this documentation as development completes.\n- Additional duties as assigned to ensure company success.\nThe compensation for this role varies depending on depth and length of experience, and can range from 27-36 LAKH.\n\nWe are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElOVEwgSW5kaWEgLSBFT1IgNmZiNTcwZjgiLCJjb21wYW55X25hbWUiOiJJbnNpZ2h0IEdsb2JhbCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiZUtoaVo5cWozWC1wLXlBVEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 days ago",
         "Data Engineer"
        ],
        [
         "R&D Data Engineer",
         "SANOFI",
         "Hyderabad, Telangana, India",
         "Position Title: R&D Data Engineer\n\nAbout the Job\n\nAt Sanofi, we’re committed to providing the next-gen healthcare that patients and customers need. It’s about harnessing data insights and leveraging AI responsibly to search deeper and solve sooner than ever before. Join our R&D Data & AI Products and Platforms Team as an R&D Data Engineer\n\nand you can help make it happen.\n\nWhat you will be doing:\n\nSanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives.\n\nThe R&D Data & AI Products and Platforms Team is a key team within R&D Digital, focused on developing and delivering Data and AI products for R&D use cases. This team plays a critical role in pursuing broader democratization of data across R&D and providing the foundation to scale AI/ML, advanced analytics, and operational analytics capabilities.\n\nAs an R&D Data Engineer, you will join this dynamic team committed to driving strategic and operational digital priorities and initiatives in R&D. You will work as a part of a Data & AI Product Delivery Pod, lead by a Product Owner, in an agile environment to deliver Data & AI Products. As a part of this team, you will be responsible for the design and development of data pipelines and workflows to ingest, curate, process, and store large volumes of complex structured and unstructured data. You will have the ability to work on multiple data products serving multiple areas of the business.\n\nOur vision for digital, data analytics and AI\n\nJoin us on our journey in enabling Sanofi’s Digital Transformation through becoming an AI first organization. This means:\n• AI Factory - Versatile Teams Operating in Cross Functional Pods: Utilizing digital and data resources to develop AI products, bringing data management, AI and product development skills to products, programs and projects to create an agile, fulfilling and meaningful work environment.\n• Leading Edge Tech Stack: Experience build products that will be deployed globally on a leading-edge tech stack.\n• World Class Mentorship and Training: Working with renown leaders and academics in machine learning to further develop your skillsets.\n\nWe are an innovative global healthcare company with one purpose: to chase the miracles of science to improve people’s lives. We’re also a company where you can flourish and grow your career, with countless opportunities to explore, make connections with people, and stretch the limits of what you thought was possible. Ready to get started?\n\nMain Responsibilities:\n\nData Product Engineering:\n• Provide input into the engineering feasibility of developing specific R&D Data/AI Products\n• Provide input to Data/AI Product Owner and Scrum Master to support with planning, capacity, and resource estimates\n• Design, build, and maintain scalable and reusable ETL / ELT pipelines to ingest, transform, clean, and load data from sources into central platforms / repositories\n• Structure and provision data to support modeling and data discovery, including filtering, tagging, joining, parsing and normalizing data\n• Collaborate with Data/AI Product Owner and Scrum Master to share Progress on engineering activities and inform of any delays, issues, bugs, or risks with proposed remediation plans\n• Design, develop, and deploy APIs, data feeds, or specific features required by product design and user stories\n• Optimize data workflows to drive high performance and reliability of implemented data products\n• Oversee and support junior engineer with Data/AI Product testing requirements and execution\n\nInnovation & Team Collaboration:\n• Stay current on industry trends, emerging technologies, and best practices in data product engineering\n• Contribute to a team culture of of innovation, collaboration, and continuous learning within the product team\n\nAbout You:\n\nKey Functional Requirements & Qualifications:\n• Bachelor’s degree in software engineering or related field, or equivalent work experience\n• 3-5 years of experience in data product engineering, software engineering, or other related field\n• Understanding of R&D business and data environment preferred\n• Excellent communication and collaboration skills\n• Working knowledge and comfort working with Agile methodologies\n\nKey Technical Requirements & Qualifications:\n• Proficiency with data analytics and statistical software (incl. SQL, Python, Java, Excel, AWS, Snowflake, Informatica)\n• Deep understanding and proven track record of developing data pipelines and workflows\n\nWhy Choose Us?\n• Bring the miracles of science to life alongside a supportive, future-focused team\n• Discover endless opportunities to grow your talent and drive your career, whether it’s through a promotion or lateral move, at home or internationally\n• Enjoy a thoughtful, well-crafted rewards package that recognizes your contribution and amplifies your impact\n• Take good care of yourself and your family, with a wide range of health and wellbeing benefits including high-quality healthcare, prevention and wellness programs\n\nPursue Progress. Discover Extraordinary.\n\nProgress doesn’t happen without people – people from different backgrounds, in different locations, doing different roles, all united by one thing: a desire to make miracles happen. You can be one of those people. Chasing change, embracing new ideas and exploring all the opportunities we have to offer. Let’s pursue progress. And let’s discover Extraordinary together.\n\nAt Sanofi, we provide equal opportunities to all regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or gender identity.\n\nWatch our ALL IN video and check out our Diversity Equity and Inclusion actions at sanofi.com!",
         "eyJqb2JfdGl0bGUiOiJSXHUwMDI2RCBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiU2Fub2ZpIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJWUzRuREMyZ2hRajk3SVhDQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "Data Engineer"
        ],
        [
         "Data Engineer-Senior II",
         "FEDERAL EXPRESS CORPORATION AMEA",
         "Hyderabad, Telangana, India",
         "Responsible for supporting the development and maintenance of business intelligence and analytics systems, implementing data warehouse architecture, enabling self-service data exploration, designing data models, implementing data security strategies, optimizing production processes, providing user training and support, and overseeing project management activities as a Data Engineer to facilitate data-driven decision-making and generate insights for business stakeholders.\n\n1. Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making.\n2. Implement of business intelligence and analytics systems, ensuring alignment with business requirements.\n3. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets.\n4. Enable self-service data exploration capabilities for users to analyze and visualize data independently.\n5. Develop reporting and analysis applications to generate insights from data for business stakeholders.\n6. Design and implement data models to organize and structure data for analytical purposes.\n7. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information.\n8. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability.\n9. Assist in training and support to users on business intelligence tools and applications.\n10. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions.\n\nEducation: Bachelors’ degree or equivalent in Computer Science, MIS, or similar discipline.\nAccreditation: Specific business accreditation for Business Intelligence.\n\nExperience: Relevant work experience in data engineering based on the following number of years:\nAssociate: Prior experience not required\nStandard I: Two (2) years\nStandard II: Three (3) years\nSenior I: Four (4) years\nSenior II: Five (5) years\n\nKnowledge, Skills and Abilities\n• Fluency in English\n• Analytical Skills\n• Accuracy & Attention to Detail\n• Numerical Skills\n• Planning & Organizing Skills\n• Presentation Skills\n\nPreferred Qualifications:\n\nPay Transparency:\n\nPay:\n\nAdditional Details:\n\nFedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n\nAll qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\nOur Company\n\nFedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World’s Most Admired Companies by \"Fortune\" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\nOur Philosophy\n\nThe People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\nOur Culture\n\nOur culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970’s. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today’s global marketplace.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVNlbmlvciBJSSIsImNvbXBhbnlfbmFtZSI6IkZlZGVyYWwgRXhwcmVzcyBDb3Jwb3JhdGlvbiBBTUVBIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJhZDU5RmU0Vl94QzdTVGFqQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "1 day ago",
         "Data Engineer"
        ],
        [
         "Lead Data Engineer(Snowflake,PowerBi)",
         "THOMSON REUTERS",
         "Hyderabad, Telangana, India (+1 other)",
         "Want to be part of the Data & Analytics organization, whose strategic goal is to create a world-class Data & Analytics company by building, embedding, and maturing a data-driven culture across Thomson Reuters.\n\nAbout The Role\nWe are looking for a highly motivated individual with strong organizational and technical skills for the position of Lead Data Engineer/ Data Engineering Manager (Snowflake). You will play a critical role working on cutting edge of Data Engineering and analytics, leveraging predictive models, machine learning and generative AI to drive business insights and facilitating informed decision-making and help Thomson Reuters rapidly scale data-driven initiatives.\n\nEffectively communicate across various levels, including Executives, and functions within the global organization.\nDemonstrate strong leadership skills with ability to drive projects/tasks to delivering value\nEngage with stakeholders, business analysts and project team to understand the data requirements.\nDesign analytical frameworks to provide insights into a business problem.\nExplore and visualize multiple data sets to understand data available and prepare data for problem solving.\nDesign database models (if a data mart or operational data store is required to aggregate data for modeling).\n\nAbout You\nYou're a fit for the Lead Data Engineer/ Data Engineering Manager (Snowflake), if your background includes:\nQualifications: B-Tech/M-Tech/MCA or equivalent\nExperience: 7-9 years of corporate experience\nLocation: Bangalore, India\nHands-on experience in developing data models for large scale data warehouse/data Lake – Snowflake, BW\nMap the data journey from operational system sources through any transformations in transit to its delivery into enterprise repositories (Warehouse, Data Lake, Master Data, etc.)\nEnabling on the overall master and reference data strategy, including the procedures to ensure the consistency and quality of Finance reference data.\nExperience across ETL, SQL and other emerging data technologies with experience in integrations of a cloud-based analytics environment\nBuild and refine end-to-end data workflows to offer actionable insights\nFair understanding of Data Strategy, Data Governance Process\nKnowledge in BI analytics and visualization tools: Power BI, Tableau\n\n#LI-NR1\n\nWhat’s in it For You?\n• Hybrid Work Model: We’ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\n• Flexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\n• Career Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow’s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\n• Industry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\n• Culture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\n• Social Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\n• Making a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\n\nAbout Us\n\nThomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\n\nWe are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\n\nAs a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\n\nWe also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here.\n\nLearn more on how to protect yourself from fraudulent job postings here.\n\nMore information about Thomson Reuters can be found on thomsonreuters.com.",
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIoU25vd2ZsYWtlLFBvd2VyQmkpIiwiY29tcGFueV9uYW1lIjoiVGhvbXNvbiBSZXV0ZXJzIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJzSFl5TXBoOXlWbDl4R0pBQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "13 days ago",
         "Data Engineer"
        ],
        [
         "Engr II-Data Engineering",
         "VERIZON",
         "Hyderabad, Telangana, India (+1 other)",
         "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat you’ll be doing…\n\nWe are looking for data engineers who can work with world class team members to help drive telecom business to its full potential . We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n\nAs a data engineer you will be collaborating with business product owners , coaches , industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n\nUnderstanding the business requirements and the technical design.\n\nWorking on Data Ingestion, Preparation and Transformation.\n\nDeveloping data streaming applications.\n\nDebugging the production failures and identifying the solution.\n\nWorking on ETL/ELT development.\n\nWhere you'll be working:\n\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nWhat we’re looking for...\n\nYou’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n\nYou’ll need to have:\n\nBachelor’s degree or one or more years of work experience.\n\nExperience with Data Warehouse concepts and Data Management life cycle.\n\nExperience in any DBMS\n\nExperience in Shell scripting, Spark, Scala.\n\nKnowledge in GCP/BigQuery.\n\nEven better if you have:\n\nTwo or more years of relevant experience.\n\nAny relevant Certification on ETL/ELT developer.\n\nCertification in GCP-Data Engineer.\n\nAccuracy and attention to detail.\n\nGood problem solving, analytical, and research capabilities.\n\nGood verbal and written communication.\n\nExperience presenting to and influence stakeholders.\n\n#AI&D\n\nWhere you’ll be working\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.",
         "eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJJZ05hWTZ2NWxkNUFrcWRyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "Data Engineer"
        ],
        [
         "Senior Data Engineer - Data Integration",
         "EPAM SYSTEMS",
         "Hyderabad, Telangana, India",
         "EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.\n\nOur company is looking for an experienced Senior Data Engineer to join our team.\n\nAs a Senior Data Engineer, you will be working on a project that focuses on data integration and ETL for cloud-based platforms. You will be responsible for designing and implementing complex data solutions, ensuring that the data is accurate, reliable, and easily accessible.\n\nRESPONSIBILITIES\n• Design and implement complex data solutions for cloud-based platforms\n• Develop ETL processes using SQL, Python, and other relevant technologies\n• Ensure that data is accurate, reliable, and easily accessible for all stakeholders\n• Collaborate with cross-functional teams to understand data integration needs and requirements\n• Develop and maintain documentation, including technical specifications, data flow diagrams, and data mappings\n• Monitor and optimize data integration processes for performance and efficiency, ensuring data accuracy and integrity\n\nREQUIREMENTS\n• Bachelor's degree in Computer Science, Electrical Engineering, or a related field\n• 5-8 years of experience in data engineering\n• Experience with cloud-native or Spark-based ETL tools such as AWS Glue, Azure Data Factory, or GCP Dataflow\n• Strong knowledge of SQL for data querying and manipulation\n• Experience with Snowflake for data warehousing\n• Experience with cloud platforms such as AWS, GCP, or Azure for data storage and processing\n• Excellent problem-solving skills and attention to detail\n• Good verbal and written communication skills in English at a B2 level\n\nNICE TO HAVE\n• Experience with ETL using Python\n\nWE OFFER\n• Opportunity to work on technical challenges that may impact across geographies\n• Vast opportunities for self-development: online university, knowledge sharing opportunities globally, learning opportunities through external certifications\n• Opportunity to share your ideas on international platforms\n• Sponsored Tech Talks & Hackathons\n• Unlimited access to LinkedIn learning solutions\n• Possibility to relocate to any EPAM office for short and long-term projects\n• Focused individual development\n• Benefit package\n• Health benefits\n• Retirement benefits\n• Paid time off\n• Flexible benefits\n• Forums to explore beyond work passion (CSR, photography, painting, sports, etc.)",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAtIERhdGEgSW50ZWdyYXRpb24iLCJjb21wYW55X25hbWUiOiJFUEFNIFN5c3RlbXMiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZnZkJvSVM5OFhDSUw3NWNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "Data Engineer"
        ],
        [
         "Python Developer – Telegram Bot Integration & Excel Automation",
         "SANGA & ASSOCIATES - EQUIDOTE",
         "Anywhere",
         "Job Title:\n\nPython Developer – Telegram Bot Integration & Excel Automation\n\nJob Description:\n\nWe are seeking a skilled and detail-oriented Python Developer to help automate the process of sending structured trade updates from an Excel file to a Telegram channel using a Telegram bot.\n\nThis is a freelance / part-time project with the potential for ongoing work based on performance.\n\nResponsibilities:\n• Read data from an Excel file that is regularly updated using Python.\n• Format and send messages to a Telegram channel or group using the Telegram Bot API.\n• Implement conditions to filter or trigger messages based on specific columns (e.g., P&L thresholds).\n• Ensure the messages are well-formatted and synchronized.\n• (Optional) Schedule the script to run at regular intervals (e.g., every 5 mins or on update).\n• Ensure error handling, avoid duplicate messages, and maintain clean logs.\n\nRequired Skills:\n• Strong experience with Python scripting\n• Proficiency in using pandas for Excel/CSV handling\n• Working knowledge of the Telegram Bot API\n• Experience with HTTP requests (requests library)\n• Ability to format dynamic messages (Markdown/HTML for Telegram)\n• (Optional but a plus) Familiarity with scheduling tools like schedule, cron jobs, or apscheduler\n\nNice to Have:\n• Understanding of stock market data or options trading (for better context)\n• Experience integrating with trading APIs or using TradingView alerts\n• Basic knowledge of Excel automation or VBA\n\nProject Details:\n• Project Type: One-time setup, with possible ongoing maintenance\n• Location: Remote (India preferred)\n• Start Date: Immediate\n\nHow to Apply:\n\nPlease apply with:\n• A short summary of your experience with Python + Telegram Bots\n• A link to any relevant projects or GitHub repos\n• Your expected rate and estimated time to complete the task\n\nJob Type: Freelance\n\nBenefits:\n• Health insurance\n• Provident Fund\n• Work from home\n\nSchedule:\n• Day shift\n\nSupplemental Pay:\n• Performance bonus\n• Yearly bonus\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBUZWxlZ3JhbSBCb3QgSW50ZWdyYXRpb24gXHUwMDI2IEV4Y2VsIEF1dG9tYXRpb24iLCJjb21wYW55X25hbWUiOiJTQU5HQSBcdTAwMjYgQVNTT0NJQVRFUyAtIEVRVUlET1RFIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNBNXdGSEplRUV3c2lQVWJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "1 day ago",
         "Python Developer"
        ],
        [
         "Python Developer - Remote",
         "XPRESS HEALTH",
         "Anywhere",
         "Job Title: Python Developer\nLocation: Remote\nSalary: Up to ₹12 LPA (based on experience and skillset)\nExperience: 3–6 years (preferred)\nEmployment Type: Full-time\n\nAbout Xpress Health\n\nXpress Health is a healthtech platform transforming clinical workforce management by connecting healthcare professionals to facilities in real time. With operations across Ireland and rapid expansion underway, we’re solving critical staffing challenges in the healthcare sector using advanced technology and automation.\n\nRole Overview\n\nWe are looking for a Python Developer with solid experience in backend development and building scalable, secure systems. You will work closely with cross-functional teams to build backend services, automate operations, and power data-driven features in our real-time healthcare staffing platform. If you're passionate about solving real-world problems through code, this role is for you.\n\nKey Responsibilities\n• Design, develop, and maintain backend services and APIs using Python (preferably with frameworks like Django or Flask).\n• Build scalable systems for real-time scheduling, user management, and analytics.\n• Integrate third-party APIs and internal services securely and efficiently.\n• Work with databases (SQL and NoSQL) to design efficient schemas and queries.\n• Optimize performance and ensure system reliability under scale.\n• Collaborate with frontend, product, and QA teams to deliver complete features.\n• Write clean, maintainable, and well-documented code.\n• Participate in code reviews, system design discussions, and architecture planning.\n\nRequirements\n• 3–6 years of professional experience with Python backend development.\n• Strong knowledge of Django, Flask, or other web frameworks.\n• Proficient in working with relational databases (MySQL/PostgreSQL) and REST APIs.\n• Experience with Git, CI/CD pipelines, and working in an Agile environment.\n• Strong debugging, testing, and problem-solving skills.\n• Good communication and ability to collaborate with remote teams.\n\nPreferred Qualifications\n• Experience in healthcare, staffing, or enterprise SaaS platforms.\n• Familiarity with containerization tools like Docker and orchestration platforms (Kubernetes).\n• Exposure to cloud platforms like AWS, GCP, or Azure.\n• Knowledge of async programming and task queues (e.g., Celery, Redis).\n• Experience working with frontend teams using React/Vue (a plus).\n\nWhat We Offer\n• Competitive salary up to ₹12 LPA, depending on experience.\n• A mission-driven environment working on meaningful, real-world problems.\n• Opportunity to shape a rapidly scaling healthtech product.\n• Flexible work culture with remote options and learning opportunities.\n• Collaborative, cross-functional team with international exposure.\n\nBe part of a product-first team building real-time tech that supports critical healthcare systems. Join Xpress Health as a Python Developer !!!\n\nJob Type: Full-time\n\nPay: Up to ₹1,200,000.00 per year\n\nBenefits:\n• Paid time off\n• Work from home\n\nLocation Type:\n• Remote\n\nSchedule:\n• Evening shift\n• Fixed shift\n• Monday to Friday\n• UK shift\n\nApplication Question(s):\n• What is your current and expected CTC?\n• Are you currently working? If yes, what is your notice period?\n\nExperience:\n• Python : 5 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIC0gUmVtb3RlIiwiY29tcGFueV9uYW1lIjoiWHByZXNzIEhlYWx0aCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJGWjc0LW4wM2NjU2xGWVVOQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "2 days ago",
         "Python Developer"
        ],
        [
         "Full Stack Developer (Python / React JS)",
         "HITACHI CAREERS",
         "India",
         "Our Company\n\nWe're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n\nOur group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n\nImagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n\nThe team\n\nWe are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n\nThe role: Full Stack Developer/Specialist\n\nResponsibilities:\n• Design, develop, and maintain applications.\n• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n• Optimise applications for performance, scalability, and user experience.\n• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n• Provide training and support to end-users and IT staff on functionalities and best practices.\n• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n• Participate in project planning, execution, and post-implementation support.\n• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n• What you'll bring\nQualifications:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n• Expertise in development and customisation.\n• Proficiency in Python and React JS\n• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n• Experience with RESTful APIs and web services.\n• Excellent problem-solving skills and attention to detail.\n• Strong communication and collaboration skills.\n• Ability to work independently and as part of a team in a fast-paced environment.\n• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n• Prior experience in building AI applications is a plus.\n• Prior experience with automation tools like UIPath is a plus.\n• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\nPreferred Skills\n• Certification in\n• Experience with other low-code/no-code platforms.\n• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n• Experience with Agile development methodologies.\nAbout us\n\nWe're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n\n#LI-MS3\n\nChampioning diversity, equity, and inclusion\n\nDiversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n\nHow we look after you\n\nWe help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\nWe're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",
         "eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "Python Developer"
        ],
        [
         "Python Developer Role",
         "PITANGENT ANALYTICS AND TECHNOLOGY SOLUTIONS PVT. LTD.",
         "India",
         "Overview\n\nPi tangent Group of Companies is an ISO:9001: 2015 Certified, CMMIL-3, Award winning Software Development Company in Eastern India. It caters to areas like AI/ML to Web development to SAAS engineering. The Group is actively seeking out talented, enthusiastic Python Developers as a value addition to the growing team at Kolkata.\n\nKey Responsibilities\n• Design and develop robust backend applications using Python.\n• Collaborate with front-end developers to integrate user-facing elements with server-side logic.\n• Implement RESTful APIs for seamless communication between server and client.\n• Write reusable, testable, and efficient code following best practices.\n• Manage and optimize multiple databases and data storage solutions.\n• Perform unit and integration testing to ensure software reliability.\n• Participate in code reviews and maintain version control in Git.\n• Gather and analyze user requirements to provide optimal solutions\n• Contribute to project documentation and specifications.\n• Collaborate with QA engineers to troubleshoot and resolve issues.\n• Maintain quality assurance processes to ensure best practices are enforced.\n• Engage in agile development practices, participating in sprints and meetings.\n• Mentor junior developers and provide guidance as needed.\n\nRequired Qualifications\n• Bachelor's degree in computer science or related field.\n• 1-2 yrs of experience in Python development.\n• Strong understanding of Django or Flask web frameworks.\n• Proficient with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB).\n• Experience with version control systems, preferably Git.\n• Solid understanding of RESTful API design principles.\n• Familiarity with front-end technologies (HTML, CSS, JavaScript).\n• Experience with containerization tools such as Docker.\n• Strong communication and teamwork abilities.\n• Familiarity with cloud services (AWS, Azure) is a plus.\n• Understanding of security principles and best practices.\n• Experience with Agile/Scrum methodologies.\n• Proven ability to manage multiple tasks and meet deadlines.\n\nSkills: agile,communication,nosql,scrum,aws lambda,docker,sql,azure,html,javascript,mongodb,css,django,aws,git,postgresql,restful apis,python,flask",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIFJvbGUiLCJjb21wYW55X25hbWUiOiJQaXRhbmdlbnQgQW5hbHl0aWNzIGFuZCBUZWNobm9sb2d5IFNvbHV0aW9ucyBQdnQuIEx0ZC4iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiLUNHQ2RNdUVNQ2V4X3F2MkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "2 days ago",
         "Python Developer"
        ],
        [
         "Python and Groovy Framework Developer",
         "APTITA",
         "India",
         "Urgent Hiring!!!\n\nRole : Python and Groovy Framework Developer\n\nMandatory Skills: Python, Appium, Groovy, Git\n\nExperience: 3 to 8 Years\n\nLocation: Bengaluru\n\nContract - 1Year\n\nJob Description:\n\nQualifications\n\n Bachelor’s or master’s degree in Computer Science, Software Engineering, or a\n\nrelated field\n\n 6 to 8 years of relevant experience in quality assurance, with a proven track record in\n\nWebKit or browser engine testing, including team leadership responsibilities.\n\n Proficiency in CI/CD and test automation frameworks such as Appium, and scripting\n\nlanguages like Python, or Shell.\n\nJob Overview\n\nWe are seeking a dynamic and experienced Lead Python & Groovy Framework developer\n\nto join our team You will be part of a fast-paced, Agile development team and work on a\n\nvariety of projects, from building new tools and solutions to improving existing ones.\n\nIn this role, you will have the chance to grow your skills and take your career to the next\n\nlevel. We offer a supportive, challenging, and exciting work environment, with\n\nopportunities for professional development, training, and advancement.\n\nIf you are a Python & Groovy Framework Developer Engineer with a passion for\n\ntechnology and a drive to continuously improve processes, we want to hear from you!\n\nIf you are passionate about browser engine technologies, performance optimization, and\n\nleadership, we encourage you to apply!\n\nPrimary Skills:\n\n Strong experience in Python Framework development, with the ability to automate\n\nand optimize processes using Jenkins Pipeline script\n\n Good knowledge in Groovy scripting\n\n Expertise in CI/CD tools such as Jenkins, CircleCI, or GitLab\n\n Good understanding of Appium.\n\nStrong Problem solving and debugging skills.\n\n Excellent communication and collaboration skills, both with technical and non-\n\ntechnical stakeholders\n\n Version Control: Familiarity with version control systems such as Git for reviewing\n\nchanges and ensuring test coverage.\n\n Communication: Strong communication and collaboration skills for working with\n\ncross-functional teams.\n\n Agile Methodologies: Experience with Agile Scrum methodologies\n\nNotice Period: Immediate- 30 Days\n\nEmail to : sharmila.m@aptita.com\n\n·",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gYW5kIEdyb292eSBGcmFtZXdvcmsgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQXB0aXRhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Imx1ckt5NFVRSi0wb3JkMi1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "2 days ago",
         "Python Developer"
        ],
        [
         "AI Python Developer",
         "ALLIANZ INSURANCE",
         "India",
         "We are seeking a skilled AI Developer proficient in Python to support various AI use cases within our dynamic team. The ideal candidate will possess a strong understanding of Python programming and a basic familiarity with PyTest. This role will also provide opportunities to work with cloud computing technologies, particularly Azure, and develop APIs.\n\nKey Responsibilities:\n• Utilize advanced Python skills to support and enhance AI use cases, ensuring the efficient and effective implementation of AI solutions.\n• Develop and execute comprehensive test scripts using PyTest to ensure the reliability and accuracy of AI models and applications.\n• Collaborate with cross-functional teams, including data scientists, engineers, and product managers, to integrate AI capabilities into existing systems and workflows.\n• Analyze and optimize AI algorithms for performance and scalability, ensuring they meet business requirements.\n• Stay updated with the latest advancements in AI and machine learning technologies and apply them to improve existing solutions.\n\nRequirements:\n\nMust-Have\n• Python: Advanced proficiency with extensive experience in writing efficient, maintainable, and scalable code. Demonstrated ability to solve complex problems using Python.\n• PyTest: Basic experience in writing and executing unit tests, with a fundamental understanding of test-driven development practices.\n\nGood-to-Have\n• Cloud Computing: Familiarity with cloud platforms and services, with hands-on experience in deploying and managing AI applications in a cloud environment.\n• Azure: Experience with Microsoft Azure, particularly in using its AI and machine learning services.\n• APIs: Knowledge of designing, developing, and consuming APIs, particularly RESTful APIs, for integrating AI solutions with other systems and applications\n\nAbout Allianz Technology\n\nAllianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 13,000 employees located in 22 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry. We oversee the full digitalization spectrum – from one of the industry’s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n\nD&I statement\n\nAllianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, race or ethnicity, age, nationality, religion, disability, or philosophy of life. Join us. Let´s care for tomorrow.",
         "eyJqb2JfdGl0bGUiOiJBSSBQeXRob24gRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQWxsaWFueiBJbnN1cmFuY2UiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQkg5Z0tqUFM2TjBGeExOb0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "24 days ago",
         "Python Developer"
        ],
        [
         "Developer- Angular, Python & Azure",
         "THE VALUE MAXIMIZER",
         "India",
         "About the Role :\n\nAs a Senior Developer, you will play a critical role in designing, implementing, and optimizing IoT applications, Salesforce development, and web solutions. This role is ideal for experienced developers eager to lead projects, mentor junior team members, and shape technical strategies across diverse platforms and technologies. You will be an important part of our international project teams, will work closely with team members from the headquarters and if qualified can lead international software development projects.\n\nKey Responsibilities :\n• Central role and expert for development of IoT frontend applications with Angular, Capacitor, and PrimeNG\n• Design and implement IoT backend systems using C#, .Net 8.0, and the Azure platform\n• Management and optimization of CI/CD pipelines using Azure DevOps and Bicep\n• Drive advanced Salesforce development efforts, including customizations, integrations, and automation\n• Manage and enhance the SharePoint Online intranet platform\n• Architect and implement Power Platform solutions tailored to business needs\n• Develop and maintain complex web applications using Django (Python) and PHP\n• Supervise website development and optimization with Typo3, PHP, and SolR, collaborating closely with the marketing department\n• Monitor and ensure the performance, compliance, and SEO optimization of global websites and portals\n\nKey Requirements :\n• 6-8 years of experience\n• Strong expertise in Angular, Python, and C#\n• Advanced experience with CI/CD pipelines and version control systems (e.g., Azure DevOps or Git)\n• Proficiency in web technologies such as PHP, MySQL, and modern frameworks\n• In-depth knowledge of Salesforce, Azure cloud solutions, and SharePoint Online\n• Proven ability to lead technical projects, mentor team members, and collaborate across departments\n• Excellent problem-solving skills, attention to detail, and a proactive approach to innovation\n• Proficiency in German is an advantage but not a requirement. Candidates must, however, have a minimum of B2-level English proficiency to communicate effectively within the international teams and our headquarters",
         "eyJqb2JfdGl0bGUiOiJEZXZlbG9wZXItIEFuZ3VsYXIsIFB5dGhvbiBcdTAwMjYgQXp1cmUiLCJjb21wYW55X25hbWUiOiJUaGUgVmFsdWUgTWF4aW1pemVyIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRteXhMclAxTG5oaWFJZEVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "Python Developer"
        ],
        [
         "Freelance Python Developer",
         "GBIM TECHNOLOGIES PVT.LTD.",
         "Anywhere",
         "We’re Hiring – Freelance Python Developer (Experienced)\nWe are currently looking for a freelance Python developer who brings hands-on experience and can help us build and debug efficient backend systems, automate processes, and solve critical issues with speed and precision.\nKey Expertise Required:\n\nPython (Backend Development)\n\nWeb Scraping & Data Extraction\n\nWeb Automation\n\nFlask | Pandas | ETL\n\nAWS (Basic to Intermediate)\n\nGoogle / Meta / LinkedIn / Third-Party API Integration\n\nProblem-solving mindset – quick in identifying & fixing bugs/errors\n\nIf you are a solution-driven individual with a solid track record in handling Python-based backend projects and automation tasks, we’d love to connect with you!\nPlease DM or share your portfolio/work samples at [hr@gbim.com / sarabjeet.mann@gbim.in ].\n#PythonDeveloper #FreelanceOpportunity #WebScraping #Automation #BackendDevelopment #Flask #AWS #APIIntegration #DataExtraction #HiringNow #LinkedInJobs\n\nJob Type: Full-time\n\nPay: ₹500.00 - ₹10,000.00 per hour\n\nLocation Type:\n• Remote\n\nSchedule:\n• Day shift\n• Monday to Friday\n\nWork Location: Remote\n\nSpeak with the employer\n+91-XXXXXXXXXX",
         "eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkdCSU0gVGVjaG5vbG9naWVzIFB2dC5MdGQuIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNtMXFFOGh4Y2xUN3Z1ck9BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "3 days ago",
         "Python Developer"
        ],
        [
         "Junior Python Developer",
         "DEHAZELABS",
         "Anywhere",
         "Location: Onsite, Kokapet, Hyderabad, Telangana.\n\nJob Type: Full-Time\n\nAbout Us: DehazeLabs is a leading company dedicated to Empowering businesses to swiftly integrate AI solutions for tangible business outcomes. We specialize in delivering innovative AI-driven solutions that drive real-world results. We are looking for a skilled Python Developer to join our team and help us build innovative software solutions.\n\nResponsibilities:\n• Develop and maintain Python applications and services\n• Collaborate with cross-functional teams to design, develop, and deploy high-quality software\n• Write clean, efficient, and well-documented code\n• Troubleshoot, debug, and optimize applications for performance and scalability\n• Participate in code reviews and maintain coding standards\n• Stay up-to-date with the latest industry trends and technologies\n\nRequirements:\n• Ability to code in Python and SQL\n• Knowledge of databases (e.g., SQL, NoSQL) and data modeling.\n• Knowledge of version control systems (e.g., Git)\n• Excellent problem-solving skills and attention to detail\n• Strong communication and teamwork abilities.\n\nPreferred Qualifications:\n• Experience with cloud platforms (e.g., AWS, Azure) is a plus.\n• Familiarity with containerization tools (e.g., Docker) and CI/CD pipelines.\n• Knowledge of RESTful APIs and microservices architecture.",
         "eyJqb2JfdGl0bGUiOiJKdW5pb3IgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkRlaGF6ZWxhYnMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTFliY1dmVTRza09FMnVoZ0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Python Developer"
        ],
        [
         "DET-Senior GIG Python Developer-GDSNF02",
         "EY",
         "India",
         "At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. Senior Python Developer – Client – Merck, H and M, TIAA, MINT Job Description Bachelor’s or master’s degree with 3+ years of strong Python development experience Design, develop, and maintain high-performance web applications using Python and related frameworks. Strong understanding of Python OOPs, Data type, Data Structure and algorithm , Exception handling, Decarotor , Generator, Iterator, Automation. Strong understanding of Python Libraries (Pandas, TensorFlow, Numpy, SciPy) Experience in Cloud Azure / AWS Develop, optimize, and manage complex APIs (RESTful or GraphQL). Collaborate with cross-functional teams to define, design, and ship new features. Troubleshoot and resolve advanced technical issues in development and production environments. Conduct technical evaluations of new tools and frameworks, recommending their adoption when appropriate. Stay ahead of emerging trends in Python development, ensuring the team remains at the forefront of innovation. Advanced proficiency in Python and frameworks like Django,Flask, or FastAPI. Good understanding of Database Postgres / MySQL & ORM Library i.e. SQL Alchemy/ any ORM libraries Understanding of Code Repository tools i.e. GIT , SVN Strong understanding of DevOps principles(Docker, Kubernetes and microservices) EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",
         "eyJqb2JfdGl0bGUiOiJERVQtU2VuaW9yIEdJRyBQeXRob24gRGV2ZWxvcGVyLUdEU05GMDIiLCJjb21wYW55X25hbWUiOiJFWSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ4Mk5pUFZHVVJ3cjczaVE1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "18 hours ago",
         "Python Developer"
        ],
        [
         "Informatica ETL Developer: Agile Dev Team Member IV",
         "CAPGEMINI",
         "Hyderabad, Telangana, India",
         "The ideal candidate will be responsible for designing, developing, and deploying scalable ETL processes using Informatica PowerCenter to support our data warehousing and analytics initiatives. You will collaborate with business and technical stakeholders to ensure high data quality, availability, and performance. Key Responsibilities:Design, develop, and maintain ETL workflows and mappings using Informatica PowerCenter or Informatica Intelligent Cloud Services (IICS).Extract, transform, and load data from various source systems (e.g., SQL Server, Oracle, flat files, cloud APIs) into data warehouses or operational data stores.Optimize ETL performance, conduct tuning, and ensure error handling and logging.Collaborate with data architects and analysts to understand data requirements and deliver high-quality data solutions.Work with QA teams to support data validation and testing efforts.Support data integration, migration, and transformation initiatives.Document ETL processes, data flows, and job schedules.Monitor daily ETL jobs and resolve production issues in a timely manner.RequirementsBachelor’s degree in Computer Science, Information Systems, or a related field (or equivalent work experience).3+ years of experience with Informatica PowerCenter or Informatica IICS.Strong SQL skills and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).Solid understanding of data warehousing concepts and dimensional modeling.Experience in performance tuning and troubleshooting ETL processes.Hands-on experience with job scheduling tools (e.g., Autosys, Control-M, Tidal).Familiarity with version control systems and DevOps practices.Preferred Qualifications:Experience with cloud data platforms (e.g., Snowflake, AWS Redshift, Azure Synapse).Exposure to data governance and data quality tools.Knowledge of scripting languages (e.g., Shell, Python).Experience working in Agile/Scrum environments.Familiarity with BI tools (e.g., Tableau, Power BI) is a plus. BenefitsThis position comes with competitive compensation and benefits package: Competitive salary and performance-based bonuses Comprehensive benefits package Home Office model Career development and training opportunities Flexible work arrangements (remote and/or office-based) Dynamic and inclusive work culture within a globally known group Private Health Insurance Pension Plan Paid Time Off Training & Development *Note: Benefits differ based on employee level",
         "eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyOiBBZ2lsZSBEZXYgVGVhbSBNZW1iZXIgSVYiLCJjb21wYW55X25hbWUiOiJDYXBnZW1pbmkiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InFZVlFqNTFEM2VzZ0lZSExBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 days ago",
         "ETL Developer"
        ],
        [
         "Senior ETL and Backend Developer (Salesforce)",
         "S&P GLOBAL",
         "Hyderabad, Telangana, India (+1 other)",
         "About the Role:\n\nGrade Level (for internal use):\n10\n\nTitle: Senior ETL and Backend Developer (Salesforce)\n\nJob Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n\nThe Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n\nThe team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n\nThe Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n\nResponsibilities:\n• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\nMonitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\nQualifications:\n\nBasic Qualifications:\n• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n• A minimum of 8+ years of experience in software engineering & Architecture.\n• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n• A minimum of 3+ years of Salesforce development, administration/Integration.\n• Proficiency in Informatica PowerCenter and other ETL tools.\n• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n• Experience with Salesforce integration and administration.\n• Proficiency in backend development languages (e.g., Java, Python, C#).\n• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n• Excellent problem-solving skills and attention to detail.\n• Ability to work independently and as part of a team.\n• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n\nPreferred Qualifications:\n• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n• Experience with other ETL tools and data integration platforms.\n• Informatica Certified Professional\nSalesforce Certified Administrator or Developer\n• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n• Excellent problem solving, analytical and technical troubleshooting skills.\n• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat’s In It For You?\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nWe're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n\nOur benefits include:\n• Health & Wellness: Health care coverage designed for the mind and body.\n• Flexible Downtime: Generous time off helps keep you energized for your time on.\n• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring and Opportunity at S&P Global:\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\n-----------------------------------------------------------\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n-----------------------------------------------------------\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n\nJob ID: 316835\nPosted On: 2025-06-03\nLocation: Hyderabad, Telangana, India",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjVGNUtpak5fRjROQXhDcERBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "19 days ago",
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "ZENSAR TECHNOLOGIES",
         "Madhavaram, Telangana, India",
         "Job Description\n\nPrimary Skill Set\n• ETL Informatica\n• SQL\n• Unix\n• Realtime Data Integration (CDC) using Power Exchange (Can be optional if candidate good in other skills)\n\nGood to Have\n\nExperience on working with Mainframe Databases/files\n\nETL Batch Scheduling tools like TWS/Tidal\n\nRoles & Responsibilities\n\nInformatica PowerCenter, Unix scripting, SQL/PLSQL\n\nKnowledge of Informatica Power Exchange is preferred\n\nExperience With Mainframe Sources/targets Is Preferred\n• Bachelor’s degree in Computer Science or similar field or equivalent work experience.\n• 5-8 years of development experience with Informatica Power Center on Data Integration projects.\n• Strong analytic, problem-solving and organizational skills.\n• Excellent SQL knowledge and ability to write the complex queries keeping performance aspect in mind.\n• Experience with analysis of business requirements, designing and writing technical specifications to design.\n• Hands-on experience to process mainframe files using Informatica Power Exchange.\n• Hands-on experience with UNIX shell scripting.\n• Participate in testing and issue resolution to validate functionality and performance.\n• Hands-on experience on any job scheduling tool, TWS is preferred.\n• Good written and verbal communication skills.\n\nLocation\n\n1 st Preference: Noida\n\n2 nd Preference: Hyderabad\n\n3 rd Preference: Gurgaon",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiWmVuc2FyIFRlY2hub2xvZ2llcyIsImFkZHJlc3NfY2l0eSI6Ik1hZGhhdmFyYW0sIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InJTQzlGVjVoRHhXZ1h3QURBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "11 days ago",
         "ETL Developer"
        ],
        [
         "Insight Global",
         "INSIGHT GLOBAL",
         "Hyderabad, Telangana, India",
         "We are seeking a highly skilled ETL Developer to join our team for an exciting project involving the upgrade of our SQL Server environment from 2012 to 2022. The ideal candidate will have extensive experience in SQL Server, SSIS, ETL processes, and .NET development. This role requires a strong technical background and the ability to work collaboratively with developers and end-users.\nResponsibilities:\nLead the migration of 300+ ETL jobs and 1,000 packages across 30 solutions to a new architecture.\nCollaborate with developers to ensure a smooth transition and integration of the upgraded system.\nEnhance SQL Server capabilities by building tables, stored procedures, views, functions, and other SQL Server objects.\nProvide support and guidance to the ETL development lead.\nEngage with end-users to ensure their needs are met during and after the upgrade process.\nUtilize your experience with SQL Server 2016 and 2022 to optimize the migration process.\nRequirements:\nMinimum of 5 years of experience in SQL Server and ETL processes- SSIS expertise and other Microsoft services.\nProven experience with SQL Server upgrades, particularly from 2012 to 2022.\nStrong SQL Server skills, including building tables, stored procedures, views, and functions.\nExperience with .NET development is highly desirable to be leveraged for other projects\nExcellent problem-solving skills and attention to detail.\nAbility to work effectively in a team environment and communicate with end-users.\nPreferred Qualifications:\nExperience with SQL Server 2016.\nExperience with Oracle\nFamiliarity with the latest features and enhancements in SQL Server 2022.\nStrong understanding of database architecture and migration strategies.\n\nWe are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "eyJqb2JfdGl0bGUiOiJJbnNpZ2h0IEdsb2JhbCIsImNvbXBhbnlfbmFtZSI6Ikluc2lnaHQgR2xvYmFsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJjRW1CZUljdmZoZlg1SEsyQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "ETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location",
         "FISERV",
         "India",
         "Calling all innovators – find your future at Fiserv.\n\nWe’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\n\nJob Title\n\nETL Developer (Hands on Microsoft SQL, SSIS, ETL, and T-SQL ) - Pune Location\n\nWork Location - PuneExperience - 5-10 YearsShift Timing - 2:00 PM - 11:00 PMWhat does a successful ETL Developer doThis role is a senior level ETL development position on Fiserv’s XD Implementations team, which requires the developer to demonstrate a broad knowledge of principles, practices and procedures associated with the implementation’s skillset. The developer will collaborate with clients, business partners, IT, and support organizations to drive the implementation of client projects. The ideal candidate is entrepreneurial, enjoys a fast-paced, problem-solving environment, and is comfortable working across departmental boundaries, while displaying initiative in the absence of management. What you will do:\n• Under minimal supervision, the Software Developer, Sr manages multiple client implementations for retail customers across the XD online banking platform.\n• Participates in client kickoff activities including requirements and data gathering sessions.\n• Ability to analyze and migrate incoming external data into Fiserv solutions.\n• Strong working knowledge of ETL processes and best practices.\n• Performs basic testing, troubleshooting, process flow enhancements and repeatable maintenance functions.\n• Documents business processes and identifies opportunities for process redesign.\n• Reports system defects and identifies opportunities for system enhancements.\n• Performs as a technical consulting resource for new clients during the implementation process.\n• Supports several project managers during the pre and post go-live activities\n• Performs root cause analysis and recommends actions to prevent future occurrences of issues that arise during the implementation process.\n• Coordinates with project managers and organizational entities to identify and monitor issues, impacts and dependencies.\n• Provides technical and analytical guidance to the project team.\n• Responds to occasional requests for additional off-hour work in an emergency client incident or critical business need.\n• The position requires working in shifts, 2 PM to 11 PM IST\n\nWhat you will need to have:\n• Minimum of 5 – 10 years of experience in software development.\n• Strong working knowledge of Microsoft SQL, SSIS, ETL, and T-SQL.\n• Experience with business process charts, flowcharts, data flow diagrams, decision tables, use cases, test scenarios, test cases, test logs, & issue logs.\n• Self-starter who can work effectively, both independently and in a team environment.\n• Strong analytical, organizational, and problem-solving skills.\n• Excellent verbal and written communication skills.\n• Ability to maintain a professional attitude and demeanor in high pressure situations.\n• Ability to multi-task and manage multiple projects simultaneously.\n• Weekend/evening availability and support (10% - 25%).\n\nWhat would be great to haveBA/BS Degree in business, financial, or technical fieldExposure to infrastructure concepts around storage, networking and virtualizationExperience with ECM as it relates to the Financial Industry\n\nThank you for considering employment with Fiserv. Please:\n• Apply using your legal name\n• Complete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\n\nWhat you should know about us:\n\nFiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune® magazine's \"World's Most Admired Companies™\" 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.\n\nOur commitment to Diversity and Inclusion:\n\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n\nWarning about fake job posts:\n\nPlease be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.\n\nAny communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.\n\nIf you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local law enforcement.",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIChIYW5kcyBvbiBNaWNyb3NvZnQgU1FMLCBTU0lTLCBFVEwsIGFuZCBULVNRTCApIC0gUHVuZSBMb2NhdGlvbiIsImNvbXBhbnlfbmFtZSI6IkZpc2VydiIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJqX2xpRm8zVlFIcXJEN19CQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "EPAM - ETL Developer - SSIS/SSRS",
         "SWATHI V",
         "Hyderabad, Telangana, India",
         "Job Title : SQL Developer / ETL Developer (SSIS/SSRS)\n\nExperience : 5+ Years\n\nLocation : Hyderabad\n\nJob Description :\n\nWe are seeking a skilled SQL Developer / ETL Developer with over 5 years of experience in Microsoft SQL Server, SSIS, and SSRS. The ideal candidate will be responsible for designing, developing, and optimizing complex SQL queries, stored procedures, and views. The role includes building and maintaining robust ETL workflows using SSIS to support data integration and warehousing requirements.\n\nYou will work closely with business analysts and data teams to ensure efficient data transformation and loading processes. Strong understanding of data warehouse concepts, performance tuning, and data modeling (normalized, de-normalized, star, snowflake) is essential. Familiarity with handling structured data formats such as JSON and XML, and scripting within SSIS is preferred.\n\nKey Responsibilities :\n\n- Design, develop, and optimize T-SQL queries, stored procedures, functions, and views.\n\n- Develop, deploy, and maintain SSIS packages for ETL processes.\n\n- Work on data warehouse design, development, and maintenance.\n\n- Ensure high performance and reliability of data integration workflows.\n\n- Troubleshoot SQL queries and ETL issues; optimize database performance.\n\n- Collaborate with cross-functional teams to define data solutions.\n\n- Work with complex data structures including JSON and XML.\n\n- Understand and implement different data models (star, snowflake, etc.).",
         "eyJqb2JfdGl0bGUiOiJFUEFNIC0gRVRMIERldmVsb3BlciAtIFNTSVMvU1NSUyIsImNvbXBhbnlfbmFtZSI6IlN3YXRoaSBWIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ2ekQ3VTVqbHNoc2EtN3luQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "Data ETL Developer / BI Engineer",
         "AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL",
         "India",
         "ETL Developer\n\nAmex GBT is a place where colleagues find inspiration in travel as a force for good and – through their work – can make an impact on our industry. We’re here to help our colleagues achieve success and offer an inclusive and collaborative culture where your voice is valued.\n\nWe are looking for an experienced Data ETL Developer / BI Engineer who loves solving complex problems across a full spectrum of data & technologies. You will lead the building effort of GBT's new BI platform and manage the legacy platform to seamlessly support our business function around data and analytics. You will create dashboards, databases, and other platforms that allow for the efficient collection and evaluation of BI data.\n\nWhat You’ll Do on a Typical Day:\n• Design, implement, and maintain systems that collect and analyze business intelligence data.\n• Design and architect an analytical data store or cluster for the enterprise and implement data pipelines that extract, transform, and load data into an information product that helps the organization reach strategic goals.\n• Create physical and logical data models to store and share data that can be easily consumed for different BI needs.\n• Develop Tableau dashboards and features.\n• Create scalable and high-performance data load and management process to make data available near real-time to support on-demand analytics and insights.\n• Translate complex technical and functional requirements into detailed designs.\n• Investigate and analyze alternative solutions to data storing, processing, etc., to ensure the most streamlined approaches are implemented.\n• Serve as a mentor to junior staff by conducting technical training sessions and reviewing project outputs\n• Design & develop, and maintain a data model implementing ETL processes.\n• Manage and maintain the database, warehouse, & cluster with other dependent infrastructure.\n• Work closely with data, products, and another team to implement data analytic solutions.\n• Support production application and Incident management.\n• Help define data governance policies and support data versioning processes\n• Maintain security and data privacy by working closely with the Data Protection Officer internally.\n• Analyze a vast number of data stores and uncover insights\n\nWhat We’re Looking For:\n• Degree in computer sciences or engineering\n• Overall, 3-5 years of experience in data & data warehouse, ETL, and data modeling.\n• 2+ years of experience working and managing large data stores, complex data pipelines, and BI solutions.\n• Strong experience in SQL and writing complex queries.\n• Hands-on experience with Tableau development.\n• Hands-on working experience on Redshift, data modeling, data warehouse, ETL tool, Python, and Shell scripting.\n• Understanding of data warehousing and data modeling techniques\n• Strong data engineering skills on the AWS Cloud Platform are essential.\n• Knowledge of Linux, SQL, and any scripting language\n• Good interpersonal skills and a positive attitude\n• Experience in travel data would be a plus.\n\nLocation\nGurgaon, India\n\nThe #TeamGBT Experience\n\nWork and life: Find your happy medium at Amex GBT.\n• Flexible benefits are tailored to each country and start the day you do. These include health and welfare insurance plans, retirement programs, parental leave, adoption assistance, and wellbeing resources to support you and your immediate family.\n• Travel perks: get a choice of deals each week from major travel providers on everything from flights to hotels to cruises and car rentals.\n• Develop the skills you want when the time is right for you, with access to over 20,000 courses on our learning platform, leadership courses, and new job openings available to internal candidates first.\n• We strive to champion Inclusion in every aspect of our business at Amex GBT. You can connect with colleagues through our global INclusion Groups, centered around common identities or initiatives, to discuss challenges, obstacles, achievements, and drive company awareness and action.\n• And much more!\n\nAll applicants will receive equal consideration for employment without regard to age, sex, gender (and characteristics related to sex and gender), pregnancy (and related medical conditions), race, color, citizenship, religion, disability, or any other class or characteristic protected by law.\n\nClick Here for Additional Disclosures in Accordance with the LA County Fair Chance Ordinance.\n\nFurthermore, we are committed to providing reasonable accommodation to qualified individuals with disabilities. Please let your recruiter know if you need an accommodation at any point during the hiring process. For details regarding how we protect your data, please consult the Amex GBT Recruitment Privacy Statement.\n\nWhat if I don’t meet every requirement? If you’re passionate about our mission and believe you’d be a phenomenal addition to our team, don’t worry about “checking every box;\" please apply anyway. You may be exactly the person we’re looking for!\nExperience Level\nMid Level\n\nMore about this Data ETL Developer / BI Engineer job\n\nAmerican Express Global Business Travel is aggressively hiring for the job profile of Data ETL Developer / BI Engineer at undefined in null locality. Kindly go through the FAQs below to get all answers related to the given job.\n\n1. How much salary can I expect as a Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n\nAns. You can expect a minimum salary of 0 INR. The salary offered will depend on your skills, experience and performance in the interview.\n\n2. What is the eligibility criteria to apply for Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n\nAns. The candidate should have completed undefined degree and people who have 3 to 5 years are eligible to apply for this job. You can apply for more jobs in undefined to get hired quickly.\n\n3. Is there any specific skill required for this job?\n\nAns. The candidate should have undefined skills and sound communication skills for this job.\n\n4. Who can apply for this job?\n\nAns. Both Male and Female candidates can apply for this job.\n\n5. Is it a work from home job?\n\nAns. No, it’s not a work from home job and can’t be done online. You can explore and apply for other work from home jobs in undefined at apna.\n\n6. Are there any charges or deposits required while applying for the role or while joining?\n\nAns. No work-related deposit needs to be made during your employment with the company.\n\n7. How can I apply for this job?\n\nAns. Go to the apna app and apply for this job. Click on the apply button and call HR directly to schedule your interview.\n\n8. What is the last date to apply?\n\nAns. The last date to apply for this job is .\n\nFor more details, download apna app and find Full Time jobs in undefined. Through apna, you can find jobs in 74 cities across India. Join NOW!",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVUTCBEZXZlbG9wZXIgLyBCSSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkFtZXJpY2FuIEV4cHJlc3MgR2xvYmFsIEJ1c2luZXNzIFRyYXZlbCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJnY3VjckFjcm5KVmRmdjRxQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "6 days ago",
         "ETL Developer"
        ],
        [
         "Informatica ETL Developer - SQL/Power Center",
         "RENOVISION AUTOMATION SERVICES PVT.LTD.",
         "Telangana, India",
         "Experience - 6+ yearsWork Mode - HybridJob Summary : We are seeking a skilled Informatica ETL Developer with 5+ years of experience in ETL and Business Intelligence projects. The ideal candidate will have a strong background in Informatica PowerCenter, a solid understanding of data warehousing concepts, and hands-on experience in SQL, performance tuning, and production support. This role involves designing and maintaining robust ETL pipelines to support digital transformation initiatives for clients in manufacturing, automotive, transportation, and engineering domains.Key Responsibilities : - Design, develop, and maintain ETL workflows using Informatica PowerCenter.- Troubleshoot and optimize ETL jobs for performance and reliability.- Analyze complex data sets and write advanced SQL queries for data validation and transformation.- Collaborate with data architects and business analysts to implement data warehousing solutions.- Apply SDLC methodologies throughout the ETL development lifecycle.- Support production environments by identifying and resolving data and performance issues.- Work with Unix shell scripting for job automation and scheduling.Required Skills : - 35 years of hands-on experience with Informatica PowerCenter.- Proficiency in SQL and familiarity with NoSQL platforms.- Experience in ETL performance tuning and troubleshooting.- Solid understanding of Unix/Linux environments and scripting.- Excellent verbal and written communication skills.Preferred Qualifications : - AWS Certification or experience with cloud-based data integration is a plus.- Exposure to data modeling and data governance practices. (ref: hirist.tech)",
         "eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyIC0gU1FML1Bvd2VyIENlbnRlciIsImNvbXBhbnlfbmFtZSI6IlJlbm92aXNpb24gQXV0b21hdGlvbiBTZXJ2aWNlcyBQdnQuTHRkLiIsImFkZHJlc3NfY2l0eSI6IlRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlVqTnczajVrcnJpS21VQjdBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "ETL Developer"
        ],
        [
         "ETL Developer- Hyderabad (2-3+ Years of Experience)",
         "A CLIENT OF ANALYTICS VIDHYA",
         "Hyderabad, Telangana, India",
         "Role Summary:\n\n•ETL Developer is responsible for Design and Development of ETL Jobs which follow standards, best practices and are maintainable, modular and reusable.\n\n•ETL Developer will analyze and review complex object and data models and the metadata repository in order to structure the processes and data for better management and efficient access.\n\n•Working on multiple projects, and delegating work to Junior Analysts to deliver projects on time.\n\n•Training and mentoring Junior Analysts and building their proficiency in the ETL process.Job Duties:\n\n•Preparing mapping document to extract, transform, and load data ensuring compatibility with all tables and requirement specifications.\n\n•Experience in ETL system design and development with Talend / Pentaho PDI is essential.\n\n•Create quality rules in Talend.\n\n•Tune Talend jobs for performance optimization.\n\n•Write relational and multidimensional database queries.\n\n•Functional Knowledge of Talend Adminstration Center, Job Servers & Load balancing setup, and all its administrative fucntions.\n\n•Develop, maintain, and enhance unit test suites to verify the accuracy of ETL processes, dimensional data, OLAP cubes and various forms of BI content including reports, dashboards, and analytical models.\n\n•Exposure in Map Reduce components of Talend / Pentaho PDI.\n\n•Creating and deploying Talend / Pentaho custom components is an add-on advantage.\n\nJob Specification:\n\n•BE, B.Tech / MS Degree in Computer Science, Engineering or a related subject.\n\n•Having an experience of 2 – 3+ years.\n\n•Comprehensive understanding and working knowledge in Data Warehouse loading, tuning, and maintenance.\n\n•Proficiency with Talend or related tools like Pentaho Data Integration / Kettle.\n\n•Working knowledge of relational database theory and dimensional database models.\n\n•Ability to write complex SQL database queries.\n\n•Ability to work independently.",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyLSBIeWRlcmFiYWQgKDItMysgWWVhcnMgb2YgRXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJBIENsaWVudCBvZiBBbmFseXRpY3MgVmlkaHlhIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJYOGlLR3lLZ0p6MDQ5UkdIQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "LUXOFT",
         "Maharashtra, India",
         "Project Description:\n\nOur client is an EU subsidiary of a Global Financial Bank working in multiple markets and asset classes.\n\nDWH/ETL developer will work closely with the Development Lead to design, build interfaces and integrate data from a variety from internal and external data sources into the new Enterprise Data Warehouse environment. The ETL Developer will be responsible for developing ETL primarily utilizing Microsoft & Azure technologies within industry recognized ETL standards, architecture, and best practices.\n\nResponsibilities:\n• Act as a technical expert in the designing, coding, unit testing, supporting, and debugging of data warehouse software components in all aspects of SDLC\n• Apply cloud and ETL engineering skills to solve problems and design approaches\n• Troubleshoot and debug ETL pipelines and creating unit tests for ETL pipelines.\n• Assess query performance and actively contribute to optimizing the code\n• Write technical documentation and specifications\n• Support internal audit by submitting required evidence\n• Create reports and dashboards in the BI portal\n• Work with Development Lead, DWH Architect and QA Engineers to plan, implement and deliver best ETL strategies\n• Work with business analysts to understand requirements to create technical design specifications, gaining a sound understanding of business processes for related applications so that integration processes fulfill the end-user requirements\n• Communicate effectively in a collaborative, complex and high performing team environment as per Agile principles\n\nMandatory Skills Description:\n• Proven work experience as an ETL Developer\n• Advanced knowledge of relational databases and dimensional Data Warehouse modelling concepts\n• Good understanding of physical and logical data modeling\n• Very good understanding of modern SaaS/PaaS data solutions in a cost conscious approach\n• Expert level of knowledge of Microsoft Data stack\n• Experience in developing and deploying data oriented solutions in Cloud (Azure/Synapse Analytics/Fabric)\n• Experience in designing and implementing data transformation and ETL layers using Data Factory, Notebooks\n• Experience with PowerBI for report & dashboard creation. PowerQuery and/or DAX is an advantage.\n• Experience in/understanding of Azure Data Lake Storage\n• Knowledge/use of CI/CD tools and principles, preferably Azure DevOps or Bamboo\n• Strong SQL knowledge, able to create complex SQL queries and good understanding of stored procedures, views, indexes, functions, etc.\n• Good working knowledge of at least one Scripting language. Python is an advantage.\n• Experience with GIT repositories and working with branches. GitHub, Azure DevOps or Bitbucket experience are preferable.\n• Ability to troubleshoot and solve complex technical problems\n• Good understanding of software development best practices\n• Working experience in Agile projects; preferably using JIRA\n• Experience in working in high priority projects preferably greenfield project experience\n• Able to communicate complex information clearly and concisely.\n• Able to work independently and also to collaborate across the organization\n• Highly developed problem-solving skills with minimal supervision\n• Understanding of data governance and enterprise concepts preferably in banking environment\n• Verbal and written communication skills in English are essential.\n\nNice-to-Have Skills Description:\n• Microsoft Fabric\n• Snowflake\n• Background in SSIS/SSAS/SSRS\n• Azure DevTest Labs, ARM templates\n• Azure PurView\n• Banking/finance experience",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiTHV4b2Z0IiwiYWRkcmVzc19jaXR5IjoiTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJlZF9YUGZ2UVNVcTZXdXltQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "ETL Developer"
        ],
        [
         "Data Engineer (Hadoop, Spark, Scala, Hive)",
         "VISA",
         "India",
         "Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n\nMake an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n\nJob Description\n\nTranslate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\n\nGood to have GenAI Exposure and Agentic AI Knowledge.\n\nWork with business partners directly to seek clarity on requirements.\n\nDefine solutions in terms of components, modules, and algorithms.\n\nDesign, develop, document, and implement new programs and subprograms, as well as enhancements, modifications and corrections to existing software.\n\nCreate technical documentation and procedures for installation and maintenance.\n\nWrite Unit Tests covering known use cases using appropriate tools.\n\nIntegrate test frameworks in the development process.\n\nWork with operations to get the solutions deployed.\n\nTake ownership of production deployment of code.\n\nCome up with Coding and Design best practices.\n\nThrive in a self-motivated, internal-innovation driven environment.\n\nAdapt quickly to new application knowledge and changes.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\nQualifications\n\nBasic Qualifications\n\n-Minimum of 6 months of work experience or a Bachelor's Degree\n\nPreferred Qualifications\n\n-Bachelor degree in Computer Science.\n\n-Minimum of 1 plus years of software development experience in Hadoop using\n\nSpark, Scala, Hive.\n\n-Expertise in Object Oriented Programming Language Java, Python.\n\n-Experience using CI CD Process, version control and bug tracking tools.\n\n-Result-oriented with strong analytical and problem-solving skills.\n\n-Experience with automation of job execution, validation and comparison of data\n\nfiles on Hadoop Environment at the field level.\n\n-Experience in leading a small team and being a team player.\n\n-Strong communication skills with proven ability to present complex ideas and\n\ndocument them in a clear and concise way.\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChIYWRvb3AsIFNwYXJrLCBTY2FsYSwgSGl2ZSkiLCJjb21wYW55X25hbWUiOiJWaXNhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImVULTZKTlc2NHVzTUxhdS1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 days ago",
         "Spark Engineer"
        ],
        [
         "Data Engineer - Spark/Python",
         "ETELLIGENS TECHNOLOGIES",
         "India",
         "Job Description\n\nWe are seeking a skilled Data Engineer with hands-on experience in Azure Data Factory, DataBricks, ETL pipelines, and strong Python/Spark scripting skills. The ideal candidate should have a solid background in data integration, migration, and data warehouse concepts, with the ability to manage and support end-to-end data engineering solutions in a modern Azure environment.\n\nKey Responsibilities\n• Design, develop, and implement ETL solutions using Azure Data Factory and DataBricks.\n• Write scalable and reusable Python and Spark scripts for data processing, cleansing, and transformation.\n• Perform data cleansing and migration from diverse sources to target systems.\n• Collaborate with stakeholders to understand requirements and convert them into technical\n\nsolutions.\n• Code using PySpark or Pysense within DataBricks to enable advanced analytics solutions.\n• Work extensively with SQL Server and other RDBMS systems, writing efficient SQL, T-SQL, and PL/SQL queries and stored procedures.\n• Support, troubleshoot, and maintain data pipelines and workflows.\n• Participate in all phases of software development lifecycle including unit testing, integration\n\ntesting, and performance testing.\n• Contribute to the modernization of data platforms and analytics tools on Azure.\n• Ensure data quality and integrity across all pipelines and systems.\n\nRequired Skills & Qualifications\n• 5 - 8 years of experience in delivering end-to-end ETL projects using SSIS or similar tools.\n• Minimum 1.5 - 2 years of experience in Azure Data Factory.\n• 1+ year of experience in Python and Spark scripting.\n• Proficiency in SQL/T-SQL/PL-SQL, stored procedures, and database performance tuning.\n• Good understanding of Data Warehousing (DWH) concepts.\n• Knowledge of modern Azure ecosystem, including Azure Data Lake, Blob Storage, and Synapse (preferred).\n• Experience with DataBricks, especially PySpark or Pysense (advantageous).\n• Hands-on experience in data cleansing, transformation, and migration projects.\n• Ability to work independently and within a team environment.\n• Microsoft Certified : Azure Data Engineer Associate\n• Databricks Certified Data : B. Tech / BCA / MCA (Computer Science)\n\nLocation : Artha SEZ, Greater Noida West\n\n(ref:hirist.tech)",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU3BhcmsvUHl0aG9uIiwiY29tcGFueV9uYW1lIjoiRXRlbGxpZ2VucyBUZWNobm9sb2dpZXMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiRGM2RFNOTE1pTllDRWw5NUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         null,
         "Spark Engineer"
        ],
        [
         "Spark Engineer",
         "STAFFINGINE LLC",
         "India",
         "- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred",
         "eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "1 day ago",
         "Spark Engineer"
        ],
        [
         "Staff Data Engineer (Spark, Python, Hadoop)",
         "VISA",
         "India",
         "Company Description\n\nVisa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n\nWhen you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.\n\nJoin Visa: A Network Working for Everyone.\nJob Description\n\nThe Payment Systems Risk team is responsible for building critical risk and fraud prevention applications and services at VISA. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate risk for Visa and Visa client payment systems.\n\nAre you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\n\nAs a Staff Data Engineer, you will be responsible to establish processes, automations, structures and big data systems based on business and technical requirements to channel multiple requirements, route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies, databases, and other applicable big data technologies as required.\n\nEssential Functions\n• Work with manager and clients to fully understand business requirements and desired business outcomes\n• Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\n• Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\n• Perform other tasks on RnD, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\n• Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\n• Execute data engineering projects ranging from small to large either individually or as part of a project team\n• Ensure project delivery within timelines and budget requirements\n• Provide coaching and mentoring to junior team members\n\nThis is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\nQualifications\n\n• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred\n• Minimum of 8 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies\n• Minimum of 3 to 4 years of experience in building large-scale applications using open source technologies. Design and coding skills with Big Data technologies like Hadoop, Spark, Hive, and Map Reduce\n• Minimum of 4 years of hands-on expertise with Java or Scala\n• Experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following database technologies: DB2, MySQL and NoSQL data warehouses such as HBase\n• Experience working in an Agile and Test Driven Development environment.\n• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable\n• Experience with SAS as a statistical package is preferred\n• Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIChTcGFyaywgUHl0aG9uLCBIYWRvb3ApIiwiY29tcGFueV9uYW1lIjoiVmlzYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJaQV9QYkhKd2VaLXRtYXJoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         null,
         "Spark Engineer"
        ],
        [
         "Databricks Engineer - Spark / PySpark",
         "ENKEFALOS TECHNOLOGIES LLP",
         "Anywhere",
         "Databricks Engineer – Spark / PySpark\n\nLocation : Remote / Mysore\n\nJoining : Immediate\n\nExperience : 5+ years\n\nResponsibilities :\n\nWill implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n\nRequirements:\n• PySpark (RDDs, DataFrames, performance tuning)\n• Building gold‐layer data models for financial reporting\n• Experience with complex joins, aggregations, GL hierarchies\n• Version handling (Actuals vs Budget), currency conversions\n\nJob Type: Full-time\n\nPay: ₹500,395.35 - ₹1,840,348.25 per year\n\nBenefits:\n• Flexible schedule\n• Paid sick time\n• Provident Fund\n• Work from home\n\nApplication Question(s):\n• Have you worked on ADF/ADLS ?\n• Do you have hands-on experience of Spark / PySpark\n\nExperience:\n• Databricks Engineering: 4 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 days ago",
         "Spark Engineer"
        ],
        [
         "Pi Square Technologies - Spark & Scala Engineer",
         "SANDEEP RAJA",
         "India",
         "Job Summary :\n\nWe are seeking a skilled and motivated Spark & Scala Engineer with 46 years of hands-on experience in building scalable data processing applications. The ideal candidate will have a strong background in big data ecosystems and a solid understanding of distributed computing using Apache Spark with Scala.\n\nRoles and Responsibilities :\n\n- Design, build, and maintain efficient, reusable, and reliable Apache Spark applications.\n\n- Optimize Spark applications for maximum speed and scalability.\n\n- Implement data ingestion and ETL processes.\n\n- Collaborate with data scientists and architects to implement complex big data solutions.\n\n- Debug and resolve issues in Spark applications.\n\n- Stay up to date with the latest trends in big data technologies and Apache Spark.\n\n- Write clean, readable, and maintainable code.\n\n- Participate in code reviews and contribute to team knowledge sharing.\n\nRequired Skills :\n\n- 46 years of experience working with Apache Spark (core, SQL, streaming).\n\n- Strong proficiency in Scala programming.\n\n- Experience in building and optimizing data pipelines and ETL workflows.\n\n- Proficient in working with big data tools and frameworks (e.g., Hive, HDFS, Kafka).",
         "eyJqb2JfdGl0bGUiOiJQaSBTcXVhcmUgVGVjaG5vbG9naWVzIC0gU3BhcmsgXHUwMDI2IFNjYWxhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoic2FuZGVlcCByYWphIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlFMNDBNZDljZDlqQkxBX3BBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "27 days ago",
         "Spark Engineer"
        ],
        [
         "Spark Developer",
         "INFOSYS",
         "India",
         "• Primary skills:Technology->Big Data - Data Processing->Spark\n\nA day in the life of an Infoscion\n• As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\n• You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\n• You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\n• You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\n• You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n• Knowledge of more than one technology\n• Basics of Architecture and Design fundamentals\n• Knowledge of Testing tools\n• Knowledge of agile methodologies\n• Understanding of Project life cycle activities on development and maintenance projects\n• Understanding of one or more Estimation methodologies, Knowledge of Quality processes\n• Basics of business domain to understand the business requirements\n• Analytical abilities, Strong Technical Skills, Good communication skills\n• Good understanding of the technology and domain\n• Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\n• Awareness of latest technologies and trends\n• Excellent problem solving, analytical and debugging skills",
         "eyJqb2JfdGl0bGUiOiJTcGFyayBEZXZlbG9wZXIiLCJjb21wYW55X25hbWUiOiJJbmZvc3lzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkZ2VmRvVC1NQnFvWHRRUmJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "16 days ago",
         "Spark Engineer"
        ],
        [
         "Data Engineer (Snowflake, Spark, AWS) - AVP",
         "12542 CITICORP SERVICES INDIA PRIVATE LIMITED",
         "India",
         "The Data Engineer is responsible for building Data Engineering Solutions using next generation data techniques. The individual will be working directly with product owners, customers and technologists to deliver data products/solutions in a collaborative and agile environment. Job Summary: We are seeking an experienced Data Engineer to join our team, responsible for designing, building, and maintaining large-scale data systems on Snowflake on AWS. The ideal candidate will have expertise in Snowflake, Spark, and various AWS services, including S3, Lambda, EKS, Glue, and Terraform (IaaC). Key Responsibilities: Design, develop, and maintain data pipelines using Snowflake, Spark, and AWS services Architect and implement data warehousing solutions using Snowflake Develop and deploy Spark applications for data processing and analytics Utilize AWS services such as S3, Lambda, EKS, and Glue for data storage, processing, and orchestration Implement infrastructure as code using Terraform for AWS resource management Collaborate with data architect to understand data requirements and deliver data solutions Ensure data quality, security, and compliance with organizational standards Requirements: 8+ years of experience in data engineering Strong expertise in Snowflake, Spark, and AWS services (S3, Lambda, EKS, Glue) Familiarity with containerization technologies like Docker and container orchestration platforms like Kubernetes. Experience with Terraform (IaaC) for infrastructure management Proficiency in programming languages such as Python, Scala, or Java Experience with data warehousing, ETL, and data pipelines Strong understanding of data modeling, data governance, and data quality Excellent problem-solving skills and attention to detail Bachelor's degree in Engineering Nice to Have: Experience with cloud-based data platforms and tools Knowledge of containerization using Docker Familiarity with Agile development methodologies Certification in AWS or Snowflake Education: Bachelor’s degree/University degree or equivalent experience ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Applications Development ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Citi is an equal opportunity and affirmative action employer. Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Citigroup Inc. and its subsidiaries (\"Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. View the \"EEO is the Law\" poster. View the EEO is the Law Supplement. View the EEO Policy Statement. View the Pay Transparency Posting",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTbm93Zmxha2UsIFNwYXJrLCBBV1MpIC0gQVZQIiwiY29tcGFueV9uYW1lIjoiMTI1NDIgQ2l0aWNvcnAgU2VydmljZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImxCUVlsUm5KamQtUlJIRjlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         null,
         "Spark Engineer"
        ],
        [
         "SW Engineer (Java and Bigdata/Hadoop/Spark) 1yr",
         "VISA",
         "India",
         "Job Description\n\nThis position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. They will be an integral part of the Payment Products Development team, focusing on the development of software solutions that leverage data to address business challenges. They will be extensively involved in hands-on activities including proof of concepts, development, and testing. They should demonstrate adaptability and readiness to change tasks according to the team's requirements.\n\nKey Responsibilities\n• Deliver high-quality products and features, taking a holistic view from the initial idea to final delivery. This involves design, development, testing, and accountability for release deployments as well as providing support post-deployment.\n• Understand the product in its entirety and ensure the timely delivery of your assigned module within an agile team setting.\n• Accurately scope efforts, identify risks, and clearly communicate trade-offs with team members and other stakeholders.\n• Coordinate and participate in Continuous Integration activities, testing automation frameworks, and other related tasks, in addition to contributing to core product code.\n• Effectively communicate status updates, issues, and potential risks accurately and promptly.\n• Perform other tasks related to data governance and system infrastructure as required.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\nQualifications\n\nBasic Qualifications\n\n-Bachelor's degree in Computer Science or equivalent field\n\n-Relevant working experience of up to 2 years in the industry\n\n-Proven experience in software development, particularly in data-centric\n\nprojects, demonstrating adherence to standard development best practices\n\n-Strong understanding and practical experience with data structures and\n\nalgorithms, with a passion for tackling complex problems\n\n-Proficiency in Java programming\n\n-Hands-on experience with Big Data technologies such as Hadoop, Spark, and\n\nHive\n\n-Strong knowledge of Unix/Linux operating systems and Shell Scripting\n\n-Proficiency in working with RDBMS and SQL\n\n-Basic knowledge of manual and automated testing\n\n-Familiarity with version control systems, specifically Git\n\n-Awareness of and experience with software design patterns\n\n-Experience working within an Agile framework\n\nPreferred Qualifications\n\n-Proficiency in Scala & Kafka programming is a good to have\n\n-Experience with Airflow for workflow management\n\n-Familiarity with AI concepts and tools, including GitHub Copilot for code\n\ndevelopment\n\n-Exposure to AI/ML development is an added advantage\n\n-Proficiency in working with In-memory Databases like Redis\n\n-Good knowledge of API development is highly advantageous\n\n-Strong verbal and written communication skills, with a proactive and self-\n\nmotivated approach to improving existing processes to enable faster\n\niterations.\n\n-Demonstrated intellectual and analytical rigor, with a keen attention to detail\n\n-Team-oriented, energetic, and collaborative approach to work, coupled with a\n\ndiplomatic and adaptable style\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "eyJqb2JfdGl0bGUiOiJTVyBFbmdpbmVlciAoSmF2YSBhbmQgQmlnZGF0YS9IYWRvb3AvU3BhcmspIDF5ciIsImNvbXBhbnlfbmFtZSI6IlZJU0EiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoidHV1TkJsMDF3amRIc0xnSEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "5 days ago",
         "Spark Engineer"
        ],
        [
         "Big Data Lead/ Lead Data Engineer/Spark Tech Lead",
         "TANISHA SYSTEMS  INC",
         "India",
         "Job Description:Mandatory:10+ years of experience in solution, design and development of applications using Java 8+/J2EE, Spring, Spring-Boot, Micro Services, RESTful Services and with experience in Big Data and with experience working in heavy data background needed.Develop, program, and maintain applications using the Apache Spark open source frameworkWork with different aspects of the Spark ecosystem, including Spark SQL, Data Frames, Datasets, and streaming.Spark Developer must have strong programming skills in Java, Scala, or PythonFamiliar with big data processing tools and techniquesProven experience as a Spark Developer or a related roleStrong programming skills in Java, Scala, or PythonFamiliarity with big data processing tools and techniquesExperience with the Hadoop ecosystemGood understanding of distributed systemsExperience with streaming data platformsMust have strong experience in Big Data and with experience working in heavy data background needed.Must be strong in Cloud AWS event-based architecture, Kubernetes, ELK (Elasticsearch, Logstash & Kibana)Must have excellent experience in designing and Implementing cloud-based solutions in various AWS Services (: s3, Lambda, Step Function, AMQ, SNS, SQS, CloudWatch Events, etc.)Must be well experienced in design and development of Microservice using Spring-Boot and REST API and with GraphQLMust have solid knowledge and experience in NoSQL (MongoDB)Good knowledge and experience in any Queue based implementations.Strong knowledge/experience in ORM Framework - JPA / HibernateGood knowledge in technical concepts – Security, Transaction, Monitoring, PerformanceShould we well have versed with TDD/ATDDShould have experience on Java, Python and Spark2+ years of experience in designing and Implementing cloud-based solutions in various AWS ServicesStrong experience in DevOps tool chain (Jenkins, Artifactory, Ansible/Chef/Puppet/Spinnaker, Maven/Gradle, Atlassian Tool suite)Very Good knowledge and experience in Non-Functional (Technical) Requirements like Security, Transaction, Performance, etc.Excellent analytical and problem-solving skills Nice to have:· Experience in Experience with OAuth implementation using Ping Identity· Experience in API Management (Apigee) or Service Mesh (Istio)· Good knowledge and experience in Queue/Topic (Active-MQ) based implementations.· Good knowledge and experience in Scheduler and Batch Jobs· Experience with scripting languages using Unix· Preferably certified in AWS",
         "eyJqb2JfdGl0bGUiOiJCaWcgRGF0YSBMZWFkLyBMZWFkIERhdGEgRW5naW5lZXIvU3BhcmsgVGVjaCBMZWFkIiwiY29tcGFueV9uYW1lIjoiVGFuaXNoYSBTeXN0ZW1zICBJbmMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQmRWaDV5cnl3aTREUFNOT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 hours ago",
         "Spark Engineer"
        ],
        [
         "Data Insights Analyst",
         "IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED",
         "India",
         "Responsible for leveraging Google Analytics 4 (GA4) to extract, analyze, and interpret data to support business objectives and decision-making processes. Requires proficient understanding and translating business requirements into actionable insights and recommendations derived from GA4 data.Key Responsibilities:1.Data Extraction & Analysis: Extract and analyze GA4 data to provide meaningful insights into user behavior, website performance, and digital marketing effectiveness.2.Reporting & Dashboarding: Develop custom reports, dashboards, and data visualizations using GA4 data to monitor key metrics and performance indicators.3.Insights and Recommendations: Providing actionable insights and recommendations to optimize website performance, user experience, and marketing campaigns.4.Customization & Configurations: Configure GA4 data streams, events, and conversions to align with business goals and KPIs (key performance indicators). Utilize custom dimensions, metrics, and attribution models to capture and analyze data relevant to unique business needs.5.Pharma knowledge: Understanding of the Pharma industry landscape, including regulatory compliance, patient journey mapping, and healthcare data privacy considerationsMinimum Requirement:·2-4 Years of experience in digital analytics, hands on experience on Google Analytics 4.·Proficiency in Google Analytics and Google Tag Manager.·Strong analytical skills and ability to interpret data.·Understanding of digital marketing concepts and website optimization techniques.·Excellent communication skills to present findings and recommendations effectively·GA4 Certification is strongly preferred",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEluc2lnaHRzIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJJTjEwIChGQ1JTID0gSU4wMTApIE5vdmFydGlzIEhlYWx0aGNhcmUgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImFiMU5uYTFGN1dHbFd4eC1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 days ago",
         "Data Analyst"
        ],
        [
         "Senior Data Management Analyst",
         "WELLS FARGO",
         "Hyderabad, Telangana, India",
         "About this role:\n\nWells Fargo is seeking a Senior Data Management Analyst\n\nIn this role, you will:\n• Lead or participate in moderately complex programs and initiatives for data quality, governance, and metadata activities\n• Design and conduct moderately complex analysis to identify and remediate data quality, data integrity, process, and control gaps\n• Analyze, assess, and test data controls and data systems to ensure quality and risk compliance standards are met and adhere to data governance standards and procedures\n• Identify data quality metrics and execute data quality audits to benchmark the state of data quality\n• Develop recommendations for optimal approaches to resolve data quality issues and implement plans for assessing the quality of new data sources leveraging domain expertise and data, business, or process analysis to inform and support solution design\n• Lead project teams and mentor less experienced staff members\n• Drive planning and coordination on moderately complex remediation efforts acting as central point of contact\n• Consult with clients to assess the current state of data and metadata quality within area of assigned responsibility\n• Participate in cross-functional groups to develop companywide data governance strategies\n• Provide input into communication routines with stakeholders, business partners, and experienced leaders\n\nRequired Qualifications:\n• 4+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\n• Experience in large enterprise data initiatives\n• Contact center business or technology experience\n• Experience implementing, managing, or utilizing big data environments (Hadoop, Big Query, etc.)\n• Experience using standard BI tools (Tableau, Power BI, MicroStrategy, etc.) preferably from big data environments\n\nPosting End Date:\n24 Jun 2025\n• Job posting may come down early due to volume of applicants.\n\nWe Value Equal Opportunity\n\nWells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n\nEmployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n\nCandidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n\nApplicants with Disabilities\n\nTo request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n\nDrug and Alcohol Policy\n\nWells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n\nWells Fargo Recruitment and Hiring Requirements:\n\na. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n\nb. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBNYW5hZ2VtZW50IEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJXZWxscyBGYXJnbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEFWbXpFcV9ySTNSUG1faUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Associate/Analyst - Data Analytics",
         "D. E. SHAW INDIA",
         "Hyderabad, Telangana, India",
         "The Financial Research (FinRes) group provides research and analysis support to the entire investment life cycle of the firm’s global proprietary trading strategies. The group caters to almost all the quantitative and qualitative strategies of the firm and its research support covers multiple asset classes, sectors, and geographies.",
         "eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUvQW5hbHlzdCAtIERhdGEgQW5hbHl0aWNzIiwiY29tcGFueV9uYW1lIjoiRC4gRS4gU2hhdyBJbmRpYSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiak9McEJja1dWaXhiWjJ4c0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "14 hours ago",
         "Data Analyst"
        ],
        [
         "Senior Analyst- Data Risk Office",
         "BRISTOL MYERS SQUIBB",
         "Hyderabad, Telangana, India",
         "Working with Us\nChallenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n\nRoles & Responsibilities\n\nFunctional and Technical\n• Execution and monitoring of data privacy office key activties.\n• Assist in the development and implementation of data privacy policies and procedures to ensure compliance with international, federal, and state regulations, including GDPR, CCPA, and HIPAA.\n• Work closely with legal, IT, HR, and other departments to address data privacy concerns and implement best practices.\n• Assist in the development and delivery of data privacy training and awareness programs across the organization. Keeps up to date with evolution of regulations impacting privacy, ethics, and data.\n• Experienced in configuring and working with various industry leading data risk and privacy tools like OneTrust, TrustArc, Microsoft Purview etc.\n• Demonstrate critical thinking around interpreting business and industry challenges and recommending best practices-based solutions to improve products, processes, systems and reduce risk.\n• Implement organizational IT controls in accordance with applicable regulations and the ability to evaluate and understand the impact of new regulations and requirements.\n\nPeople Management:\n• Responsible for training and mentoring junior staff to meet BMS standards.\n• Preferred experience with working in a multi-cultural, multi-location and diverse environments.\n\nQualifications & Experience\n• B.E./B.Tech. or equivalent in computer science, engineering, life science field\n• Recognized privacy/DLP certifications and experience preferred.\n• At least years of privacy program management, compliance, or strong operations management experience (regulated or healthcare or tech preferred).\n• Knowledge of HIPAA, GDPR, CPRA, PIPL etc., and other privacy regulations is a must.\n• Ability to make decisions that impact own work and other groups/teams and works under minimal supervision.\n• Demonstrates openness to learning and developing. Takes a responsibility for their own and team’s development and growth.\n• Demonstrates an understanding of factors driving team performance and how they contribute to the team's overall success.\n• Excellent English Oral and written communication skills including the ability to deliver clear and articulate presentations.\n• Ability to use PowerPoint, Excel, Word, or other technologies to communicate complex topics to stakeholders, manage personal workload, and track projects and issues.\n\nIf you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n\nUniquely Interesting Work, Life-changing Careers\nWith a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n\nOn-site Protocol\n\nBMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n\nSite-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n\nBMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n\nIf you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHlzdC0gRGF0YSBSaXNrIE9mZmljZSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJsWm9Rdlc1dnAyWjZyYVlMQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst II – Product Information Capabilities | Digital & Technology",
         "GENERAL MILLS INDIA",
         "India",
         "India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.\n\nPosition Title\n\nSoftware Engineer II – Product Information Capability\n\nFunction/Group\n\nDigital & Technology\n\nLocation\n\nMumbai\n\nShift Timing\n\nRegular\n\nRole Reports to\n\nD&T Manager – Product Information Capability\n\nRemote/Hybrid/in-Office\n\nHybrid\n\nAbout General Mills\n\nWe make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and Häagen-Dazs, we’ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\n\nHow we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\n\nus into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\n\nGeneral Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\n\nWith our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital & Technology (D&T) Innovation, Technology & Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy & Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\n\nWe advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\n\nJob Overview\n\nFunction Overview\n\nThe Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The team's expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\n\nThe team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the \"Work with Heart\" philosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the following Link\n\nPurpose of the role\n\nThis is an exciting time to work in General Mills' Supply Chain & ITQ Organization! We are accelerating Digital Transformation of our Product Information Capabilities (PIC) capabilities to provide a competitive advantage to our business. To this end, we are seeking a skilled and motivated STIBO Developer to join our Product Information Capabilities team. As a STIBO Developer, you will play a crucial role in designing, developing, and implementing solutions within our STIBO STEP platform to support our product information management (PIM) and master data management (MDM) processes. You will collaborate with cross-functional teams to understand business requirements, translate them into technical specifications, and deliver high-quality solutions that meet our business needs.\n\nKey Accountabilities\n• Design, develop, and maintain STIBO STEP solutions to support product information management processes.\n• Write and maintain code for business rules to ensure data quality and consistency.\n• Configure outbound and inbound integrations to exchange data with other systems.\n• Configure gateway endpoints for seamless data flow.\n• Develop and maintain data models within STIBO STEP to accurately represent product information.\n• Build web UI screens for data entry, validation, and reporting.\n• Develop solutions based on documented requirements and specifications.\n• Participate in agile project development, including sprint planning, daily stand-ups, and retrospectives.\n• Collaborate with business analysts, data architects, and other developers to ensure solutions align with business needs and technical standards.\n• Troubleshoot and resolve issues related to STIBO STEP implementations.\n• Stay up-to-date with the latest STIBO STEP features and best practices.\n• Create and maintain technical documentation for STIBO STEP solutions.\n\nMinimum Qualifications\n• Education – Full time graduation from an accredited university (Mandatory- Note: This is the minimum education criteria which cannot be altered)\n• Experience with tools and concepts related to MDM and/or Digital Product Publishing, such as data governance, data quality, data integration and data exporting\n• Exposure to Product Information Management Systems (PIM/MDM)\n• Technical expertise into Stibo platform\n• Experience with Data Syndication partners such as 1WorldSync, Syndigo and Salsify.\n• Exposure to GDSN Standards\n• Strong team player and collaborator with core focus on solution making; Effective analytical and technical skills; Ability to work in a cross functional team environment\n\nPreferred Qualifications\n• Product Information Management / Master Data Management\n• STIBO STEP certification\n• Business Analysis skills\n• SQL, Cloud GCP\n• Agile / SCRUM Delivery\n• Familiarity with Service Bus Integration\n• Preferably experience in Consumer Goods industry.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUkg4oCTIFByb2R1Y3QgSW5mb3JtYXRpb24gQ2FwYWJpbGl0aWVzIHwgRGlnaXRhbCBcdTAwMjYgVGVjaG5vbG9neSIsImNvbXBhbnlfbmFtZSI6IkdlbmVyYWwgTWlsbHMgSW5kaWEiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTWplT1ByZWI4UVZPLVpDVUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "2 days ago",
         "Data Analyst"
        ],
        [
         "Lead Data Management Analyst",
         "WELLS FARGO",
         "Hyderabad, Telangana, India",
         "About this role:\n\nWells Fargo is seeking a Lead Data Management Analyst\n\nIn this role, you will:\n• Organize and lead complex companywide initiatives to ensure that data quality is maintained so that data can effectively support business processes\n• Oversee analysis and reporting in support of regulatory requirements\n• Identify and recommend analysis of data quality or integrity issues\n• Evaluate data quality metrics and data quality audits to benchmark the state of data quality\n• Make decisions in complex and multi-faceted situations requiring solid understanding of data governance standards and procedures\n• Identify new data sources and develop recommendations for assessing the quality of new data\n• Lead project teams and mentor less experienced staff members\n• Recommend remediation of process or control gaps that align to management strategy\n• Serve as relationship manager for a line of business\n• Consult with and provide recommendations to senior management regarding assessments of the current state of data and metadata quality within area of assigned responsibility\n• Represent client in cross-functional groups to develop companywide data governance strategies\n• Strategically collaborate and consult with peers, colleagues, and mid-level to senior managers to coordinate and drive collaboration on solution design and remediation execution\n\nRequired Qualifications:\n• 5+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\n• Experience in Data Management, Business Analysis, Analytics, Project Management.\n\nPosting End Date:\n24 Jun 2025\n• Job posting may come down early due to volume of applicants.\n\nWe Value Equal Opportunity\n\nWells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n\nEmployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n\nCandidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n\nApplicants with Disabilities\n\nTo request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n\nDrug and Alcohol Policy\n\nWells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n\nWells Fargo Recruitment and Hiring Requirements:\n\na. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n\nb. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgTWFuYWdlbWVudCBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiV2VsbHMgRmFyZ28iLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ik5YbnE1ZlFtc3M5WXNVQ0JBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Senior Data Analyst, Marketing Science",
         "CRUNCHYROLL",
         "Hyderabad, Telangana, India",
         "About the role\n\nWe are looking for a hands-on Senior Marketing Science Analyst with a passion for understanding data, tracking business trends, and storytelling to join the Center for Data and Insights team. This position will report to the Vice President, Business Intelligence & Analytics, and will collaborate with data engineers, campaign managers, and marketing leaders to investigate campaign trends, build business insights, and recommend proactive measures to enable better business decisions every day. The right person for this position is a service-minded, empathetic problem-solver who will be motivated by the opportunity to build a centralized insights service team from the ground up!\n• Partner with Branch/ Adjust MMPs, direct response, and media buying agencies to define and implement campaign measurement of integrated marketing campaigns, including TV, OOH, Digital, and Social Media across a wide range of business lines (theatrical, streaming, e-commerce, etc.)\n• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n• Evaluating media measurement leveraging MMM (Marketing Mix Modeling), MTA (Multi-Touch Attribution), and Incrementality A/B Testing\n• Lead / Assist in development of testing roadmaps, measurement plans, KPI & KLI reporting based on set marketing objectives linked to key campaigns and always-on digital marketing tactics.\n• Deliver ad hoc analysis for client stakeholders throughout marketing, working with the channel leads, planning, and client service departments.\n• Partner with paid marketing teams in building weekly insight decks for the Marketing leadership team by collaborating effectively across teams, documenting the impact of strategic initiatives on benchmarks, and tracking the progress towards company goals.\n• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n• Connect ideas into cohesive, well-grounded recommendations, using creative, structured, and analytical thinking with the help of effective data visualization\n• Work with offshore and onsite teams and lead the sprint planning/management\n• Maintain a culture of high-quality output and outstanding customer service by effectively communicating at all levels, ensuring that work gets done, and responding effectively to\n\nAbout You\n• 5+ years of experience with data analysis, paid campaign analytics, statistics, experimentation, and optimization.\n• 3+ years of experience in writing complex SQL queries, experience in marketing data automation.\n• 3+ years of experience with data visualization tools like Tableau, Superset, etc.\n• Understanding of install and user-level paid campaign tracking to support multi-touch attribution via UTMs and MMP data for all major marketing channels\n• Proficiency in data analysis, including defining critical metrics, statistical and predictive modeling concepts, descriptive statistics, and experimental design\n• Experience in Marketing analytics tools like Google Analytics, Adjust, Braze, Branch, etc.\n• Experience working with large data sets (Terabytes of data/ billions of records).\n• Deep expertise in measuring marketing performance against lifetime value metrics.\n• Outstanding teamwork skills: You have excellent interpersonal skills. You’re a good listener. You place the success of the company ahead of any particular idea. Proven ability to work across a globally, matrixed organization.\n• BS in Statistics, Computer Science, Information Systems, or a related field\n\nAbout the Team\n\nThe Center for Data and Insights (CDI) is the centralized team of data engineering, BI, analytics, and data science experts, passionate about servicing the organization with timely and certified reports and insights! The mission of the group is to inspire, support, and guide our stakeholders to be data-aware as well as build out the systems of intelligence to discover insights and act on them.\n\nWhy you will love working at Crunchyroll\n\nIn addition to getting to work with fun, passionate and inspired colleagues, you will also enjoy the following benefits and perks:\n• Best-in class medical, dental, and vision private insurance healthcare coverage\n• Access to counseling & mental health sessions 24/7 through our Employee Assistance Program (EAP)\n• Free premium access to Crunchyroll\n• Professional Development\n• Company's Paid Parental Leave\n• up to 26 weeks for birthing parents\n• up to 12 weeks for non-birthing parents\n• Hybrid Work Schedule\n• Paid Time Off\n• Flex Time Off\n• 5 Yasumi Days\n• Half-Day Fridays during the summer\n• Winter Break\n\n#LifeAtCrunchyroll #LI-Hybrid",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBBbmFseXN0LCBNYXJrZXRpbmcgU2NpZW5jZSIsImNvbXBhbnlfbmFtZSI6IkNydW5jaHlyb2xsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiI3QU16TGktYmRTM1gybEV4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Principal Data Analyst",
         "STORABLE",
         "Serilingampalle (M), Hyderabad, Telangana, India",
         "About the Role:\nWe’re seeking a strategic, data-driven professional to lead our Business Analysis initiatives for our Marketplace product within the Self-Storage vertical. This is a ‘hands on keyboard’ high-impact role that bridges business strategy, analytical rigor, and data visualization —ideal for someone who thrives on translating data into actionable insights that drive performance. As the Principal Data Analyst, you will own our marketplace performance metrics, lead the development of world-class dashboards and reporting frameworks, and partner closely with Product, Marketing, Operations, and Finance teams to inform strategic decisions. You’ll drive both hands-on analysis and organizational data literacy while mentoring a team of analysts and collaborating with engineering on data infrastructure.\nKey Responsibilities:\n\nOwn and continuously optimize key Marketplace KPIs (conversion, revenue yield, inventory, pricing, etc.).\nServe as the steward of metric definitions and business logic—ensuring consistency, accuracy, and alignment across stakeholders.\nHave a deep understanding of how to run a BI environment. Proactive, insightful, curious.\nBuild and scale world class self-service dashboards and Tableau reports for internal and executive stakeholders.\nLead cross-functional analysis projects to uncover trends, diagnose problems, and identify growth and efficiency opportunities.\nTranslate complex data into clear, actionable insights and concise narratives for business and executive audiences.\nDrive rigorous SQL-based analysis from raw datasets, ensuring high-quality, reproducible outputs.\nGuide and collaborate with junior analysts in other groups and collaborate with engineering to shape data pipelines and definitions.\nCollaborate with cross functional partners and teams (Product,Marketing, Finance, Business Insights) to test hypotheses, evaluate feature/campaign performance, and measure outcomes.\nIdentify data gaps or integrity risks and work with stakeholders to improve reliability and accuracy.\nChampion a culture of curiosity, experimentation, and evidence-based decision-making.\nProactively keep other teams in the loop on new data capabilities unlocked and changes being made.\n\nRequirements:\n\n5-7 years of experience in business analysis, data analytics, or related roles, ideally with at least 2+ years in a leadership capacity.\nAdvanced proficiency in SQL with experience working with complex data models and the ability to query complex relational datasets.\nOther data engineering experience is a significant plus to facilitate sourcing/formating of data.\nDeep understanding of performance metrics and business levers in a supply/demand or marketplace environment.\nExperience leading analytics efforts in partnership with cross-functional teams including Product, Marketing, Finance, and Engineering.\nProven track record of developing intuitive, scalable dashboards using Tableau (or similar BI platforms).\nStrong business acumen with the ability to connect the dots between business objectives and analytical frameworks.\nExceptional communication skills with the ability to distill technical findings for non-technical audiences.\nCapable of influencing and informing executive stakeholders with clear, concise insights.\nDemonstrated experience owning KPIs, building data products, and drive insights into action in a fast paced environment.\nAbility to navigate ambiguity, manage and prioritize competing needs, and work cross-functionally.\n\nPreferred Qualifications:\n\nExperience in the storage, real estate, or marketplace industries strongly preferred\nFamiliarity with modern data stack tools such as Snowflake, dbt, or similar Experience analyzing marketplace dynamics or supply/demand business models\nExposure to experimentation frameworks, A/B testing, or uplift modeling\nPrior exposure to high-growth SaaS or Marketplace operations\nData engineering capabilities—especially in sourcing, transforming, or centralizing disparate datasets.\n\nAbout Us:\nAt Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\nWe leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.\nImportant Notice: Protect Yourself from Fraudulent Activities Targeting Job Seekers\nWe’ve been made aware of fraudulent activities where an individual or group is pretending to represent Storable, attempting to deceive job seekers by using our company’s good name and stellar reputation.\nTo protect yourself, please consider the following guidelines:\n– Official Communication: All genuine communication from Storable will come from official email addresses, ending in “@storable.com.” Be cautious of any communication that doesn’t match this criteria.– No Unsolicited Offers: We do not extend job offers without a formal interview process. If you receive an unsolicited job offer claiming to be from Storable or any of its representatives, it’s a red flag.– Verification: If you’re uncertain about the legitimacy of any job offer or communication claiming to be from Storable, please directly contact our HR department directly at POps@storable.com for verification.\nYour security and trust are paramount to us. If you suspect you’ve been contacted by someone falsely claiming to be from Storable or using our company’s name for any dubious purpose, please immediately report the incident to POps@storable.com\nWe’re committed to ensuring a transparent and secure hiring process.\nThank you for your vigilance and interest in joining our team.",
         "eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiU3RvcmFibGUiLCJhZGRyZXNzX2NpdHkiOiJTZXJpbGluZ2FtcGFsbGUgKE0pLCBIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InljaTdhWW0wWnR2MEMzbkZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "18 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst 1",
         "UNITEDHEALTH GROUP",
         "India",
         "At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.\n\nPrimary Responsibilities:\n• Validate data with administrative source systems (source of truth)\n• Analyze complex datasets\n• Generate actionable insights and recommendations based on data analysis\n• Database Management:\n• Develop and maintain data models, data dictionaries, and other documentation\n• Troubleshoot and resolve database-related issues\n• Data Extraction and Transformation:\n• Support capital project that will, in part, import and transform data from various sources for broker bonus calculations\n• Ensure data integrity and quality through rigorous validation and testing\n• Data Visualization and Reporting:\n• Create visually appealing and informative dashboards and reports\n• Present findings and insights to both technical and non-technical stakeholders in a clear and concise manner\n• Continuous Learning and Improvement:\n• Stay up to date with the latest data analysis techniques and tools\n• Identify opportunities to improve data analysis processes and methodologies\n• Actively participate in knowledge sharing and mentoring within the team\n• Comply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\n• Undergraduate degree or equivalent experience\n• 4+ years Experience as SAS Data Analyst\n• Experience with relational database management systems (e.g., MySQL, Oracle, SQL Server)\n• Experience with statistical analysis\n• Familiarity with data visualization tools (e.g., Tableau, Power BI)\n• Proven excellent problem-solving and critical thinking skills\n• Proven solid communication and presentation skills to effectively convey complex data analysis findings to both technical and non-technical stakeholders\n• Proven ability to work independently and collaboratively in a fast-paced, deadline-driven environment\n• Proven detail-oriented with a focus on accuracy and data integrity\n\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone - of every race, gender, sexuality, age, location and income - deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.\n\n#NTRQ",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IlVuaXRlZEhlYWx0aCBHcm91cCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJhdHFPMEswckdadGwwUy1xQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "3 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst – Competitive Benchmarking & Reporting",
         "REPUTATION",
         "Hyderabad, Telangana, India",
         "Why Work at Reputation?\n• Reputation reached over $100m in Annual Recurring Revenue (ARR) in 2022 and continues to grow worldwide.\n• We've raised over $200 million in funding from A-list venture capital firms such as Bessemer Venture Partner and Kleiner Perkins, including $150 million in equity financing from Marlin Equity Partners in January 2022.\n• Reputation is trusted by more than 250 partners, including Google, Facebook, Salesforce, J.D. Power, Amazon and Web.com.\n• Our industry leading platform has been recognized by Forrester and Gartner as a vendor of choice in Voice of the Customer, Customer Feedback Management, and Social Suites research reports.\n• The platform is used by 10+ major automotive OEMs and 16,000 auto dealerships, more than 250 healthcare systems, and over 100 leading property management firms.\n• Our executive management team is committed to building a performance-based culture where excellence is rewarded and careers are developed.\n• Who thrives at Reputation? Managers who embody a player-coach mentality. Employees who value teamwork and cross-functional collaboration. People who emphasize perseverance and hustle over quick wins and luck.\n• Our Mission: We exist to forge relationships between companies and communities.\n\nWe are seeking a Data Analyst with expertise in data stewardship and analysis to manage our competitive benchmark data tenants and produce customer-facing reports for marketing and sales. The ideal candidate is proficient in BigQuery (SQL), detail-oriented, and capable of working independently on complex data projects.\n\nResponsibilities:\n• Data Stewardship & Governance: Ensure data accuracy, integrity, and accessibility by managing competitive benchmark data tenants, implementing verification processes, and maintaining governance best practices.\n• Data Pipeline Management: Develop and maintain data ingestion pipelines, including API integrations and web scraping, ensuring timely updates and scalability.\n• Data Validation & Quality Control: Oversee large-scale data input and validation, balancing hands-on data manipulation with automation strategies. Collaborate with data owners to resolve quality issues.\n• Advanced Analytics & Reporting: Conduct in-depth data analysis to uncover trends, patterns, and insights that drive business decisions. Scope, design, and execute analysis projects with minimal supervision.\n• Industry & Market Insights: Develop data-driven industry reports for marketing and sales, leveraging analytics to provide insights on market trends and competitive landscapes.\n• Cross-Functional Collaboration: Work closely with product managers, engineers, marketing, and leadership to resolve data issues, provide training, and ensure alignment on data best practices.\n• Tool & Process Optimization: Design scalable internal tools and reporting solutions to enhance data accessibility, automation, and usability across teams.\n• Stakeholder Communication: Translate complex data insights into actionable recommendations for key stakeholders, including non-technical audiences.\n\nQualifications:\n• 5+ years of proven experience in data analysis, data stewardship, and managing large-scale datasets, with a strong focus on ensuring data accuracy, integrity, and accessibility.\n• Must be proficient with SQL and NoSQL (BigQuery), as well as have the ability to thrive in an environment that often requires patience and creativity to query, extract and validate data in which there are high levels of uncertainty.\n• Strong prior professional experience managing databases and using applicable tools is required.\n• Experience with and knowledge of ETL processes and data migration.\n• Understanding of and prior experience with General Data Protection Regulation.\n• Demonstrated experience and proficiency in using Python in real-world, business applications are substantial advantages to candidates.\n• Highly articulate with outstanding verbal and written communication skills; you are able to explain data-driven insights and analyses intuitively to clients, executives, and technical and non-technical coworkers in both large and small groups.\n• You should be comfortable taking feedback from everyone – Product Managers, Engineers, customers, the CEO, etc. – to help understand business objectives and develop the most powerful analyses and tools possible.\n• Proven ability to operate in a fast-paced, data-driven environment.\n\nWhen you join Reputation, you can expect:\n• Flexible working arrangements.\n• Career growth with paid training tuition opportunities.\n• Active Employee Resource Groups (ERGs) to engage with.\n• An equitable work environment.\n\nOur employees say it best:\n\nAccording to Glassdoor, 94% of our employees approve of our CEO, Joe Burton.\n\nOur employees highlight our:\n• Ample Opportunities- “There are many opportunities to learn and grow. Many open roles are replaced with internal promotions.”\n• Positive Culture- \"Great opportunity and exceptional culture.\" “You will never have a better culture anywhere else. Period.”\n• Training and Tools- “All managers truly want you to succeed, and you are given great tools and training to be successful in your role.”\n• Balance- “Great work life balance and awesome team environment!”\n\nDiversity Programs & Initiatives:\n\nOur Reputation Nation spans around the world. This global perspective allows us to intentionally unlock the magic that comes from diversity of experience to contribute to our success.\n\nAt Reputation, we believe in:\n• Diversity: Embracing a culture that values uniqueness.\n• Inclusion: Inviting diverse groups to take part in company life.\n• Belonging: Helping each individual feel accepted for who they are.\n\n\"At Reputation, we see diversity and inclusion as the foundation for an equitable workplace. Our goal is to empower all of our employees, regardless of their background, to make an impact in their work each and every day.\" - Joe Burton, CEO, Reputation\n\nAdditionally, we offer a variety of benefits and perks, such as:\n• Health Insurance & Wellness Benefits: Group Health Insurance: Medical Insurance with floater policy of up to 10,00,000 for employee + spouse + 2 dependent children + 2 parents / parent-in-laws\n• Maternity Benefits: Medical insurance up to 75,000 INR, 26 weeks of leave for birth, adoption or surrogacy\n• Life Insurance: Insurance at 3x annual cost to the company (Term Insurance, GPA)\n• Accident/Disability Insurance: Insured at 3x base salary for permanent total disability, permanent partial disability and temporary total disability (GPA)\n• OPD: of 7500 per annum per employee\n\nLeaves\n• 10 Company observed holidays a year (Refer to the Holiday Calendar for the Year)\n• 12 Casual/Sick leaves (Pro-rata calculated)\n• 02 Earned Leaves per Month (Pro-rata calculated)\n• 04 Employee Recharge days (aka company holiday/office closed)\n• Maternity & Paternity (6 months)\n• Bereavement Leave (10 Days)\n\nCar Lease:\nReputation offers Car Lease Program that allows employees to lease a car with no upfront cost or down payment. They benefit from a fixed monthly lease rental and 20-30% tax savings.\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\nTo learn more about how we handle the personal data of applicants, visit our Candidate Privacy Notice.\n\nApplicants only - No 3rd party agency candidates.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3Qg4oCTIENvbXBldGl0aXZlIEJlbmNobWFya2luZyBcdTAwMjYgUmVwb3J0aW5nIiwiY29tcGFueV9uYW1lIjoiUmVwdXRhdGlvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiT0NPR3UtU1dxWjh4X0gzS0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 days ago",
         "Data Analyst"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "job_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "posted_at",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "search_role",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "silver_df=silver_df.withColumn(\"company_name\",trim(upper(col(\"company_name\"))))\n",
    "\n",
    "#display(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "262ccf73-0b52-484e-b30b-b8c6bbbeea7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- ### Drop rows where posted_at is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ac2e33-d04a-4b59-b45f-d0ea8b4266ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>company_name</th><th>location</th><th>description</th><th>job_id</th><th>posted_at</th><th>search_role</th></tr></thead><tbody><tr><td>Lead Consultant - Technical Lead - Fullstack Data Engineer</td><td>ASTRAZENECA</td><td>Chennai, Tamil Nadu, India</td><td>Job Title: Lead Consultant - Technical Lead - Data, Analytics & AI\n",
       "Career Level: E\n",
       "\n",
       "Introduction to role: Are you ready to redefine an industry and change lives? Join our Operations IT organization, where your work will have a direct impact on patients by redefining our ability to develop life-changing medicines. We empower the business to perform at its peak, combining innovative science with leading digital technology platforms and data. As part of the Data Analytics and AI (DA&AI) group, you'll deliver innovative solutions that drive the transformation of medicine development. As the Technical Lead, you'll oversee the technical delivery of products and projects aligned with business objectives, tapping into your expertise in Data and Cloud Engineering, Software Engineering, AI, and more. Collaborate with stakeholders to ensure seamless delivery and maximum business impact!\n",
       "\n",
       "Accountabilities:\n",
       "• Bridge business needs with technical solutions by leading IT application design and implementation.\n",
       "• Collaborate with stakeholders to define and deliver requirements, translating them into detailed specifications.\n",
       "• Own the technical vision of products and compile detailed technical designs, refining user Epics/stories.\n",
       "• Optimize performance, mitigate risks, and ensure alignment to timelines and resource allocations.\n",
       "• Advise on industry trends and standard methodologies to enhance performance and business outcomes.\n",
       "• Provide technical direction and guidance to IT teams and business units.\n",
       "• Contribute to Data & Software Engineering standards and best practices.\n",
       "• Research new technologies to boost system performance and scalability.\n",
       "• Lead and mentor technical teams (Data Engineering, Cloud Engineering, Software Engineering) and work with AI/GenAI leads to foster collaboration and innovation.\n",
       "• Ensure platform stability, scalability, and simplicity while adhering to regulatory requirements and data security standards.\n",
       "• Foster continuous improvement and innovation.\n",
       "• Supervise technical leadership throughout the software development lifecycle and provide hands-on support in development, technical delivery, review, and testing while collaborating with global teams.\n",
       "• Apply central Enterprise Data Platforms and guide DevOps, DataOps, and MLOps teams to ensure standard methodologies.\n",
       "• Ensure data solutions align with FAIR principles and support end-to-end data science and machine learning.\n",
       "\n",
       "Essential Skills/Experience:\n",
       "• Minimum 10 years of experience in the design, development, and delivery of software and data engineering solutions.\n",
       "• Extensive technical expertise in Data Engineering, Software Engineering, and Cloud Engineering.\n",
       "• Strong foundational knowledge of AI Engineering principles and practices.\n",
       "• Deep understanding of DevOps, MLOps, and DataOps methodologies with practical implementation experience.\n",
       "• Demonstrated success in product development and/or product management, delivering complex solutions end-to-end.\n",
       "• Demonstrable ability to provide technical thought leadership across Data, Analytics, and AI domains.\n",
       "• Exceptional communication, customer management, and multi-functional collaboration skills.\n",
       "• Robust analytical and problem-solving abilities with a collaborative, team-oriented approach.\n",
       "• Hands-on experience driving innovation throughout the full product development lifecycle.\n",
       "• Solid understanding of Data Mesh and Data Product concepts and architectures.\n",
       "• Proficiency in Agile methodologies and facilitating iterative, multi-functional team delivery.\n",
       "• Hands-on experience designing, implementing, and optimizing data pipelines using leading ETL tools.\n",
       "• Skilled in architecting, deploying, and managing scalable, secure AWS cloud environments.\n",
       "• Proficient in workflow orchestration tools such as Apache Airflow.\n",
       "• Practical experience implementing DataOps practices with tools like DataOps.Live.\n",
       "• Strong expertise in data storage and analytics platforms such as Snowflake.\n",
       "• Ability to deliver actionable insights through business intelligence tools, including Power BI.\n",
       "• Extensive full-stack development experience, including backend proficiency with Node.js and Python and frontend expertise with ReactJS or NextJS.\n",
       "• Experience designing and deploying Generative AI solutions.\n",
       "• Hands-on implementation of AI/ML models using platforms such as Amazon SageMaker.\n",
       "• Advanced programming skills, especially in Python.\n",
       "• Solid knowledge of both SQL and NoSQL database technologies.\n",
       "• Familiarity with agile ways of working and iterative development environments.\n",
       "• Experience working in large, multinational organizations or pharmaceutical environments is highly desirable.\n",
       "• Demonstrated leadership and mentoring skills, with a demonstrable ability to develop high-performing technical teams.\n",
       "\n",
       "Desirable Skills/Experience:\n",
       "• Bachelor's or master's degree in health sciences, Life Sciences, Data Management, IT, or a related field.\n",
       "• Experience in the pharmaceutical industry or a similar multinational environment.\n",
       "• AWS Cloud or relevant data/software engineering certifications.\n",
       "• Awareness of use case specific GenAI tools available in the market and their application in day-to-day work scenarios.\n",
       "• Possess working knowledge of basic prompting techniques and continuously improve these skills.\n",
       "• Stay up to date with developments in AI and GenAI, applying new insights to work-related situations.\n",
       "\n",
       "When we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That's why we work, on average, a minimum of three days per week from the office. But that doesn't mean we're not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\n",
       "\n",
       "At AstraZeneca, we are at a crucial stage of our journey to become a digital and data-led enterprise. Our commitment to innovation empowers us to make the impossible possible by building partnerships and ecosystems that drive scale and speed for exponential growth. With investment backing us all the way, we are focused on disrupting the industry while making a meaningful impact through our work. Here you'll find countless opportunities to learn, grow, and contribute to developing life-changing medicines.\n",
       "\n",
       "Ready to make a difference? Apply now to join our team!\n",
       "\n",
       "Date Posted\n",
       "30-Jun-2025\n",
       "\n",
       "Closing Date\n",
       "\n",
       "AstraZeneca embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills. We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements.</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIENvbnN1bHRhbnQgLSBUZWNobmljYWwgTGVhZCAtIEZ1bGxzdGFjayBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiQXN0cmFaZW5lY2EiLCJhZGRyZXNzX2NpdHkiOiJDaGVubmFpLCBUYW1pbCBOYWR1LCBJbmRpYSIsImh0aWRvY2lkIjoiRGVXa2pFZDgxNXcwSUpJY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>3 days ago</td><td>Data Engineer</td></tr><tr><td>AWS Data Engineer</td><td>COGNIZANT</td><td>Hyderabad, Telangana, India (+1 other)</td><td>Job Summary:\n",
       "\n",
       "Experience : 4 - 8 years\n",
       "\n",
       "Location : Bangalore\n",
       "\n",
       "The Data Engineer will contribute to building state-of-the-art data Lakehouse platforms in AWS, leveraging Python and Spark. You will be part of a dynamic team, building innovative and scalable data solutions in a supportive and hybrid work environment. You will design, implement, and optimize workflows using Python and Spark, contributing to our robust data Lakehouse architecture on AWS. Success in this role requires previous experience of building data products using AWS services, familiarity with Python and Spark, problem-solving skills, and the ability to collaborate effectively within an agile team.\n",
       "\n",
       "Must Have Tech Skills:\n",
       "\n",
       "· Demonstrable previous experience as a data engineer.\n",
       "• Technical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\n",
       "\n",
       "· Proficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n",
       "\n",
       "Nice To Have Tech Skills:\n",
       "\n",
       "· Familiar with data services in a Lakehouse architecture.\n",
       "\n",
       "· Familiar with technical design practices, allowing for the creation of scalable, reliable data products that meet both technical and business requirements\n",
       "\n",
       "· A master’s degree or relevant certifications (e.g., AWS Certified Solutions Architect, Certified Data Analytics) is advantageous\n",
       "\n",
       "Key Accountabilities:\n",
       "• Writes high quality code, ensuring solutions meet business requirements and technical standards.\n",
       "• Works with architects, Product Owners, and Development leads to decompose solutions into Epics, assisting the design and planning of these components.\n",
       "• Creates clear, comprehensive technical documentation that supports knowledge sharing and compliance. Experience in decomposing solutions into components (Epics, stories) to streamline development.\n",
       "• Actively contributes to technical discussions, supporting a culture of continuous learning and innovation.\n",
       "\n",
       "Key Skills:\n",
       "• Proficient in Python and familiar with a variety of development technologies.\n",
       "• Previous experience of implementing data pipelines, including use of ETL tools to streamline data ingestion, transformation, and loading.\n",
       "• Solid understanding of AWS services and cloud solutions, particularly as they pertain to data engineering practices. Familiar with AWS solutions including IAM, Step Functions, Glue, Lambda, RDS, SQS, API Gateway, Athena.\n",
       "• Proficient in quality assurance practices, including code reviews, automated testing, and best practices for data validation.\n",
       "• Experienced in Agile development, including sprint planning, reviews, and retrospectives\n",
       "\n",
       "Educational Background:\n",
       "• Bachelor’s degree in computer science, Software Engineering, or related essential.\n",
       "\n",
       "Bonus Skills:\n",
       "• Financial Services expertise preferred, working with Equity and Fixed Income asset classes and a working knowledge of Indices.\n",
       "• Familiar with implementing and optimizing CI/CD pipelines. Understands the processes that enable rapid, reliable releases, minimizing manual effort and supporting agile development cycles.</td><td>eyJqb2JfdGl0bGUiOiJBV1MgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkNvZ25pemFudCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiWjYyVVNZZkhtUHk0dHdwNkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 days ago</td><td>Data Engineer</td></tr><tr><td>Data Engineer (Python, Pyspark, and Azure Databricks) || (4 - 6 Years)</td><td>EMIDS</td><td>Bengaluru, Karnataka, India</td><td>Hi All,\n",
       "\n",
       "Greetings for the day!!\n",
       "\n",
       "We are currently hiring for Data Engineer (Python, Pyspark, and Azure Databricks) for Emids(MNC) at Bangalore location.\n",
       "\n",
       "Role: Data Engineer\n",
       "\n",
       "Exp: 5 to 8 Years\n",
       "\n",
       "Location: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\n",
       "\n",
       "NP: Immediate to 15 Days (Try to find only immediate joiners)\n",
       "\n",
       "Note: Candidate Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.\n",
       "\n",
       "Role Overview:\n",
       "\n",
       "We are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Lead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\n",
       "• Architect scalable data streaming and processing solutions to support healthcare data workflows.\n",
       "• Develop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\n",
       "• Ensure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\n",
       "• Collaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\n",
       "• Troubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\n",
       "• Mentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\n",
       "• Stay updated with the latest cloud technologies, big data frameworks, and industry trends.\n",
       "\n",
       "Required Skills & Qualifications:\n",
       "• 4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\n",
       "• Expertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\n",
       "• Experience with Azure Databricks (or willingness to learn and adopt it quickly).\n",
       "• Hands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\n",
       "• Proficiency in SQL, NoSQL databases, and data modeling for big data processing.\n",
       "• Knowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\n",
       "• Experience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\n",
       "• Strong analytical skills, problem-solving mindset, and ability to lead complex data projects.\n",
       "• Excellent communication and stakeholder management skills.\n",
       "\n",
       "Note: This is not a contract position, this will be a permanent position with Emids.\n",
       "\n",
       "Interested candidates Can Share Your Updated Profile with details for below Email.\n",
       "\n",
       "NAME:\n",
       "\n",
       "CCTC:\n",
       "\n",
       "ECTC:\n",
       "\n",
       "Notice Period:\n",
       "\n",
       "Offers in Hand :\n",
       "\n",
       "Email ID: Ravi.chekka@emids.com</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChQeXRob24sIFB5c3BhcmssIGFuZCBBenVyZSBEYXRhYnJpY2tzKSB8fCAoNCAtIDYgWWVhcnMpIiwiY29tcGFueV9uYW1lIjoiRW1pZHMiLCJhZGRyZXNzX2NpdHkiOiJCZW5nYWx1cnUsIEthcm5hdGFrYSwgSW5kaWEiLCJodGlkb2NpZCI6Im9TMEEyREhLeU9wd0lVdnVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>8 hours ago</td><td>Data Engineer</td></tr><tr><td>Lead Data Engineer - Data Engineering</td><td>CENCORA</td><td>India</td><td>Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\n",
       "\n",
       "Job Details\n",
       "\n",
       "PRIMARY DUTIES AND RESPONSIBILITIES:\n",
       "• Leads design, development, and automation of scalable Analytic data engineering products leveraging Machine Learning, cloud infrastructure\n",
       "• Leads the design, build and operationalization of monitoring and tracking of data quality and data flow dynamics\n",
       "• Optimizes existing data processes and implements best-in-class data transformation capabilities\n",
       "• Leverages advanced statistical and computational methodologies to deliver insights and identify strategic opportunities\n",
       "• Implements Data Ingestion Framework and other re-usable components leveraged for delivering data pipeline and supporting ad-hoc data ingestions\n",
       "• Assists with development and storage of analytics-ready data for development of analytic deliverables\n",
       "• Recommends data products to solve business problems meeting multiple stakeholder requirements\n",
       "• Drives project planning processes, delegates non-complex tasks to junior team members\n",
       "• Mentors other team members and assists them with priority setting and issue resolution\n",
       "• Maintains data and information environment that provides consistent and accurate reporting of the client’s data\n",
       "• Leverages Machine Learning to enhance the developed solution\n",
       "• Collaborates with Business and Client representatives to target strategic analytics opportunities and define deliverable scope\n",
       "• Facilitates the resolution of issues regarding projects being worked on to include proper documentation of recommendations for those issues\n",
       "• Manages multiple initiatives in parallel partnering with other Analytics team members and conducts review meetings with stakeholders to drive prioritization effort\n",
       "• Analyzes model errors and design strategies to overcome them\n",
       "• Implements best practices for data engineering to ensure quality delivery of enterprise solutions\n",
       "• Brings expert knowledge of data visualization tools and techniques to drive business analytics and semantic data access requirements.\n",
       "• Work closely with business users, vendors, and delivery teams to understand the business requirements that drive the analysis and design of business analytics and reporting solutions.\n",
       "• Conceptualize, design, and develop data visualization solutions that synthesize data concepts into clear communications for key business stakeholders.\n",
       "• Drives business stakeholder adoption of insights-driven decision making and/or business process innovation.\n",
       "• Leads knowledge transfer around using data visualizations to business stakeholders.\n",
       "• Assist in developing best practices for data presentation and sharing across the organization.\n",
       "• Ensures data visualization standards are maintained and implemented.\n",
       "• Demonstrate an ability to reduce data to the bare minimum of what is needed to optimally communicate a message.\n",
       "• Provides technical leadership, coaching and mentoring to team members and business users.\n",
       "• Participates in POC projects and provides business analytics solutions recommendations.\n",
       "• Evaluates new visualization tools and performs research on best practices.\n",
       "• Contributes to strategic planning meetings and provides guidance and expertise on system options, risk, cost vs. benefits, and impacts on business processes and goals.\n",
       "• Has expertise in multiple technical environments and possesses business knowledge that spans multiple business areas.\n",
       "• Responsible for BI Tool administration & security functions as designated\n",
       "\n",
       ".\n",
       "\n",
       "EDUCATIONAL QUALIFICATIONS:\n",
       "\n",
       "Bachelor’s Degree in Statistics, Computer Science, Information Technology or any other related discipline or equivalent related experience.\n",
       "\n",
       "Preferred Certifications:\n",
       "• Advanced Data Analytics Certifications\n",
       "• AI and ML Certifications\n",
       "• SAS Statistical Business Analyst Professional Certification\n",
       "\n",
       "WORK EXPERIENCE:\n",
       "6+ years of directly-related or relevant experience, preferably in healthcare data analytics or data engineering.\n",
       "\n",
       "Working Hours:\n",
       "\n",
       "7PM IST to 2AM IST; Hybrid Working Model\n",
       "\n",
       "SKILLS & KNOWLEDGE:\n",
       "\n",
       "Behavioral Skills:\n",
       "• Conflict Resolution\n",
       "• Creativity & Innovation\n",
       "• Decision Making\n",
       "• Planning\n",
       "• Presentation Skills\n",
       "• Risk-taking\n",
       "\n",
       "Technical Skills:\n",
       "• Advanced Data Visualization Techniques\n",
       "• Advanced Statistical Analysis\n",
       "• Big Data Analysis Tools and Techniques\n",
       "• Data Governance\n",
       "• Data Management\n",
       "• Data Modelling\n",
       "• Data Quality Assurance\n",
       "• Machine Learning and AI Fundamentals\n",
       "• Programming languages like SQL, R, Python\n",
       "\n",
       "Tools Knowledge:\n",
       "• Business Intelligence Software like Tableau, Power BI, Alteryx, QlikSense\n",
       "• Data Visualization Tools\n",
       "• Microsoft Office Suite\n",
       "• Statistical Analytics tools (SAS, SPSS3)\n",
       "\n",
       "What Cencora offers\n",
       "\n",
       "​Benefit offerings outside the US may vary by country and will be aligned to local market practice. The eligibility and effective date may differ for some benefits and for team members covered under collective bargaining agreements.\n",
       "\n",
       "Full time\n",
       "\n",
       "Affiliated Companies\n",
       "Affiliated Companies: CENCORA INDIA TECHNOLOGY SERVICES PRIVATE LIMITED\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Cencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\n",
       "\n",
       "The company’s continued success depends on the full and effective utilization of qualified individuals. Therefore, harassment is prohibited and all matters related to recruiting, training, compensation, benefits, promotions and transfers comply with equal opportunity principles and are non-discriminatory.\n",
       "\n",
       "Cencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com. We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgLSBEYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiQ2VuY29yYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJHdzRzcW9FbWlPdEVnTDAwQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>3 days ago</td><td>Data Engineer</td></tr><tr><td>Azure Data Engineer – Azure Databricks</td><td>AIPRUS SOFTWARE PRIVATE LIMITED</td><td>Bengaluru, Karnataka, India</td><td>Job Title: Azure Data Engineer – Azure Databricks\n",
       "\n",
       "Location: Bangalore, India\n",
       "\n",
       "Experience: 5 to 10 Years\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "As a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Design and develop scalable data pipelines using Azure Databricks and PySpark.\n",
       "• Transform raw data into actionable insights through advanced data engineering techniques.\n",
       "• Build, deploy, and maintain machine learning models using MLlib, TensorFlow, and MLflow.\n",
       "• Optimize data integration workflows from Azure Blob Storage, Data Lake, and SQL/NoSQL sources.\n",
       "• Execute large-scale data processing using Spark Pools, fine-tuning configurations for performance and cost-efficiency.\n",
       "• Collaborate with data scientists, analysts, and business stakeholders to deliver robust data solutions.\n",
       "• Maintain and enhance Databricks notebooks and Delta Lake architectures.\n",
       "\n",
       "Required Skills & Qualifications:\n",
       "• Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field.\n",
       "• 7+ years of experience in data engineering, with at least 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n",
       "• Strong proficiency in:\n",
       "• Python, PySpark, Pandas, NumPy, SciPy\n",
       "• Spark SQL, DataFrames, RDDs\n",
       "• Delta Lake, Databricks Notebooks, MLflow\n",
       "• Hands-on experience with:\n",
       "• Azure Data Lake, Blob Storage, Synapse Analytics\n",
       "• Excellent problem-solving and communication skills.\n",
       "• Ability to work independently and in a collaborative team environment.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with CI/CD pipelines for data workflows.\n",
       "• Familiarity with data governance and security best practices in Azure.\n",
       "• Knowledge of real-time data processing and streaming technologies.</td><td>eyJqb2JfdGl0bGUiOiJBenVyZSBEYXRhIEVuZ2luZWVyIOKAkyBBenVyZSBEYXRhYnJpY2tzIiwiY29tcGFueV9uYW1lIjoiQWlwcnVzIFNvZnR3YXJlIFByaXZhdGUgTGltaXRlZCIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiek5tRUxOMG5Iam5zazl6TkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 hours ago</td><td>Data Engineer</td></tr><tr><td>Principle Software Engineer for Data Platform - 31866</td><td>SPLUNK</td><td>Bengaluru, Karnataka, India</td><td>Splunk, a Cisco company, is building a safer and more resilient digital world with an end-to-end full stack platform made for a hybrid, multi-cloud world. Leading enterprises use our unified security and observability platform to keep their digital systems secure and reliable. Our customers love our technology, but it's our caring employees that make Splunk stand out as an amazing career destination. No matter where in the world or what level of the organization, we approach our work with kindness. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Come help organizations be their best, while you reach new heights with a team that has your back.Role SummarySplunk AppDynamics, a leader in observability solutions for both cloud and on-premises environments, empowers customers with end-to-end visibility across their entire application ecosystems, cloud, on-premises, or hybrid. Our team is dedicated to developing, maintaining, and supporting SaaS and on-premises offerings within the observability product suite.We're looking for a Senior Software Engineer to build upon our application and data platform as we continue to innovate on application observability. We move fast and iterate quickly. We are passionate about solving customers’ problems. We have ambitious goals to build best-of-its-kind products. You will help the team win in a fast-growing market. If you are passionate about innovation and embrace the challenge of working on highly scalable systems that handle large volumes of data, this position is for you.Meet the Team (Optional)Data platform is the heart of the AppDynamics architecture. We process more than 100 billion data packets per day, more than 5 peta byte of data flows through our platform. We are in 8 regions having 40 clusters of 7000 nodes. We ingest, process and store all the data that power the dashboards that our customers rely on for observing their infrastructure and their applications.What you'll get to do\n",
       "• Design and build highly scalable solutions\n",
       "• Work with a team of exceptionally capable and dedicated peers, all the way from engineering to product management and customer support\n",
       "• Work in an open environment, work together to get things done and adapt to the team's changing needs\n",
       "• Leverage technologies including Kafka, ElasticSearch, Docker, and Kubernetes across different cloud environments like AWS and Azure\n",
       "• lead critical initiatives for the organisation\n",
       "Must-have Qualifications\n",
       "• 15+ years of full-stack developer experience in designing and developing highly scalable, distributed applications, products, and services.\n",
       "• Expertise in Java programming language.\n",
       "• Strong proficiency in data structures, algorithms, threads, concurrent programming\n",
       "• Extensive knowledge of SQL and at least one relational database engine: MySQL. Hands on experience in RDS or NoSQL (Dynamo, MongoDB) is a big plus\n",
       "• Experience of building applications using microservices architecture with expertise in Dockers and Kubernetes.\n",
       "• Strong communication skills, both verbal and written. Ability to multi-task and adapt quickly to changing requirements, scope, and priorities.\n",
       "• Optimize data pipelines, storage systems, and query engines for performance and efficiency.\n",
       "• Work closely with product teams, data engineers, software developers, and product managers to deliver on business goals.\n",
       "• Mentor team members in architecture principles, coding best practices, and system design.\n",
       "• Ensure robust monitoring, logging, and alerting systems for proactive issue detection and resolution.\n",
       "• Support CI/CD processes and automate testing for data systems\n",
       "• Stay abreast of emerging trends in big data, machine learning, and distributed systems to recommend innovative solutions.\n",
       "Nice-to-have QualificationsWe’ve taken special care to separate the must-have qualifications from the nice-to-haves. “Nice-to-have” means just that: Nice. To. Have. So, don’t worry if you can’t check off every box. We’re not hiring a list of bullet points–we’re interested in the whole you.\n",
       "• Added advantage of having an experience in working on Cloud Observability Space.\n",
       "• experience of other languages like python, etc\n",
       "• experience of front-end technologies\n",
       "Splunk is an Equal Opportunity EmployerSplunk, a Cisco company, is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.\n",
       "\n",
       "Note:</td><td>eyJqb2JfdGl0bGUiOiJQcmluY2lwbGUgU29mdHdhcmUgRW5naW5lZXIgZm9yIERhdGEgUGxhdGZvcm0gLSAzMTg2NiIsImNvbXBhbnlfbmFtZSI6IlNwbHVuayIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiY2k4emNxSnNNanNSemNYUEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>9 days ago</td><td>Data Engineer</td></tr><tr><td>Software Developer- Python</td><td>BNP PARIBAS INDIA SOLUTIONS</td><td>India</td><td>About BNP Paribas India Solutions:\n",
       "\n",
       "Established in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n",
       "\n",
       "About BNP Paribas Group:\n",
       "\n",
       "BNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n",
       "\n",
       "Commitment to Diversity and Inclusion\n",
       "\n",
       "At BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n",
       "\n",
       "About Business line/Function:\n",
       "\n",
       "The Intermediate Holding Company (“IHC”) program structured at the U.S. level across poles of activities of BNP Paribas provides guidance, supports the analysis, impact assessment and drives adjustments of the U.S. platform’s operating model due to the drastic changes introduced by the Enhanced Prudential Standards (“EPS”) for Foreign Banking Organizations (“FBOs”) finalized by the Federal Reserve in February 2014, implementing Section 165 of U.S. Dodd-Frank Act.\n",
       "\n",
       "The IT Transversal Team is part of the Information Technology Group which works simultaneously on a wide range of projects arising from business, strategic initiatives, and regulatory changes and reengineering of existing applications to improve functionality and efficiency.\n",
       "\n",
       "Job Title:\n",
       "\n",
       "Python Developer\n",
       "\n",
       "Date:\n",
       "\n",
       "June-25\n",
       "\n",
       "Department:\n",
       "\n",
       "ITG- Fresh\n",
       "\n",
       "Location:\n",
       "\n",
       "Chennai, Mumbai\n",
       "\n",
       "Business Line / Function:\n",
       "\n",
       "Finance Dedicated Solutions\n",
       "\n",
       "Reports to:\n",
       "\n",
       "(Direct)\n",
       "\n",
       "Grade:\n",
       "\n",
       "(if applicable)\n",
       "\n",
       "(Functional)\n",
       "\n",
       "Number of Direct Reports:\n",
       "\n",
       "NA\n",
       "\n",
       "Directorship / Registration:\n",
       "\n",
       "NA\n",
       "Position Purpose\n",
       "\n",
       "The Python Developer will play a critical role in building and maintaining financial applications and tools that support data processing, analysis, and reporting within a fast-paced financial services environment. This position involves developing scalable and secure systems. The developer will collaborate with business analysts, finance users/or finance BA to translate complex business requirements into efficient, high-quality software solutions.\n",
       "\n",
       "A strong understanding of financial concepts, data integrity, and regulatory compliance is essential. The detailed responsibilities are mentioned below.\n",
       "\n",
       "Responsibilities\n",
       "\n",
       "Direct Responsibilities\n",
       "\n",
       "- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n",
       "\n",
       "- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n",
       "\n",
       "- Expertise in PySpark for large-scale data processing and loading into databases.\n",
       "\n",
       "- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n",
       "\n",
       "- Strong communication skills to effectively collaborate with team members and stakeholders.\n",
       "\n",
       "- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n",
       "Technical & Behavioral Competencies\n",
       "\n",
       "- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n",
       "\n",
       "- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n",
       "\n",
       "- Expertise in PySpark for large-scale data processing and loading into databases.\n",
       "\n",
       "- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n",
       "\n",
       "- Strong communication skills to effectively collaborate with team members and stakeholders.\n",
       "\n",
       "- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n",
       "\n",
       "- Good analytical, problem solving, & communication skills\n",
       "\n",
       "- Engage in technical discussions and to help in improving the system, process etc\n",
       "\n",
       "Nice to Have\n",
       "\n",
       "- Familiarity with Plotly and Matplotlib for data visualization of large datasets.\n",
       "\n",
       "- Skilled in API programming, handling JSON, CSV, and other unstructured data from various systems.\n",
       "\n",
       "- Familiarity with JavaScript, CSS, and HTML.\n",
       "\n",
       "- Experience with cloud architecture applications such as Dataiku or Databricks; competency with ETL tools.\n",
       "\n",
       "- Knowledge of regulatory frameworks, RISK, CCAR, and GDPR.\n",
       "Specific Qualifications (if required)\n",
       "\n",
       "Skills Referential\n",
       "\n",
       "Behavioural Skills: (Please select up to 4 skills)\n",
       "\n",
       "Ability to collaborate / Teamwork\n",
       "\n",
       "Critical thinking\n",
       "\n",
       "Ability to deliver / Results driven\n",
       "\n",
       "Communication skills - oral & written\n",
       "\n",
       "Transversal Skills: (Please select up to 5 skills)\n",
       "\n",
       "Analytical Ability\n",
       "\n",
       "Ability to develop and adapt a process\n",
       "\n",
       "Ability to understand, explain and support change\n",
       "\n",
       "Ability to develop others & improve their skills\n",
       "\n",
       "Choose an item.\n",
       "\n",
       "Education Level:\n",
       "\n",
       "Bachelor Degree or equivalent\n",
       "\n",
       "Experience Level\n",
       "\n",
       "At least 5 years\n",
       "\n",
       "Other/Specific Qualifications (if required)</td><td>eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBEZXZlbG9wZXItIFB5dGhvbiIsImNvbXBhbnlfbmFtZSI6IkJOUCBQYXJpYmFzIEluZGlhIFNvbHV0aW9ucyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ6VTdCX1R2emRzczk0M0ZoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>7 days ago</td><td>Python Developer</td></tr><tr><td>Freelance Python Developer</td><td>TEQLAWN</td><td>Anywhere</td><td>We’re looking for an experienced Python Developer (4–7 years) to build scalable web and application solutions, with opportunities to integrate AI capabilities. Experience in AI integration is a must.\n",
       "\n",
       "Responsibilities:\n",
       "• Develop scalable web and application solutions using Python, with integration of AI/ML components\n",
       "• Collaborate with clients to understand project goals and technical requirements\n",
       "• Write clean, maintainable, and well-documented code\n",
       "• Troubleshoot, debug, and optimize applications for performance and reliability\n",
       "• Ensure timely and efficient delivery of milestones and final deliverables\n",
       "• Participate in code reviews and contribute to maintaining coding standards and best practices\n",
       "• Work with relevant frameworks and libraries such as Django, Flask, FastAPI, NumPy, pandas, and scikit-learn\n",
       "\n",
       "Note: Please share the link to your portfolio along with your application.\n",
       "\n",
       "Job Types: Full-time, Contractual / Temporary, Freelance\n",
       "Contract length: 2 months\n",
       "\n",
       "Pay: ₹50,000.00 - ₹80,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Work from home\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "• Monday to Friday\n",
       "\n",
       "Experience:\n",
       "• Python Development: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlRlcWxhd24iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiY20xcUU4aHhjbFQ3dnVyT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>1 day ago</td><td>Python Developer</td></tr><tr><td>Python Developer — Full-Time | 1-2 years Exp | In-Office | Bangalore</td><td>SERP HAWK</td><td>India</td><td>\uD83D\uDE80 We’re Hiring: Python Developer\n",
       "\n",
       "SERP Hawk is looking for a talented Python Developer to join our dynamic team in Bangalore!\n",
       "\n",
       "\uD83C\uDF1F About Us\n",
       "\n",
       "SERP Hawk is a leading digital marketing and technology solutions company based in Bangalore. We specialize in building robust backend systems, scalable applications, and AI-powered solutions for clients across various industries.\n",
       "\n",
       "\uD83D\uDCCD Location:B-1, Bannerghatta Slip Road, KEB Colony, New Gurappana Palya, 1st Stage, BTM Layout 1, Bengaluru, Karnataka 560029\n",
       "\n",
       "\uD83C\uDF10 Website: www.serphawk.com\n",
       "\n",
       "\uD83D\uDCBC What You’ll Do\n",
       "• Design and develop scalable backend architectures.\n",
       "• Write clean, efficient Python code.\n",
       "• Integrate APIs and databases.\n",
       "• Implement CI/CD pipelines and automated tests.\n",
       "• Ensure high performance, security, and reliability.\n",
       "\n",
       "✅ What We’re Looking For\n",
       "\n",
       "✔️ 1–2 years of experience in Python development.\n",
       "\n",
       "✔️ Proficiency in frameworks like Django, Flask, or FastAPI.\n",
       "\n",
       "✔️ Strong understanding of APIs and databases.\n",
       "\n",
       "✔️ Experience with CI/CD tools and best practices.\n",
       "\n",
       "✔️ Excellent problem-solving skills and a collaborative mindset.\n",
       "\n",
       "\uD83D\uDCA1 Nice to Have\n",
       "\n",
       "⭐ Experience with AI/chatbots.\n",
       "\n",
       "⭐ Knowledge of cloud services and containerization.\n",
       "\n",
       "\uD83D\uDCB0 Salary\n",
       "• ₹20,000 – ₹25,000 per month (based on skills and experience).\n",
       "\n",
       "\uD83D\uDCCC Additional Details\n",
       "\n",
       "\uD83D\uDCBC This is a full-time, in-office role in Bangalore.\n",
       "\n",
       "\uD83C\uDFE2 Candidates must report to the office daily.\n",
       "\n",
       "\uD83C\uDF10 Should be flexible to work and attend meetings or calls as per client time zones.\n",
       "\n",
       "✨ Apply now and grow with us!</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAlCBGdWxsLVRpbWUgfCAxLTIgeWVhcnMgRXhwIHwgSW4tT2ZmaWNlIHwgQmFuZ2Fsb3JlIiwiY29tcGFueV9uYW1lIjoiU0VSUCBIYXdrIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjNBMWtiUmowZFhsRVZsWnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 hours ago</td><td>Python Developer</td></tr><tr><td>SQL + Python</td><td>WISSEN TECHNOLOGY</td><td>India</td><td>Wissen Technology is Hiring for SQL With Python\n",
       "\n",
       "About Wissen Technology:\n",
       "\n",
       "Wissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges.\n",
       "\n",
       "Role Overview:\n",
       "\n",
       "We are looking for a skilled and detail-oriented candidate with a strong foundation in SQL, Python, and data processing techniques. The ideal candidate is passionate about transforming raw data into meaningful insights and has hands-on experience across the data pipeline—from data wrangling to visualization.\n",
       "\n",
       "Experience: 3-7 Years\n",
       "\n",
       "Location: Bengaluru\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with SQL (e.g., joins, subqueries, CTEs, window functions).\n",
       "• Proficiency in Python for data manipulation (e.g., pandas, NumPy).\n",
       "• Experience working with relational databases like MySQL, PostgreSQL, SQL Server, or Oracle.\n",
       "• Hands-on experience in data wrangling, cleaning, and feature engineering.\n",
       "• Understanding of ETL processes and tools.\n",
       "• Familiarity with version control systems like Git.\n",
       "• Knowledge of data visualization techniques and tools.\n",
       "• Strong problem-solving and analytical skills.\n",
       "\n",
       "The Wissen Group was founded in the year 2000. Wissen Technology, a part of Wissen Group, was established in the year 2015. Wissen Technology is a specialized technology company that delivers high-end consulting for organizations in the Banking & Finance, Telecom, and Healthcare domains. We help clients build world class products.\n",
       "\n",
       "We offer an array of services including Core Business Application Development, Artificial Intelligence & Machine Learning, Big Data & Analytics, Visualization & Business Intelligence, Robotic Process Automation, Cloud Adoption, Mobility, Digital Adoption, Agile & DevOps, Quality Assurance & Test Automation.\n",
       "\n",
       "Over the years, Wissen Group has successfully delivered $1 billion worth of projects for more than 20 of the Fortune 500 companies. Wissen Technology provides exceptional value in mission critical projects for its clients, through thought leadership, ownership, and assured on-time deliveries that are always ‘first time right’.\n",
       "\n",
       "The technology and thought leadership that the company commands in the industry is the direct result of the kind of people Wissen has been able to attract. Wissen is committed to providing them with the best possible opportunities and careers, which extends to providing the best possible experience and value to our clients.\n",
       "\n",
       "We have been certified as a Great Place to Work® company for two consecutive years (2020-2022) and voted as the Top 20 AI/ML vendor by CIO Insider. Great Place to Work® Certification is recognized world over by employees and employers alike and is considered the ‘Gold Standard’. Wissen Technology has created a Great Place to Work by excelling in all dimensions - High-Trust, High-Performance Culture, Credibility, Respect, Fairness, Pride and Camaraderie.\n",
       "\n",
       "Website: www.wissen.com\n",
       "\n",
       "LinkedIn: https://www.linkedin.com/company/wissen-technology\n",
       "\n",
       "Wissen Leadership: https://www.wissen.com/company/leadership-team/\n",
       "\n",
       "Wissen Live: https://www.linkedin.com/company/wissen-technology/posts/feedView=All\n",
       "\n",
       "Wissen Thought Leadership: https://www.wissen.com/articles/\n",
       "\n",
       "Employee Speak:\n",
       "\n",
       "https://www.ambitionbox.com/overview/wissen-technology-overview\n",
       "\n",
       "https://www.glassdoor.com/Reviews/Wissen-Infotech-Reviews-E287365.htm\n",
       "\n",
       "Great Place to Work:\n",
       "\n",
       "https://www.wissen.com/blog/wissen-is-a-great-place-to-work-says-the-great-place-to-work-institute-india/\n",
       "\n",
       "https://www.linkedin.com/posts/wissen-infotech_wissen-leadership-wissenites-activity-6935459546131763200-xF2k\n",
       "\n",
       "About Wissen Interview Process:\n",
       "\n",
       "https://www.wissen.com/blog/we-work-on-highly-complex-technology-projects-here-is-how-it-changes-whom-we-hire/\n",
       "\n",
       "Latest in Wissen in CIO Insider:\n",
       "\n",
       "https://www.cioinsiderindia.com/vendor/wissen-technology-setting-new-benchmarks-in-technology-consulting-cid-1064.html</td><td>eyJqb2JfdGl0bGUiOiJTUUwgKyBQeXRob24iLCJjb21wYW55X25hbWUiOiJXaXNzZW4gVGVjaG5vbG9neSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJITW5vZUwxWlF1QWVma0hlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>7 hours ago</td><td>Python Developer</td></tr><tr><td>Python Developer Full time</td><td>VARIANCE TECHNOLOGIES PRIVATE LIMITED</td><td>Anywhere</td><td>Job Opportunity: Python Developer at Variance Technologies Private Limited!\n",
       "\n",
       "Role: Python Developer\n",
       "\n",
       "Duration: 1 Months\n",
       "\n",
       "Location: Hybrid / Remote\n",
       "\n",
       "Responsibilities:\n",
       "\n",
       "Collaborate with our development team to develop and maintain Python-based applications tailored for finance-related tasks\n",
       "\n",
       "Implement object-oriented programming principles to ensure the scalability and maintainability of codebase\n",
       "\n",
       "Gain hands-on experience with Pandas for data manipulation and analysis within finance contexts\n",
       "\n",
       "Support integration of REST and WebSocket APIs for seamless communication with financial data providers and platforms\n",
       "\n",
       "Assist in automating financial modeling and analysis workflows using Python, including Excel automation for enhanced efficiency\n",
       "\n",
       "Requirements:\n",
       "\n",
       "Currently pursuing or recently completed a degree in Computer Science, Finance, or a related field\n",
       "\n",
       "Basic proficiency in Python programming language, with a strong willingness to learn and grow\n",
       "\n",
       "Exceptional attention to detail and proactive attitude towards problem-solving\n",
       "\n",
       "Genuine interest in the intersection of finance and technology\n",
       "\n",
       "Bonus Skills:\n",
       "\n",
       "Familiarity with fundamental financial concepts and markets\n",
       "\n",
       "Exposure to Python libraries such as Pandas, NumPy, or SciPy\n",
       "\n",
       "Demonstrated interest in financial data analysis and visualization techniques\n",
       "\n",
       "Basic understanding of REST and WebSocket APIs\n",
       "\n",
       "Perks:\n",
       "\n",
       "Hands-on experience working on real-world projects at the forefront of finance and technology\n",
       "\n",
       "Mentorship and guidance from seasoned professionals in the field\n",
       "\n",
       "Networking opportunities with industry experts to expand your professional connections\n",
       "\n",
       "Flexible scheduling to accommodate academic commitments\n",
       "\n",
       "Potential for transition to a full-time position based on exceptional performance and availability\n",
       "\n",
       "Ready to kickstart your career with a prestigious 1-month internship at Variance Technologies Private Limited? Apply now by sending your resume and a tailored cover letter expressing your interest and qualifications to careers@variancefintech.com. Join our team and embark on a journey towards shaping the future of finance through technology!\n",
       "\n",
       "Variance Technologies Private Limited is committed to fostering a diverse and inclusive workplace where all individuals are empowered to thrive. We welcome applicants from all backgrounds and identities to apply.\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: From ₹35,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Work from home\n",
       "\n",
       "Schedule:\n",
       "• Monday to Friday\n",
       "\n",
       "Education:\n",
       "• Bachelor's (Preferred)\n",
       "\n",
       "Experience:\n",
       "• Python: 1 year (Preferred)\n",
       "• total work: 1 year (Preferred)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIEZ1bGwgdGltZSIsImNvbXBhbnlfbmFtZSI6IlZhcmlhbmNlIFRlY2hub2xvZ2llcyBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiaXZCN3JUQTN5YnkxRzV2MUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>5 days ago</td><td>Python Developer</td></tr><tr><td>Python Developer – AI & LLM Integrations</td><td>DISCOVER WEBTECH PRIVATE LIMITED</td><td>India</td><td>We are hiring a skilled Python Developer with a minimum of 3 years of experience, who has hands-on expertise in developing AI modules, building intelligent agents using LLMs, and working with cutting-edge frameworks such as LangChain, LangGraph, and LangSmith.\n",
       "\n",
       "The ideal candidate should be capable of designing, developing, and deploying backend services as well as intelligent AI-driven tools and systems.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design and implement intelligent agents using LLM-based frameworks like LangChain, LangGraph, and LangSmith.\n",
       "• Build backend systems using Python (Django, FastAPI, or Flask).\n",
       "• Develop and integrate APIs, third-party tools, and cloud services.\n",
       "• Create AI modules that interact with knowledge bases, APIs, and perform multi-step reasoning.\n",
       "• Implement prompt engineering, memory chains, and agent behavior logic.\n",
       "• Collaborate with cross-functional teams to deliver robust AI features.\n",
       "• Optimize code for scalability, performance, and reliability.\n",
       "\n",
       "Required Skills and Qualifications\n",
       "• 3+ years of hands-on experience with Python.\n",
       "• Proficiency in LangChain, LangGraph, or LangSmith.\n",
       "• Strong experience in working with LLMs like OpenAI, Cohere, or Anthropic.\n",
       "• Deep understanding of prompt engineering and agent orchestration.\n",
       "• Experience with APIs, JSON, and external integrations.\n",
       "• Knowledge of data storage systems (PostgreSQL, MongoDB).\n",
       "• Familiarity with Docker, Git, and CI/CD tools.\n",
       "• Excellent problem-solving and debugging skills.\n",
       "\n",
       "Preferred Qualifications\n",
       "• Knowledge of vector databases (e.g., Pinecone, Weaviate, FAISS).\n",
       "• Experience with microservices, asynchronous programming, and message queues (Celery, RabbitMQ).\n",
       "• Familiarity with frontend technologies (React.js or Vue.js) is a plus.\n",
       "• Exposure to cloud platforms such as AWS, GCP, or Azure.\n",
       "\n",
       "Job Types: Full-time, Permanent\n",
       "\n",
       "Pay: ₹30,000.00 - ₹70,000.00 per month\n",
       "\n",
       "Benefits:\n",
       "• Health insurance\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "\n",
       "Work Location: In person</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBBSSBcdTAwMjYgTExNIEludGVncmF0aW9ucyIsImNvbXBhbnlfbmFtZSI6IkRpc2NvdmVyIFdlYlRlY2ggUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Im1JUlJqRWJSV0pCSkdQR0VBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>3 days ago</td><td>Python Developer</td></tr><tr><td>Full Stack Developer (Python / React JS)</td><td>HITACHI CAREERS</td><td>India</td><td>Our Company\n",
       "\n",
       "We're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n",
       "\n",
       "Our group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n",
       "\n",
       "Imagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n",
       "\n",
       "The team\n",
       "\n",
       "We are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "\n",
       "The role: Full Stack Developer/Specialist\n",
       "\n",
       "Responsibilities:\n",
       "• Design, develop, and maintain applications.\n",
       "• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n",
       "• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n",
       "• Optimise applications for performance, scalability, and user experience.\n",
       "• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n",
       "• Provide training and support to end-users and IT staff on functionalities and best practices.\n",
       "• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n",
       "• Participate in project planning, execution, and post-implementation support.\n",
       "• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n",
       "• What you'll bring\n",
       "Qualifications:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n",
       "• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n",
       "• Expertise in development and customisation.\n",
       "• Proficiency in Python and React JS\n",
       "• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n",
       "• Experience with RESTful APIs and web services.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Strong communication and collaboration skills.\n",
       "• Ability to work independently and as part of a team in a fast-paced environment.\n",
       "• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n",
       "• Prior experience in building AI applications is a plus.\n",
       "• Prior experience with automation tools like UIPath is a plus.\n",
       "• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\n",
       "Preferred Skills\n",
       "• Certification in\n",
       "• Experience with other low-code/no-code platforms.\n",
       "• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n",
       "• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n",
       "• Experience with Agile development methodologies.\n",
       "About us\n",
       "\n",
       "We're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n",
       "\n",
       "#LI-MS3\n",
       "\n",
       "Championing diversity, equity, and inclusion\n",
       "\n",
       "Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n",
       "\n",
       "How we look after you\n",
       "\n",
       "We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n",
       "\n",
       "We're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.</td><td>eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>14 days ago</td><td>Python Developer</td></tr><tr><td>Generative AI & Backend Developer(python)</td><td>INTELLYPOD</td><td>Anywhere</td><td>Job Description (JD) For Gen Ai with Python:\n",
       "\n",
       "We're Hiring: GenAI & Backend Developer (Python)\n",
       "\n",
       "Work Location: Remote (Work From Home)\n",
       "\n",
       "Experience: 2+ Years\n",
       "\n",
       "Immediate Joiners Preferred\n",
       "\n",
       "Company: IntellyPod\n",
       "\n",
       "Apply at: hrd@intellypod.com | hr@intellypod.com\n",
       "\n",
       "About the Role:\n",
       "\n",
       "IntellyPod is looking for a passionate Backend Developer with hands-on experience in GenAI, LLMs, and Python. If you’re excited about building scalable APIs, integrating AI models, and working with the latest in backend and GenAI technologies — we’d love to connect!\n",
       "\n",
       "Key Responsibilities:\n",
       "\n",
       "· Develop and maintain Python-based backend services.\n",
       "\n",
       "· Design and implement RESTful APIs.\n",
       "\n",
       "· Integrate GenAI/LLM solutions into applications.\n",
       "\n",
       "· Manage and optimize SQL/NoSQL databases.\n",
       "\n",
       "· Collaborate with cross-functional tech teams.\n",
       "\n",
       "Must-Have Skills:\n",
       "\n",
       "· 2+ years of experience in backend development (Python).\n",
       "\n",
       "· Experience with GenAI and Large Language Models (e.g., GPT, LLaMA).\n",
       "\n",
       "· Strong knowledge of REST APIs and database design.\n",
       "\n",
       "· Familiarity with Git and backend architecture best practices.\n",
       "\n",
       "Need to Have:\n",
       "\n",
       "· Experience with AWS/GCP/Azure.\n",
       "\n",
       "· Docker, Kubernetes, or CI/CD exposure.\n",
       "\n",
       "· Familiarity with vector databases (e.g., Pinecone, FAISS).\n",
       "\n",
       "· Prompt engineering or LLM fine-tuning knowledge.\n",
       "\n",
       "Why Join Us?\n",
       "\n",
       "· 100% Remote – Flexible work setup\n",
       "\n",
       "· Work on next-gen AI products\n",
       "\n",
       "· Fast-growing, collaborative tech team\n",
       "\n",
       "· Opportunity to innovate with emerging AI tools\n",
       "\n",
       "Ready to build the future with us? Send your resume to: hrd@intellypod.com | hr@intellypod.com #GenAI #BackendDeveloper #PythonJobs #LLM #RemoteJob #HiringNow #IntellyPod\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: Up to ₹70,000.00 per month\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Application Question(s):\n",
       "• Are an immediate joiner -\n",
       "\n",
       "Are on notice period if yes [Then how many days]\n",
       "• Write YES or NO\n",
       "\n",
       "1) Need to ask have you worked on LLM based project -\n",
       "\n",
       "2) Have you worked on chatbot types apps -\n",
       "\n",
       "3) Have you strong knowleged of OOps and Python basic -\n",
       "\n",
       "4) Have you knowledge of Rest APi development -\n",
       "\n",
       "Experience:\n",
       "• 5G: 3 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJHZW5lcmF0aXZlIEFJIFx1MDAyNiBCYWNrZW5kIERldmVsb3BlcihweXRob24pIiwiY29tcGFueV9uYW1lIjoiSW50ZWxseXBvZCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiI4ekp1Sk9ZUFlTbWtOcFRuQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>5 hours ago</td><td>Python Developer</td></tr><tr><td>Etl Developer</td><td>VIVID RESOURCING</td><td>Bilaspur, Chhattisgarh, India</td><td>Job Title:\n",
       "Data Engineer / ETL Developer\n",
       "\n",
       "Location:\n",
       "US, remote from India\n",
       "\n",
       "Department:\n",
       "IT / Data & Analytics\n",
       "\n",
       "Reports To:\n",
       "Head of Data & Analytics\n",
       "\n",
       "Employment Type:\n",
       "Contract, 12 months\n",
       "\n",
       "Pay:\n",
       "$28-35 per hour payrolled or self-employed\n",
       "\n",
       "About the Role\n",
       "\n",
       "We are seeking a Data Engineer / ETL Developer to help design, build, and maintain data pipelines that support our internal reporting and analytics needs. This role will focus on integrating data from systems such as ERP, MES , and production databases into a centralized data platform to enable reliable and timely insights across the business. You will work alongside BI developers, IT staff, and business users to ensure the smooth flow of data and contribute to the foundation of a modern business intelligence environment.\n",
       "\n",
       "Key Responsibilities\n",
       "\n",
       "ETL & Data Pipeline Development\n",
       "• Develop and maintain ETL/ELT pipelines to move data from source systems (ERP, MES, SQL databases, flat files) into our centralized platform.\n",
       "• Use tools such as Azure Data Factory , SSIS , or similar to orchestrate and automate data workflows.\n",
       "\n",
       "Data Modeling & Integration\n",
       "• Assist in designing and building data models and data marts optimized for Power BI dashboards.\n",
       "• Support the creation and maintenance of dataflows and datasets in Power BI Service.\n",
       "\n",
       "Data Quality & Documentation\n",
       "• Implement data validation, transformation, and cleansing logic to ensure high-quality, reliable data.\n",
       "• Document data processes, business rules, and data mappings to support knowledge sharing and governance.\n",
       "\n",
       "Cross-Functional Collaboration\n",
       "• Work with internal teams (production, quality, operations, finance) to understand data requirements and reporting needs.\n",
       "• Collaborate with BI developers to ensure the data pipeline supports efficient and user-friendly reporting.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\n",
       "• 2+ years of experience in data engineering or ETL development roles.\n",
       "• Solid experience with SQL and ETL tools (e.G., Azure Data Factory , SSIS , Informatica , etc.).\n",
       "• Familiarity with Power BI , including working with datasets, dataflows, or basic DAX.\n",
       "• Understanding of data integration, transformation, and warehousing concepts.\n",
       "• Experience working with or integrating manufacturing systems (ERP, MES) is a strong plus.\n",
       "\n",
       "Preferred Skills\n",
       "• Experience with cloud data platforms (especially Microsoft Azure ).\n",
       "• Exposure to Python or scripting for automation.\n",
       "• Familiarity with data governance and documentation practices.\n",
       "• Experience with manufacturing environments or industrial data is beneficial.\n",
       "\n",
       "Soft Skills\n",
       "• Strong attention to detail and a logical, structured approach to problem-solving.\n",
       "• Willingness to learn and grow in a fast-paced environment.\n",
       "• Good communication and collaboration skills across technical and non-technical teams.\n",
       "• Proactive and solutions-oriented mindset.</td><td>eyJqb2JfdGl0bGUiOiJFdGwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiVml2aWQgUmVzb3VyY2luZyIsImFkZHJlc3NfY2l0eSI6IkJpbGFzcHVyLCBDaGhhdHRpc2dhcmgsIEluZGlhIiwiaHRpZG9jaWQiOiJueDl3cW1oMXpfcnBSX2dFQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>14 hours ago</td><td>ETL Developer</td></tr><tr><td>Senior ETL and Backend Developer (Salesforce)</td><td>S&P GLOBAL</td><td>Hyderabad, Telangana, India (+1 other)</td><td>About the Role:\n",
       "\n",
       "Grade Level (for internal use):\n",
       "10\n",
       "\n",
       "Title: Senior ETL and Backend Developer (Salesforce)\n",
       "\n",
       "Job Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n",
       "\n",
       "The Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n",
       "\n",
       "The team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n",
       "\n",
       "The Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n",
       "\n",
       "Responsibilities:\n",
       "• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n",
       "• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n",
       "• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n",
       "• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n",
       "• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n",
       "• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\n",
       "Monitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n",
       "• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\n",
       "Qualifications:\n",
       "\n",
       "Basic Qualifications:\n",
       "• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n",
       "• A minimum of 8+ years of experience in software engineering & Architecture.\n",
       "• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n",
       "• A minimum of 3+ years of Salesforce development, administration/Integration.\n",
       "• Proficiency in Informatica PowerCenter and other ETL tools.\n",
       "• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n",
       "• Experience with Salesforce integration and administration.\n",
       "• Proficiency in backend development languages (e.g., Java, Python, C#).\n",
       "• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Ability to work independently and as part of a team.\n",
       "• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n",
       "• Experience with other ETL tools and data integration platforms.\n",
       "• Informatica Certified Professional\n",
       "Salesforce Certified Administrator or Developer\n",
       "• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n",
       "• Excellent problem solving, analytical and technical troubleshooting skills.\n",
       "• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n",
       "• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n",
       "\n",
       "About S&P Global Commodity Insights\n",
       "At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n",
       "\n",
       "We’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n",
       "\n",
       "S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n",
       "\n",
       "For more information, visit http://www.spglobal.com/commodity-insights.\n",
       "\n",
       "What’s In It For You?\n",
       "\n",
       "Our Purpose:\n",
       "\n",
       "Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n",
       "\n",
       "Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n",
       "\n",
       "Our People:\n",
       "\n",
       "We're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n",
       "\n",
       "From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n",
       "\n",
       "Our Values:\n",
       "\n",
       "Integrity, Discovery, Partnership\n",
       "\n",
       "At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n",
       "\n",
       "Benefits:\n",
       "\n",
       "We take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n",
       "\n",
       "Our benefits include:\n",
       "• Health & Wellness: Health care coverage designed for the mind and body.\n",
       "• Flexible Downtime: Generous time off helps keep you energized for your time on.\n",
       "• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n",
       "• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n",
       "• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n",
       "• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n",
       "\n",
       "For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n",
       "\n",
       "Global Hiring and Opportunity at S&P Global:\n",
       "\n",
       "At S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "Equal Opportunity Employer\n",
       "\n",
       "S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n",
       "\n",
       "If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n",
       "\n",
       "US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n",
       "\n",
       "Job ID: 316835\n",
       "Posted On: 2025-06-03\n",
       "Location: Hyderabad, Telangana, India</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjJlUWpKemNOclM4MXR6QzhBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>30 days ago</td><td>ETL Developer</td></tr><tr><td>Senior Etl Developer</td><td>VIVID RESOURCING</td><td>Bilaspur, Chhattisgarh, India</td><td>Job Title:\n",
       "Senior Data Engineer / ETL Developer\n",
       "\n",
       "Location:\n",
       "US, from India\n",
       "\n",
       "Department:\n",
       "IT / Data & Analytics\n",
       "\n",
       "Reports To:\n",
       "Director of Data & Analytics\n",
       "\n",
       "Employment Type:\n",
       "Contract, 12 months\n",
       "\n",
       "Pay:\n",
       "$30-38 per hour payrolled or self-employed\n",
       "\n",
       "About the Role\n",
       "\n",
       "We are seeking a skilled and motivated Senior Data Engineer / ETL Developer to lead the design and implementation of end-to-end data integration solutions powering this new platform. You will work closely with IT, business stakeholders, and BI developers to build scalable data pipelines, robust data models, and a high-performance reporting environment centered around Power BI.\n",
       "\n",
       "This role is a key contributor in shaping our data infrastructure and delivering timely, accurate insights across the organization—from the shop floor to the boardroom.\n",
       "\n",
       "Key Responsibilities\n",
       "\n",
       "Data Engineering & Integration\n",
       "• Design, build, and maintain scalable ETL/ELT pipelines to extract, transform, and load data from diverse manufacturing systems (ERP, MES, IoT sensors, SQL databases, flat files, APIs).\n",
       "• Develop and manage data workflows using tools such as Azure Data Factory, SQL Server Integration Services (SSIS), or other modern data orchestration platforms.\n",
       "\n",
       "Platform & Architecture Support\n",
       "• Contribute to the design and architecture of the new internal analytics platform, ensuring flexibility, scalability, and cost-efficiency.\n",
       "• Design and optimize data lake and data warehouse solutions using cloud-native tools (preferably Microsoft Azure).\n",
       "\n",
       "Power BI Enablement\n",
       "• Build and optimize semantic data models, dataflows, and datasets for use in Power BI.\n",
       "• Collaborate with Power BI developers and business analysts to ensure data models meet reporting requirements, including DAX performance tuning and efficient data structure design.\n",
       "\n",
       "Data Governance & Quality\n",
       "• Implement data validation, cleansing, and monitoring processes to ensure data quality, reliability, and accuracy.\n",
       "• Contribute to data governance policies, including documentation, lineage tracking, and security controls.\n",
       "\n",
       "Collaboration & Mentorship\n",
       "• Work closely with cross-functional teams (IT, production, quality, finance, supply chain) to gather data requirements and deliver meaningful solutions.\n",
       "• Provide technical leadership and mentor junior developers or analysts in best practices for data engineering and analytics.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor’s or Master’s degree in Computer Science, Information Systems, Engineering, or a related field.\n",
       "• 5+ years of experience in data engineering, ETL development, or database architecture roles.\n",
       "• Proven experience with ETL tools such as Azure Data Factory, SSIS, or Informatica.\n",
       "• Advanced SQL skills, including performance tuning, stored procedures, and query optimization.\n",
       "• Strong experience with Power BI, including DAX, data modeling, and dataset optimization.\n",
       "• Experience with data warehousing (e.G., Azure Synapse, SQL Server, Snowflake) and cloud platforms (preferably Azure).\n",
       "• Understanding of manufacturing systems and data (e.G., ERP, MES, shop floor data, SCADA, historians).\n",
       "\n",
       "Preferred Skills\n",
       "• Experience with Python or other scripting languages for automation and data manipulation.\n",
       "• Familiarity with time-series data and integration from IoT or edge devices.\n",
       "• Knowledge of DevOps practices, CI/CD for data pipelines, and version control using Git.\n",
       "• Exposure to data governance frameworks and tools like Purview, Alation, or Collibra.\n",
       "• Power BI Service administration experience and integration with Power Platform (e.G., Power Automate, Power Apps)\n",
       "\n",
       "Key Competencies\n",
       "• Strong analytical and problem-solving skills with attention to detail.\n",
       "• Excellent communication skills, with the ability to bridge technical and business domains.\n",
       "• Self-starter with the ability to lead initiatives and work independently or collaboratively.\n",
       "• A passion for continuous improvement and innovation in a manufacturing setting.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRXRsIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlZpdmlkIFJlc291cmNpbmciLCJhZGRyZXNzX2NpdHkiOiJCaWxhc3B1ciwgQ2hoYXR0aXNnYXJoLCBJbmRpYSIsImh0aWRvY2lkIjoiMnQxYXo2bnUzRU1abUlLLUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>14 hours ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer – IBM DataStage</td><td>TATA CONSULTANCY SERVICES</td><td>Hyderabad, Telangana, India</td><td>Job Title: ETL Developer – IBM DataStage\n",
       "\n",
       "Experience: 5 to 10 years\n",
       "\n",
       "Location: Hyderabad, Chennai, Mumbai, Bangalore, Ahmedabad, Indore\n",
       "\n",
       "Employment Type: Full-time\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "We are seeking a skilled ETL Developer with hands-on experience in IBM DataStage to join our dynamic data engineering team. The ideal candidate will have a strong background in ETL development, data warehousing concepts, and performance optimization. This role involves designing and implementing robust ETL solutions, collaborating with cross-functional teams, and ensuring high data quality and integrity.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Design, develop, and implement ETL processes using IBM DataStage.\n",
       "• Work across the full project lifecycle including low-level design, development, testing, and deployment.\n",
       "• Develop parallel jobs using various DataStage stages such as Copy, Join, Merge, Lookup, Funnel, Filter, Sort, Remove Duplicates, Aggregator, Change Capture, and Transformer.\n",
       "• Integrate data from heterogeneous sources including Oracle, SQL Server, and flat files.\n",
       "• Optimize and troubleshoot existing ETL processes for performance improvements.\n",
       "• Collaborate with data analysts and architects to understand business requirements and translate them into technical solutions.\n",
       "• Ensure data quality and integrity across multiple data sources.\n",
       "• Create and maintain technical documentation for ETL processes.\n",
       "• Participate in code reviews and adhere to ETL best practices.\n",
       "• Work in Agile environments and use tools like JIRA for tracking tasks and issues.\n",
       "• Demonstrate strong problem-solving skills and the ability to troubleshoot technical issues effectively.\n",
       "• Understand and support operational requirements as part of business delivery.\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with IBM DataStage for ETL development and migration.\n",
       "• Solid understanding of database and data warehousing concepts.\n",
       "• Proficiency in SQL and UNIX.\n",
       "• Experience working with large datasets and complex data transformations.\n",
       "• Familiarity with Agile methodologies and tools like JIRA.\n",
       "• Excellent communication and collaboration skills.</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIOKAkyBJQk0gRGF0YVN0YWdlIiwiY29tcGFueV9uYW1lIjoiVGF0YSBDb25zdWx0YW5jeSBTZXJ2aWNlcyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoibkdQWmp0dFNpdlVKUWVEYkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>7 days ago</td><td>ETL Developer</td></tr><tr><td>Senior Informatica Developer</td><td>EVERESTDX INC</td><td>Hyderabad, Telangana, India</td><td>About the Company:\n",
       "\n",
       "Everest DX – We are a Digital Platform Services company, headquartered in Stamford. Our Platform/Solution includes Orchestration, Intelligent operations with BOTs’, AI-powered analytics for Enterprise IT. Our vision is to enable Digital Transformation for enterprises to deliver seamless customer experience, business efficiency and actionable insights through an integrated set of futuristic digital technologies.\n",
       "\n",
       "Digital Transformation Services - Specialized in Design, Build, Develop, Integrate, and Manage cloud solutions and modernize Data centers, build a Cloud-native application and migrate existing applications into secure, multi-cloud environments to support digital transformation. Our Digital\n",
       "\n",
       "Platform Services enable organizations to reduce IT resource requirements and improve productivity, in addition to lowering costs and speeding digital transformation.\n",
       "\n",
       "Digital Platform - Cloud Intelligent Management (CiM) - An Autonomous Hybrid Cloud Management Platform that works across multi-cloud environments. helps enterprise Digital Transformation get most out of the cloud strategy while reducing Cost, Risk and Speed.\n",
       "\n",
       "To know more please visit: http://www.everestdx.com\n",
       "\n",
       "Responsibilities:\n",
       "• Candidate should hands-on experience on ETL and SQL.\n",
       "• Design, develop, and optimize ETL workflows using Informatica PowerCenter.\n",
       "• Implement cloud-based ETL solutions using Informatica IDMC and IICS.\n",
       "• Should have expertise on all transformations in Power Center and IDMC/IICS.\n",
       "• Should have experience or knowledge on the PC to IICS migration using CDI PC tool or some other tool.\n",
       "• Lead data migration projects, transitioning data from on-premise to cloud environments.\n",
       "• Write complex SQL queries and perform data validation and transformation.\n",
       "• Conduct detailed data analysis to ensure accuracy and integrity of migrated data.\n",
       "• Troubleshoot and optimize ETL processes for performance and error handling.\n",
       "• Collaborate with cross-functional teams to gather requirements and design solutions.\n",
       "• Create and maintain documentation for ETL processes and system configurations.\n",
       "• Implement industry best practices for data integration and performance tuning.\n",
       "\n",
       "Required Skills:\n",
       "• Hands-on experience with Informatica Power Center, IDMC and IICS.\n",
       "• Strong expertise in writing complex SQL queries and database management.\n",
       "• Experience in data migration projects (on-premise to cloud).\n",
       "• Strong data analysis skills for large datasets and ensuring accuracy.\n",
       "• Solid understanding of ETL design & development concepts.\n",
       "• Familiarity with cloud platforms (AWS, Azure).\n",
       "• Experience with version control tools (e.g., Git) and deployment processes.\n",
       "\n",
       "Preferred Skills:\n",
       "• Experience with data lakes, data warehousing, or big data platforms.\n",
       "• Familiarity with Agile methodologies.\n",
       "• Knowledge of other ETL tools</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgSW5mb3JtYXRpY2EgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiRXZlcmVzdERYIEluYyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNDdnQ200aEh5aHpSWlhDOEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>2 days ago</td><td>ETL Developer</td></tr><tr><td>Spark Engineer</td><td>STAFFINGINE LLC</td><td>India</td><td>- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>Spark Engineer</td></tr><tr><td>Databricks Engineer - Spark / PySpark</td><td>ENKEFALOS TECHNOLOGIES LLP</td><td>Anywhere</td><td>Databricks Engineer – Spark / PySpark\n",
       "\n",
       "Location : Remote / Mysore\n",
       "\n",
       "Joining : Immediate\n",
       "\n",
       "Experience : 5+ years\n",
       "\n",
       "Responsibilities :\n",
       "\n",
       "Will implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n",
       "\n",
       "Requirements:\n",
       "• PySpark (RDDs, DataFrames, performance tuning)\n",
       "• Building gold‐layer data models for financial reporting\n",
       "• Experience with complex joins, aggregations, GL hierarchies\n",
       "• Version handling (Actuals vs Budget), currency conversions\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500,395.35 - ₹1,840,348.25 per year\n",
       "\n",
       "Benefits:\n",
       "• Flexible schedule\n",
       "• Paid sick time\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Application Question(s):\n",
       "• Have you worked on ADF/ADLS ?\n",
       "• Do you have hands-on experience of Spark / PySpark\n",
       "\n",
       "Experience:\n",
       "• Databricks Engineering: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>15 days ago</td><td>Spark Engineer</td></tr><tr><td>Senior PySpark Data Engineer (Big Data, Cloud Data Solutions, & Python)</td><td>SYNECHRON</td><td>India</td><td>Job Summary\n",
       "\n",
       "Synechron is seeking a skilled PySpark Data Engineer to design, develop, and optimize data processing solutions leveraging modern big data technologies. In this role, you will lead efforts to build scalable data pipelines, support data integration initiatives, and work closely with cross-functional teams to enable data-driven decision-making. Your expertise will contribute to enhancing business insights and operational efficiency, positioning Synechron as a pioneer in adopting emerging data technologies.\n",
       "\n",
       "Software Requirements\n",
       "\n",
       "Required Software Skills:\n",
       "• PySpark (Apache Spark with Python) – experience in developing data pipelines\n",
       "• Apache Spark ecosystem knowledge\n",
       "• Python programming (versions 3.7 or higher)\n",
       "• SQL and relational database management systems (e.g., PostgreSQL, MySQL)\n",
       "• Cloud platforms (preferably AWS or Azure)\n",
       "• Version control: GIT\n",
       "• Data workflow orchestration tools like Apache Airflow\n",
       "• Data management tools: SQL Developer or equivalent\n",
       "\n",
       "Preferred Software Skills:\n",
       "• Experience with Hadoop ecosystem components\n",
       "• Knowledge of containerization (Docker, Kubernetes)\n",
       "• Familiarity with data lake and data warehouse solutions (e.g., AWS S3, Redshift, Snowflake)\n",
       "• Monitoring and logging tools (e.g., Prometheus, Grafana)\n",
       "\n",
       "Overall Responsibilities\n",
       "• Lead the design and implementation of large-scale data processing solutions using PySpark and related technologies\n",
       "• Collaborate with data scientists, analysts, and business teams to understand data requirements and deliver scalable pipelines\n",
       "• Mentor junior team members on best practices in data engineering and emerging technologies\n",
       "• Evaluate new tools and methodologies to optimize data workflows and improve data quality\n",
       "• Ensure data solutions are robust, scalable, and aligned with organizational data governance policies\n",
       "• Stay informed on industry trends and technological advancements in big data and analytics\n",
       "• Support production environment stability and performance tuning of data pipelines\n",
       "• Drive innovative approaches to extract value from large and complex datasets\n",
       "\n",
       "Technical Skills (By Category)\n",
       "\n",
       "Programming Languages:\n",
       "• Required: Python (PySpark experience minimum 2 years)\n",
       "• Preferred: Scala (for Spark), SQL, Bash scripting\n",
       "\n",
       "Databases/Data Management:\n",
       "• Relational databases (PostgreSQL, MySQL)\n",
       "• Distributed storage solutions (HDFS, cloud object storage like S3 or Azure Blob Storage)\n",
       "• Data warehousing platforms (Snowflake, Redshift – preferred)\n",
       "\n",
       "Cloud Technologies:\n",
       "• Required: Experience deploying and managing data solutions on AWS or Azure\n",
       "• Preferred: Knowledge of cloud-native services like EMR, Data Factory, or Azure Data Lake\n",
       "\n",
       "Frameworks and Libraries:\n",
       "• Apache Spark (PySpark)\n",
       "• Airflow or similar orchestration tools\n",
       "• Data processing frameworks (Kafka, Spark Streaming – preferred)\n",
       "\n",
       "Development Tools and Methodologies:\n",
       "• Version control with GIT\n",
       "• Agile management tools: Jira, Confluence\n",
       "• Continuous integration/deployment pipelines (Jenkins, GitLab CI)\n",
       "\n",
       "Security Protocols:\n",
       "• Understanding of data security, access controls, and GDPR compliance in cloud environments\n",
       "\n",
       "Experience Requirements\n",
       "• Minimum of 5+ years in data engineering, with hands-on PySpark experience\n",
       "• Proven track record of developing, deploying, and maintaining scalable data pipelines\n",
       "• Experience working with data lakes, data warehouses, and cloud data services\n",
       "• Demonstrated leadership in projects involving big data technologies\n",
       "• Experience mentoring junior team members and collaborating across teams\n",
       "• Prior experience in financial, healthcare, or retail sectors is beneficial but not mandatory\n",
       "\n",
       "Day-to-Day Activities\n",
       "• Develop, optimize, and deploy big data pipelines using PySpark and related tools\n",
       "• Collaborate with data analysts, data scientists, and business teams to define data requirements\n",
       "• Conduct code reviews, troubleshoot pipeline issues, and optimize performance\n",
       "• Mentor junior team members on best practices and emerging technologies\n",
       "• Design solutions for data ingestion, transformation, and storage\n",
       "• Evaluate new tools and frameworks for continuous improvement\n",
       "• Maintain documentation, monitor system health, and ensure security compliance\n",
       "• Participate in sprint planning, daily stand-ups, and project retrospectives to align priorities\n",
       "\n",
       "Qualifications\n",
       "• Bachelor’s or Master’s degree in Computer Science, Information Technology, or related discipline\n",
       "• Relevant industry certifications (e.g., AWS Data Analytics, GCP Professional Data Engineer) preferred\n",
       "• Proven experience working with PySpark and big data ecosystems\n",
       "• Strong understanding of software development lifecycle and data governance standards\n",
       "• Commitment to continuous learning and professional development in data engineering technologies\n",
       "\n",
       "Professional Competencies\n",
       "• Analytical mindset and problem-solving acumen for complex data challenges\n",
       "• Effective leadership and team management skills\n",
       "• Excellent communication skills tailored to technical and non-technical audiences\n",
       "• Adaptability in fast-evolving technological landscapes\n",
       "• Strong organizational skills to prioritize tasks and manage multiple projects\n",
       "• Innovation-driven with a passion for leveraging emerging data technologies\n",
       "\n",
       "S YNECHRON’S DIVERSITY & INCLUSION STATEMENT\n",
       "\n",
       "Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.\n",
       "\n",
       "All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.\n",
       "\n",
       "Candidate Application Notice</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgUHlTcGFyayBEYXRhIEVuZ2luZWVyIChCaWcgRGF0YSwgQ2xvdWQgRGF0YSBTb2x1dGlvbnMsIFx1MDAyNiBQeXRob24pIiwiY29tcGFueV9uYW1lIjoiU3luZWNocm9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjJYOEpqZUhEQmxiRndHQklBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>1 day ago</td><td>Spark Engineer</td></tr><tr><td>Senior Data Engineer (Delta Lake, Spark & Unity Catalog)</td><td>306 - GOTO TECHNOLOGIES INDIA PRIVATE LIMITED</td><td>India</td><td>Job Description Where you’ll work: India (Remote) Engineering at GoTo We’re the trailblazers of remote work technology. We build powerful, flexible work software that empowers everyone to live their best life, at work and beyond. And blaze even more trails along the way. There’s ample room for growth – so you can blaze your own trail here too. When you join a GoTo product team, you’ll take on a key role in this process and see your work be used by millions of users worldwide. Your Day to Day As a Senior Data Engineer, you would be: Design, develop, and maintain robust, scalable, and efficient ETL/ELT data pipelines to process structured and unstructured data from diverse sources. Architect and implement cloud-native data solutions using AWS services including S3, EMR, Lambda, and EKS. Build and optimize large-scale data processing workflows using Apache Spark and Databricks, with hands-on experience implementing and managing Delta Lake tables for high-volume batch and streaming use cases. Manage and govern data using Unity Catalog, maintaining strong data lineage, access controls, and metadata management. Develop and maintain data models (relational and dimensional) to support analytics, reporting, and machine learning use cases. Schedule, monitor, and orchestrate workflows using Apache Airflow or similar orchestration tools. Implement data quality checks, logging, monitoring, and alerting to ensure reliability and visibility of data pipelines. Collaborate with analysts, data scientists, and business stakeholders to deliver high-quality, trusted data for downstream applications and enable self-service BI tooling. Ensure adherence to best practices in data governance, security, and compliance. Mentor junior engineers and contribute to engineering standards, including CI/CD, automated testing, and documentation What We’re Looking For As an Senior Data Engineer, your background will look like: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. 5+ years of relevant experience in data engineering or software development, with a proven track record designing and maintaining production-grade data pipelines. Proficiency in Python programming and SQL for data transformation and analytics. Strong hands-on experience with Apache Spark and Databricks, including direct exposure to Delta Lake for data lake management, ACID transactions, schema enforcement and evolution, and time travel functionalities. Experience managing and organizing data access with Unity Catalog. In-depth experience with AWS services—specifically S3, EMR, Lambda, and EKS—and an excellent understanding of cloud-based data architecture and security best practices. Strong data modeling skills (dimensional, normalized) and understanding of data warehousing and lakehouse paradigms. Hands-on experience orchestrating workflows using Apache Airflow or similar orchestration frameworks. Familiarity with BI and data visualization tools (e.g., Tableau, Power BI). Experience establishing data quality processes, monitoring, and observability for pipelines. Exceptional communication, collaboration, and problem-solving skills, with the ability to thrive in an agile, multicultural team environment. At GoTo, authenticity and inclusive culture are key to our thriving workplace, where diverse perspectives drive innovation and growth. Our team of GoGetters is passionate about learning, exploring, and working together to achieve success while staying committed to delivering exceptional experiences for our customers. We take pride in supporting our employees with comprehensive benefits, wellness programs, and global opportunities for professional and personal development. By maintaining an inclusive environment, we empower our teams to do their best work, make a meaningful impact, and grow their career. Learn more. At GoTo, we’re bold enough to imagine a world of work without limits—where curiosity and AI-driven innovation fuel our constant growth. As the leader in cloud communications and IT, we solve real-world challenges through practical, cutting-edge solutions and an unwavering customer-first mindset. Our culture is rooted in inclusion, ownership, and transparency, fueling an environment where every voice contributes to both personal and collective achievement. Here, collaboration sparks bold ideas, and authenticity is celebrated—empowering you to adapt, evolve, and make a real impact. Join GoTo, and help shape the future of work while accelerating your own growth alongside exceptional people who are redefining what’s possible.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAoRGVsdGEgTGFrZSwgU3BhcmsgXHUwMDI2IFVuaXR5IENhdGFsb2cpIiwiY29tcGFueV9uYW1lIjoiMzA2IC0gR29UbyBUZWNobm9sb2dpZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlJxSFNWX2s0aVZkRWhBNGRBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>10 days ago</td><td>Spark Engineer</td></tr><tr><td>Data Engineer-Python,PySpark,SQL ,Spark Architecture,Azure Databricks</td><td>SIEMENS HEALTHINEERS</td><td>India</td><td>jobid\n",
       "• 460574\n",
       "\n",
       "jobfamily\n",
       "• Research & Development\n",
       "\n",
       "company\n",
       "• Siemens Healthcare Private Limited\n",
       "\n",
       "organization\n",
       "• Siemens Healthineers\n",
       "\n",
       "jobType\n",
       "• Full-time\n",
       "\n",
       "experienceLevel\n",
       "• Experienced Professional\n",
       "\n",
       "contractType\n",
       "• Permanent\n",
       "\n",
       "As a Data Engineer , you are required to:\n",
       "\n",
       "Design, build, and maintain data pipelines that efficiently process and transport data from various sources to storage systems or processing environments while ensuring data integrity, consistency, and accuracy across the entire data pipeline.\n",
       "\n",
       "Integrate data from different systems, often involving data cleaning, transformation (ETL), and validation. Design the structure of databases and data storage systems, including the design of schemas, tables, and relationships between datasets to enable efficient querying. Work closely with data scientists, analysts, and other stakeholders to understand their data needs and ensure that the data is structured in a way that makes it accessible and usable.\n",
       "\n",
       "Stay up-to-date with the latest trends and technologies in the data engineering space, such as new data storage solutions, processing frameworks, and cloud technologies. Evaluate and implement new tools to improve data engineering processes.\n",
       "\n",
       "Qualification : Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Science, Engineering is desirable.\n",
       "\n",
       "Experience level : At least 3 - 5 years hands-on experience in Data Engineering\n",
       "\n",
       "Desired Knowledge & Experience :\n",
       "• Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\n",
       "• Knowing Spark internals: Catalyst/Tungsten/Photon\n",
       "• Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\n",
       "• IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\n",
       "• Test: pytest, Great Expectations\n",
       "• CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\n",
       "• Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\n",
       "• Languages: Python/Functional Programming (FP)\n",
       "• SQL : TSQL/Spark SQL/HiveQL\n",
       "• Storage : Data Lake and Big Data Storage Design\n",
       "\n",
       "additionally it is helpful to know basics of:\n",
       "• Data Pipelines : ADF/Synapse Pipelines/Oozie/Airflow\n",
       "• Languages: Scala, Java\n",
       "• NoSQL : Cosmos, Mongo, Cassandra\n",
       "• Cubes : SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\n",
       "• SQL Server : TSQL, Stored Procedures\n",
       "• Hadoop : HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\n",
       "• Data Catalog : Azure Purview, Apache Atlas, Informatica\n",
       "\n",
       "Required Soft skills & Other Capabilities :\n",
       "\n",
       "Great attention to detail and good analytical abilities.\n",
       "\n",
       "Good planning and organizational skills\n",
       "\n",
       "Collaborative approach to sharing ideas and finding solutions\n",
       "\n",
       "Ability to work independently and also in a global team environment.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVB5dGhvbixQeVNwYXJrLFNRTCAsU3BhcmsgQXJjaGl0ZWN0dXJlLEF6dXJlIERhdGFicmlja3MiLCJjb21wYW55X25hbWUiOiJTaWVtZW5zIEhlYWx0aGluZWVycyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJXVkxrZFF1QU4tSGFXQjIzQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>8 days ago</td><td>Spark Engineer</td></tr><tr><td>Cloud Data Engineer (Spark/Databricks)</td><td>ANTAL JOB BOARD</td><td>Nagpur, Maharashtra, India</td><td>Vacancy No\n",
       "VN1228\n",
       "\n",
       "Business Unit\n",
       "EMEA\n",
       "\n",
       "Job Location\n",
       "India\n",
       "\n",
       "Employment Type\n",
       "Full Time\n",
       "\n",
       "Job Details and Responsibilities\n",
       "We are seeking an experienced Cloud Data Engineer with a strong background in AWS, Azure, and GCP. The ideal candidate will have extensive experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, and other ETL tools like Informatica, SAP Data Intelligence, etc. You will be responsible for designing, implementing, and maintaining robust data pipelines and building scalable data lakes. Experience with various data platforms like Redshift, Snowflake, Databricks, Synapse, Snowflake and others is essential. Familiarity with data extraction from SAP or ERP systems is a plus.\n",
       "\n",
       "Key Responsibilities:\n",
       "\n",
       "Design and Development:\n",
       "• Design, develop, and maintain scalable ETL pipelines using cloud-native tools (AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.).\n",
       "• Architect and implement data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n",
       "• Develop and optimize data ingestion, transformation, and loading processes using Databricks, Snowflake, Redshift, BigQuery and Azure Synapse.\n",
       "• Implement ETL processes using tools like Informatica, SAP Data Intelligence, and others.\n",
       "• Develop and optimize data processing jobs using Spark Scala.\n",
       "Data Integration and Management:\n",
       "• Integrate various data sources, including relational databases, APIs, unstructured data, and ERP systems into the data lake.\n",
       "• Ensure data quality and integrity through rigorous testing and validation.\n",
       "• Perform data extraction from SAP or ERP systems when necessary.\n",
       "Performance Optimization:\n",
       "• Monitor and optimize the performance of data pipelines and ETL processes.\n",
       "• Implement best practices for data management, including data governance, security, and compliance.\n",
       "Collaboration and Communication:\n",
       "• Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\n",
       "• Collaborate with cross-functional teams to design and implement data solutions that meet business needs.\n",
       "Documentation and Maintenance:\n",
       "• Document technical solutions, processes, and workflows.\n",
       "• Maintain and troubleshoot existing ETL pipelines and data integrations.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Education:\n",
       "\n",
       "Bachelor’s degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n",
       "\n",
       "Experience:\n",
       "• 7+ years of experience as a Data Engineer or in a similar role.\n",
       "• Proven experience with cloud platforms: AWS, Azure, and GCP.\n",
       "• Hands-on experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.\n",
       "• Experience with other ETL tools like Informatica, SAP Data Intelligence, etc.\n",
       "• Experience in building and managing data lakes and data warehouses.\n",
       "• Proficiency with data platforms like Redshift, Snowflake, BigQuery, Databricks, and Azure Synapse.\n",
       "• Experience with data extraction from SAP or ERP systems is a plus.\n",
       "• Strong experience with Spark and Scala for data processing.\n",
       "\n",
       "Skills:\n",
       "• Strong programming skills in Python, Java, or Scala.\n",
       "• Proficient in SQL and query optimization techniques.\n",
       "• Familiarity with data modeling, ETL/ELT processes, and data warehousing concepts.\n",
       "• Knowledge of data governance, security, and compliance best practices.\n",
       "• Excellent problem-solving and analytical skills.\n",
       "• Strong communication and collaboration skills.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Experience with other data tools and technologies such as Apache Spark, or Hadoop.\n",
       "• Certifications in cloud platforms (AWS Certified Data Analytics – Specialty, Google Professional Data Engineer, Microsoft Certified: Azure Data Engineer Associate).\n",
       "• Experience with CI/CD pipelines and DevOps practices for data engineering\n",
       "• Selected applicant will be subject to a background investigation, which will be conducted and the results of which will be used in compliance with applicable law.\n",
       "\n",
       "What we offer in return:\n",
       "• Remote Working: Lemongrass always has been and always will offer 100% remote work\n",
       "• Flexibility: Work where and when you like most of the time\n",
       "• Training: A subscription to A Cloud Guru and generous budget for taking certifications and other resources you’ll find helpful\n",
       "• State of the art tech: An opportunity to learn and run the latest industry standard tools\n",
       "• Team: Colleagues who will challenge you giving the chance to learn from them and them from you\n",
       "\n",
       "Lemongrass Consulting is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate on the basis of race, religion, color, national origin, religious creed, gender, sexual orientation, gender identity, gender expression, age, genetic information, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics\n",
       "\n",
       "About Lemongrass\n",
       "Lemongrass (lemongrasscloud.com) is a global leader in SAP consulting, focused on helping organizations transform their business processes through innovative solutions and technologies. With a strong commitment to customer success, Lemongrass partners with companies to drive their digital transformation journeys, enabling them to unlock the full potential of their SAP investments.\n",
       "\n",
       "We do this with our continuous innovation, automation, migration and operation, delivered on the world's most comprehensive cloud platforms – AWS, Azure and GCP and SAP Cloud ERP. We have been working with AWS and SAP since 2010 and we are a Premier Amazon Partner Network (APN) Consulting Partner. We are also a Microsoft Gold Partner, a Google Cloud Partner and an SAP Certified Silver Partner.\n",
       "\n",
       "Our team is what makes Lemongrass exceptional and why we have the excellent reputation in the market that we enjoy today. At Lemongrass, you will work with the smartest and most motivated people in the business. We take pride in our culture of innovation and collaboration that drives us to deliver exceptional benefits to our clients every day.</td><td>eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyIChTcGFyay9EYXRhYnJpY2tzKSIsImNvbXBhbnlfbmFtZSI6IkFudGFsIEpvYiBCb2FyZCIsImFkZHJlc3NfY2l0eSI6Ik5hZ3B1ciwgTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJCQXpseTdfU0lsQjByTmF2QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>28 days ago</td><td>Spark Engineer</td></tr><tr><td>Data Analyst III</td><td>BRISTOL MYERS SQUIBB</td><td>Hyderabad, Telangana, India</td><td>Working with Us\n",
       "Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n",
       "\n",
       "Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n",
       "\n",
       "The US Commercial Analytics, Market Access Data Analyst will play a crucial role in supporting data-driven decision-making processes, generating insights, and providing strategic guidance to optimize our Market Access & Pricing activities in the US. This position requires a deep expertise in secondary analytics and statistical analytic techniques, along with a firm grasp of the US healthcare landscape, especially market access and pricing dynamics. The Business Analyst III will typically be responsible for leading support for multiple therapeutic areas in Market Access and will collaborate with cross-functional teams to enhance understanding of business performance and drive growth.\n",
       "\n",
       "Roles and Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Perform statistical analyses, data mining and predictive modelling to uncover trends and drivers of performance\n",
       "• Collaborate with stakeholders (US based Market Access Business Insights & Analytics team and US Market Access team members) to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects including payer, PBM and patient level subnational & claims data analyses, formulary and policy level analysis, and ad-hoc analytics based on secondary data to support US Market Access strategy\n",
       "• Prepare reports, dashboards and presentations to communicate findings, insights and recommendations to stakeholders\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up-to-date with industry trends (US access and health care landscape), best practices, and emerging technologies\n",
       "• Partner with US based Market Access BIA/Commercial team members (for a given project scope)\n",
       "• Provide overall project oversight and training/mentorship to junior analysts and team members as required\n",
       "• Supports culture of fact-based decision making through application of best-in-class, innovative, and appropriate methodologies to address key business questions\n",
       "\n",
       "Skills & Competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions, around US pricing and reimbursement strategies\n",
       "• Proficiency in statistical analysis techniques and applications of those to US commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and strategic thinking, with the ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome Candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• 5-8 years prior Pharmaceutical industry experience with 1-2 years within US Market Access & Pricing Analytics\n",
       "• Experience working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims, medical policy and formulary data), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with reimbursement and regulatory requirements and compliance in the US biopharma industry\n",
       "• Certification or training in relevant analytics or business intelligence tools is a plus\n",
       "\n",
       "If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n",
       "\n",
       "Uniquely Interesting Work, Life-changing Careers\n",
       "With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n",
       "\n",
       "On-site Protocol\n",
       "\n",
       "BMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n",
       "\n",
       "Site-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n",
       "\n",
       "BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n",
       "\n",
       "BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n",
       "\n",
       "BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n",
       "\n",
       "If you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n",
       "\n",
       "Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUlJIiwiY29tcGFueV9uYW1lIjoiQnJpc3RvbCBNeWVycyBTcXVpYmIiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZHU3JGRGo4OTcybHdnQWpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>3 days ago</td><td>Data Analyst</td></tr><tr><td>Data Science Analyst</td><td>IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED</td><td>India</td><td>About the Role:Within Insights and Commercial Solutions, below are the key service groups:  Marketing Effectiveness (Business Analytics, Forecasting, Competitive Intelligence)  Field Force Operations  Commercial Consulting  Brand Insights  Patient access services (Pricing and market access, etc.)  Primary Market Research Provide analytics support to Novartis internal customers (CPOs & Regional marketing and sales teams) on various projects • Support and facilitate data enabled decision making for Novartis internal customers using data analysis and data science techniques/methods on internal and external (3rd party) data to solve business problems • Support I&CS business in building capabilities by involving in various initiatives like knowledge sharing, on-boarding and training support, support in all business-related tasks/activities, building process documentation and knowledge repositories.Analyst, Data ScienceLocation – Hyderabad #LI Hybrid Key Responsibilities:Delivering projects and managing internal customer expectations across multi-channel marketing analytics, Portfolio Analytics, Targeting and Segmentation, Predictive Analytics, Resource Allocation and Optimization, Other ad-hoc requests.Create and deliver customer requirements as agreed SLAs (timeliness, accuracy, quality, etc.) and drive excellent customer satisfaction. Deliver advanced analytical and statistical solutions for various projects related to promotion evaluation, multi-channel marketing (MCM) campaign design, return on investment (ROI) analysis, resource allocation, segmentation, targeting, and other ad-hoc business questionsSupport exploratory research to identify new areas of application of advanced analytics/data science in providing enhanced decision-making support. Deliver services through a structured project management approach with appropriate documentation and communication throughout the delivery of servicesAutomation of project codes and development of front-end delivery solutions. Support in creation and maintenance of standard operating procedures (SOPs), quality checklists that will enable excellent quality outputs within the function.Support in developing and maintaining knowledge repositories that captures qualitative and quantitative reports of brands, disease areas, macroeconomic trends of Novartis operating markets, etc.Comply with all internal functional operating procedures like time tracking, KPI tracking and reporting, and other internal systems and processes. Comply to all Novartis operating procedures as per legal/IT/HR requirementsEssential Requirements:Technical Skills: Statistical Modeling Experience (Covering but not limited to, Regression, Classification, Supervised and Unsupervised Learning Methods, Optimization techniques, Predictive Models, Deep learning Models, e.g. Neural Networks, Natural Language Processing, Text Mining etc.)Programming languages/tools – R, SAS, Python, SQLVisualization tools – Qlikview, Qliksense, TableauWorking knowledge of MS-Office (MS Excel, VBA, PowerPoint, Access)Pharmaceutical industry domain/datasets knowledge – desirableEducation:Graduation/Post Graduation in Business Administration, Mathematics, Economics, Statistics, Engineering or Quantitative streamsExperience: Experience (Minimum 2years) in analytics in marketing analytics, experience in pharma industry is preferableShould have strong quantitative and systems backgroundStrong analytical thinking with problem solving approachShould have good ability to understand new data sources in short timeframe and embed them into standard analysesA good understanding of pharmaceutical domain and data would be ideal. Should have worked in an international company with exposure to working in a cross-cultural environmentStrong and proactive business results focus, and proven ability to provide insights that increase productivityWhy Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook. https://www.novartis.com/careers/benefits-rewards Commitment to Diversity and Inclusion: Novartis is committed to building an outstanding, inclusive work environment and diverse teams' representative of the patients and communities we serve. Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVuY2UgQW5hbHlzdCIsImNvbXBhbnlfbmFtZSI6IklOMTAgKEZDUlMgPSBJTjAxMCkgTm92YXJ0aXMgSGVhbHRoY2FyZSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiejA4Y3NEMUZUdmdvZExfWkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>7 days ago</td><td>Data Analyst</td></tr><tr><td>Data Sr.Modeler/Data Analyst( Immediate Joiner)</td><td>THE TALENT QUEST</td><td>Hyderabad, Telangana, India</td><td>Vacancy with a company focused on digital transformation, specializing in intelligent automation, digitalization, data science & analytics, and mobile enablement. They help businesses improve cost efficiency, productivity, and agility by reducing turnaround time and errors. The company provides services and solutions including operations digital transformation consulting, next-gen shared services setup consulting, cognitive RPA deployment, and AI-enabled CX enhancement. Founded in 2020 ;with HQ in Gurugram, India; the Company is now operating from Noida, Mumbai, Hyderabad, and Bengaluru as well.\n",
       "\n",
       "Job Role:We are seeking a highly skilled and detail-oriented Data Modeler to join our Data\n",
       "\n",
       "Management team. The ideal candidate will be responsible for designing and\n",
       "\n",
       "implementing logical and physical data models to support enterprise data\n",
       "\n",
       "initiatives. This role requires close collaboration with business stakeholders, data\n",
       "\n",
       "architects, and engineers to ensure data is structured and accessible for analytics,\n",
       "\n",
       "reporting, and operational needs.\n",
       "\n",
       "The successful candidate will:\n",
       "\n",
       "Provides technical expertise in needs identification, data modelling, data\n",
       "\n",
       "movement and transformation mapping (source to target), automation and testing\n",
       "\n",
       "strategies, translating business needs into technical solutions with adherence to\n",
       "\n",
       "established data guidelines and approaches from a business unit or project\n",
       "\n",
       "perspective.\n",
       "\n",
       "7-10 Years industry implementation experience with one or more data\n",
       "\n",
       "modelling tools such as Erwin, ERStudio, PowerDesigner etc.\n",
       "\n",
       " Minimum of 8 years of data architecture, data modelling or similar\n",
       "\n",
       "experience\n",
       "\n",
       " 5-7 years of management experience required\n",
       "\n",
       " 5-7 years consulting experience preferred\n",
       "\n",
       " Experience working with dimensionally modelled data\n",
       "\n",
       " Bachelor’s degree or equivalent experience, Master’s Degree Preferred\n",
       "\n",
       " Understanding of cloud (Azure, AWS, GCP, Snowflake preferred) and on\n",
       "\n",
       "premises architectures\n",
       "\n",
       "Job Types: Full-time, Permanent\n",
       "\n",
       "Pay: Up to ₹3,000,000.00 per year\n",
       "\n",
       "Benefits:\n",
       "• Cell phone reimbursement\n",
       "• Internet reimbursement\n",
       "• Life insurance\n",
       "• Paid sick time\n",
       "• Paid time off\n",
       "• Work from home\n",
       "\n",
       "Work Location: In person</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIFNyLk1vZGVsZXIvRGF0YSBBbmFseXN0KCBJbW1lZGlhdGUgSm9pbmVyKSIsImNvbXBhbnlfbmFtZSI6IlRoZSBUYWxlbnQgUXVlc3QiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ino0YkdrY3RHOUdZdmFTRVVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>2 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst - INTL - Mexico or India</td><td>INSIGHT GLOBAL</td><td>Hyderabad, Telangana, India</td><td>Project Background:\n",
       "Mosaic is our financial planning and reporting transformation program, bringing an aligned business process and toolset to all key business units enabling us to get consistent and standardized financial metrics across geographies. Started in 2019 the program is already rolled out to Europe and most of North America and continues across the world.\n",
       "There are three key components to the program:\n",
       "1. A standardized planning tool IBM Cognos TM1\n",
       "2. A global data warehouse based on Azure Cloud know as SPOT (single point of truth). The SPOT team is made up of functional data analysts & data engineers\n",
       "3. An extensive suite of dashboards known as Cockpit using Tableau. The Cockpit team are generally finance analysts with Tableau skills\n",
       "Role Background:\n",
       "We are looking for an additional functional/data analyst to join the existing team of functional/data analysts working on the SPOT data warehouse.\n",
       "The role of the functional/data analyst is to be the bridge between the Cockpit team, Data Engineering team, and Cognos TM1 team. The analyst understands the business data requirements and translates these into data definition and data transformation documents that are then build by the data engineers. The analyst can also be involved in testing, data validation and any sustain activity related to the flow and accuracy of the data in SPOT & Cockpit.\n",
       "Typically, the functional data analyst receives requirements for new and/or amended data pipelines from the Cockpit team or works with the Cognos TM1 team on new data sets that need to flow to SPOT & Cockpit.\n",
       "The current functional data analysts come from a range of backgrounds from data engineers to business analysts. What unites the team is a passion for data, attention to detail, ability to navigate and understand complex data flows, and a customer-centric mindset. This is a high profile and at times high pressure transformation program and everyone on the team must be responsive to our stakeholders needs.\n",
       "Key Accountabilities:\n",
       "This role will primarily work with the North America Cockpit team primarily based in Mexico City, but as part of the team they will be expected to build a good knowledge of all data pipelines and work on any area as needed. The key elements of the role are:\n",
       "Develop and maintain SPOT solution design & data architecture:\n",
       "o Ensure SPOT solution design & data model is up to date with latest business requirements\n",
       "o Contribute to the delivery of current technical roadmap that can support the predicted growth in data volumes\n",
       "Translate and communicate business requirements across all IT delivery teams and/or partners:\n",
       "o Collaborate closely with SPOT Cockpit teams to ensure business requirements received from Global & Regional Planning teams are well-understood and translated into the SPOT solution design (functional & data model)\n",
       "Act as an owner in the diagnosis and resolution of any data issues raised with the team, partnering with other teams as needed.\n",
       "\n",
       "We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBJTlRMIC0gTWV4aWNvIG9yIEluZGlhIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Il96ZU5oSUE0Ujg3ak1KZHNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>6 days ago</td><td>Data Analyst</td></tr><tr><td>IN Specialist 3- Data Analyst - TRAS -Assurance– Bangalore</td><td>PWC</td><td>India</td><td>Line of ServiceAssuranceIndustry/SectorNot ApplicableSpecialismAssuranceManagement LevelSpecialistJob Description & SummaryAt PwC, our people in audit and assurance focus on providing independent and objective assessments of financial statements, internal controls, and other assurable information enhancing the credibility and reliability of this information with a variety of stakeholders. They evaluate compliance with regulations including assessing governance and risk management processes and related controls.In digital assurance at PwC, you will focus on providing assurance services over clients' digital environment, including processes and controls, cyber security measures, data and AI systems, and their associated governance, to help organisations and their stakeholders build trust in their technology while complying with relevant regulations.*Why PWCAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm’s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations. \"Job Description & SummaryABOUT TECHNOLOGY RISK ASSURANCE SERVICES:The Technology Risk Assurance Services practice in the firm, provides both audit and non-audit services related to controls around the financial reporting process, including financial business process and IT management controls.SPA provides:· Financial and operation applications/business process controls reviews· Database security controls reviews· IT general controls reviews· Infrastructure security reviews· Third party assurance and opinion services· Sarbanes-Oxley readiness, process improvement and sustainability services· Due diligence on systems and controls· Pre- and post-implementation systems reviews· Project assurance services· Data services (e.g., CAATs, data quality reviews)· Computer security reviewsResponsibilities (essential functions and responsibilities of the job):The tasks, roles and responsibilities concerned with the job will include the following:Executing various audit engagements as a support to field in-charge andApplying The Firm’s methodology while executing various job rolesApply current knowledge of IT trends and systems to identify security and risk management issues, and other opportunities for improvementAssist in the planning and execution of business process control reviews, information systems audits, and other Assurance offerings across a variety of industriesA good understanding of clients Business Risks, Audit Risks, IT Risks and the mitigating controls in addressing these risksGain strong comprehension of client operations, processes, and business objectives and utilize that knowledge on engagements.Evaluate and test business processes and controls and identify areas of risks.Perform general computer and application controls reviews including ERPs like SAP, Oracle Application, JD Edwards, MFG Pro, etc.Comply with requirements of ICAI Auditing Standards like AAS 29Data Analysis using CAATs tool like Excel, ACL, SQL etc*Mandatory skill sets - Data Analyst*Preferred Skill Sets – Business Analyst*Years of experience required – 0-1 Years*Education Qualification – Post Graduation in Data ScienceEducation (if blank, degree and/or field of study not specified)Degrees/Field of Study required: Postgraduate (Diploma)Degrees/Field of Study preferred:Certifications (if blank, certifications not specified)Required SkillsData AnalyticsOptional SkillsAccepting Feedback, Accepting Feedback, Active Listening, Artificial Intelligence (AI) Platform, Auditing Methodologies, Cloud Engineering, Communication, Compliance and Governance, Compliance and Standards, Compliance Auditing, Corporate Governance, Cybersecurity Governance, Cybersecurity Risk Management, Data Quality, Data Quality Assessment, Data Quality Assurance Testing, Data Validation, Emotional Regulation, Empathy, Governance Framework, Inclusion, Information Assurance, Information Security Governance, Intellectual Curiosity, Internal Controls {+ 11 more}Desired Languages (If blank, desired languages not specified)Travel RequirementsNot SpecifiedAvailable for Work Visa Sponsorship?NoGovernment Clearance Required?NoJob Posting End Date</td><td>eyJqb2JfdGl0bGUiOiJJTiBTcGVjaWFsaXN0IDMtIERhdGEgQW5hbHlzdCAtIFRSQVMgLUFzc3VyYW5jZeKAkyBCYW5nYWxvcmUiLCJjb21wYW55X25hbWUiOiJQd0MiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiZTlNdVBjTnNJWHlmb3ZrSkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>1 day ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst 1</td><td>BRISTOL MYERS SQUIBB</td><td>Hyderabad, Telangana, India</td><td>The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n",
       "\n",
       "Roles & Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up to date with industry trends, best practices, and emerging technologies\n",
       "\n",
       "Skills and competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n",
       "• commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• Proven experience (1-3 years) in a similar business analyst role\n",
       "• Prior Pharmaceutical industry and/or healthcare consulting experience required\n",
       "• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with regulatory requirements and compliance in the US biopharma industry\n",
       "\n",
       "The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n",
       "\n",
       "Roles & Responsibilities:\n",
       "• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n",
       "• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n",
       "• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n",
       "• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n",
       "• Stay up to date with industry trends, best practices, and emerging technologies\n",
       "\n",
       "Skills and competencies:\n",
       "• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n",
       "• commercial business problems\n",
       "• Strong project management skills and the ability to work independently or as part of a team\n",
       "• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n",
       "• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n",
       "\n",
       "Experience:\n",
       "• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n",
       "• Proven experience (1-3 years) in a similar business analyst role\n",
       "• Prior Pharmaceutical industry and/or healthcare consulting experience required\n",
       "• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n",
       "• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n",
       "• Familiarity with regulatory requirements and compliance in the US biopharma industry</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ1ZUlVZURnaVJaU1ludkdlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>16 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst Finance (VBA, Power Query, Power BI, Python) 4+ years of experience)</td><td>DUPONT</td><td>Hyderabad, Telangana, India</td><td>At DuPont, our purpose is to empower the world with essential innovations to thrive. We work on things that matter. Whether it’s providing clean water to more than a billion people on the planet, producing materials that are essential in everyday technology devices from smartphones to electric vehicles, or protecting workers around the world. Discover the many reasons the world’s most talented people are choosing to work at DuPont. Why Join Us | DuPont Careers\n",
       "\n",
       "Job Summary:\n",
       "\n",
       "The Data Analyst/Developer will play a crucial role in developing automated data solutions, creating comprehensive dashboards, and utilizing advanced Excel functionalities. This position will require collaboration with various teams to translate data needs into actionable intelligence.\n",
       "\n",
       "Key Areas of Expertise and Responsibilities:\n",
       "\n",
       "1. Visual Basic for Applications (VBA)\n",
       "• Responsibilities:\n",
       "• Develop and maintain complex VBA applications to automate repetitive tasks.\n",
       "• Incorporate SAP Scripting within VBA to optimize business processes.\n",
       "• Troubleshoot and debug existing VBA code to ensure smooth functionality.\n",
       "• Criteria:\n",
       "• Advanced proficiency in VBA programming.\n",
       "• Demonstrated experience with SAP interfaces and scripting.\n",
       "• Ability to write modular, efficient, and maintainable code.\n",
       "• Knowledge of Excel object model and its functionalities.\n",
       "\n",
       "2. Power Query\n",
       "• Responsibilities:\n",
       "• Utilize Power Query for data extraction, transformation, and loading (ETL) processes.\n",
       "• Develop and maintain data models in Excel to streamline data preparation.\n",
       "• Create and optimize Power Query scripts for efficient data processing.\n",
       "• Criteria:\n",
       "• Intermediate experience with Power Query including M language for data transformation.\n",
       "• Familiarity with connecting Power Query to various data sources (e.g., databases, APIs).\n",
       "• Ability to perform data cleansing and manipulation through Power Query.\n",
       "\n",
       "3. Power BI\n",
       "• Responsibilities:\n",
       "• Create interactive, user-friendly dashboards and reports using Power BI.\n",
       "• Collaborate with stakeholders to gather reporting requirements and deliver insights.\n",
       "• Optimize Power BI reports for performance and usability.\n",
       "• Criteria:\n",
       "• Intermediate knowledge of Power BI Desktop and Power BI Service.\n",
       "• Ability to create DAX measures and calculated columns for enhanced analytics.\n",
       "• Familiarity with data visualization best practices and techniques.\n",
       "\n",
       "4. Python\n",
       "• Responsibilities:\n",
       "• Develop Python scripts to automate data manipulation and Excel-related tasks.\n",
       "• Utilize libraries such as Pandas and NumPy for data analysis and automation.\n",
       "• Collaborate with the data team to integrate Python solutions with existing tools.\n",
       "• Criteria:\n",
       "• Intermediate proficiency in Python, especially in data manipulation and automation.\n",
       "• Experience with libraries like Pandas, NumPy, and openpyxl for Excel automation.\n",
       "• Understanding of APIs and ability to retrieve data programmatically.\n",
       "\n",
       "Qualifications:\n",
       "• Bachelor’s degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n",
       "• A minimum of 4 of experience in a data-related role, focusing on the technologies mentioned.\n",
       "• Strong analytical and problem-solving skills with attention to detail.\n",
       "• Excellent communication skills and the ability to work collaboratively with diverse teams.\n",
       "\n",
       "Preferred Skills:\n",
       "• Experience with SQL and relational databases for data querying and data management.\n",
       "• Familiarity with other business intelligence tools (like Tableau or QlikView) is a plus.\n",
       "• Knowledge of machine learning principles is an advantage.\n",
       "• Understanding of data warehousing concepts and methodologies.\n",
       "\n",
       "Join our Talent Community to stay connected with us!\n",
       "\n",
       "On May 22, 2024, we announced a plan to separate our Electronics and Water businesses in a tax-free manner to its shareholders. On January 15, 2025, we announced that we are targeting November 1, 2025, for the completion of the intended separation of the Electronics business (the “Intended Electronics Separation”)*. We also announced that we would retain the Water business. We are committed to ensuring a smooth and successful separation process for the Future Electronics business. We look forward to welcoming new talent interested in contributing to the continued success and growth of our evolving organization.\n",
       "\n",
       "(1)The separation transactions are subject to satisfaction of customary conditions, including final approval by DuPont's Board of Directors, receipt of tax opinion from counsel, the filing and effectiveness of Form 10 registration statements with the U.S. Securities and Exchange Commission, applicable regulatory approvals, and satisfactory completion of financing.  For further discussion of risks, uncertainties and assumptions that could impact the achievement, expected timing and intended benefits of the separation transactions, see DuPont’s announcement.\n",
       "\n",
       "DuPont is an equal opportunity employer. Qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability or any other protected class. If you need a reasonable accommodation to search or apply for a position, please visit our Accessibility Page for Contact Information.\n",
       "\n",
       "DuPont offers a comprehensive pay and benefits package. To learn more visit the Compensation and Benefits page.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgRmluYW5jZSAoVkJBLCBQb3dlciBRdWVyeSwgUG93ZXIgQkksIFB5dGhvbikgNCsgeWVhcnMgb2YgZXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJEdXBvbnQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImNGeEdTMkhJQXpJRW5kb2ZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>2 days ago</td><td>Data Analyst</td></tr><tr><td>Sustainability Data Analyst</td><td>CARRIER</td><td>Hyderabad, Telangana, India</td><td>Role: Sustainability Data Analyst\n",
       "\n",
       "Location: Hyderabad, India\n",
       "\n",
       "Full/ Part-time: Full time\n",
       "\n",
       "Build a career with confidence\n",
       "\n",
       "Carrier Global Corporation, global leader in intelligent climate and energy solutions is committed to creating solutions that matter for people and our planet for generations to come. From the beginning, we've led in inventing new technologies and entirely new industries. Today, we continue to lead because we have a world-class, diverse workforce that puts the customer at the center of everything we do.\n",
       "\n",
       "About the role\n",
       "\n",
       "We are seeking a results-driven Sustainability Data Analyst to join Carrier’s product sustainability analytics team. The ideal candidate will have a strong background in sustainability practices and data analytics, with a focus on analyzing environmental impact and identifying opportunities for improvement. This role involves conducting comprehensive sustainability/energy audits, analyzing data, and developing strategies to enhance sustainability performance.\n",
       "\n",
       "Key responsibilities:\n",
       "• Perform detailed sustainability audits and analyze data to identify trends and areas for improvement.\n",
       "• Develop and implement strategies to enhance sustainability and reduce environmental impact.\n",
       "• Monitor and evaluate the performance of sustainability initiatives using data analytics.\n",
       "• Collaborate with cross-functional teams to optimize sustainability practices.\n",
       "• Prepare reports and presentations on sustainability metrics and audit findings.\n",
       "• Stay updated on industry trends and best practices in sustainability and data analytics.\n",
       "\n",
       "Minimum Requirements:\n",
       "\n",
       "Education: Bachelor’s or Master’s degree in Mechanical, Energy Engineering, Environmental Science, Sustainability, Data Analytics, or a related field.\n",
       "\n",
       "Experience: atleast 6 years in sustainability, data analytics, or BI development roles to be able to drive himself towards business goals independently.\n",
       "\n",
       "Key Skills:\n",
       "• Strong analytical skills, attention to detail and ability to think from first principles.\n",
       "• Excellent communication and teamwork abilities.\n",
       "• Proficiency in data analytics software and tools. Knowledge of python, SQL, Power-BI, would be added advantage.\n",
       "• Knowledge of relevant regulations and standards in sustainability.\n",
       "• Familiarity with data visualization tools and techniques.\n",
       "• Willingness to be flexible, learn new tools, techniques and deliver.\n",
       "\n",
       "Benefits\n",
       "\n",
       "We are committed to offering competitive benefits programs for all of our employees, and enhancing our programs when necessary.\n",
       "• Enjoy your best years with our retirement savings plan\n",
       "• Have peace of mind and body with our health insurance\n",
       "• Make yourself a priority with flexible schedules, parental leave and our holiday purchase scheme\n",
       "• Drive forward your career through professional development opportunities\n",
       "• Achieve your personal goals with our Employee Assistance Programme.\n",
       "\n",
       "Our commitment to you\n",
       "\n",
       "Our greatest assets are the expertise, creativity and passion of our employees. We strive to provide a great place to work that attracts, develops and retains the best talent, promotes employee engagement, fosters teamwork and ultimately drives innovation for the benefit of our customers. We strive to create an environment where you feel that you belong, with diversity and inclusion as the engine to growth and innovation. We develop and deploy best-in-class programs and practices, providing enriching career opportunities, listening to employee feedback and always challenging ourselves to do better. This is The Carrier Way.\n",
       "\n",
       "Join us and make a difference.\n",
       "\n",
       "Apply Now!\n",
       "\n",
       "Carrier is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.\n",
       "\n",
       "Job Applicant's Privacy Notice:\n",
       "\n",
       "Click on this link to read the Job Applicant's Privacy Notice</td><td>eyJqb2JfdGl0bGUiOiJTdXN0YWluYWJpbGl0eSBEYXRhIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJDYXJyaWVyIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJPMkpEMHBVcG0xc3dxYnc1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>9 days ago</td><td>Data Analyst</td></tr><tr><td>Sr. Data Analyst</td><td>ICIMS TALENT ACQUISITION</td><td>Rai Durg, Telangana, India</td><td>Job Overview\n",
       "\n",
       "The Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.\n",
       "\n",
       "About Us\n",
       "\n",
       "When you join iCIMS, you join the team helping global companies transform business and the world through the power of talent. Our customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile. As the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent. We’re passionate about helping companies build a diverse, winning workforce and about building our home team. We're dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\n",
       "\n",
       "Responsibilities\n",
       "• Perform various data analysis functions to analyze data from a variety of sources including external labor market data and research and internal data sets from our platforms \n",
       "• Incorporate information from a variety of systems to produce comprehensive and compelling narratives for thought-leadership initiatives and customer engagements\n",
       "• Demonstrate critical thinking - identify the story in context using multiple data sets, and present results. A strong proficiency in data storytelling will be critical to success in this role.\n",
       "• Understand principles of quality data visualization and apply them in Tableau to create and maintain custom dashboards for consumption by other employees\n",
       "• Find and investigate data quality issues, root causes and recommend remedies to be implemented by the data scientists and engineers  \n",
       "• Liaise with teams around our business to understand their problems, determine how our team can help, then use our database to produce the content they need\n",
       "• Identify data mapping and enrichment requirements. Familiarity with SQL, especially the logic behind different types of data joins and writing efficient queries, will be necessary\n",
       "• Consistently ensure that business is always conducted with integrity and that behavior aligns with iCIMS policies, procedures, and core competencies\n",
       "\n",
       "Additional Job Responsibilities: \n",
       "• Produce and adapt data visualizations in response to business requests for internal and external use\n",
       "• Shows good judgement in prioritizing their own commitments and those of the larger team, while demonstrating initiative and appropriate urgency when needed\n",
       "• Mentor junior team members in best practices for analytics, data visualization, and data storytelling. Exemplify these standards and guide teammates in following them.\n",
       "• Think creatively to produce unique, actionable insights from complex datasets, which can deliver value to our business and to our customers.\n",
       "\n",
       "Qualifications\n",
       "• 5-10 years professional experience working in an analytics capacity\n",
       "• Excellent communication skills, especially with regards to data storytelling – finding insights from complex datasets and sharing those findings with key stakeholders\n",
       "• Strong data analytics and visualization skills\n",
       "• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n",
       "• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n",
       "• Advanced knowledge of Excel (Pivot tables, VLOOKUPs, IF statements)\n",
       "• Familiarity with data guardrails to ensure compliance with applicable data governance regulations and privacy laws (i.e., GDPR)\n",
       "\n",
       "Preferred\n",
       "• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n",
       "• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n",
       "\n",
       "EEO Statement\n",
       "\n",
       "iCIMS is a place where everyone belongs. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at iCIMS.\n",
       "\n",
       "We are proud to be an equal opportunity and affirmative action employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at careers@icims.com.\n",
       "\n",
       "Compensation and Benefits\n",
       "\n",
       "Competitive health and wellness benefits include medical insurance (employee and dependent family members), personal accident and group term life insurance, bonding and parental leave, lifestyle spending account reimbursements, wellness services offerings, sick and casual/emergency days, paid holidays, tuition reimbursement, retirals (PF - employer contribution) and gratuity. Benefits and eligibility may vary by location, role, and tenure. Learn more here: https://careers.icims.com/benefits</td><td>eyJqb2JfdGl0bGUiOiJTci4gRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiaUNJTVMgVGFsZW50IEFjcXVpc2l0aW9uIiwiYWRkcmVzc19jaXR5IjoiUmFpIER1cmcsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImZMUlRBQjlESVFWdFdOeGpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>21 days ago</td><td>Data Analyst</td></tr><tr><td>Master Data Analyst</td><td>C32511 ALFA LAVAL INDIA PRIVATE LIMITED</td><td>India</td><td>Purpose of the Job: To provide direct support to the Sales and Order Handling teams by managing customer master data and administering the Order Management System. This role ensures data integrity, system readiness, and compliance with internal controls during order booking and execution processes. Key Responsibilities: Customer Master Data Management Manage the full lifecycle of customer codes, including creation, amendment, and deactivation. Conduct background checks using the Global World Check system and coordinate with the Export Control Super User for compliance validation. Ensure customer data is configured correctly in the system for seamless order booking and execution. Maintain data accuracy and cleanliness in line with Alfa Laval’s global data standards. System Administration Act as the system administrator for the Customer Order Management system. Maintain system master data and troubleshoot operational issues. Ensure the system is up-to-date and aligned with business requirements. Order Booking Compliance Perform basic compliance checks during order booking. Verify completeness and correctness of mandatory information. Ensure all required approvals are in place as per the authorization matrix. Process Activation Initiate internal processes required for order execution. Collaborate with cross-functional teams to ensure timely and accurate order processing. Key Competencies: Strong attention to detail and data accuracy Knowledge of compliance and export control processes Proficiency in ERP and order management systems Analytical and problem-solving skills Effective communication and collaboration across teams Qualifications & Experience: Bachelor’s degree in Business Administration, Information Systems, or related field 3+ years of experience in data management or order processing roles Familiarity with data governance and compliance frameworks Experience with customer master data and ERP systems (e.g., SAP) \"We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\" Every day, we get opportunities to make a positive impact – on our colleagues, partners, customers and society. Together, we’re pioneering the solutions of the future and unlocking the full potential of precious resources. Trusted to act on initiative, we challenge conventional thinking to develop world-leading technologies that inspire progress in vital areas, including energy, food, water and shipping. As we push forward, the innovative, open spirit that fuels our 140-year-old start-up culture and rapid growth also drives our personal growth. So, as we shape a more resourceful, less wasteful world, we build our careers too. Meet our dedicated people who always go the extra mile Alfa Laval Career Site Or sign up for Job Alerts to stay in touch Sign up for Job Alerts</td><td>eyJqb2JfdGl0bGUiOiJNYXN0ZXIgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiQzMyNTExIEFsZmEgTGF2YWwgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImhJMnc5MHYxS1E3QXVQTkNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>6 days ago</td><td>Data Analyst</td></tr><tr><td>Engr II-Data Engineering</td><td>VERIZON</td><td>India</td><td>When you join Verizon\n",
       "\n",
       "You want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n",
       "\n",
       "What You’ll Be Doing...\n",
       "\n",
       "As a Data Engineer with ETL/ELT expertise for our growing data platform and analytics teams, you will understand and enable the required data sets from different sources. This includes both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n",
       "• Understanding the business requirements.\n",
       "• Transforming technical design.\n",
       "• Working on data ingestion, preparation and transformation.\n",
       "• Developing the scripts for data sourcing and parsing.\n",
       "• Developing data streaming applications.\n",
       "• Debugging the production failures and identifying the solution.\n",
       "• Working on ETL/ELT development.\n",
       "\n",
       "What We’re Looking For...\n",
       "\n",
       "You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems.\n",
       "\n",
       "You'll Need To Have\n",
       "• Bachelor’s degree or one or more years of relevant experience required, demonstrated through work experience and/or military experience.\n",
       "• Experience with Data Warehouse concepts and Data Management life cycle.\n",
       "\n",
       "Even better if you have one or more of the following:\n",
       "• Any related Certification on ETL/ELT developer.\n",
       "• Accuracy and attention to detail.\n",
       "• Good problem solving, analytical, and research capabilities.\n",
       "• Good verbal and written communication.\n",
       "• Experience presenting to and influencing partners.\n",
       "\n",
       "Why Verizon?\n",
       "\n",
       "Verizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.\n",
       "• We are a ‘pay for performance’ company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.\n",
       "• Your benefits are market competitive and delivered by some of the best providers.\n",
       "• You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.\n",
       "• We offer generous paid time off benefits to help you manage your work life balance and opportunities for flexible working arrangements*.\n",
       "• Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.\n",
       "• You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.\n",
       "\n",
       "Your benefits package will vary depending on the country in which you work.\n",
       "• subject to business approval\n",
       "\n",
       "If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.\n",
       "\n",
       "Where you’ll be working\n",
       "\n",
       "In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n",
       "\n",
       "Scheduled Weekly Hours\n",
       "\n",
       "40\n",
       "\n",
       "Equal Employment Opportunity\n",
       "\n",
       "Verizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.</td><td>eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkpaY3lnNzJpTUJGcHpfUTJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>1 day ago</td><td>Data Engineer</td></tr><tr><td>Associate Analyst - Data Engineer</td><td>PEPSICO</td><td>Hyderabad, Telangana, India</td><td>Overview\n",
       "\n",
       "PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT.  The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics, and new product development.  PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\n",
       "\n",
       "What PepsiCo Data Management and Operations does:\n",
       "\n",
       "Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\n",
       "\n",
       "Responsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\n",
       "\n",
       "Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\n",
       "\n",
       "Increase awareness about available data and democratize access to it across the company.\n",
       "\n",
       " \n",
       "\n",
       "               As a data engineer, you will be the key technical expert building PepsiCo's data products to drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help developing very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n",
       "\n",
       "Responsibilities\n",
       "• Act as a subject matter expert across different digital projects.\n",
       "• Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\n",
       "• Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\n",
       "• Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\n",
       "• Responsible for implementing best practices around systems integration, security, performance, and data management.\n",
       "• Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\n",
       "• Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\n",
       "• Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\n",
       "• Develop and optimize procedures to “productionalize” data science models.\n",
       "• Define and manage SLA’s for data products and processes running in production.\n",
       "• Support large-scale experimentation done by data scientists.\n",
       "• Prototype new approaches and build solutions at scale.\n",
       "• Research in state-of-the-art methodologies.\n",
       "• Create documentation for learnings and knowledge transfer.\n",
       "• Create and audit reusable packages or libraries.\n",
       "\n",
       "Qualifications\n",
       "• 4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n",
       "• 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n",
       "• 3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n",
       "• 2+ years in cloud data engineering experience in Azure.\n",
       "• Fluent with Azure cloud services. Azure Certification is a plus.\n",
       "• Experience in Azure Log Analytics\n",
       "• Experience with integration of multi cloud services with on-premises technologies.\n",
       "• Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\n",
       "• Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\n",
       "• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n",
       "• Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\n",
       "• Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\n",
       "• Experience with Statistical/ML techniques is a plus.\n",
       "• Experience with building solutions in the retail or in the supply chain space is a plus.\n",
       "• Experience with version control systems like Github and deployment & CI tools.\n",
       "• Working knowledge of agile development, including DevOps and DataOps concepts.\n",
       "• B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.\n",
       "\n",
       " Skills, Abilities, Knowledge:\n",
       "• Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\n",
       "• Strong change manager. Comfortable with change, especially that which arises through company growth.\n",
       "• Ability to understand and translate business requirements into data and technical requirements.\n",
       "• High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\n",
       "• Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\n",
       "• Strong organizational and interpersonal skills; comfortable managing trade-offs.</td><td>eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUgQW5hbHlzdCAtIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJQZXBzaUNvIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJkdUI4NkhSU1JwRWNPdlBCQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>1 month ago</td><td>Data Engineer</td></tr><tr><td>Senior Big Data Engineer</td><td>QUALCOMM</td><td>Hyderabad, Telangana, India</td><td>Company:\n",
       "Qualcomm India Private Limited\n",
       "\n",
       "Job Area:\n",
       "Engineering Group, Engineering Group > Software Engineering\n",
       "\n",
       "General Summary:\n",
       "\n",
       "As a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n",
       "\n",
       "Minimum Qualifications:\n",
       "• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\n",
       "OR\n",
       "Master's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\n",
       "OR\n",
       "PhD in Engineering, Information Systems, Computer Science, or related field.\n",
       "\n",
       "• 2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n",
       "\n",
       "General Summary:\n",
       "\n",
       "Preferred Qualifications\n",
       "• 3+ years of experience as a Data Engineer or in a similar role\n",
       "• Experience with data modeling, data warehousing, and building ETL pipelines\n",
       "• Solid working experience with Python, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n",
       "• Experience with Big Data tools, platforms and architecture with solid working experience with SQL\n",
       "• Experience working in a very large data warehousing environment, Distributed System.\n",
       "• Solid understanding on various data exchange formats and complexities\n",
       "• Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n",
       "• Strong data visualization skills\n",
       "• Basic understanding of Machine Learning; Prior experience in ML Engineering a plus\n",
       "• Ability to manage on-premises data and make it inter-operate with AWS based pipelines\n",
       "• Ability to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n",
       "\n",
       "Education\n",
       "• Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n",
       "• Preferred Qualifications: Masters in CS/ECE with a Data Science / ML Specialization\n",
       "\n",
       "Minimum Qualifications:\n",
       "• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n",
       "\n",
       "OR\n",
       "\n",
       "Master's degree in Engineering, Information Systems, Computer Science, or related field\n",
       "\n",
       "OR\n",
       "\n",
       "PhD in Engineering, Information Systems, Computer Science, or related field.\n",
       "• 3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n",
       "\n",
       "Develops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n",
       "\n",
       "Principal Duties and Responsibilities:\n",
       "• Completes assigned coding tasks to specifications on time without significant errors or bugs.\n",
       "• Adapts to changes and setbacks in order to manage pressure and meet deadlines.\n",
       "• Collaborates with others inside project team to accomplish project objectives.\n",
       "• Communicates with project lead to provide status and information about impending obstacles.\n",
       "• Quickly resolves complex software issues and bugs.\n",
       "• Gathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n",
       "• Seeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n",
       "• Participates in technical conversations with tech leads/managers.\n",
       "• Anticipates and communicates issues with project team to maintain open communication.\n",
       "• Makes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n",
       "• Prioritizes project deadlines and deliverables with minimal supervision.\n",
       "• Resolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n",
       "• Writes readable code for large features or significant bug fixes to support collaboration with other engineers.\n",
       "• Determines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n",
       "• Unit tests own code to verify the stability and functionality of a feature.\n",
       "\n",
       "Applicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n",
       "\n",
       "Qualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n",
       "\n",
       "To all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n",
       "\n",
       "If you would like more information about this role, please contact Qualcomm Careers.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQmlnIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJRdWFsY29tbSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieVRVTHdrQjB2Ujk5bmJQUUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 days ago</td><td>Data Engineer</td></tr><tr><td>Manager Data Engineer - AWS Databricks</td><td>BLEND360</td><td>Hyderabad, Telangana, India</td><td>Company Description\n",
       "\n",
       "Blend is a premier AI services provider, committed to co-creating meaningful impact for its clients through the power of data science, AI, technology, and people. With a mission to fuel bold visions, Blend tackles significant challenges by seamlessly aligning human expertise with artificial intelligence. The company is dedicated to unlocking value and fostering innovation for its clients by harnessing world-class people and data-driven strategy. We believe that the power of people and AI can have a meaningful impact on your world, creating more fulfilling work and projects for our people and clients. For more information, visit www.blend360.com\n",
       "\n",
       "Job Description\n",
       "\n",
       "We are seeking a seasoned Data Engineering Manager with 8+ years of experience to lead and grow our data engineering capabilities. This role demands strong hands-on expertise in Python, SQL, Spark, and advanced proficiency in AWS and Databricks. As a technical leader, you will be responsible for architecting and optimizing scalable data solutions that enable analytics, data science, and business intelligence across the organization.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Lead the design, development, and optimization of scalable and secure data pipelines using AWS services such as Glue, S3, Lambda, EMR, and Databricks Notebooks, Jobs, and Workflows.\n",
       "• Oversee the development and maintenance of data lakes on AWS Databricks, ensuring performance and scalability.\n",
       "• Build and manage robust ETL/ELT workflows using Python and SQL, handling both structured and semi-structured data.\n",
       "• Implement distributed data processing solutions using Apache Spark/PySpark for large-scale data transformation.\n",
       "• Collaborate with cross-functional teams including data scientists, analysts, and product managers to ensure data is accurate, accessible, and well-structured.\n",
       "• Enforce best practices for data quality, governance, security, and compliance across the entire data ecosystem.\n",
       "• Monitor system performance, troubleshoot issues, and drive continuous improvements in data infrastructure.\n",
       "• Conduct code reviews, define coding standards, and promote engineering excellence across the team.\n",
       "• Mentor and guide junior data engineers, fostering a culture of technical growth and innovation.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Requirements\n",
       "• 8+ years of experience in data engineering with proven leadership in managing data projects and teams.\n",
       "• Expertise in Python, SQL, Spark (PySpark), and experience with AWS and Databricks in production environments.\n",
       "• Strong understanding of modern data architecture, distributed systems, and cloud-native solutions.\n",
       "• Excellent problem-solving, communication, and collaboration skills.\n",
       "• Prior experience mentoring team members and contributing to strategic technical decisions is highly desirable.</td><td>eyJqb2JfdGl0bGUiOiJNYW5hZ2VyIERhdGEgRW5naW5lZXIgLSBBV1MgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJsZW5kMzYwIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJxelhlVGQtWWc2ekE5U3R4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>3 days ago</td><td>Data Engineer</td></tr><tr><td>Data Engineer INTL India - EOR 6fb570f8</td><td>INSIGHT GLOBAL</td><td>Hyderabad, Telangana, India</td><td>- In this role, you will be building data pipeline solutions by designing, adopting, and applying big data strategies and architectures. You must have knowledge of scalable system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration, data transformation, data modeling, and data cleansing.\n",
       "- The Sr. Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for business stakeholders.\n",
       "- Design and develop a logical data model for a large multi-tenant application with intent to facilitate consistency and efficiency.\n",
       "- Collaborate with analysts and internal clients to understand the goals of the overall system data architecture, and further develop on business needs/issues.\n",
       "- Test/troubleshoot problems and conduct root cause analysis.\n",
       "- Own communications with technical contacts at software vendors to escalate software issues, recommend enhancements, and find sustainable workarounds to issues.\n",
       "- Work in tandem with analysts and other stakeholders to develop and execute necessary processes and controls around the flow of data to meet data governance standards.\n",
       "- Verify accuracy of table changes and data transformation processes\n",
       "- Deliver fully tested code prior to prod-deployment when appropriate.\n",
       "- Recommend and implement enhancements that address system performance requirements, streamline processes and improve data integrity.\n",
       "- Create sound technical documentation and train peer developers on this documentation as development completes.\n",
       "- Additional duties as assigned to ensure company success.\n",
       "The compensation for this role varies depending on depth and length of experience, and can range from 27-36 LAKH.\n",
       "\n",
       "We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n",
       "\n",
       "To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElOVEwgSW5kaWEgLSBFT1IgNmZiNTcwZjgiLCJjb21wYW55X25hbWUiOiJJbnNpZ2h0IEdsb2JhbCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiZUtoaVo5cWozWC1wLXlBVEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 days ago</td><td>Data Engineer</td></tr><tr><td>Data Engineer-Senior II</td><td>FEDERAL EXPRESS CORPORATION AMEA</td><td>Hyderabad, Telangana, India</td><td>Responsible for supporting the development and maintenance of business intelligence and analytics systems, implementing data warehouse architecture, enabling self-service data exploration, designing data models, implementing data security strategies, optimizing production processes, providing user training and support, and overseeing project management activities as a Data Engineer to facilitate data-driven decision-making and generate insights for business stakeholders.\n",
       "\n",
       "1. Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making.\n",
       "2. Implement of business intelligence and analytics systems, ensuring alignment with business requirements.\n",
       "3. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets.\n",
       "4. Enable self-service data exploration capabilities for users to analyze and visualize data independently.\n",
       "5. Develop reporting and analysis applications to generate insights from data for business stakeholders.\n",
       "6. Design and implement data models to organize and structure data for analytical purposes.\n",
       "7. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information.\n",
       "8. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability.\n",
       "9. Assist in training and support to users on business intelligence tools and applications.\n",
       "10. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions.\n",
       "\n",
       "Education: Bachelors’ degree or equivalent in Computer Science, MIS, or similar discipline.\n",
       "Accreditation: Specific business accreditation for Business Intelligence.\n",
       "\n",
       "Experience: Relevant work experience in data engineering based on the following number of years:\n",
       "Associate: Prior experience not required\n",
       "Standard I: Two (2) years\n",
       "Standard II: Three (3) years\n",
       "Senior I: Four (4) years\n",
       "Senior II: Five (5) years\n",
       "\n",
       "Knowledge, Skills and Abilities\n",
       "• Fluency in English\n",
       "• Analytical Skills\n",
       "• Accuracy & Attention to Detail\n",
       "• Numerical Skills\n",
       "• Planning & Organizing Skills\n",
       "• Presentation Skills\n",
       "\n",
       "Preferred Qualifications:\n",
       "\n",
       "Pay Transparency:\n",
       "\n",
       "Pay:\n",
       "\n",
       "Additional Details:\n",
       "\n",
       "FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n",
       "\n",
       "All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\n",
       "Our Company\n",
       "\n",
       "FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World’s Most Admired Companies by \"Fortune\" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\n",
       "Our Philosophy\n",
       "\n",
       "The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\n",
       "Our Culture\n",
       "\n",
       "Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970’s. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today’s global marketplace.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVNlbmlvciBJSSIsImNvbXBhbnlfbmFtZSI6IkZlZGVyYWwgRXhwcmVzcyBDb3Jwb3JhdGlvbiBBTUVBIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJhZDU5RmU0Vl94QzdTVGFqQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>1 day ago</td><td>Data Engineer</td></tr><tr><td>Lead Data Engineer(Snowflake,PowerBi)</td><td>THOMSON REUTERS</td><td>Hyderabad, Telangana, India (+1 other)</td><td>Want to be part of the Data & Analytics organization, whose strategic goal is to create a world-class Data & Analytics company by building, embedding, and maturing a data-driven culture across Thomson Reuters.\n",
       "\n",
       "About The Role\n",
       "We are looking for a highly motivated individual with strong organizational and technical skills for the position of Lead Data Engineer/ Data Engineering Manager (Snowflake). You will play a critical role working on cutting edge of Data Engineering and analytics, leveraging predictive models, machine learning and generative AI to drive business insights and facilitating informed decision-making and help Thomson Reuters rapidly scale data-driven initiatives.\n",
       "\n",
       "Effectively communicate across various levels, including Executives, and functions within the global organization.\n",
       "Demonstrate strong leadership skills with ability to drive projects/tasks to delivering value\n",
       "Engage with stakeholders, business analysts and project team to understand the data requirements.\n",
       "Design analytical frameworks to provide insights into a business problem.\n",
       "Explore and visualize multiple data sets to understand data available and prepare data for problem solving.\n",
       "Design database models (if a data mart or operational data store is required to aggregate data for modeling).\n",
       "\n",
       "About You\n",
       "You're a fit for the Lead Data Engineer/ Data Engineering Manager (Snowflake), if your background includes:\n",
       "Qualifications: B-Tech/M-Tech/MCA or equivalent\n",
       "Experience: 7-9 years of corporate experience\n",
       "Location: Bangalore, India\n",
       "Hands-on experience in developing data models for large scale data warehouse/data Lake – Snowflake, BW\n",
       "Map the data journey from operational system sources through any transformations in transit to its delivery into enterprise repositories (Warehouse, Data Lake, Master Data, etc.)\n",
       "Enabling on the overall master and reference data strategy, including the procedures to ensure the consistency and quality of Finance reference data.\n",
       "Experience across ETL, SQL and other emerging data technologies with experience in integrations of a cloud-based analytics environment\n",
       "Build and refine end-to-end data workflows to offer actionable insights\n",
       "Fair understanding of Data Strategy, Data Governance Process\n",
       "Knowledge in BI analytics and visualization tools: Power BI, Tableau\n",
       "\n",
       "#LI-NR1\n",
       "\n",
       "What’s in it For You?\n",
       "• Hybrid Work Model: We’ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\n",
       "• Flexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\n",
       "• Career Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow’s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\n",
       "• Industry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\n",
       "• Culture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\n",
       "• Social Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\n",
       "• Making a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\n",
       "\n",
       "About Us\n",
       "\n",
       "Thomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\n",
       "\n",
       "We are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\n",
       "\n",
       "As a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\n",
       "\n",
       "We also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here.\n",
       "\n",
       "Learn more on how to protect yourself from fraudulent job postings here.\n",
       "\n",
       "More information about Thomson Reuters can be found on thomsonreuters.com.</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIoU25vd2ZsYWtlLFBvd2VyQmkpIiwiY29tcGFueV9uYW1lIjoiVGhvbXNvbiBSZXV0ZXJzIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJzSFl5TXBoOXlWbDl4R0pBQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>13 days ago</td><td>Data Engineer</td></tr><tr><td>Python Developer – Telegram Bot Integration & Excel Automation</td><td>SANGA & ASSOCIATES - EQUIDOTE</td><td>Anywhere</td><td>Job Title:\n",
       "\n",
       "Python Developer – Telegram Bot Integration & Excel Automation\n",
       "\n",
       "Job Description:\n",
       "\n",
       "We are seeking a skilled and detail-oriented Python Developer to help automate the process of sending structured trade updates from an Excel file to a Telegram channel using a Telegram bot.\n",
       "\n",
       "This is a freelance / part-time project with the potential for ongoing work based on performance.\n",
       "\n",
       "Responsibilities:\n",
       "• Read data from an Excel file that is regularly updated using Python.\n",
       "• Format and send messages to a Telegram channel or group using the Telegram Bot API.\n",
       "• Implement conditions to filter or trigger messages based on specific columns (e.g., P&L thresholds).\n",
       "• Ensure the messages are well-formatted and synchronized.\n",
       "• (Optional) Schedule the script to run at regular intervals (e.g., every 5 mins or on update).\n",
       "• Ensure error handling, avoid duplicate messages, and maintain clean logs.\n",
       "\n",
       "Required Skills:\n",
       "• Strong experience with Python scripting\n",
       "• Proficiency in using pandas for Excel/CSV handling\n",
       "• Working knowledge of the Telegram Bot API\n",
       "• Experience with HTTP requests (requests library)\n",
       "• Ability to format dynamic messages (Markdown/HTML for Telegram)\n",
       "• (Optional but a plus) Familiarity with scheduling tools like schedule, cron jobs, or apscheduler\n",
       "\n",
       "Nice to Have:\n",
       "• Understanding of stock market data or options trading (for better context)\n",
       "• Experience integrating with trading APIs or using TradingView alerts\n",
       "• Basic knowledge of Excel automation or VBA\n",
       "\n",
       "Project Details:\n",
       "• Project Type: One-time setup, with possible ongoing maintenance\n",
       "• Location: Remote (India preferred)\n",
       "• Start Date: Immediate\n",
       "\n",
       "How to Apply:\n",
       "\n",
       "Please apply with:\n",
       "• A short summary of your experience with Python + Telegram Bots\n",
       "• A link to any relevant projects or GitHub repos\n",
       "• Your expected rate and estimated time to complete the task\n",
       "\n",
       "Job Type: Freelance\n",
       "\n",
       "Benefits:\n",
       "• Health insurance\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "\n",
       "Supplemental Pay:\n",
       "• Performance bonus\n",
       "• Yearly bonus\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBUZWxlZ3JhbSBCb3QgSW50ZWdyYXRpb24gXHUwMDI2IEV4Y2VsIEF1dG9tYXRpb24iLCJjb21wYW55X25hbWUiOiJTQU5HQSBcdTAwMjYgQVNTT0NJQVRFUyAtIEVRVUlET1RFIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNBNXdGSEplRUV3c2lQVWJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>1 day ago</td><td>Python Developer</td></tr><tr><td>Python Developer - Remote</td><td>XPRESS HEALTH</td><td>Anywhere</td><td>Job Title: Python Developer\n",
       "Location: Remote\n",
       "Salary: Up to ₹12 LPA (based on experience and skillset)\n",
       "Experience: 3–6 years (preferred)\n",
       "Employment Type: Full-time\n",
       "\n",
       "About Xpress Health\n",
       "\n",
       "Xpress Health is a healthtech platform transforming clinical workforce management by connecting healthcare professionals to facilities in real time. With operations across Ireland and rapid expansion underway, we’re solving critical staffing challenges in the healthcare sector using advanced technology and automation.\n",
       "\n",
       "Role Overview\n",
       "\n",
       "We are looking for a Python Developer with solid experience in backend development and building scalable, secure systems. You will work closely with cross-functional teams to build backend services, automate operations, and power data-driven features in our real-time healthcare staffing platform. If you're passionate about solving real-world problems through code, this role is for you.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design, develop, and maintain backend services and APIs using Python (preferably with frameworks like Django or Flask).\n",
       "• Build scalable systems for real-time scheduling, user management, and analytics.\n",
       "• Integrate third-party APIs and internal services securely and efficiently.\n",
       "• Work with databases (SQL and NoSQL) to design efficient schemas and queries.\n",
       "• Optimize performance and ensure system reliability under scale.\n",
       "• Collaborate with frontend, product, and QA teams to deliver complete features.\n",
       "• Write clean, maintainable, and well-documented code.\n",
       "• Participate in code reviews, system design discussions, and architecture planning.\n",
       "\n",
       "Requirements\n",
       "• 3–6 years of professional experience with Python backend development.\n",
       "• Strong knowledge of Django, Flask, or other web frameworks.\n",
       "• Proficient in working with relational databases (MySQL/PostgreSQL) and REST APIs.\n",
       "• Experience with Git, CI/CD pipelines, and working in an Agile environment.\n",
       "• Strong debugging, testing, and problem-solving skills.\n",
       "• Good communication and ability to collaborate with remote teams.\n",
       "\n",
       "Preferred Qualifications\n",
       "• Experience in healthcare, staffing, or enterprise SaaS platforms.\n",
       "• Familiarity with containerization tools like Docker and orchestration platforms (Kubernetes).\n",
       "• Exposure to cloud platforms like AWS, GCP, or Azure.\n",
       "• Knowledge of async programming and task queues (e.g., Celery, Redis).\n",
       "• Experience working with frontend teams using React/Vue (a plus).\n",
       "\n",
       "What We Offer\n",
       "• Competitive salary up to ₹12 LPA, depending on experience.\n",
       "• A mission-driven environment working on meaningful, real-world problems.\n",
       "• Opportunity to shape a rapidly scaling healthtech product.\n",
       "• Flexible work culture with remote options and learning opportunities.\n",
       "• Collaborative, cross-functional team with international exposure.\n",
       "\n",
       "Be part of a product-first team building real-time tech that supports critical healthcare systems. Join Xpress Health as a Python Developer !!!\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: Up to ₹1,200,000.00 per year\n",
       "\n",
       "Benefits:\n",
       "• Paid time off\n",
       "• Work from home\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Evening shift\n",
       "• Fixed shift\n",
       "• Monday to Friday\n",
       "• UK shift\n",
       "\n",
       "Application Question(s):\n",
       "• What is your current and expected CTC?\n",
       "• Are you currently working? If yes, what is your notice period?\n",
       "\n",
       "Experience:\n",
       "• Python : 5 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIC0gUmVtb3RlIiwiY29tcGFueV9uYW1lIjoiWHByZXNzIEhlYWx0aCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJGWjc0LW4wM2NjU2xGWVVOQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>2 days ago</td><td>Python Developer</td></tr><tr><td>Full Stack Developer (Python / React JS)</td><td>HITACHI CAREERS</td><td>India</td><td>Our Company\n",
       "\n",
       "We're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n",
       "\n",
       "Our group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n",
       "\n",
       "Imagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n",
       "\n",
       "The team\n",
       "\n",
       "We are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "\n",
       "The role: Full Stack Developer/Specialist\n",
       "\n",
       "Responsibilities:\n",
       "• Design, develop, and maintain applications.\n",
       "• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n",
       "• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n",
       "• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n",
       "• Optimise applications for performance, scalability, and user experience.\n",
       "• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n",
       "• Provide training and support to end-users and IT staff on functionalities and best practices.\n",
       "• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n",
       "• Participate in project planning, execution, and post-implementation support.\n",
       "• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n",
       "• What you'll bring\n",
       "Qualifications:\n",
       "• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n",
       "• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n",
       "• Expertise in development and customisation.\n",
       "• Proficiency in Python and React JS\n",
       "• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n",
       "• Experience with RESTful APIs and web services.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Strong communication and collaboration skills.\n",
       "• Ability to work independently and as part of a team in a fast-paced environment.\n",
       "• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n",
       "• Prior experience in building AI applications is a plus.\n",
       "• Prior experience with automation tools like UIPath is a plus.\n",
       "• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\n",
       "Preferred Skills\n",
       "• Certification in\n",
       "• Experience with other low-code/no-code platforms.\n",
       "• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n",
       "• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n",
       "• Experience with Agile development methodologies.\n",
       "About us\n",
       "\n",
       "We're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n",
       "\n",
       "#LI-MS3\n",
       "\n",
       "Championing diversity, equity, and inclusion\n",
       "\n",
       "Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n",
       "\n",
       "How we look after you\n",
       "\n",
       "We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n",
       "\n",
       "We're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.</td><td>eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>Python Developer</td></tr><tr><td>Python Developer Role</td><td>PITANGENT ANALYTICS AND TECHNOLOGY SOLUTIONS PVT. LTD.</td><td>India</td><td>Overview\n",
       "\n",
       "Pi tangent Group of Companies is an ISO:9001: 2015 Certified, CMMIL-3, Award winning Software Development Company in Eastern India. It caters to areas like AI/ML to Web development to SAAS engineering. The Group is actively seeking out talented, enthusiastic Python Developers as a value addition to the growing team at Kolkata.\n",
       "\n",
       "Key Responsibilities\n",
       "• Design and develop robust backend applications using Python.\n",
       "• Collaborate with front-end developers to integrate user-facing elements with server-side logic.\n",
       "• Implement RESTful APIs for seamless communication between server and client.\n",
       "• Write reusable, testable, and efficient code following best practices.\n",
       "• Manage and optimize multiple databases and data storage solutions.\n",
       "• Perform unit and integration testing to ensure software reliability.\n",
       "• Participate in code reviews and maintain version control in Git.\n",
       "• Gather and analyze user requirements to provide optimal solutions\n",
       "• Contribute to project documentation and specifications.\n",
       "• Collaborate with QA engineers to troubleshoot and resolve issues.\n",
       "• Maintain quality assurance processes to ensure best practices are enforced.\n",
       "• Engage in agile development practices, participating in sprints and meetings.\n",
       "• Mentor junior developers and provide guidance as needed.\n",
       "\n",
       "Required Qualifications\n",
       "• Bachelor's degree in computer science or related field.\n",
       "• 1-2 yrs of experience in Python development.\n",
       "• Strong understanding of Django or Flask web frameworks.\n",
       "• Proficient with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB).\n",
       "• Experience with version control systems, preferably Git.\n",
       "• Solid understanding of RESTful API design principles.\n",
       "• Familiarity with front-end technologies (HTML, CSS, JavaScript).\n",
       "• Experience with containerization tools such as Docker.\n",
       "• Strong communication and teamwork abilities.\n",
       "• Familiarity with cloud services (AWS, Azure) is a plus.\n",
       "• Understanding of security principles and best practices.\n",
       "• Experience with Agile/Scrum methodologies.\n",
       "• Proven ability to manage multiple tasks and meet deadlines.\n",
       "\n",
       "Skills: agile,communication,nosql,scrum,aws lambda,docker,sql,azure,html,javascript,mongodb,css,django,aws,git,postgresql,restful apis,python,flask</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIFJvbGUiLCJjb21wYW55X25hbWUiOiJQaXRhbmdlbnQgQW5hbHl0aWNzIGFuZCBUZWNobm9sb2d5IFNvbHV0aW9ucyBQdnQuIEx0ZC4iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiLUNHQ2RNdUVNQ2V4X3F2MkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>2 days ago</td><td>Python Developer</td></tr><tr><td>Python and Groovy Framework Developer</td><td>APTITA</td><td>India</td><td>Urgent Hiring!!!\n",
       "\n",
       "Role : Python and Groovy Framework Developer\n",
       "\n",
       "Mandatory Skills: Python, Appium, Groovy, Git\n",
       "\n",
       "Experience: 3 to 8 Years\n",
       "\n",
       "Location: Bengaluru\n",
       "\n",
       "Contract - 1Year\n",
       "\n",
       "Job Description:\n",
       "\n",
       "Qualifications\n",
       "\n",
       " Bachelor’s or master’s degree in Computer Science, Software Engineering, or a\n",
       "\n",
       "related field\n",
       "\n",
       " 6 to 8 years of relevant experience in quality assurance, with a proven track record in\n",
       "\n",
       "WebKit or browser engine testing, including team leadership responsibilities.\n",
       "\n",
       " Proficiency in CI/CD and test automation frameworks such as Appium, and scripting\n",
       "\n",
       "languages like Python, or Shell.\n",
       "\n",
       "Job Overview\n",
       "\n",
       "We are seeking a dynamic and experienced Lead Python & Groovy Framework developer\n",
       "\n",
       "to join our team You will be part of a fast-paced, Agile development team and work on a\n",
       "\n",
       "variety of projects, from building new tools and solutions to improving existing ones.\n",
       "\n",
       "In this role, you will have the chance to grow your skills and take your career to the next\n",
       "\n",
       "level. We offer a supportive, challenging, and exciting work environment, with\n",
       "\n",
       "opportunities for professional development, training, and advancement.\n",
       "\n",
       "If you are a Python & Groovy Framework Developer Engineer with a passion for\n",
       "\n",
       "technology and a drive to continuously improve processes, we want to hear from you!\n",
       "\n",
       "If you are passionate about browser engine technologies, performance optimization, and\n",
       "\n",
       "leadership, we encourage you to apply!\n",
       "\n",
       "Primary Skills:\n",
       "\n",
       " Strong experience in Python Framework development, with the ability to automate\n",
       "\n",
       "and optimize processes using Jenkins Pipeline script\n",
       "\n",
       " Good knowledge in Groovy scripting\n",
       "\n",
       " Expertise in CI/CD tools such as Jenkins, CircleCI, or GitLab\n",
       "\n",
       " Good understanding of Appium.\n",
       "\n",
       "Strong Problem solving and debugging skills.\n",
       "\n",
       " Excellent communication and collaboration skills, both with technical and non-\n",
       "\n",
       "technical stakeholders\n",
       "\n",
       " Version Control: Familiarity with version control systems such as Git for reviewing\n",
       "\n",
       "changes and ensuring test coverage.\n",
       "\n",
       " Communication: Strong communication and collaboration skills for working with\n",
       "\n",
       "cross-functional teams.\n",
       "\n",
       " Agile Methodologies: Experience with Agile Scrum methodologies\n",
       "\n",
       "Notice Period: Immediate- 30 Days\n",
       "\n",
       "Email to : sharmila.m@aptita.com\n",
       "\n",
       "·</td><td>eyJqb2JfdGl0bGUiOiJQeXRob24gYW5kIEdyb292eSBGcmFtZXdvcmsgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQXB0aXRhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Imx1ckt5NFVRSi0wb3JkMi1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>2 days ago</td><td>Python Developer</td></tr><tr><td>AI Python Developer</td><td>ALLIANZ INSURANCE</td><td>India</td><td>We are seeking a skilled AI Developer proficient in Python to support various AI use cases within our dynamic team. The ideal candidate will possess a strong understanding of Python programming and a basic familiarity with PyTest. This role will also provide opportunities to work with cloud computing technologies, particularly Azure, and develop APIs.\n",
       "\n",
       "Key Responsibilities:\n",
       "• Utilize advanced Python skills to support and enhance AI use cases, ensuring the efficient and effective implementation of AI solutions.\n",
       "• Develop and execute comprehensive test scripts using PyTest to ensure the reliability and accuracy of AI models and applications.\n",
       "• Collaborate with cross-functional teams, including data scientists, engineers, and product managers, to integrate AI capabilities into existing systems and workflows.\n",
       "• Analyze and optimize AI algorithms for performance and scalability, ensuring they meet business requirements.\n",
       "• Stay updated with the latest advancements in AI and machine learning technologies and apply them to improve existing solutions.\n",
       "\n",
       "Requirements:\n",
       "\n",
       "Must-Have\n",
       "• Python: Advanced proficiency with extensive experience in writing efficient, maintainable, and scalable code. Demonstrated ability to solve complex problems using Python.\n",
       "• PyTest: Basic experience in writing and executing unit tests, with a fundamental understanding of test-driven development practices.\n",
       "\n",
       "Good-to-Have\n",
       "• Cloud Computing: Familiarity with cloud platforms and services, with hands-on experience in deploying and managing AI applications in a cloud environment.\n",
       "• Azure: Experience with Microsoft Azure, particularly in using its AI and machine learning services.\n",
       "• APIs: Knowledge of designing, developing, and consuming APIs, particularly RESTful APIs, for integrating AI solutions with other systems and applications\n",
       "\n",
       "About Allianz Technology\n",
       "\n",
       "Allianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 13,000 employees located in 22 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry. We oversee the full digitalization spectrum – from one of the industry’s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n",
       "\n",
       "D&I statement\n",
       "\n",
       "Allianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, race or ethnicity, age, nationality, religion, disability, or philosophy of life. Join us. Let´s care for tomorrow.</td><td>eyJqb2JfdGl0bGUiOiJBSSBQeXRob24gRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQWxsaWFueiBJbnN1cmFuY2UiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQkg5Z0tqUFM2TjBGeExOb0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>24 days ago</td><td>Python Developer</td></tr><tr><td>Developer- Angular, Python & Azure</td><td>THE VALUE MAXIMIZER</td><td>India</td><td>About the Role :\n",
       "\n",
       "As a Senior Developer, you will play a critical role in designing, implementing, and optimizing IoT applications, Salesforce development, and web solutions. This role is ideal for experienced developers eager to lead projects, mentor junior team members, and shape technical strategies across diverse platforms and technologies. You will be an important part of our international project teams, will work closely with team members from the headquarters and if qualified can lead international software development projects.\n",
       "\n",
       "Key Responsibilities :\n",
       "• Central role and expert for development of IoT frontend applications with Angular, Capacitor, and PrimeNG\n",
       "• Design and implement IoT backend systems using C#, .Net 8.0, and the Azure platform\n",
       "• Management and optimization of CI/CD pipelines using Azure DevOps and Bicep\n",
       "• Drive advanced Salesforce development efforts, including customizations, integrations, and automation\n",
       "• Manage and enhance the SharePoint Online intranet platform\n",
       "• Architect and implement Power Platform solutions tailored to business needs\n",
       "• Develop and maintain complex web applications using Django (Python) and PHP\n",
       "• Supervise website development and optimization with Typo3, PHP, and SolR, collaborating closely with the marketing department\n",
       "• Monitor and ensure the performance, compliance, and SEO optimization of global websites and portals\n",
       "\n",
       "Key Requirements :\n",
       "• 6-8 years of experience\n",
       "• Strong expertise in Angular, Python, and C#\n",
       "• Advanced experience with CI/CD pipelines and version control systems (e.g., Azure DevOps or Git)\n",
       "• Proficiency in web technologies such as PHP, MySQL, and modern frameworks\n",
       "• In-depth knowledge of Salesforce, Azure cloud solutions, and SharePoint Online\n",
       "• Proven ability to lead technical projects, mentor team members, and collaborate across departments\n",
       "• Excellent problem-solving skills, attention to detail, and a proactive approach to innovation\n",
       "• Proficiency in German is an advantage but not a requirement. Candidates must, however, have a minimum of B2-level English proficiency to communicate effectively within the international teams and our headquarters</td><td>eyJqb2JfdGl0bGUiOiJEZXZlbG9wZXItIEFuZ3VsYXIsIFB5dGhvbiBcdTAwMjYgQXp1cmUiLCJjb21wYW55X25hbWUiOiJUaGUgVmFsdWUgTWF4aW1pemVyIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRteXhMclAxTG5oaWFJZEVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>Python Developer</td></tr><tr><td>Freelance Python Developer</td><td>GBIM TECHNOLOGIES PVT.LTD.</td><td>Anywhere</td><td>We’re Hiring – Freelance Python Developer (Experienced)\n",
       "We are currently looking for a freelance Python developer who brings hands-on experience and can help us build and debug efficient backend systems, automate processes, and solve critical issues with speed and precision.\n",
       "Key Expertise Required:\n",
       "\n",
       "Python (Backend Development)\n",
       "\n",
       "Web Scraping & Data Extraction\n",
       "\n",
       "Web Automation\n",
       "\n",
       "Flask | Pandas | ETL\n",
       "\n",
       "AWS (Basic to Intermediate)\n",
       "\n",
       "Google / Meta / LinkedIn / Third-Party API Integration\n",
       "\n",
       "Problem-solving mindset – quick in identifying & fixing bugs/errors\n",
       "\n",
       "If you are a solution-driven individual with a solid track record in handling Python-based backend projects and automation tasks, we’d love to connect with you!\n",
       "Please DM or share your portfolio/work samples at [hr@gbim.com / sarabjeet.mann@gbim.in ].\n",
       "#PythonDeveloper #FreelanceOpportunity #WebScraping #Automation #BackendDevelopment #Flask #AWS #APIIntegration #DataExtraction #HiringNow #LinkedInJobs\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500.00 - ₹10,000.00 per hour\n",
       "\n",
       "Location Type:\n",
       "• Remote\n",
       "\n",
       "Schedule:\n",
       "• Day shift\n",
       "• Monday to Friday\n",
       "\n",
       "Work Location: Remote\n",
       "\n",
       "Speak with the employer\n",
       "+91-XXXXXXXXXX</td><td>eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkdCSU0gVGVjaG5vbG9naWVzIFB2dC5MdGQuIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNtMXFFOGh4Y2xUN3Z1ck9BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>3 days ago</td><td>Python Developer</td></tr><tr><td>DET-Senior GIG Python Developer-GDSNF02</td><td>EY</td><td>India</td><td>At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. Senior Python Developer – Client – Merck, H and M, TIAA, MINT Job Description Bachelor’s or master’s degree with 3+ years of strong Python development experience Design, develop, and maintain high-performance web applications using Python and related frameworks. Strong understanding of Python OOPs, Data type, Data Structure and algorithm , Exception handling, Decarotor , Generator, Iterator, Automation. Strong understanding of Python Libraries (Pandas, TensorFlow, Numpy, SciPy) Experience in Cloud Azure / AWS Develop, optimize, and manage complex APIs (RESTful or GraphQL). Collaborate with cross-functional teams to define, design, and ship new features. Troubleshoot and resolve advanced technical issues in development and production environments. Conduct technical evaluations of new tools and frameworks, recommending their adoption when appropriate. Stay ahead of emerging trends in Python development, ensuring the team remains at the forefront of innovation. Advanced proficiency in Python and frameworks like Django,Flask, or FastAPI. Good understanding of Database Postgres / MySQL & ORM Library i.e. SQL Alchemy/ any ORM libraries Understanding of Code Repository tools i.e. GIT , SVN Strong understanding of DevOps principles(Docker, Kubernetes and microservices) EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.</td><td>eyJqb2JfdGl0bGUiOiJERVQtU2VuaW9yIEdJRyBQeXRob24gRGV2ZWxvcGVyLUdEU05GMDIiLCJjb21wYW55X25hbWUiOiJFWSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ4Mk5pUFZHVVJ3cjczaVE1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>18 hours ago</td><td>Python Developer</td></tr><tr><td>Informatica ETL Developer: Agile Dev Team Member IV</td><td>CAPGEMINI</td><td>Hyderabad, Telangana, India</td><td>The ideal candidate will be responsible for designing, developing, and deploying scalable ETL processes using Informatica PowerCenter to support our data warehousing and analytics initiatives. You will collaborate with business and technical stakeholders to ensure high data quality, availability, and performance. Key Responsibilities:Design, develop, and maintain ETL workflows and mappings using Informatica PowerCenter or Informatica Intelligent Cloud Services (IICS).Extract, transform, and load data from various source systems (e.g., SQL Server, Oracle, flat files, cloud APIs) into data warehouses or operational data stores.Optimize ETL performance, conduct tuning, and ensure error handling and logging.Collaborate with data architects and analysts to understand data requirements and deliver high-quality data solutions.Work with QA teams to support data validation and testing efforts.Support data integration, migration, and transformation initiatives.Document ETL processes, data flows, and job schedules.Monitor daily ETL jobs and resolve production issues in a timely manner.RequirementsBachelor’s degree in Computer Science, Information Systems, or a related field (or equivalent work experience).3+ years of experience with Informatica PowerCenter or Informatica IICS.Strong SQL skills and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).Solid understanding of data warehousing concepts and dimensional modeling.Experience in performance tuning and troubleshooting ETL processes.Hands-on experience with job scheduling tools (e.g., Autosys, Control-M, Tidal).Familiarity with version control systems and DevOps practices.Preferred Qualifications:Experience with cloud data platforms (e.g., Snowflake, AWS Redshift, Azure Synapse).Exposure to data governance and data quality tools.Knowledge of scripting languages (e.g., Shell, Python).Experience working in Agile/Scrum environments.Familiarity with BI tools (e.g., Tableau, Power BI) is a plus. BenefitsThis position comes with competitive compensation and benefits package: Competitive salary and performance-based bonuses Comprehensive benefits package Home Office model Career development and training opportunities Flexible work arrangements (remote and/or office-based) Dynamic and inclusive work culture within a globally known group Private Health Insurance Pension Plan Paid Time Off Training & Development *Note: Benefits differ based on employee level</td><td>eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyOiBBZ2lsZSBEZXYgVGVhbSBNZW1iZXIgSVYiLCJjb21wYW55X25hbWUiOiJDYXBnZW1pbmkiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InFZVlFqNTFEM2VzZ0lZSExBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 days ago</td><td>ETL Developer</td></tr><tr><td>Senior ETL and Backend Developer (Salesforce)</td><td>S&P GLOBAL</td><td>Hyderabad, Telangana, India (+1 other)</td><td>About the Role:\n",
       "\n",
       "Grade Level (for internal use):\n",
       "10\n",
       "\n",
       "Title: Senior ETL and Backend Developer (Salesforce)\n",
       "\n",
       "Job Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n",
       "\n",
       "The Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n",
       "\n",
       "The team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n",
       "\n",
       "The Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n",
       "\n",
       "Responsibilities:\n",
       "• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n",
       "• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n",
       "• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n",
       "• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n",
       "• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n",
       "• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\n",
       "Monitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n",
       "• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\n",
       "Qualifications:\n",
       "\n",
       "Basic Qualifications:\n",
       "• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n",
       "• A minimum of 8+ years of experience in software engineering & Architecture.\n",
       "• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n",
       "• A minimum of 3+ years of Salesforce development, administration/Integration.\n",
       "• Proficiency in Informatica PowerCenter and other ETL tools.\n",
       "• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n",
       "• Experience with Salesforce integration and administration.\n",
       "• Proficiency in backend development languages (e.g., Java, Python, C#).\n",
       "• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n",
       "• Excellent problem-solving skills and attention to detail.\n",
       "• Ability to work independently and as part of a team.\n",
       "• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n",
       "\n",
       "Preferred Qualifications:\n",
       "• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n",
       "• Experience with other ETL tools and data integration platforms.\n",
       "• Informatica Certified Professional\n",
       "Salesforce Certified Administrator or Developer\n",
       "• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n",
       "• Excellent problem solving, analytical and technical troubleshooting skills.\n",
       "• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n",
       "• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n",
       "\n",
       "About S&P Global Commodity Insights\n",
       "At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n",
       "\n",
       "We’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n",
       "\n",
       "S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n",
       "\n",
       "For more information, visit http://www.spglobal.com/commodity-insights.\n",
       "\n",
       "What’s In It For You?\n",
       "\n",
       "Our Purpose:\n",
       "\n",
       "Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n",
       "\n",
       "Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n",
       "\n",
       "Our People:\n",
       "\n",
       "We're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n",
       "\n",
       "From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n",
       "\n",
       "Our Values:\n",
       "\n",
       "Integrity, Discovery, Partnership\n",
       "\n",
       "At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n",
       "\n",
       "Benefits:\n",
       "\n",
       "We take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n",
       "\n",
       "Our benefits include:\n",
       "• Health & Wellness: Health care coverage designed for the mind and body.\n",
       "• Flexible Downtime: Generous time off helps keep you energized for your time on.\n",
       "• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n",
       "• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n",
       "• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n",
       "• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n",
       "\n",
       "For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n",
       "\n",
       "Global Hiring and Opportunity at S&P Global:\n",
       "\n",
       "At S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "Equal Opportunity Employer\n",
       "\n",
       "S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n",
       "\n",
       "If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n",
       "\n",
       "US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n",
       "\n",
       "-----------------------------------------------------------\n",
       "\n",
       "20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n",
       "\n",
       "Job ID: 316835\n",
       "Posted On: 2025-06-03\n",
       "Location: Hyderabad, Telangana, India</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjVGNUtpak5fRjROQXhDcERBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>19 days ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>ZENSAR TECHNOLOGIES</td><td>Madhavaram, Telangana, India</td><td>Job Description\n",
       "\n",
       "Primary Skill Set\n",
       "• ETL Informatica\n",
       "• SQL\n",
       "• Unix\n",
       "• Realtime Data Integration (CDC) using Power Exchange (Can be optional if candidate good in other skills)\n",
       "\n",
       "Good to Have\n",
       "\n",
       "Experience on working with Mainframe Databases/files\n",
       "\n",
       "ETL Batch Scheduling tools like TWS/Tidal\n",
       "\n",
       "Roles & Responsibilities\n",
       "\n",
       "Informatica PowerCenter, Unix scripting, SQL/PLSQL\n",
       "\n",
       "Knowledge of Informatica Power Exchange is preferred\n",
       "\n",
       "Experience With Mainframe Sources/targets Is Preferred\n",
       "• Bachelor’s degree in Computer Science or similar field or equivalent work experience.\n",
       "• 5-8 years of development experience with Informatica Power Center on Data Integration projects.\n",
       "• Strong analytic, problem-solving and organizational skills.\n",
       "• Excellent SQL knowledge and ability to write the complex queries keeping performance aspect in mind.\n",
       "• Experience with analysis of business requirements, designing and writing technical specifications to design.\n",
       "• Hands-on experience to process mainframe files using Informatica Power Exchange.\n",
       "• Hands-on experience with UNIX shell scripting.\n",
       "• Participate in testing and issue resolution to validate functionality and performance.\n",
       "• Hands-on experience on any job scheduling tool, TWS is preferred.\n",
       "• Good written and verbal communication skills.\n",
       "\n",
       "Location\n",
       "\n",
       "1 st Preference: Noida\n",
       "\n",
       "2 nd Preference: Hyderabad\n",
       "\n",
       "3 rd Preference: Gurgaon</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiWmVuc2FyIFRlY2hub2xvZ2llcyIsImFkZHJlc3NfY2l0eSI6Ik1hZGhhdmFyYW0sIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InJTQzlGVjVoRHhXZ1h3QURBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>11 days ago</td><td>ETL Developer</td></tr><tr><td>Data ETL Developer / BI Engineer</td><td>AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL</td><td>India</td><td>ETL Developer\n",
       "\n",
       "Amex GBT is a place where colleagues find inspiration in travel as a force for good and – through their work – can make an impact on our industry. We’re here to help our colleagues achieve success and offer an inclusive and collaborative culture where your voice is valued.\n",
       "\n",
       "We are looking for an experienced Data ETL Developer / BI Engineer who loves solving complex problems across a full spectrum of data & technologies. You will lead the building effort of GBT's new BI platform and manage the legacy platform to seamlessly support our business function around data and analytics. You will create dashboards, databases, and other platforms that allow for the efficient collection and evaluation of BI data.\n",
       "\n",
       "What You’ll Do on a Typical Day:\n",
       "• Design, implement, and maintain systems that collect and analyze business intelligence data.\n",
       "• Design and architect an analytical data store or cluster for the enterprise and implement data pipelines that extract, transform, and load data into an information product that helps the organization reach strategic goals.\n",
       "• Create physical and logical data models to store and share data that can be easily consumed for different BI needs.\n",
       "• Develop Tableau dashboards and features.\n",
       "• Create scalable and high-performance data load and management process to make data available near real-time to support on-demand analytics and insights.\n",
       "• Translate complex technical and functional requirements into detailed designs.\n",
       "• Investigate and analyze alternative solutions to data storing, processing, etc., to ensure the most streamlined approaches are implemented.\n",
       "• Serve as a mentor to junior staff by conducting technical training sessions and reviewing project outputs\n",
       "• Design & develop, and maintain a data model implementing ETL processes.\n",
       "• Manage and maintain the database, warehouse, & cluster with other dependent infrastructure.\n",
       "• Work closely with data, products, and another team to implement data analytic solutions.\n",
       "• Support production application and Incident management.\n",
       "• Help define data governance policies and support data versioning processes\n",
       "• Maintain security and data privacy by working closely with the Data Protection Officer internally.\n",
       "• Analyze a vast number of data stores and uncover insights\n",
       "\n",
       "What We’re Looking For:\n",
       "• Degree in computer sciences or engineering\n",
       "• Overall, 3-5 years of experience in data & data warehouse, ETL, and data modeling.\n",
       "• 2+ years of experience working and managing large data stores, complex data pipelines, and BI solutions.\n",
       "• Strong experience in SQL and writing complex queries.\n",
       "• Hands-on experience with Tableau development.\n",
       "• Hands-on working experience on Redshift, data modeling, data warehouse, ETL tool, Python, and Shell scripting.\n",
       "• Understanding of data warehousing and data modeling techniques\n",
       "• Strong data engineering skills on the AWS Cloud Platform are essential.\n",
       "• Knowledge of Linux, SQL, and any scripting language\n",
       "• Good interpersonal skills and a positive attitude\n",
       "• Experience in travel data would be a plus.\n",
       "\n",
       "Location\n",
       "Gurgaon, India\n",
       "\n",
       "The #TeamGBT Experience\n",
       "\n",
       "Work and life: Find your happy medium at Amex GBT.\n",
       "• Flexible benefits are tailored to each country and start the day you do. These include health and welfare insurance plans, retirement programs, parental leave, adoption assistance, and wellbeing resources to support you and your immediate family.\n",
       "• Travel perks: get a choice of deals each week from major travel providers on everything from flights to hotels to cruises and car rentals.\n",
       "• Develop the skills you want when the time is right for you, with access to over 20,000 courses on our learning platform, leadership courses, and new job openings available to internal candidates first.\n",
       "• We strive to champion Inclusion in every aspect of our business at Amex GBT. You can connect with colleagues through our global INclusion Groups, centered around common identities or initiatives, to discuss challenges, obstacles, achievements, and drive company awareness and action.\n",
       "• And much more!\n",
       "\n",
       "All applicants will receive equal consideration for employment without regard to age, sex, gender (and characteristics related to sex and gender), pregnancy (and related medical conditions), race, color, citizenship, religion, disability, or any other class or characteristic protected by law.\n",
       "\n",
       "Click Here for Additional Disclosures in Accordance with the LA County Fair Chance Ordinance.\n",
       "\n",
       "Furthermore, we are committed to providing reasonable accommodation to qualified individuals with disabilities. Please let your recruiter know if you need an accommodation at any point during the hiring process. For details regarding how we protect your data, please consult the Amex GBT Recruitment Privacy Statement.\n",
       "\n",
       "What if I don’t meet every requirement? If you’re passionate about our mission and believe you’d be a phenomenal addition to our team, don’t worry about “checking every box;\" please apply anyway. You may be exactly the person we’re looking for!\n",
       "Experience Level\n",
       "Mid Level\n",
       "\n",
       "More about this Data ETL Developer / BI Engineer job\n",
       "\n",
       "American Express Global Business Travel is aggressively hiring for the job profile of Data ETL Developer / BI Engineer at undefined in null locality. Kindly go through the FAQs below to get all answers related to the given job.\n",
       "\n",
       "1. How much salary can I expect as a Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n",
       "\n",
       "Ans. You can expect a minimum salary of 0 INR. The salary offered will depend on your skills, experience and performance in the interview.\n",
       "\n",
       "2. What is the eligibility criteria to apply for Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n",
       "\n",
       "Ans. The candidate should have completed undefined degree and people who have 3 to 5 years are eligible to apply for this job. You can apply for more jobs in undefined to get hired quickly.\n",
       "\n",
       "3. Is there any specific skill required for this job?\n",
       "\n",
       "Ans. The candidate should have undefined skills and sound communication skills for this job.\n",
       "\n",
       "4. Who can apply for this job?\n",
       "\n",
       "Ans. Both Male and Female candidates can apply for this job.\n",
       "\n",
       "5. Is it a work from home job?\n",
       "\n",
       "Ans. No, it’s not a work from home job and can’t be done online. You can explore and apply for other work from home jobs in undefined at apna.\n",
       "\n",
       "6. Are there any charges or deposits required while applying for the role or while joining?\n",
       "\n",
       "Ans. No work-related deposit needs to be made during your employment with the company.\n",
       "\n",
       "7. How can I apply for this job?\n",
       "\n",
       "Ans. Go to the apna app and apply for this job. Click on the apply button and call HR directly to schedule your interview.\n",
       "\n",
       "8. What is the last date to apply?\n",
       "\n",
       "Ans. The last date to apply for this job is .\n",
       "\n",
       "For more details, download apna app and find Full Time jobs in undefined. Through apna, you can find jobs in 74 cities across India. Join NOW!</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVUTCBEZXZlbG9wZXIgLyBCSSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkFtZXJpY2FuIEV4cHJlc3MgR2xvYmFsIEJ1c2luZXNzIFRyYXZlbCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJnY3VjckFjcm5KVmRmdjRxQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>6 days ago</td><td>ETL Developer</td></tr><tr><td>Informatica ETL Developer - SQL/Power Center</td><td>RENOVISION AUTOMATION SERVICES PVT.LTD.</td><td>Telangana, India</td><td>Experience - 6+ yearsWork Mode - HybridJob Summary : We are seeking a skilled Informatica ETL Developer with 5+ years of experience in ETL and Business Intelligence projects. The ideal candidate will have a strong background in Informatica PowerCenter, a solid understanding of data warehousing concepts, and hands-on experience in SQL, performance tuning, and production support. This role involves designing and maintaining robust ETL pipelines to support digital transformation initiatives for clients in manufacturing, automotive, transportation, and engineering domains.Key Responsibilities : - Design, develop, and maintain ETL workflows using Informatica PowerCenter.- Troubleshoot and optimize ETL jobs for performance and reliability.- Analyze complex data sets and write advanced SQL queries for data validation and transformation.- Collaborate with data architects and business analysts to implement data warehousing solutions.- Apply SDLC methodologies throughout the ETL development lifecycle.- Support production environments by identifying and resolving data and performance issues.- Work with Unix shell scripting for job automation and scheduling.Required Skills : - 35 years of hands-on experience with Informatica PowerCenter.- Proficiency in SQL and familiarity with NoSQL platforms.- Experience in ETL performance tuning and troubleshooting.- Solid understanding of Unix/Linux environments and scripting.- Excellent verbal and written communication skills.Preferred Qualifications : - AWS Certification or experience with cloud-based data integration is a plus.- Exposure to data modeling and data governance practices. (ref: hirist.tech)</td><td>eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyIC0gU1FML1Bvd2VyIENlbnRlciIsImNvbXBhbnlfbmFtZSI6IlJlbm92aXNpb24gQXV0b21hdGlvbiBTZXJ2aWNlcyBQdnQuTHRkLiIsImFkZHJlc3NfY2l0eSI6IlRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlVqTnczajVrcnJpS21VQjdBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>ETL Developer</td></tr><tr><td>ETL Developer</td><td>LUXOFT</td><td>Maharashtra, India</td><td>Project Description:\n",
       "\n",
       "Our client is an EU subsidiary of a Global Financial Bank working in multiple markets and asset classes.\n",
       "\n",
       "DWH/ETL developer will work closely with the Development Lead to design, build interfaces and integrate data from a variety from internal and external data sources into the new Enterprise Data Warehouse environment. The ETL Developer will be responsible for developing ETL primarily utilizing Microsoft & Azure technologies within industry recognized ETL standards, architecture, and best practices.\n",
       "\n",
       "Responsibilities:\n",
       "• Act as a technical expert in the designing, coding, unit testing, supporting, and debugging of data warehouse software components in all aspects of SDLC\n",
       "• Apply cloud and ETL engineering skills to solve problems and design approaches\n",
       "• Troubleshoot and debug ETL pipelines and creating unit tests for ETL pipelines.\n",
       "• Assess query performance and actively contribute to optimizing the code\n",
       "• Write technical documentation and specifications\n",
       "• Support internal audit by submitting required evidence\n",
       "• Create reports and dashboards in the BI portal\n",
       "• Work with Development Lead, DWH Architect and QA Engineers to plan, implement and deliver best ETL strategies\n",
       "• Work with business analysts to understand requirements to create technical design specifications, gaining a sound understanding of business processes for related applications so that integration processes fulfill the end-user requirements\n",
       "• Communicate effectively in a collaborative, complex and high performing team environment as per Agile principles\n",
       "\n",
       "Mandatory Skills Description:\n",
       "• Proven work experience as an ETL Developer\n",
       "• Advanced knowledge of relational databases and dimensional Data Warehouse modelling concepts\n",
       "• Good understanding of physical and logical data modeling\n",
       "• Very good understanding of modern SaaS/PaaS data solutions in a cost conscious approach\n",
       "• Expert level of knowledge of Microsoft Data stack\n",
       "• Experience in developing and deploying data oriented solutions in Cloud (Azure/Synapse Analytics/Fabric)\n",
       "• Experience in designing and implementing data transformation and ETL layers using Data Factory, Notebooks\n",
       "• Experience with PowerBI for report & dashboard creation. PowerQuery and/or DAX is an advantage.\n",
       "• Experience in/understanding of Azure Data Lake Storage\n",
       "• Knowledge/use of CI/CD tools and principles, preferably Azure DevOps or Bamboo\n",
       "• Strong SQL knowledge, able to create complex SQL queries and good understanding of stored procedures, views, indexes, functions, etc.\n",
       "• Good working knowledge of at least one Scripting language. Python is an advantage.\n",
       "• Experience with GIT repositories and working with branches. GitHub, Azure DevOps or Bitbucket experience are preferable.\n",
       "• Ability to troubleshoot and solve complex technical problems\n",
       "• Good understanding of software development best practices\n",
       "• Working experience in Agile projects; preferably using JIRA\n",
       "• Experience in working in high priority projects preferably greenfield project experience\n",
       "• Able to communicate complex information clearly and concisely.\n",
       "• Able to work independently and also to collaborate across the organization\n",
       "• Highly developed problem-solving skills with minimal supervision\n",
       "• Understanding of data governance and enterprise concepts preferably in banking environment\n",
       "• Verbal and written communication skills in English are essential.\n",
       "\n",
       "Nice-to-Have Skills Description:\n",
       "• Microsoft Fabric\n",
       "• Snowflake\n",
       "• Background in SSIS/SSAS/SSRS\n",
       "• Azure DevTest Labs, ARM templates\n",
       "• Azure PurView\n",
       "• Banking/finance experience</td><td>eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiTHV4b2Z0IiwiYWRkcmVzc19jaXR5IjoiTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJlZF9YUGZ2UVNVcTZXdXltQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>ETL Developer</td></tr><tr><td>Data Engineer (Hadoop, Spark, Scala, Hive)</td><td>VISA</td><td>India</td><td>Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n",
       "\n",
       "Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n",
       "\n",
       "Job Description\n",
       "\n",
       "Translate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\n",
       "\n",
       "Good to have GenAI Exposure and Agentic AI Knowledge.\n",
       "\n",
       "Work with business partners directly to seek clarity on requirements.\n",
       "\n",
       "Define solutions in terms of components, modules, and algorithms.\n",
       "\n",
       "Design, develop, document, and implement new programs and subprograms, as well as enhancements, modifications and corrections to existing software.\n",
       "\n",
       "Create technical documentation and procedures for installation and maintenance.\n",
       "\n",
       "Write Unit Tests covering known use cases using appropriate tools.\n",
       "\n",
       "Integrate test frameworks in the development process.\n",
       "\n",
       "Work with operations to get the solutions deployed.\n",
       "\n",
       "Take ownership of production deployment of code.\n",
       "\n",
       "Come up with Coding and Design best practices.\n",
       "\n",
       "Thrive in a self-motivated, internal-innovation driven environment.\n",
       "\n",
       "Adapt quickly to new application knowledge and changes.\n",
       "\n",
       "This is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Basic Qualifications\n",
       "\n",
       "-Minimum of 6 months of work experience or a Bachelor's Degree\n",
       "\n",
       "Preferred Qualifications\n",
       "\n",
       "-Bachelor degree in Computer Science.\n",
       "\n",
       "-Minimum of 1 plus years of software development experience in Hadoop using\n",
       "\n",
       "Spark, Scala, Hive.\n",
       "\n",
       "-Expertise in Object Oriented Programming Language Java, Python.\n",
       "\n",
       "-Experience using CI CD Process, version control and bug tracking tools.\n",
       "\n",
       "-Result-oriented with strong analytical and problem-solving skills.\n",
       "\n",
       "-Experience with automation of job execution, validation and comparison of data\n",
       "\n",
       "files on Hadoop Environment at the field level.\n",
       "\n",
       "-Experience in leading a small team and being a team player.\n",
       "\n",
       "-Strong communication skills with proven ability to present complex ideas and\n",
       "\n",
       "document them in a clear and concise way.\n",
       "\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChIYWRvb3AsIFNwYXJrLCBTY2FsYSwgSGl2ZSkiLCJjb21wYW55X25hbWUiOiJWaXNhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImVULTZKTlc2NHVzTUxhdS1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 days ago</td><td>Spark Engineer</td></tr><tr><td>Spark Engineer</td><td>STAFFINGINE LLC</td><td>India</td><td>- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>1 day ago</td><td>Spark Engineer</td></tr><tr><td>Databricks Engineer - Spark / PySpark</td><td>ENKEFALOS TECHNOLOGIES LLP</td><td>Anywhere</td><td>Databricks Engineer – Spark / PySpark\n",
       "\n",
       "Location : Remote / Mysore\n",
       "\n",
       "Joining : Immediate\n",
       "\n",
       "Experience : 5+ years\n",
       "\n",
       "Responsibilities :\n",
       "\n",
       "Will implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n",
       "\n",
       "Requirements:\n",
       "• PySpark (RDDs, DataFrames, performance tuning)\n",
       "• Building gold‐layer data models for financial reporting\n",
       "• Experience with complex joins, aggregations, GL hierarchies\n",
       "• Version handling (Actuals vs Budget), currency conversions\n",
       "\n",
       "Job Type: Full-time\n",
       "\n",
       "Pay: ₹500,395.35 - ₹1,840,348.25 per year\n",
       "\n",
       "Benefits:\n",
       "• Flexible schedule\n",
       "• Paid sick time\n",
       "• Provident Fund\n",
       "• Work from home\n",
       "\n",
       "Application Question(s):\n",
       "• Have you worked on ADF/ADLS ?\n",
       "• Do you have hands-on experience of Spark / PySpark\n",
       "\n",
       "Experience:\n",
       "• Databricks Engineering: 4 years (Required)\n",
       "\n",
       "Work Location: Remote</td><td>eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 days ago</td><td>Spark Engineer</td></tr><tr><td>Pi Square Technologies - Spark & Scala Engineer</td><td>SANDEEP RAJA</td><td>India</td><td>Job Summary :\n",
       "\n",
       "We are seeking a skilled and motivated Spark & Scala Engineer with 46 years of hands-on experience in building scalable data processing applications. The ideal candidate will have a strong background in big data ecosystems and a solid understanding of distributed computing using Apache Spark with Scala.\n",
       "\n",
       "Roles and Responsibilities :\n",
       "\n",
       "- Design, build, and maintain efficient, reusable, and reliable Apache Spark applications.\n",
       "\n",
       "- Optimize Spark applications for maximum speed and scalability.\n",
       "\n",
       "- Implement data ingestion and ETL processes.\n",
       "\n",
       "- Collaborate with data scientists and architects to implement complex big data solutions.\n",
       "\n",
       "- Debug and resolve issues in Spark applications.\n",
       "\n",
       "- Stay up to date with the latest trends in big data technologies and Apache Spark.\n",
       "\n",
       "- Write clean, readable, and maintainable code.\n",
       "\n",
       "- Participate in code reviews and contribute to team knowledge sharing.\n",
       "\n",
       "Required Skills :\n",
       "\n",
       "- 46 years of experience working with Apache Spark (core, SQL, streaming).\n",
       "\n",
       "- Strong proficiency in Scala programming.\n",
       "\n",
       "- Experience in building and optimizing data pipelines and ETL workflows.\n",
       "\n",
       "- Proficient in working with big data tools and frameworks (e.g., Hive, HDFS, Kafka).</td><td>eyJqb2JfdGl0bGUiOiJQaSBTcXVhcmUgVGVjaG5vbG9naWVzIC0gU3BhcmsgXHUwMDI2IFNjYWxhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoic2FuZGVlcCByYWphIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlFMNDBNZDljZDlqQkxBX3BBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>27 days ago</td><td>Spark Engineer</td></tr><tr><td>Spark Developer</td><td>INFOSYS</td><td>India</td><td>• Primary skills:Technology->Big Data - Data Processing->Spark\n",
       "\n",
       "A day in the life of an Infoscion\n",
       "• As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\n",
       "• You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\n",
       "• You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\n",
       "• You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\n",
       "• You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n",
       "• Knowledge of more than one technology\n",
       "• Basics of Architecture and Design fundamentals\n",
       "• Knowledge of Testing tools\n",
       "• Knowledge of agile methodologies\n",
       "• Understanding of Project life cycle activities on development and maintenance projects\n",
       "• Understanding of one or more Estimation methodologies, Knowledge of Quality processes\n",
       "• Basics of business domain to understand the business requirements\n",
       "• Analytical abilities, Strong Technical Skills, Good communication skills\n",
       "• Good understanding of the technology and domain\n",
       "• Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\n",
       "• Awareness of latest technologies and trends\n",
       "• Excellent problem solving, analytical and debugging skills</td><td>eyJqb2JfdGl0bGUiOiJTcGFyayBEZXZlbG9wZXIiLCJjb21wYW55X25hbWUiOiJJbmZvc3lzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkZ2VmRvVC1NQnFvWHRRUmJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>16 days ago</td><td>Spark Engineer</td></tr><tr><td>SW Engineer (Java and Bigdata/Hadoop/Spark) 1yr</td><td>VISA</td><td>India</td><td>Job Description\n",
       "\n",
       "This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. They will be an integral part of the Payment Products Development team, focusing on the development of software solutions that leverage data to address business challenges. They will be extensively involved in hands-on activities including proof of concepts, development, and testing. They should demonstrate adaptability and readiness to change tasks according to the team's requirements.\n",
       "\n",
       "Key Responsibilities\n",
       "• Deliver high-quality products and features, taking a holistic view from the initial idea to final delivery. This involves design, development, testing, and accountability for release deployments as well as providing support post-deployment.\n",
       "• Understand the product in its entirety and ensure the timely delivery of your assigned module within an agile team setting.\n",
       "• Accurately scope efforts, identify risks, and clearly communicate trade-offs with team members and other stakeholders.\n",
       "• Coordinate and participate in Continuous Integration activities, testing automation frameworks, and other related tasks, in addition to contributing to core product code.\n",
       "• Effectively communicate status updates, issues, and potential risks accurately and promptly.\n",
       "• Perform other tasks related to data governance and system infrastructure as required.\n",
       "\n",
       "This is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n",
       "\n",
       "Qualifications\n",
       "\n",
       "Basic Qualifications\n",
       "\n",
       "-Bachelor's degree in Computer Science or equivalent field\n",
       "\n",
       "-Relevant working experience of up to 2 years in the industry\n",
       "\n",
       "-Proven experience in software development, particularly in data-centric\n",
       "\n",
       "projects, demonstrating adherence to standard development best practices\n",
       "\n",
       "-Strong understanding and practical experience with data structures and\n",
       "\n",
       "algorithms, with a passion for tackling complex problems\n",
       "\n",
       "-Proficiency in Java programming\n",
       "\n",
       "-Hands-on experience with Big Data technologies such as Hadoop, Spark, and\n",
       "\n",
       "Hive\n",
       "\n",
       "-Strong knowledge of Unix/Linux operating systems and Shell Scripting\n",
       "\n",
       "-Proficiency in working with RDBMS and SQL\n",
       "\n",
       "-Basic knowledge of manual and automated testing\n",
       "\n",
       "-Familiarity with version control systems, specifically Git\n",
       "\n",
       "-Awareness of and experience with software design patterns\n",
       "\n",
       "-Experience working within an Agile framework\n",
       "\n",
       "Preferred Qualifications\n",
       "\n",
       "-Proficiency in Scala & Kafka programming is a good to have\n",
       "\n",
       "-Experience with Airflow for workflow management\n",
       "\n",
       "-Familiarity with AI concepts and tools, including GitHub Copilot for code\n",
       "\n",
       "development\n",
       "\n",
       "-Exposure to AI/ML development is an added advantage\n",
       "\n",
       "-Proficiency in working with In-memory Databases like Redis\n",
       "\n",
       "-Good knowledge of API development is highly advantageous\n",
       "\n",
       "-Strong verbal and written communication skills, with a proactive and self-\n",
       "\n",
       "motivated approach to improving existing processes to enable faster\n",
       "\n",
       "iterations.\n",
       "\n",
       "-Demonstrated intellectual and analytical rigor, with a keen attention to detail\n",
       "\n",
       "-Team-oriented, energetic, and collaborative approach to work, coupled with a\n",
       "\n",
       "diplomatic and adaptable style\n",
       "\n",
       "Additional Information\n",
       "\n",
       "Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td><td>eyJqb2JfdGl0bGUiOiJTVyBFbmdpbmVlciAoSmF2YSBhbmQgQmlnZGF0YS9IYWRvb3AvU3BhcmspIDF5ciIsImNvbXBhbnlfbmFtZSI6IlZJU0EiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoidHV1TkJsMDF3amRIc0xnSEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>5 days ago</td><td>Spark Engineer</td></tr><tr><td>Big Data Lead/ Lead Data Engineer/Spark Tech Lead</td><td>TANISHA SYSTEMS  INC</td><td>India</td><td>Job Description:Mandatory:10+ years of experience in solution, design and development of applications using Java 8+/J2EE, Spring, Spring-Boot, Micro Services, RESTful Services and with experience in Big Data and with experience working in heavy data background needed.Develop, program, and maintain applications using the Apache Spark open source frameworkWork with different aspects of the Spark ecosystem, including Spark SQL, Data Frames, Datasets, and streaming.Spark Developer must have strong programming skills in Java, Scala, or PythonFamiliar with big data processing tools and techniquesProven experience as a Spark Developer or a related roleStrong programming skills in Java, Scala, or PythonFamiliarity with big data processing tools and techniquesExperience with the Hadoop ecosystemGood understanding of distributed systemsExperience with streaming data platformsMust have strong experience in Big Data and with experience working in heavy data background needed.Must be strong in Cloud AWS event-based architecture, Kubernetes, ELK (Elasticsearch, Logstash & Kibana)Must have excellent experience in designing and Implementing cloud-based solutions in various AWS Services (: s3, Lambda, Step Function, AMQ, SNS, SQS, CloudWatch Events, etc.)Must be well experienced in design and development of Microservice using Spring-Boot and REST API and with GraphQLMust have solid knowledge and experience in NoSQL (MongoDB)Good knowledge and experience in any Queue based implementations.Strong knowledge/experience in ORM Framework - JPA / HibernateGood knowledge in technical concepts – Security, Transaction, Monitoring, PerformanceShould we well have versed with TDD/ATDDShould have experience on Java, Python and Spark2+ years of experience in designing and Implementing cloud-based solutions in various AWS ServicesStrong experience in DevOps tool chain (Jenkins, Artifactory, Ansible/Chef/Puppet/Spinnaker, Maven/Gradle, Atlassian Tool suite)Very Good knowledge and experience in Non-Functional (Technical) Requirements like Security, Transaction, Performance, etc.Excellent analytical and problem-solving skills Nice to have:· Experience in Experience with OAuth implementation using Ping Identity· Experience in API Management (Apigee) or Service Mesh (Istio)· Good knowledge and experience in Queue/Topic (Active-MQ) based implementations.· Good knowledge and experience in Scheduler and Batch Jobs· Experience with scripting languages using Unix· Preferably certified in AWS</td><td>eyJqb2JfdGl0bGUiOiJCaWcgRGF0YSBMZWFkLyBMZWFkIERhdGEgRW5naW5lZXIvU3BhcmsgVGVjaCBMZWFkIiwiY29tcGFueV9uYW1lIjoiVGFuaXNoYSBTeXN0ZW1zICBJbmMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQmRWaDV5cnl3aTREUFNOT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 hours ago</td><td>Spark Engineer</td></tr><tr><td>Data Insights Analyst</td><td>IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED</td><td>India</td><td>Responsible for leveraging Google Analytics 4 (GA4) to extract, analyze, and interpret data to support business objectives and decision-making processes. Requires proficient understanding and translating business requirements into actionable insights and recommendations derived from GA4 data.Key Responsibilities:1.Data Extraction & Analysis: Extract and analyze GA4 data to provide meaningful insights into user behavior, website performance, and digital marketing effectiveness.2.Reporting & Dashboarding: Develop custom reports, dashboards, and data visualizations using GA4 data to monitor key metrics and performance indicators.3.Insights and Recommendations: Providing actionable insights and recommendations to optimize website performance, user experience, and marketing campaigns.4.Customization & Configurations: Configure GA4 data streams, events, and conversions to align with business goals and KPIs (key performance indicators). Utilize custom dimensions, metrics, and attribution models to capture and analyze data relevant to unique business needs.5.Pharma knowledge: Understanding of the Pharma industry landscape, including regulatory compliance, patient journey mapping, and healthcare data privacy considerationsMinimum Requirement:·2-4 Years of experience in digital analytics, hands on experience on Google Analytics 4.·Proficiency in Google Analytics and Google Tag Manager.·Strong analytical skills and ability to interpret data.·Understanding of digital marketing concepts and website optimization techniques.·Excellent communication skills to present findings and recommendations effectively·GA4 Certification is strongly preferred</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEluc2lnaHRzIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJJTjEwIChGQ1JTID0gSU4wMTApIE5vdmFydGlzIEhlYWx0aGNhcmUgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImFiMU5uYTFGN1dHbFd4eC1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>5 days ago</td><td>Data Analyst</td></tr><tr><td>Senior Data Management Analyst</td><td>WELLS FARGO</td><td>Hyderabad, Telangana, India</td><td>About this role:\n",
       "\n",
       "Wells Fargo is seeking a Senior Data Management Analyst\n",
       "\n",
       "In this role, you will:\n",
       "• Lead or participate in moderately complex programs and initiatives for data quality, governance, and metadata activities\n",
       "• Design and conduct moderately complex analysis to identify and remediate data quality, data integrity, process, and control gaps\n",
       "• Analyze, assess, and test data controls and data systems to ensure quality and risk compliance standards are met and adhere to data governance standards and procedures\n",
       "• Identify data quality metrics and execute data quality audits to benchmark the state of data quality\n",
       "• Develop recommendations for optimal approaches to resolve data quality issues and implement plans for assessing the quality of new data sources leveraging domain expertise and data, business, or process analysis to inform and support solution design\n",
       "• Lead project teams and mentor less experienced staff members\n",
       "• Drive planning and coordination on moderately complex remediation efforts acting as central point of contact\n",
       "• Consult with clients to assess the current state of data and metadata quality within area of assigned responsibility\n",
       "• Participate in cross-functional groups to develop companywide data governance strategies\n",
       "• Provide input into communication routines with stakeholders, business partners, and experienced leaders\n",
       "\n",
       "Required Qualifications:\n",
       "• 4+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n",
       "\n",
       "Desired Qualifications:\n",
       "• Experience in large enterprise data initiatives\n",
       "• Contact center business or technology experience\n",
       "• Experience implementing, managing, or utilizing big data environments (Hadoop, Big Query, etc.)\n",
       "• Experience using standard BI tools (Tableau, Power BI, MicroStrategy, etc.) preferably from big data environments\n",
       "\n",
       "Posting End Date:\n",
       "24 Jun 2025\n",
       "• Job posting may come down early due to volume of applicants.\n",
       "\n",
       "We Value Equal Opportunity\n",
       "\n",
       "Wells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n",
       "\n",
       "Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n",
       "\n",
       "Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n",
       "\n",
       "Applicants with Disabilities\n",
       "\n",
       "To request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n",
       "\n",
       "Drug and Alcohol Policy\n",
       "\n",
       "Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n",
       "\n",
       "Wells Fargo Recruitment and Hiring Requirements:\n",
       "\n",
       "a. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n",
       "\n",
       "b. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBNYW5hZ2VtZW50IEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJXZWxscyBGYXJnbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEFWbXpFcV9ySTNSUG1faUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Associate/Analyst - Data Analytics</td><td>D. E. SHAW INDIA</td><td>Hyderabad, Telangana, India</td><td>The Financial Research (FinRes) group provides research and analysis support to the entire investment life cycle of the firm’s global proprietary trading strategies. The group caters to almost all the quantitative and qualitative strategies of the firm and its research support covers multiple asset classes, sectors, and geographies.</td><td>eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUvQW5hbHlzdCAtIERhdGEgQW5hbHl0aWNzIiwiY29tcGFueV9uYW1lIjoiRC4gRS4gU2hhdyBJbmRpYSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiak9McEJja1dWaXhiWjJ4c0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>14 hours ago</td><td>Data Analyst</td></tr><tr><td>Senior Analyst- Data Risk Office</td><td>BRISTOL MYERS SQUIBB</td><td>Hyderabad, Telangana, India</td><td>Working with Us\n",
       "Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n",
       "\n",
       "Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n",
       "\n",
       "Roles & Responsibilities\n",
       "\n",
       "Functional and Technical\n",
       "• Execution and monitoring of data privacy office key activties.\n",
       "• Assist in the development and implementation of data privacy policies and procedures to ensure compliance with international, federal, and state regulations, including GDPR, CCPA, and HIPAA.\n",
       "• Work closely with legal, IT, HR, and other departments to address data privacy concerns and implement best practices.\n",
       "• Assist in the development and delivery of data privacy training and awareness programs across the organization. Keeps up to date with evolution of regulations impacting privacy, ethics, and data.\n",
       "• Experienced in configuring and working with various industry leading data risk and privacy tools like OneTrust, TrustArc, Microsoft Purview etc.\n",
       "• Demonstrate critical thinking around interpreting business and industry challenges and recommending best practices-based solutions to improve products, processes, systems and reduce risk.\n",
       "• Implement organizational IT controls in accordance with applicable regulations and the ability to evaluate and understand the impact of new regulations and requirements.\n",
       "\n",
       "People Management:\n",
       "• Responsible for training and mentoring junior staff to meet BMS standards.\n",
       "• Preferred experience with working in a multi-cultural, multi-location and diverse environments.\n",
       "\n",
       "Qualifications & Experience\n",
       "• B.E./B.Tech. or equivalent in computer science, engineering, life science field\n",
       "• Recognized privacy/DLP certifications and experience preferred.\n",
       "• At least years of privacy program management, compliance, or strong operations management experience (regulated or healthcare or tech preferred).\n",
       "• Knowledge of HIPAA, GDPR, CPRA, PIPL etc., and other privacy regulations is a must.\n",
       "• Ability to make decisions that impact own work and other groups/teams and works under minimal supervision.\n",
       "• Demonstrates openness to learning and developing. Takes a responsibility for their own and team’s development and growth.\n",
       "• Demonstrates an understanding of factors driving team performance and how they contribute to the team's overall success.\n",
       "• Excellent English Oral and written communication skills including the ability to deliver clear and articulate presentations.\n",
       "• Ability to use PowerPoint, Excel, Word, or other technologies to communicate complex topics to stakeholders, manage personal workload, and track projects and issues.\n",
       "\n",
       "If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n",
       "\n",
       "Uniquely Interesting Work, Life-changing Careers\n",
       "With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n",
       "\n",
       "On-site Protocol\n",
       "\n",
       "BMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n",
       "\n",
       "Site-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n",
       "\n",
       "BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n",
       "\n",
       "BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n",
       "\n",
       "BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n",
       "\n",
       "If you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n",
       "\n",
       "Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHlzdC0gRGF0YSBSaXNrIE9mZmljZSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJsWm9Rdlc1dnAyWjZyYVlMQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst II – Product Information Capabilities | Digital & Technology</td><td>GENERAL MILLS INDIA</td><td>India</td><td>India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.\n",
       "\n",
       "Position Title\n",
       "\n",
       "Software Engineer II – Product Information Capability\n",
       "\n",
       "Function/Group\n",
       "\n",
       "Digital & Technology\n",
       "\n",
       "Location\n",
       "\n",
       "Mumbai\n",
       "\n",
       "Shift Timing\n",
       "\n",
       "Regular\n",
       "\n",
       "Role Reports to\n",
       "\n",
       "D&T Manager – Product Information Capability\n",
       "\n",
       "Remote/Hybrid/in-Office\n",
       "\n",
       "Hybrid\n",
       "\n",
       "About General Mills\n",
       "\n",
       "We make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and Häagen-Dazs, we’ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\n",
       "\n",
       "How we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\n",
       "\n",
       "us into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\n",
       "\n",
       "General Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\n",
       "\n",
       "With our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital & Technology (D&T) Innovation, Technology & Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy & Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\n",
       "\n",
       "We advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\n",
       "\n",
       "Job Overview\n",
       "\n",
       "Function Overview\n",
       "\n",
       "The Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The team's expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\n",
       "\n",
       "The team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the \"Work with Heart\" philosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the following Link\n",
       "\n",
       "Purpose of the role\n",
       "\n",
       "This is an exciting time to work in General Mills' Supply Chain & ITQ Organization! We are accelerating Digital Transformation of our Product Information Capabilities (PIC) capabilities to provide a competitive advantage to our business. To this end, we are seeking a skilled and motivated STIBO Developer to join our Product Information Capabilities team. As a STIBO Developer, you will play a crucial role in designing, developing, and implementing solutions within our STIBO STEP platform to support our product information management (PIM) and master data management (MDM) processes. You will collaborate with cross-functional teams to understand business requirements, translate them into technical specifications, and deliver high-quality solutions that meet our business needs.\n",
       "\n",
       "Key Accountabilities\n",
       "• Design, develop, and maintain STIBO STEP solutions to support product information management processes.\n",
       "• Write and maintain code for business rules to ensure data quality and consistency.\n",
       "• Configure outbound and inbound integrations to exchange data with other systems.\n",
       "• Configure gateway endpoints for seamless data flow.\n",
       "• Develop and maintain data models within STIBO STEP to accurately represent product information.\n",
       "• Build web UI screens for data entry, validation, and reporting.\n",
       "• Develop solutions based on documented requirements and specifications.\n",
       "• Participate in agile project development, including sprint planning, daily stand-ups, and retrospectives.\n",
       "• Collaborate with business analysts, data architects, and other developers to ensure solutions align with business needs and technical standards.\n",
       "• Troubleshoot and resolve issues related to STIBO STEP implementations.\n",
       "• Stay up-to-date with the latest STIBO STEP features and best practices.\n",
       "• Create and maintain technical documentation for STIBO STEP solutions.\n",
       "\n",
       "Minimum Qualifications\n",
       "• Education – Full time graduation from an accredited university (Mandatory- Note: This is the minimum education criteria which cannot be altered)\n",
       "• Experience with tools and concepts related to MDM and/or Digital Product Publishing, such as data governance, data quality, data integration and data exporting\n",
       "• Exposure to Product Information Management Systems (PIM/MDM)\n",
       "• Technical expertise into Stibo platform\n",
       "• Experience with Data Syndication partners such as 1WorldSync, Syndigo and Salsify.\n",
       "• Exposure to GDSN Standards\n",
       "• Strong team player and collaborator with core focus on solution making; Effective analytical and technical skills; Ability to work in a cross functional team environment\n",
       "\n",
       "Preferred Qualifications\n",
       "• Product Information Management / Master Data Management\n",
       "• STIBO STEP certification\n",
       "• Business Analysis skills\n",
       "• SQL, Cloud GCP\n",
       "• Agile / SCRUM Delivery\n",
       "• Familiarity with Service Bus Integration\n",
       "• Preferably experience in Consumer Goods industry.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUkg4oCTIFByb2R1Y3QgSW5mb3JtYXRpb24gQ2FwYWJpbGl0aWVzIHwgRGlnaXRhbCBcdTAwMjYgVGVjaG5vbG9neSIsImNvbXBhbnlfbmFtZSI6IkdlbmVyYWwgTWlsbHMgSW5kaWEiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTWplT1ByZWI4UVZPLVpDVUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>2 days ago</td><td>Data Analyst</td></tr><tr><td>Lead Data Management Analyst</td><td>WELLS FARGO</td><td>Hyderabad, Telangana, India</td><td>About this role:\n",
       "\n",
       "Wells Fargo is seeking a Lead Data Management Analyst\n",
       "\n",
       "In this role, you will:\n",
       "• Organize and lead complex companywide initiatives to ensure that data quality is maintained so that data can effectively support business processes\n",
       "• Oversee analysis and reporting in support of regulatory requirements\n",
       "• Identify and recommend analysis of data quality or integrity issues\n",
       "• Evaluate data quality metrics and data quality audits to benchmark the state of data quality\n",
       "• Make decisions in complex and multi-faceted situations requiring solid understanding of data governance standards and procedures\n",
       "• Identify new data sources and develop recommendations for assessing the quality of new data\n",
       "• Lead project teams and mentor less experienced staff members\n",
       "• Recommend remediation of process or control gaps that align to management strategy\n",
       "• Serve as relationship manager for a line of business\n",
       "• Consult with and provide recommendations to senior management regarding assessments of the current state of data and metadata quality within area of assigned responsibility\n",
       "• Represent client in cross-functional groups to develop companywide data governance strategies\n",
       "• Strategically collaborate and consult with peers, colleagues, and mid-level to senior managers to coordinate and drive collaboration on solution design and remediation execution\n",
       "\n",
       "Required Qualifications:\n",
       "• 5+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n",
       "\n",
       "Desired Qualifications:\n",
       "• Experience in Data Management, Business Analysis, Analytics, Project Management.\n",
       "\n",
       "Posting End Date:\n",
       "24 Jun 2025\n",
       "• Job posting may come down early due to volume of applicants.\n",
       "\n",
       "We Value Equal Opportunity\n",
       "\n",
       "Wells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n",
       "\n",
       "Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n",
       "\n",
       "Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n",
       "\n",
       "Applicants with Disabilities\n",
       "\n",
       "To request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n",
       "\n",
       "Drug and Alcohol Policy\n",
       "\n",
       "Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n",
       "\n",
       "Wells Fargo Recruitment and Hiring Requirements:\n",
       "\n",
       "a. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n",
       "\n",
       "b. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.</td><td>eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgTWFuYWdlbWVudCBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiV2VsbHMgRmFyZ28iLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ik5YbnE1ZlFtc3M5WXNVQ0JBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Senior Data Analyst, Marketing Science</td><td>CRUNCHYROLL</td><td>Hyderabad, Telangana, India</td><td>About the role\n",
       "\n",
       "We are looking for a hands-on Senior Marketing Science Analyst with a passion for understanding data, tracking business trends, and storytelling to join the Center for Data and Insights team. This position will report to the Vice President, Business Intelligence & Analytics, and will collaborate with data engineers, campaign managers, and marketing leaders to investigate campaign trends, build business insights, and recommend proactive measures to enable better business decisions every day. The right person for this position is a service-minded, empathetic problem-solver who will be motivated by the opportunity to build a centralized insights service team from the ground up!\n",
       "• Partner with Branch/ Adjust MMPs, direct response, and media buying agencies to define and implement campaign measurement of integrated marketing campaigns, including TV, OOH, Digital, and Social Media across a wide range of business lines (theatrical, streaming, e-commerce, etc.)\n",
       "• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n",
       "• Evaluating media measurement leveraging MMM (Marketing Mix Modeling), MTA (Multi-Touch Attribution), and Incrementality A/B Testing\n",
       "• Lead / Assist in development of testing roadmaps, measurement plans, KPI & KLI reporting based on set marketing objectives linked to key campaigns and always-on digital marketing tactics.\n",
       "• Deliver ad hoc analysis for client stakeholders throughout marketing, working with the channel leads, planning, and client service departments.\n",
       "• Partner with paid marketing teams in building weekly insight decks for the Marketing leadership team by collaborating effectively across teams, documenting the impact of strategic initiatives on benchmarks, and tracking the progress towards company goals.\n",
       "• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n",
       "• Connect ideas into cohesive, well-grounded recommendations, using creative, structured, and analytical thinking with the help of effective data visualization\n",
       "• Work with offshore and onsite teams and lead the sprint planning/management\n",
       "• Maintain a culture of high-quality output and outstanding customer service by effectively communicating at all levels, ensuring that work gets done, and responding effectively to\n",
       "\n",
       "About You\n",
       "• 5+ years of experience with data analysis, paid campaign analytics, statistics, experimentation, and optimization.\n",
       "• 3+ years of experience in writing complex SQL queries, experience in marketing data automation.\n",
       "• 3+ years of experience with data visualization tools like Tableau, Superset, etc.\n",
       "• Understanding of install and user-level paid campaign tracking to support multi-touch attribution via UTMs and MMP data for all major marketing channels\n",
       "• Proficiency in data analysis, including defining critical metrics, statistical and predictive modeling concepts, descriptive statistics, and experimental design\n",
       "• Experience in Marketing analytics tools like Google Analytics, Adjust, Braze, Branch, etc.\n",
       "• Experience working with large data sets (Terabytes of data/ billions of records).\n",
       "• Deep expertise in measuring marketing performance against lifetime value metrics.\n",
       "• Outstanding teamwork skills: You have excellent interpersonal skills. You’re a good listener. You place the success of the company ahead of any particular idea. Proven ability to work across a globally, matrixed organization.\n",
       "• BS in Statistics, Computer Science, Information Systems, or a related field\n",
       "\n",
       "About the Team\n",
       "\n",
       "The Center for Data and Insights (CDI) is the centralized team of data engineering, BI, analytics, and data science experts, passionate about servicing the organization with timely and certified reports and insights! The mission of the group is to inspire, support, and guide our stakeholders to be data-aware as well as build out the systems of intelligence to discover insights and act on them.\n",
       "\n",
       "Why you will love working at Crunchyroll\n",
       "\n",
       "In addition to getting to work with fun, passionate and inspired colleagues, you will also enjoy the following benefits and perks:\n",
       "• Best-in class medical, dental, and vision private insurance healthcare coverage\n",
       "• Access to counseling & mental health sessions 24/7 through our Employee Assistance Program (EAP)\n",
       "• Free premium access to Crunchyroll\n",
       "• Professional Development\n",
       "• Company's Paid Parental Leave\n",
       "• up to 26 weeks for birthing parents\n",
       "• up to 12 weeks for non-birthing parents\n",
       "• Hybrid Work Schedule\n",
       "• Paid Time Off\n",
       "• Flex Time Off\n",
       "• 5 Yasumi Days\n",
       "• Half-Day Fridays during the summer\n",
       "• Winter Break\n",
       "\n",
       "#LifeAtCrunchyroll #LI-Hybrid</td><td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBBbmFseXN0LCBNYXJrZXRpbmcgU2NpZW5jZSIsImNvbXBhbnlfbmFtZSI6IkNydW5jaHlyb2xsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiI3QU16TGktYmRTM1gybEV4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>4 days ago</td><td>Data Analyst</td></tr><tr><td>Principal Data Analyst</td><td>STORABLE</td><td>Serilingampalle (M), Hyderabad, Telangana, India</td><td>About the Role:\n",
       "We’re seeking a strategic, data-driven professional to lead our Business Analysis initiatives for our Marketplace product within the Self-Storage vertical. This is a ‘hands on keyboard’ high-impact role that bridges business strategy, analytical rigor, and data visualization —ideal for someone who thrives on translating data into actionable insights that drive performance. As the Principal Data Analyst, you will own our marketplace performance metrics, lead the development of world-class dashboards and reporting frameworks, and partner closely with Product, Marketing, Operations, and Finance teams to inform strategic decisions. You’ll drive both hands-on analysis and organizational data literacy while mentoring a team of analysts and collaborating with engineering on data infrastructure.\n",
       "Key Responsibilities:\n",
       "\n",
       "Own and continuously optimize key Marketplace KPIs (conversion, revenue yield, inventory, pricing, etc.).\n",
       "Serve as the steward of metric definitions and business logic—ensuring consistency, accuracy, and alignment across stakeholders.\n",
       "Have a deep understanding of how to run a BI environment. Proactive, insightful, curious.\n",
       "Build and scale world class self-service dashboards and Tableau reports for internal and executive stakeholders.\n",
       "Lead cross-functional analysis projects to uncover trends, diagnose problems, and identify growth and efficiency opportunities.\n",
       "Translate complex data into clear, actionable insights and concise narratives for business and executive audiences.\n",
       "Drive rigorous SQL-based analysis from raw datasets, ensuring high-quality, reproducible outputs.\n",
       "Guide and collaborate with junior analysts in other groups and collaborate with engineering to shape data pipelines and definitions.\n",
       "Collaborate with cross functional partners and teams (Product,Marketing, Finance, Business Insights) to test hypotheses, evaluate feature/campaign performance, and measure outcomes.\n",
       "Identify data gaps or integrity risks and work with stakeholders to improve reliability and accuracy.\n",
       "Champion a culture of curiosity, experimentation, and evidence-based decision-making.\n",
       "Proactively keep other teams in the loop on new data capabilities unlocked and changes being made.\n",
       "\n",
       "Requirements:\n",
       "\n",
       "5-7 years of experience in business analysis, data analytics, or related roles, ideally with at least 2+ years in a leadership capacity.\n",
       "Advanced proficiency in SQL with experience working with complex data models and the ability to query complex relational datasets.\n",
       "Other data engineering experience is a significant plus to facilitate sourcing/formating of data.\n",
       "Deep understanding of performance metrics and business levers in a supply/demand or marketplace environment.\n",
       "Experience leading analytics efforts in partnership with cross-functional teams including Product, Marketing, Finance, and Engineering.\n",
       "Proven track record of developing intuitive, scalable dashboards using Tableau (or similar BI platforms).\n",
       "Strong business acumen with the ability to connect the dots between business objectives and analytical frameworks.\n",
       "Exceptional communication skills with the ability to distill technical findings for non-technical audiences.\n",
       "Capable of influencing and informing executive stakeholders with clear, concise insights.\n",
       "Demonstrated experience owning KPIs, building data products, and drive insights into action in a fast paced environment.\n",
       "Ability to navigate ambiguity, manage and prioritize competing needs, and work cross-functionally.\n",
       "\n",
       "Preferred Qualifications:\n",
       "\n",
       "Experience in the storage, real estate, or marketplace industries strongly preferred\n",
       "Familiarity with modern data stack tools such as Snowflake, dbt, or similar Experience analyzing marketplace dynamics or supply/demand business models\n",
       "Exposure to experimentation frameworks, A/B testing, or uplift modeling\n",
       "Prior exposure to high-growth SaaS or Marketplace operations\n",
       "Data engineering capabilities—especially in sourcing, transforming, or centralizing disparate datasets.\n",
       "\n",
       "About Us:\n",
       "At Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\n",
       "We leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.\n",
       "Important Notice: Protect Yourself from Fraudulent Activities Targeting Job Seekers\n",
       "We’ve been made aware of fraudulent activities where an individual or group is pretending to represent Storable, attempting to deceive job seekers by using our company’s good name and stellar reputation.\n",
       "To protect yourself, please consider the following guidelines:\n",
       "– Official Communication: All genuine communication from Storable will come from official email addresses, ending in “@storable.com.” Be cautious of any communication that doesn’t match this criteria.– No Unsolicited Offers: We do not extend job offers without a formal interview process. If you receive an unsolicited job offer claiming to be from Storable or any of its representatives, it’s a red flag.– Verification: If you’re uncertain about the legitimacy of any job offer or communication claiming to be from Storable, please directly contact our HR department directly at POps@storable.com for verification.\n",
       "Your security and trust are paramount to us. If you suspect you’ve been contacted by someone falsely claiming to be from Storable or using our company’s name for any dubious purpose, please immediately report the incident to POps@storable.com\n",
       "We’re committed to ensuring a transparent and secure hiring process.\n",
       "Thank you for your vigilance and interest in joining our team.</td><td>eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiU3RvcmFibGUiLCJhZGRyZXNzX2NpdHkiOiJTZXJpbGluZ2FtcGFsbGUgKE0pLCBIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InljaTdhWW0wWnR2MEMzbkZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9</td><td>18 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst 1</td><td>UNITEDHEALTH GROUP</td><td>India</td><td>At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.\n",
       "\n",
       "Primary Responsibilities:\n",
       "• Validate data with administrative source systems (source of truth)\n",
       "• Analyze complex datasets\n",
       "• Generate actionable insights and recommendations based on data analysis\n",
       "• Database Management:\n",
       "• Develop and maintain data models, data dictionaries, and other documentation\n",
       "• Troubleshoot and resolve database-related issues\n",
       "• Data Extraction and Transformation:\n",
       "• Support capital project that will, in part, import and transform data from various sources for broker bonus calculations\n",
       "• Ensure data integrity and quality through rigorous validation and testing\n",
       "• Data Visualization and Reporting:\n",
       "• Create visually appealing and informative dashboards and reports\n",
       "• Present findings and insights to both technical and non-technical stakeholders in a clear and concise manner\n",
       "• Continuous Learning and Improvement:\n",
       "• Stay up to date with the latest data analysis techniques and tools\n",
       "• Identify opportunities to improve data analysis processes and methodologies\n",
       "• Actively participate in knowledge sharing and mentoring within the team\n",
       "• Comply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n",
       "\n",
       "Required Qualifications:\n",
       "• Undergraduate degree or equivalent experience\n",
       "• 4+ years Experience as SAS Data Analyst\n",
       "• Experience with relational database management systems (e.g., MySQL, Oracle, SQL Server)\n",
       "• Experience with statistical analysis\n",
       "• Familiarity with data visualization tools (e.g., Tableau, Power BI)\n",
       "• Proven excellent problem-solving and critical thinking skills\n",
       "• Proven solid communication and presentation skills to effectively convey complex data analysis findings to both technical and non-technical stakeholders\n",
       "• Proven ability to work independently and collaboratively in a fast-paced, deadline-driven environment\n",
       "• Proven detail-oriented with a focus on accuracy and data integrity\n",
       "\n",
       "At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone - of every race, gender, sexuality, age, location and income - deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.\n",
       "\n",
       "#NTRQ</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IlVuaXRlZEhlYWx0aCBHcm91cCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJhdHFPMEswckdadGwwUy1xQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==</td><td>3 days ago</td><td>Data Analyst</td></tr><tr><td>Data Analyst – Competitive Benchmarking & Reporting</td><td>REPUTATION</td><td>Hyderabad, Telangana, India</td><td>Why Work at Reputation?\n",
       "• Reputation reached over $100m in Annual Recurring Revenue (ARR) in 2022 and continues to grow worldwide.\n",
       "• We've raised over $200 million in funding from A-list venture capital firms such as Bessemer Venture Partner and Kleiner Perkins, including $150 million in equity financing from Marlin Equity Partners in January 2022.\n",
       "• Reputation is trusted by more than 250 partners, including Google, Facebook, Salesforce, J.D. Power, Amazon and Web.com.\n",
       "• Our industry leading platform has been recognized by Forrester and Gartner as a vendor of choice in Voice of the Customer, Customer Feedback Management, and Social Suites research reports.\n",
       "• The platform is used by 10+ major automotive OEMs and 16,000 auto dealerships, more than 250 healthcare systems, and over 100 leading property management firms.\n",
       "• Our executive management team is committed to building a performance-based culture where excellence is rewarded and careers are developed.\n",
       "• Who thrives at Reputation? Managers who embody a player-coach mentality. Employees who value teamwork and cross-functional collaboration. People who emphasize perseverance and hustle over quick wins and luck.\n",
       "• Our Mission: We exist to forge relationships between companies and communities.\n",
       "\n",
       "We are seeking a Data Analyst with expertise in data stewardship and analysis to manage our competitive benchmark data tenants and produce customer-facing reports for marketing and sales. The ideal candidate is proficient in BigQuery (SQL), detail-oriented, and capable of working independently on complex data projects.\n",
       "\n",
       "Responsibilities:\n",
       "• Data Stewardship & Governance: Ensure data accuracy, integrity, and accessibility by managing competitive benchmark data tenants, implementing verification processes, and maintaining governance best practices.\n",
       "• Data Pipeline Management: Develop and maintain data ingestion pipelines, including API integrations and web scraping, ensuring timely updates and scalability.\n",
       "• Data Validation & Quality Control: Oversee large-scale data input and validation, balancing hands-on data manipulation with automation strategies. Collaborate with data owners to resolve quality issues.\n",
       "• Advanced Analytics & Reporting: Conduct in-depth data analysis to uncover trends, patterns, and insights that drive business decisions. Scope, design, and execute analysis projects with minimal supervision.\n",
       "• Industry & Market Insights: Develop data-driven industry reports for marketing and sales, leveraging analytics to provide insights on market trends and competitive landscapes.\n",
       "• Cross-Functional Collaboration: Work closely with product managers, engineers, marketing, and leadership to resolve data issues, provide training, and ensure alignment on data best practices.\n",
       "• Tool & Process Optimization: Design scalable internal tools and reporting solutions to enhance data accessibility, automation, and usability across teams.\n",
       "• Stakeholder Communication: Translate complex data insights into actionable recommendations for key stakeholders, including non-technical audiences.\n",
       "\n",
       "Qualifications:\n",
       "• 5+ years of proven experience in data analysis, data stewardship, and managing large-scale datasets, with a strong focus on ensuring data accuracy, integrity, and accessibility.\n",
       "• Must be proficient with SQL and NoSQL (BigQuery), as well as have the ability to thrive in an environment that often requires patience and creativity to query, extract and validate data in which there are high levels of uncertainty.\n",
       "• Strong prior professional experience managing databases and using applicable tools is required.\n",
       "• Experience with and knowledge of ETL processes and data migration.\n",
       "• Understanding of and prior experience with General Data Protection Regulation.\n",
       "• Demonstrated experience and proficiency in using Python in real-world, business applications are substantial advantages to candidates.\n",
       "• Highly articulate with outstanding verbal and written communication skills; you are able to explain data-driven insights and analyses intuitively to clients, executives, and technical and non-technical coworkers in both large and small groups.\n",
       "• You should be comfortable taking feedback from everyone – Product Managers, Engineers, customers, the CEO, etc. – to help understand business objectives and develop the most powerful analyses and tools possible.\n",
       "• Proven ability to operate in a fast-paced, data-driven environment.\n",
       "\n",
       "When you join Reputation, you can expect:\n",
       "• Flexible working arrangements.\n",
       "• Career growth with paid training tuition opportunities.\n",
       "• Active Employee Resource Groups (ERGs) to engage with.\n",
       "• An equitable work environment.\n",
       "\n",
       "Our employees say it best:\n",
       "\n",
       "According to Glassdoor, 94% of our employees approve of our CEO, Joe Burton.\n",
       "\n",
       "Our employees highlight our:\n",
       "• Ample Opportunities- “There are many opportunities to learn and grow. Many open roles are replaced with internal promotions.”\n",
       "• Positive Culture- \"Great opportunity and exceptional culture.\" “You will never have a better culture anywhere else. Period.”\n",
       "• Training and Tools- “All managers truly want you to succeed, and you are given great tools and training to be successful in your role.”\n",
       "• Balance- “Great work life balance and awesome team environment!”\n",
       "\n",
       "Diversity Programs & Initiatives:\n",
       "\n",
       "Our Reputation Nation spans around the world. This global perspective allows us to intentionally unlock the magic that comes from diversity of experience to contribute to our success.\n",
       "\n",
       "At Reputation, we believe in:\n",
       "• Diversity: Embracing a culture that values uniqueness.\n",
       "• Inclusion: Inviting diverse groups to take part in company life.\n",
       "• Belonging: Helping each individual feel accepted for who they are.\n",
       "\n",
       "\"At Reputation, we see diversity and inclusion as the foundation for an equitable workplace. Our goal is to empower all of our employees, regardless of their background, to make an impact in their work each and every day.\" - Joe Burton, CEO, Reputation\n",
       "\n",
       "Additionally, we offer a variety of benefits and perks, such as:\n",
       "• Health Insurance & Wellness Benefits: Group Health Insurance: Medical Insurance with floater policy of up to 10,00,000 for employee + spouse + 2 dependent children + 2 parents / parent-in-laws\n",
       "• Maternity Benefits: Medical insurance up to 75,000 INR, 26 weeks of leave for birth, adoption or surrogacy\n",
       "• Life Insurance: Insurance at 3x annual cost to the company (Term Insurance, GPA)\n",
       "• Accident/Disability Insurance: Insured at 3x base salary for permanent total disability, permanent partial disability and temporary total disability (GPA)\n",
       "• OPD: of 7500 per annum per employee\n",
       "\n",
       "Leaves\n",
       "• 10 Company observed holidays a year (Refer to the Holiday Calendar for the Year)\n",
       "• 12 Casual/Sick leaves (Pro-rata calculated)\n",
       "• 02 Earned Leaves per Month (Pro-rata calculated)\n",
       "• 04 Employee Recharge days (aka company holiday/office closed)\n",
       "• Maternity & Paternity (6 months)\n",
       "• Bereavement Leave (10 Days)\n",
       "\n",
       "Car Lease:\n",
       "Reputation offers Car Lease Program that allows employees to lease a car with no upfront cost or down payment. They benefit from a fixed monthly lease rental and 20-30% tax savings.\n",
       "\n",
       "We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n",
       "\n",
       "To learn more about how we handle the personal data of applicants, visit our Candidate Privacy Notice.\n",
       "\n",
       "Applicants only - No 3rd party agency candidates.</td><td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3Qg4oCTIENvbXBldGl0aXZlIEJlbmNobWFya2luZyBcdTAwMjYgUmVwb3J0aW5nIiwiY29tcGFueV9uYW1lIjoiUmVwdXRhdGlvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiT0NPR3UtU1dxWjh4X0gzS0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=</td><td>6 days ago</td><td>Data Analyst</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Lead Consultant - Technical Lead - Fullstack Data Engineer",
         "ASTRAZENECA",
         "Chennai, Tamil Nadu, India",
         "Job Title: Lead Consultant - Technical Lead - Data, Analytics & AI\nCareer Level: E\n\nIntroduction to role: Are you ready to redefine an industry and change lives? Join our Operations IT organization, where your work will have a direct impact on patients by redefining our ability to develop life-changing medicines. We empower the business to perform at its peak, combining innovative science with leading digital technology platforms and data. As part of the Data Analytics and AI (DA&AI) group, you'll deliver innovative solutions that drive the transformation of medicine development. As the Technical Lead, you'll oversee the technical delivery of products and projects aligned with business objectives, tapping into your expertise in Data and Cloud Engineering, Software Engineering, AI, and more. Collaborate with stakeholders to ensure seamless delivery and maximum business impact!\n\nAccountabilities:\n• Bridge business needs with technical solutions by leading IT application design and implementation.\n• Collaborate with stakeholders to define and deliver requirements, translating them into detailed specifications.\n• Own the technical vision of products and compile detailed technical designs, refining user Epics/stories.\n• Optimize performance, mitigate risks, and ensure alignment to timelines and resource allocations.\n• Advise on industry trends and standard methodologies to enhance performance and business outcomes.\n• Provide technical direction and guidance to IT teams and business units.\n• Contribute to Data & Software Engineering standards and best practices.\n• Research new technologies to boost system performance and scalability.\n• Lead and mentor technical teams (Data Engineering, Cloud Engineering, Software Engineering) and work with AI/GenAI leads to foster collaboration and innovation.\n• Ensure platform stability, scalability, and simplicity while adhering to regulatory requirements and data security standards.\n• Foster continuous improvement and innovation.\n• Supervise technical leadership throughout the software development lifecycle and provide hands-on support in development, technical delivery, review, and testing while collaborating with global teams.\n• Apply central Enterprise Data Platforms and guide DevOps, DataOps, and MLOps teams to ensure standard methodologies.\n• Ensure data solutions align with FAIR principles and support end-to-end data science and machine learning.\n\nEssential Skills/Experience:\n• Minimum 10 years of experience in the design, development, and delivery of software and data engineering solutions.\n• Extensive technical expertise in Data Engineering, Software Engineering, and Cloud Engineering.\n• Strong foundational knowledge of AI Engineering principles and practices.\n• Deep understanding of DevOps, MLOps, and DataOps methodologies with practical implementation experience.\n• Demonstrated success in product development and/or product management, delivering complex solutions end-to-end.\n• Demonstrable ability to provide technical thought leadership across Data, Analytics, and AI domains.\n• Exceptional communication, customer management, and multi-functional collaboration skills.\n• Robust analytical and problem-solving abilities with a collaborative, team-oriented approach.\n• Hands-on experience driving innovation throughout the full product development lifecycle.\n• Solid understanding of Data Mesh and Data Product concepts and architectures.\n• Proficiency in Agile methodologies and facilitating iterative, multi-functional team delivery.\n• Hands-on experience designing, implementing, and optimizing data pipelines using leading ETL tools.\n• Skilled in architecting, deploying, and managing scalable, secure AWS cloud environments.\n• Proficient in workflow orchestration tools such as Apache Airflow.\n• Practical experience implementing DataOps practices with tools like DataOps.Live.\n• Strong expertise in data storage and analytics platforms such as Snowflake.\n• Ability to deliver actionable insights through business intelligence tools, including Power BI.\n• Extensive full-stack development experience, including backend proficiency with Node.js and Python and frontend expertise with ReactJS or NextJS.\n• Experience designing and deploying Generative AI solutions.\n• Hands-on implementation of AI/ML models using platforms such as Amazon SageMaker.\n• Advanced programming skills, especially in Python.\n• Solid knowledge of both SQL and NoSQL database technologies.\n• Familiarity with agile ways of working and iterative development environments.\n• Experience working in large, multinational organizations or pharmaceutical environments is highly desirable.\n• Demonstrated leadership and mentoring skills, with a demonstrable ability to develop high-performing technical teams.\n\nDesirable Skills/Experience:\n• Bachelor's or master's degree in health sciences, Life Sciences, Data Management, IT, or a related field.\n• Experience in the pharmaceutical industry or a similar multinational environment.\n• AWS Cloud or relevant data/software engineering certifications.\n• Awareness of use case specific GenAI tools available in the market and their application in day-to-day work scenarios.\n• Possess working knowledge of basic prompting techniques and continuously improve these skills.\n• Stay up to date with developments in AI and GenAI, applying new insights to work-related situations.\n\nWhen we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That's why we work, on average, a minimum of three days per week from the office. But that doesn't mean we're not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\n\nAt AstraZeneca, we are at a crucial stage of our journey to become a digital and data-led enterprise. Our commitment to innovation empowers us to make the impossible possible by building partnerships and ecosystems that drive scale and speed for exponential growth. With investment backing us all the way, we are focused on disrupting the industry while making a meaningful impact through our work. Here you'll find countless opportunities to learn, grow, and contribute to developing life-changing medicines.\n\nReady to make a difference? Apply now to join our team!\n\nDate Posted\n30-Jun-2025\n\nClosing Date\n\nAstraZeneca embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills. We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements.",
         "eyJqb2JfdGl0bGUiOiJMZWFkIENvbnN1bHRhbnQgLSBUZWNobmljYWwgTGVhZCAtIEZ1bGxzdGFjayBEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiQXN0cmFaZW5lY2EiLCJhZGRyZXNzX2NpdHkiOiJDaGVubmFpLCBUYW1pbCBOYWR1LCBJbmRpYSIsImh0aWRvY2lkIjoiRGVXa2pFZDgxNXcwSUpJY0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "3 days ago",
         "Data Engineer"
        ],
        [
         "AWS Data Engineer",
         "COGNIZANT",
         "Hyderabad, Telangana, India (+1 other)",
         "Job Summary:\n\nExperience : 4 - 8 years\n\nLocation : Bangalore\n\nThe Data Engineer will contribute to building state-of-the-art data Lakehouse platforms in AWS, leveraging Python and Spark. You will be part of a dynamic team, building innovative and scalable data solutions in a supportive and hybrid work environment. You will design, implement, and optimize workflows using Python and Spark, contributing to our robust data Lakehouse architecture on AWS. Success in this role requires previous experience of building data products using AWS services, familiarity with Python and Spark, problem-solving skills, and the ability to collaborate effectively within an agile team.\n\nMust Have Tech Skills:\n\n· Demonstrable previous experience as a data engineer.\n• Technical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\n\n· Proficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nNice To Have Tech Skills:\n\n· Familiar with data services in a Lakehouse architecture.\n\n· Familiar with technical design practices, allowing for the creation of scalable, reliable data products that meet both technical and business requirements\n\n· A master’s degree or relevant certifications (e.g., AWS Certified Solutions Architect, Certified Data Analytics) is advantageous\n\nKey Accountabilities:\n• Writes high quality code, ensuring solutions meet business requirements and technical standards.\n• Works with architects, Product Owners, and Development leads to decompose solutions into Epics, assisting the design and planning of these components.\n• Creates clear, comprehensive technical documentation that supports knowledge sharing and compliance. Experience in decomposing solutions into components (Epics, stories) to streamline development.\n• Actively contributes to technical discussions, supporting a culture of continuous learning and innovation.\n\nKey Skills:\n• Proficient in Python and familiar with a variety of development technologies.\n• Previous experience of implementing data pipelines, including use of ETL tools to streamline data ingestion, transformation, and loading.\n• Solid understanding of AWS services and cloud solutions, particularly as they pertain to data engineering practices. Familiar with AWS solutions including IAM, Step Functions, Glue, Lambda, RDS, SQS, API Gateway, Athena.\n• Proficient in quality assurance practices, including code reviews, automated testing, and best practices for data validation.\n• Experienced in Agile development, including sprint planning, reviews, and retrospectives\n\nEducational Background:\n• Bachelor’s degree in computer science, Software Engineering, or related essential.\n\nBonus Skills:\n• Financial Services expertise preferred, working with Equity and Fixed Income asset classes and a working knowledge of Indices.\n• Familiar with implementing and optimizing CI/CD pipelines. Understands the processes that enable rapid, reliable releases, minimizing manual effort and supporting agile development cycles.",
         "eyJqb2JfdGl0bGUiOiJBV1MgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkNvZ25pemFudCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiWjYyVVNZZkhtUHk0dHdwNkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 days ago",
         "Data Engineer"
        ],
        [
         "Data Engineer (Python, Pyspark, and Azure Databricks) || (4 - 6 Years)",
         "EMIDS",
         "Bengaluru, Karnataka, India",
         "Hi All,\n\nGreetings for the day!!\n\nWe are currently hiring for Data Engineer (Python, Pyspark, and Azure Databricks) for Emids(MNC) at Bangalore location.\n\nRole: Data Engineer\n\nExp: 5 to 8 Years\n\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\n\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\nNote: Candidate Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.\n\nRole Overview:\n\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\n• Lead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\n• Architect scalable data streaming and processing solutions to support healthcare data workflows.\n• Develop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\n• Ensure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\n• Collaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\n• Troubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\n• Mentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\n• Stay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\nRequired Skills & Qualifications:\n• 4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\n• Expertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\n• Experience with Azure Databricks (or willingness to learn and adopt it quickly).\n• Hands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\n• Proficiency in SQL, NoSQL databases, and data modeling for big data processing.\n• Knowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\n• Experience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\n• Strong analytical skills, problem-solving mindset, and ability to lead complex data projects.\n• Excellent communication and stakeholder management skills.\n\nNote: This is not a contract position, this will be a permanent position with Emids.\n\nInterested candidates Can Share Your Updated Profile with details for below Email.\n\nNAME:\n\nCCTC:\n\nECTC:\n\nNotice Period:\n\nOffers in Hand :\n\nEmail ID: Ravi.chekka@emids.com",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChQeXRob24sIFB5c3BhcmssIGFuZCBBenVyZSBEYXRhYnJpY2tzKSB8fCAoNCAtIDYgWWVhcnMpIiwiY29tcGFueV9uYW1lIjoiRW1pZHMiLCJhZGRyZXNzX2NpdHkiOiJCZW5nYWx1cnUsIEthcm5hdGFrYSwgSW5kaWEiLCJodGlkb2NpZCI6Im9TMEEyREhLeU9wd0lVdnVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "8 hours ago",
         "Data Engineer"
        ],
        [
         "Lead Data Engineer - Data Engineering",
         "CENCORA",
         "India",
         "Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\n\nJob Details\n\nPRIMARY DUTIES AND RESPONSIBILITIES:\n• Leads design, development, and automation of scalable Analytic data engineering products leveraging Machine Learning, cloud infrastructure\n• Leads the design, build and operationalization of monitoring and tracking of data quality and data flow dynamics\n• Optimizes existing data processes and implements best-in-class data transformation capabilities\n• Leverages advanced statistical and computational methodologies to deliver insights and identify strategic opportunities\n• Implements Data Ingestion Framework and other re-usable components leveraged for delivering data pipeline and supporting ad-hoc data ingestions\n• Assists with development and storage of analytics-ready data for development of analytic deliverables\n• Recommends data products to solve business problems meeting multiple stakeholder requirements\n• Drives project planning processes, delegates non-complex tasks to junior team members\n• Mentors other team members and assists them with priority setting and issue resolution\n• Maintains data and information environment that provides consistent and accurate reporting of the client’s data\n• Leverages Machine Learning to enhance the developed solution\n• Collaborates with Business and Client representatives to target strategic analytics opportunities and define deliverable scope\n• Facilitates the resolution of issues regarding projects being worked on to include proper documentation of recommendations for those issues\n• Manages multiple initiatives in parallel partnering with other Analytics team members and conducts review meetings with stakeholders to drive prioritization effort\n• Analyzes model errors and design strategies to overcome them\n• Implements best practices for data engineering to ensure quality delivery of enterprise solutions\n• Brings expert knowledge of data visualization tools and techniques to drive business analytics and semantic data access requirements.\n• Work closely with business users, vendors, and delivery teams to understand the business requirements that drive the analysis and design of business analytics and reporting solutions.\n• Conceptualize, design, and develop data visualization solutions that synthesize data concepts into clear communications for key business stakeholders.\n• Drives business stakeholder adoption of insights-driven decision making and/or business process innovation.\n• Leads knowledge transfer around using data visualizations to business stakeholders.\n• Assist in developing best practices for data presentation and sharing across the organization.\n• Ensures data visualization standards are maintained and implemented.\n• Demonstrate an ability to reduce data to the bare minimum of what is needed to optimally communicate a message.\n• Provides technical leadership, coaching and mentoring to team members and business users.\n• Participates in POC projects and provides business analytics solutions recommendations.\n• Evaluates new visualization tools and performs research on best practices.\n• Contributes to strategic planning meetings and provides guidance and expertise on system options, risk, cost vs. benefits, and impacts on business processes and goals.\n• Has expertise in multiple technical environments and possesses business knowledge that spans multiple business areas.\n• Responsible for BI Tool administration & security functions as designated\n\n.\n\nEDUCATIONAL QUALIFICATIONS:\n\nBachelor’s Degree in Statistics, Computer Science, Information Technology or any other related discipline or equivalent related experience.\n\nPreferred Certifications:\n• Advanced Data Analytics Certifications\n• AI and ML Certifications\n• SAS Statistical Business Analyst Professional Certification\n\nWORK EXPERIENCE:\n6+ years of directly-related or relevant experience, preferably in healthcare data analytics or data engineering.\n\nWorking Hours:\n\n7PM IST to 2AM IST; Hybrid Working Model\n\nSKILLS & KNOWLEDGE:\n\nBehavioral Skills:\n• Conflict Resolution\n• Creativity & Innovation\n• Decision Making\n• Planning\n• Presentation Skills\n• Risk-taking\n\nTechnical Skills:\n• Advanced Data Visualization Techniques\n• Advanced Statistical Analysis\n• Big Data Analysis Tools and Techniques\n• Data Governance\n• Data Management\n• Data Modelling\n• Data Quality Assurance\n• Machine Learning and AI Fundamentals\n• Programming languages like SQL, R, Python\n\nTools Knowledge:\n• Business Intelligence Software like Tableau, Power BI, Alteryx, QlikSense\n• Data Visualization Tools\n• Microsoft Office Suite\n• Statistical Analytics tools (SAS, SPSS3)\n\nWhat Cencora offers\n\n​Benefit offerings outside the US may vary by country and will be aligned to local market practice. The eligibility and effective date may differ for some benefits and for team members covered under collective bargaining agreements.\n\nFull time\n\nAffiliated Companies\nAffiliated Companies: CENCORA INDIA TECHNOLOGY SERVICES PRIVATE LIMITED\n\nEqual Employment Opportunity\n\nCencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\n\nThe company’s continued success depends on the full and effective utilization of qualified individuals. Therefore, harassment is prohibited and all matters related to recruiting, training, compensation, benefits, promotions and transfers comply with equal opportunity principles and are non-discriminatory.\n\nCencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com. We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned",
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgLSBEYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiQ2VuY29yYSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJHdzRzcW9FbWlPdEVnTDAwQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "3 days ago",
         "Data Engineer"
        ],
        [
         "Azure Data Engineer – Azure Databricks",
         "AIPRUS SOFTWARE PRIVATE LIMITED",
         "Bengaluru, Karnataka, India",
         "Job Title: Azure Data Engineer – Azure Databricks\n\nLocation: Bangalore, India\n\nExperience: 5 to 10 Years\n\nJob Summary:\n\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n\nKey Responsibilities:\n• Design and develop scalable data pipelines using Azure Databricks and PySpark.\n• Transform raw data into actionable insights through advanced data engineering techniques.\n• Build, deploy, and maintain machine learning models using MLlib, TensorFlow, and MLflow.\n• Optimize data integration workflows from Azure Blob Storage, Data Lake, and SQL/NoSQL sources.\n• Execute large-scale data processing using Spark Pools, fine-tuning configurations for performance and cost-efficiency.\n• Collaborate with data scientists, analysts, and business stakeholders to deliver robust data solutions.\n• Maintain and enhance Databricks notebooks and Delta Lake architectures.\n\nRequired Skills & Qualifications:\n• Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field.\n• 7+ years of experience in data engineering, with at least 3+ years specializing in Azure Databricks, PySpark, and Spark Pools.\n• Strong proficiency in:\n• Python, PySpark, Pandas, NumPy, SciPy\n• Spark SQL, DataFrames, RDDs\n• Delta Lake, Databricks Notebooks, MLflow\n• Hands-on experience with:\n• Azure Data Lake, Blob Storage, Synapse Analytics\n• Excellent problem-solving and communication skills.\n• Ability to work independently and in a collaborative team environment.\n\nPreferred Qualifications:\n• Experience with CI/CD pipelines for data workflows.\n• Familiarity with data governance and security best practices in Azure.\n• Knowledge of real-time data processing and streaming technologies.",
         "eyJqb2JfdGl0bGUiOiJBenVyZSBEYXRhIEVuZ2luZWVyIOKAkyBBenVyZSBEYXRhYnJpY2tzIiwiY29tcGFueV9uYW1lIjoiQWlwcnVzIFNvZnR3YXJlIFByaXZhdGUgTGltaXRlZCIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiek5tRUxOMG5Iam5zazl6TkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 hours ago",
         "Data Engineer"
        ],
        [
         "Principle Software Engineer for Data Platform - 31866",
         "SPLUNK",
         "Bengaluru, Karnataka, India",
         "Splunk, a Cisco company, is building a safer and more resilient digital world with an end-to-end full stack platform made for a hybrid, multi-cloud world. Leading enterprises use our unified security and observability platform to keep their digital systems secure and reliable. Our customers love our technology, but it's our caring employees that make Splunk stand out as an amazing career destination. No matter where in the world or what level of the organization, we approach our work with kindness. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Come help organizations be their best, while you reach new heights with a team that has your back.Role SummarySplunk AppDynamics, a leader in observability solutions for both cloud and on-premises environments, empowers customers with end-to-end visibility across their entire application ecosystems, cloud, on-premises, or hybrid. Our team is dedicated to developing, maintaining, and supporting SaaS and on-premises offerings within the observability product suite.We're looking for a Senior Software Engineer to build upon our application and data platform as we continue to innovate on application observability. We move fast and iterate quickly. We are passionate about solving customers’ problems. We have ambitious goals to build best-of-its-kind products. You will help the team win in a fast-growing market. If you are passionate about innovation and embrace the challenge of working on highly scalable systems that handle large volumes of data, this position is for you.Meet the Team (Optional)Data platform is the heart of the AppDynamics architecture. We process more than 100 billion data packets per day, more than 5 peta byte of data flows through our platform. We are in 8 regions having 40 clusters of 7000 nodes. We ingest, process and store all the data that power the dashboards that our customers rely on for observing their infrastructure and their applications.What you'll get to do\n• Design and build highly scalable solutions\n• Work with a team of exceptionally capable and dedicated peers, all the way from engineering to product management and customer support\n• Work in an open environment, work together to get things done and adapt to the team's changing needs\n• Leverage technologies including Kafka, ElasticSearch, Docker, and Kubernetes across different cloud environments like AWS and Azure\n• lead critical initiatives for the organisation\nMust-have Qualifications\n• 15+ years of full-stack developer experience in designing and developing highly scalable, distributed applications, products, and services.\n• Expertise in Java programming language.\n• Strong proficiency in data structures, algorithms, threads, concurrent programming\n• Extensive knowledge of SQL and at least one relational database engine: MySQL. Hands on experience in RDS or NoSQL (Dynamo, MongoDB) is a big plus\n• Experience of building applications using microservices architecture with expertise in Dockers and Kubernetes.\n• Strong communication skills, both verbal and written. Ability to multi-task and adapt quickly to changing requirements, scope, and priorities.\n• Optimize data pipelines, storage systems, and query engines for performance and efficiency.\n• Work closely with product teams, data engineers, software developers, and product managers to deliver on business goals.\n• Mentor team members in architecture principles, coding best practices, and system design.\n• Ensure robust monitoring, logging, and alerting systems for proactive issue detection and resolution.\n• Support CI/CD processes and automate testing for data systems\n• Stay abreast of emerging trends in big data, machine learning, and distributed systems to recommend innovative solutions.\nNice-to-have QualificationsWe’ve taken special care to separate the must-have qualifications from the nice-to-haves. “Nice-to-have” means just that: Nice. To. Have. So, don’t worry if you can’t check off every box. We’re not hiring a list of bullet points–we’re interested in the whole you.\n• Added advantage of having an experience in working on Cloud Observability Space.\n• experience of other languages like python, etc\n• experience of front-end technologies\nSplunk is an Equal Opportunity EmployerSplunk, a Cisco company, is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.\n\nNote:",
         "eyJqb2JfdGl0bGUiOiJQcmluY2lwbGUgU29mdHdhcmUgRW5naW5lZXIgZm9yIERhdGEgUGxhdGZvcm0gLSAzMTg2NiIsImNvbXBhbnlfbmFtZSI6IlNwbHVuayIsImFkZHJlc3NfY2l0eSI6IkJlbmdhbHVydSwgS2FybmF0YWthLCBJbmRpYSIsImh0aWRvY2lkIjoiY2k4emNxSnNNanNSemNYUEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "9 days ago",
         "Data Engineer"
        ],
        [
         "Software Developer- Python",
         "BNP PARIBAS INDIA SOLUTIONS",
         "India",
         "About BNP Paribas India Solutions:\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union’s leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n\nAbout BNP Paribas Group:\n\nBNP Paribas is the European Union’s leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group’s commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n\nCommitment to Diversity and Inclusion\n\nAt BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n\nAbout Business line/Function:\n\nThe Intermediate Holding Company (“IHC”) program structured at the U.S. level across poles of activities of BNP Paribas provides guidance, supports the analysis, impact assessment and drives adjustments of the U.S. platform’s operating model due to the drastic changes introduced by the Enhanced Prudential Standards (“EPS”) for Foreign Banking Organizations (“FBOs”) finalized by the Federal Reserve in February 2014, implementing Section 165 of U.S. Dodd-Frank Act.\n\nThe IT Transversal Team is part of the Information Technology Group which works simultaneously on a wide range of projects arising from business, strategic initiatives, and regulatory changes and reengineering of existing applications to improve functionality and efficiency.\n\nJob Title:\n\nPython Developer\n\nDate:\n\nJune-25\n\nDepartment:\n\nITG- Fresh\n\nLocation:\n\nChennai, Mumbai\n\nBusiness Line / Function:\n\nFinance Dedicated Solutions\n\nReports to:\n\n(Direct)\n\nGrade:\n\n(if applicable)\n\n(Functional)\n\nNumber of Direct Reports:\n\nNA\n\nDirectorship / Registration:\n\nNA\nPosition Purpose\n\nThe Python Developer will play a critical role in building and maintaining financial applications and tools that support data processing, analysis, and reporting within a fast-paced financial services environment. This position involves developing scalable and secure systems. The developer will collaborate with business analysts, finance users/or finance BA to translate complex business requirements into efficient, high-quality software solutions.\n\nA strong understanding of financial concepts, data integrity, and regulatory compliance is essential. The detailed responsibilities are mentioned below.\n\nResponsibilities\n\nDirect Responsibilities\n\n- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n\n- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n\n- Expertise in PySpark for large-scale data processing and loading into databases.\n\n- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n\n- Strong communication skills to effectively collaborate with team members and stakeholders.\n\n- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\nTechnical & Behavioral Competencies\n\n- Proficient in object-oriented programming, especially Python, with a minimum of 6-8 years of core python development experience.\n\n- Strong competency with Python libraries such as Pandas and NumPy for data wrangling, analysis, and manipulation.\n\n- Expertise in PySpark for large-scale data processing and loading into databases.\n\n- Proficiency in data querying and manipulation with Oracle and PostgreSQL.\n\n- Strong communication skills to effectively collaborate with team members and stakeholders.\n\n- Familiarity with the Software Development Life Cycle (SDLC) process and its various stages, including experience with JIRA and Confluence.\n\n- Good analytical, problem solving, & communication skills\n\n- Engage in technical discussions and to help in improving the system, process etc\n\nNice to Have\n\n- Familiarity with Plotly and Matplotlib for data visualization of large datasets.\n\n- Skilled in API programming, handling JSON, CSV, and other unstructured data from various systems.\n\n- Familiarity with JavaScript, CSS, and HTML.\n\n- Experience with cloud architecture applications such as Dataiku or Databricks; competency with ETL tools.\n\n- Knowledge of regulatory frameworks, RISK, CCAR, and GDPR.\nSpecific Qualifications (if required)\n\nSkills Referential\n\nBehavioural Skills: (Please select up to 4 skills)\n\nAbility to collaborate / Teamwork\n\nCritical thinking\n\nAbility to deliver / Results driven\n\nCommunication skills - oral & written\n\nTransversal Skills: (Please select up to 5 skills)\n\nAnalytical Ability\n\nAbility to develop and adapt a process\n\nAbility to understand, explain and support change\n\nAbility to develop others & improve their skills\n\nChoose an item.\n\nEducation Level:\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 5 years\n\nOther/Specific Qualifications (if required)",
         "eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBEZXZlbG9wZXItIFB5dGhvbiIsImNvbXBhbnlfbmFtZSI6IkJOUCBQYXJpYmFzIEluZGlhIFNvbHV0aW9ucyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ6VTdCX1R2emRzczk0M0ZoQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "7 days ago",
         "Python Developer"
        ],
        [
         "Freelance Python Developer",
         "TEQLAWN",
         "Anywhere",
         "We’re looking for an experienced Python Developer (4–7 years) to build scalable web and application solutions, with opportunities to integrate AI capabilities. Experience in AI integration is a must.\n\nResponsibilities:\n• Develop scalable web and application solutions using Python, with integration of AI/ML components\n• Collaborate with clients to understand project goals and technical requirements\n• Write clean, maintainable, and well-documented code\n• Troubleshoot, debug, and optimize applications for performance and reliability\n• Ensure timely and efficient delivery of milestones and final deliverables\n• Participate in code reviews and contribute to maintaining coding standards and best practices\n• Work with relevant frameworks and libraries such as Django, Flask, FastAPI, NumPy, pandas, and scikit-learn\n\nNote: Please share the link to your portfolio along with your application.\n\nJob Types: Full-time, Contractual / Temporary, Freelance\nContract length: 2 months\n\nPay: ₹50,000.00 - ₹80,000.00 per month\n\nBenefits:\n• Work from home\n\nLocation Type:\n• Remote\n\nSchedule:\n• Day shift\n• Monday to Friday\n\nExperience:\n• Python Development: 4 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlRlcWxhd24iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiY20xcUU4aHhjbFQ3dnVyT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "1 day ago",
         "Python Developer"
        ],
        [
         "Python Developer — Full-Time | 1-2 years Exp | In-Office | Bangalore",
         "SERP HAWK",
         "India",
         "\uD83D\uDE80 We’re Hiring: Python Developer\n\nSERP Hawk is looking for a talented Python Developer to join our dynamic team in Bangalore!\n\n\uD83C\uDF1F About Us\n\nSERP Hawk is a leading digital marketing and technology solutions company based in Bangalore. We specialize in building robust backend systems, scalable applications, and AI-powered solutions for clients across various industries.\n\n\uD83D\uDCCD Location:B-1, Bannerghatta Slip Road, KEB Colony, New Gurappana Palya, 1st Stage, BTM Layout 1, Bengaluru, Karnataka 560029\n\n\uD83C\uDF10 Website: www.serphawk.com\n\n\uD83D\uDCBC What You’ll Do\n• Design and develop scalable backend architectures.\n• Write clean, efficient Python code.\n• Integrate APIs and databases.\n• Implement CI/CD pipelines and automated tests.\n• Ensure high performance, security, and reliability.\n\n✅ What We’re Looking For\n\n✔️ 1–2 years of experience in Python development.\n\n✔️ Proficiency in frameworks like Django, Flask, or FastAPI.\n\n✔️ Strong understanding of APIs and databases.\n\n✔️ Experience with CI/CD tools and best practices.\n\n✔️ Excellent problem-solving skills and a collaborative mindset.\n\n\uD83D\uDCA1 Nice to Have\n\n⭐ Experience with AI/chatbots.\n\n⭐ Knowledge of cloud services and containerization.\n\n\uD83D\uDCB0 Salary\n• ₹20,000 – ₹25,000 per month (based on skills and experience).\n\n\uD83D\uDCCC Additional Details\n\n\uD83D\uDCBC This is a full-time, in-office role in Bangalore.\n\n\uD83C\uDFE2 Candidates must report to the office daily.\n\n\uD83C\uDF10 Should be flexible to work and attend meetings or calls as per client time zones.\n\n✨ Apply now and grow with us!",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAlCBGdWxsLVRpbWUgfCAxLTIgeWVhcnMgRXhwIHwgSW4tT2ZmaWNlIHwgQmFuZ2Fsb3JlIiwiY29tcGFueV9uYW1lIjoiU0VSUCBIYXdrIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjNBMWtiUmowZFhsRVZsWnlBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 hours ago",
         "Python Developer"
        ],
        [
         "SQL + Python",
         "WISSEN TECHNOLOGY",
         "India",
         "Wissen Technology is Hiring for SQL With Python\n\nAbout Wissen Technology:\n\nWissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges.\n\nRole Overview:\n\nWe are looking for a skilled and detail-oriented candidate with a strong foundation in SQL, Python, and data processing techniques. The ideal candidate is passionate about transforming raw data into meaningful insights and has hands-on experience across the data pipeline—from data wrangling to visualization.\n\nExperience: 3-7 Years\n\nLocation: Bengaluru\n\nRequired Skills:\n• Strong experience with SQL (e.g., joins, subqueries, CTEs, window functions).\n• Proficiency in Python for data manipulation (e.g., pandas, NumPy).\n• Experience working with relational databases like MySQL, PostgreSQL, SQL Server, or Oracle.\n• Hands-on experience in data wrangling, cleaning, and feature engineering.\n• Understanding of ETL processes and tools.\n• Familiarity with version control systems like Git.\n• Knowledge of data visualization techniques and tools.\n• Strong problem-solving and analytical skills.\n\nThe Wissen Group was founded in the year 2000. Wissen Technology, a part of Wissen Group, was established in the year 2015. Wissen Technology is a specialized technology company that delivers high-end consulting for organizations in the Banking & Finance, Telecom, and Healthcare domains. We help clients build world class products.\n\nWe offer an array of services including Core Business Application Development, Artificial Intelligence & Machine Learning, Big Data & Analytics, Visualization & Business Intelligence, Robotic Process Automation, Cloud Adoption, Mobility, Digital Adoption, Agile & DevOps, Quality Assurance & Test Automation.\n\nOver the years, Wissen Group has successfully delivered $1 billion worth of projects for more than 20 of the Fortune 500 companies. Wissen Technology provides exceptional value in mission critical projects for its clients, through thought leadership, ownership, and assured on-time deliveries that are always ‘first time right’.\n\nThe technology and thought leadership that the company commands in the industry is the direct result of the kind of people Wissen has been able to attract. Wissen is committed to providing them with the best possible opportunities and careers, which extends to providing the best possible experience and value to our clients.\n\nWe have been certified as a Great Place to Work® company for two consecutive years (2020-2022) and voted as the Top 20 AI/ML vendor by CIO Insider. Great Place to Work® Certification is recognized world over by employees and employers alike and is considered the ‘Gold Standard’. Wissen Technology has created a Great Place to Work by excelling in all dimensions - High-Trust, High-Performance Culture, Credibility, Respect, Fairness, Pride and Camaraderie.\n\nWebsite: www.wissen.com\n\nLinkedIn: https://www.linkedin.com/company/wissen-technology\n\nWissen Leadership: https://www.wissen.com/company/leadership-team/\n\nWissen Live: https://www.linkedin.com/company/wissen-technology/posts/feedView=All\n\nWissen Thought Leadership: https://www.wissen.com/articles/\n\nEmployee Speak:\n\nhttps://www.ambitionbox.com/overview/wissen-technology-overview\n\nhttps://www.glassdoor.com/Reviews/Wissen-Infotech-Reviews-E287365.htm\n\nGreat Place to Work:\n\nhttps://www.wissen.com/blog/wissen-is-a-great-place-to-work-says-the-great-place-to-work-institute-india/\n\nhttps://www.linkedin.com/posts/wissen-infotech_wissen-leadership-wissenites-activity-6935459546131763200-xF2k\n\nAbout Wissen Interview Process:\n\nhttps://www.wissen.com/blog/we-work-on-highly-complex-technology-projects-here-is-how-it-changes-whom-we-hire/\n\nLatest in Wissen in CIO Insider:\n\nhttps://www.cioinsiderindia.com/vendor/wissen-technology-setting-new-benchmarks-in-technology-consulting-cid-1064.html",
         "eyJqb2JfdGl0bGUiOiJTUUwgKyBQeXRob24iLCJjb21wYW55X25hbWUiOiJXaXNzZW4gVGVjaG5vbG9neSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJITW5vZUwxWlF1QWVma0hlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "7 hours ago",
         "Python Developer"
        ],
        [
         "Python Developer Full time",
         "VARIANCE TECHNOLOGIES PRIVATE LIMITED",
         "Anywhere",
         "Job Opportunity: Python Developer at Variance Technologies Private Limited!\n\nRole: Python Developer\n\nDuration: 1 Months\n\nLocation: Hybrid / Remote\n\nResponsibilities:\n\nCollaborate with our development team to develop and maintain Python-based applications tailored for finance-related tasks\n\nImplement object-oriented programming principles to ensure the scalability and maintainability of codebase\n\nGain hands-on experience with Pandas for data manipulation and analysis within finance contexts\n\nSupport integration of REST and WebSocket APIs for seamless communication with financial data providers and platforms\n\nAssist in automating financial modeling and analysis workflows using Python, including Excel automation for enhanced efficiency\n\nRequirements:\n\nCurrently pursuing or recently completed a degree in Computer Science, Finance, or a related field\n\nBasic proficiency in Python programming language, with a strong willingness to learn and grow\n\nExceptional attention to detail and proactive attitude towards problem-solving\n\nGenuine interest in the intersection of finance and technology\n\nBonus Skills:\n\nFamiliarity with fundamental financial concepts and markets\n\nExposure to Python libraries such as Pandas, NumPy, or SciPy\n\nDemonstrated interest in financial data analysis and visualization techniques\n\nBasic understanding of REST and WebSocket APIs\n\nPerks:\n\nHands-on experience working on real-world projects at the forefront of finance and technology\n\nMentorship and guidance from seasoned professionals in the field\n\nNetworking opportunities with industry experts to expand your professional connections\n\nFlexible scheduling to accommodate academic commitments\n\nPotential for transition to a full-time position based on exceptional performance and availability\n\nReady to kickstart your career with a prestigious 1-month internship at Variance Technologies Private Limited? Apply now by sending your resume and a tailored cover letter expressing your interest and qualifications to careers@variancefintech.com. Join our team and embark on a journey towards shaping the future of finance through technology!\n\nVariance Technologies Private Limited is committed to fostering a diverse and inclusive workplace where all individuals are empowered to thrive. We welcome applicants from all backgrounds and identities to apply.\n\nJob Type: Full-time\n\nPay: From ₹35,000.00 per month\n\nBenefits:\n• Work from home\n\nSchedule:\n• Monday to Friday\n\nEducation:\n• Bachelor's (Preferred)\n\nExperience:\n• Python: 1 year (Preferred)\n• total work: 1 year (Preferred)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIEZ1bGwgdGltZSIsImNvbXBhbnlfbmFtZSI6IlZhcmlhbmNlIFRlY2hub2xvZ2llcyBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiaXZCN3JUQTN5YnkxRzV2MUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "5 days ago",
         "Python Developer"
        ],
        [
         "Python Developer – AI & LLM Integrations",
         "DISCOVER WEBTECH PRIVATE LIMITED",
         "India",
         "We are hiring a skilled Python Developer with a minimum of 3 years of experience, who has hands-on expertise in developing AI modules, building intelligent agents using LLMs, and working with cutting-edge frameworks such as LangChain, LangGraph, and LangSmith.\n\nThe ideal candidate should be capable of designing, developing, and deploying backend services as well as intelligent AI-driven tools and systems.\n\nKey Responsibilities\n• Design and implement intelligent agents using LLM-based frameworks like LangChain, LangGraph, and LangSmith.\n• Build backend systems using Python (Django, FastAPI, or Flask).\n• Develop and integrate APIs, third-party tools, and cloud services.\n• Create AI modules that interact with knowledge bases, APIs, and perform multi-step reasoning.\n• Implement prompt engineering, memory chains, and agent behavior logic.\n• Collaborate with cross-functional teams to deliver robust AI features.\n• Optimize code for scalability, performance, and reliability.\n\nRequired Skills and Qualifications\n• 3+ years of hands-on experience with Python.\n• Proficiency in LangChain, LangGraph, or LangSmith.\n• Strong experience in working with LLMs like OpenAI, Cohere, or Anthropic.\n• Deep understanding of prompt engineering and agent orchestration.\n• Experience with APIs, JSON, and external integrations.\n• Knowledge of data storage systems (PostgreSQL, MongoDB).\n• Familiarity with Docker, Git, and CI/CD tools.\n• Excellent problem-solving and debugging skills.\n\nPreferred Qualifications\n• Knowledge of vector databases (e.g., Pinecone, Weaviate, FAISS).\n• Experience with microservices, asynchronous programming, and message queues (Celery, RabbitMQ).\n• Familiarity with frontend technologies (React.js or Vue.js) is a plus.\n• Exposure to cloud platforms such as AWS, GCP, or Azure.\n\nJob Types: Full-time, Permanent\n\nPay: ₹30,000.00 - ₹70,000.00 per month\n\nBenefits:\n• Health insurance\n\nSchedule:\n• Day shift\n\nWork Location: In person",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBBSSBcdTAwMjYgTExNIEludGVncmF0aW9ucyIsImNvbXBhbnlfbmFtZSI6IkRpc2NvdmVyIFdlYlRlY2ggUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Im1JUlJqRWJSV0pCSkdQR0VBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "3 days ago",
         "Python Developer"
        ],
        [
         "Full Stack Developer (Python / React JS)",
         "HITACHI CAREERS",
         "India",
         "Our Company\n\nWe're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n\nOur group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n\nImagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n\nThe team\n\nWe are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n\nThe role: Full Stack Developer/Specialist\n\nResponsibilities:\n• Design, develop, and maintain applications.\n• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n• Optimise applications for performance, scalability, and user experience.\n• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n• Provide training and support to end-users and IT staff on functionalities and best practices.\n• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n• Participate in project planning, execution, and post-implementation support.\n• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n• What you'll bring\nQualifications:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n• Expertise in development and customisation.\n• Proficiency in Python and React JS\n• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n• Experience with RESTful APIs and web services.\n• Excellent problem-solving skills and attention to detail.\n• Strong communication and collaboration skills.\n• Ability to work independently and as part of a team in a fast-paced environment.\n• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n• Prior experience in building AI applications is a plus.\n• Prior experience with automation tools like UIPath is a plus.\n• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\nPreferred Skills\n• Certification in\n• Experience with other low-code/no-code platforms.\n• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n• Experience with Agile development methodologies.\nAbout us\n\nWe're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n\n#LI-MS3\n\nChampioning diversity, equity, and inclusion\n\nDiversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n\nHow we look after you\n\nWe help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\nWe're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",
         "eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "14 days ago",
         "Python Developer"
        ],
        [
         "Generative AI & Backend Developer(python)",
         "INTELLYPOD",
         "Anywhere",
         "Job Description (JD) For Gen Ai with Python:\n\nWe're Hiring: GenAI & Backend Developer (Python)\n\nWork Location: Remote (Work From Home)\n\nExperience: 2+ Years\n\nImmediate Joiners Preferred\n\nCompany: IntellyPod\n\nApply at: hrd@intellypod.com | hr@intellypod.com\n\nAbout the Role:\n\nIntellyPod is looking for a passionate Backend Developer with hands-on experience in GenAI, LLMs, and Python. If you’re excited about building scalable APIs, integrating AI models, and working with the latest in backend and GenAI technologies — we’d love to connect!\n\nKey Responsibilities:\n\n· Develop and maintain Python-based backend services.\n\n· Design and implement RESTful APIs.\n\n· Integrate GenAI/LLM solutions into applications.\n\n· Manage and optimize SQL/NoSQL databases.\n\n· Collaborate with cross-functional tech teams.\n\nMust-Have Skills:\n\n· 2+ years of experience in backend development (Python).\n\n· Experience with GenAI and Large Language Models (e.g., GPT, LLaMA).\n\n· Strong knowledge of REST APIs and database design.\n\n· Familiarity with Git and backend architecture best practices.\n\nNeed to Have:\n\n· Experience with AWS/GCP/Azure.\n\n· Docker, Kubernetes, or CI/CD exposure.\n\n· Familiarity with vector databases (e.g., Pinecone, FAISS).\n\n· Prompt engineering or LLM fine-tuning knowledge.\n\nWhy Join Us?\n\n· 100% Remote – Flexible work setup\n\n· Work on next-gen AI products\n\n· Fast-growing, collaborative tech team\n\n· Opportunity to innovate with emerging AI tools\n\nReady to build the future with us? Send your resume to: hrd@intellypod.com | hr@intellypod.com #GenAI #BackendDeveloper #PythonJobs #LLM #RemoteJob #HiringNow #IntellyPod\n\nJob Type: Full-time\n\nPay: Up to ₹70,000.00 per month\n\nLocation Type:\n• Remote\n\nApplication Question(s):\n• Are an immediate joiner -\n\nAre on notice period if yes [Then how many days]\n• Write YES or NO\n\n1) Need to ask have you worked on LLM based project -\n\n2) Have you worked on chatbot types apps -\n\n3) Have you strong knowleged of OOps and Python basic -\n\n4) Have you knowledge of Rest APi development -\n\nExperience:\n• 5G: 3 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJHZW5lcmF0aXZlIEFJIFx1MDAyNiBCYWNrZW5kIERldmVsb3BlcihweXRob24pIiwiY29tcGFueV9uYW1lIjoiSW50ZWxseXBvZCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiI4ekp1Sk9ZUFlTbWtOcFRuQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "5 hours ago",
         "Python Developer"
        ],
        [
         "Etl Developer",
         "VIVID RESOURCING",
         "Bilaspur, Chhattisgarh, India",
         "Job Title:\nData Engineer / ETL Developer\n\nLocation:\nUS, remote from India\n\nDepartment:\nIT / Data & Analytics\n\nReports To:\nHead of Data & Analytics\n\nEmployment Type:\nContract, 12 months\n\nPay:\n$28-35 per hour payrolled or self-employed\n\nAbout the Role\n\nWe are seeking a Data Engineer / ETL Developer to help design, build, and maintain data pipelines that support our internal reporting and analytics needs. This role will focus on integrating data from systems such as ERP, MES , and production databases into a centralized data platform to enable reliable and timely insights across the business. You will work alongside BI developers, IT staff, and business users to ensure the smooth flow of data and contribute to the foundation of a modern business intelligence environment.\n\nKey Responsibilities\n\nETL & Data Pipeline Development\n• Develop and maintain ETL/ELT pipelines to move data from source systems (ERP, MES, SQL databases, flat files) into our centralized platform.\n• Use tools such as Azure Data Factory , SSIS , or similar to orchestrate and automate data workflows.\n\nData Modeling & Integration\n• Assist in designing and building data models and data marts optimized for Power BI dashboards.\n• Support the creation and maintenance of dataflows and datasets in Power BI Service.\n\nData Quality & Documentation\n• Implement data validation, transformation, and cleansing logic to ensure high-quality, reliable data.\n• Document data processes, business rules, and data mappings to support knowledge sharing and governance.\n\nCross-Functional Collaboration\n• Work with internal teams (production, quality, operations, finance) to understand data requirements and reporting needs.\n• Collaborate with BI developers to ensure the data pipeline supports efficient and user-friendly reporting.\n\nRequired Qualifications\n• Bachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\n• 2+ years of experience in data engineering or ETL development roles.\n• Solid experience with SQL and ETL tools (e.G., Azure Data Factory , SSIS , Informatica , etc.).\n• Familiarity with Power BI , including working with datasets, dataflows, or basic DAX.\n• Understanding of data integration, transformation, and warehousing concepts.\n• Experience working with or integrating manufacturing systems (ERP, MES) is a strong plus.\n\nPreferred Skills\n• Experience with cloud data platforms (especially Microsoft Azure ).\n• Exposure to Python or scripting for automation.\n• Familiarity with data governance and documentation practices.\n• Experience with manufacturing environments or industrial data is beneficial.\n\nSoft Skills\n• Strong attention to detail and a logical, structured approach to problem-solving.\n• Willingness to learn and grow in a fast-paced environment.\n• Good communication and collaboration skills across technical and non-technical teams.\n• Proactive and solutions-oriented mindset.",
         "eyJqb2JfdGl0bGUiOiJFdGwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiVml2aWQgUmVzb3VyY2luZyIsImFkZHJlc3NfY2l0eSI6IkJpbGFzcHVyLCBDaGhhdHRpc2dhcmgsIEluZGlhIiwiaHRpZG9jaWQiOiJueDl3cW1oMXpfcnBSX2dFQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "14 hours ago",
         "ETL Developer"
        ],
        [
         "Senior ETL and Backend Developer (Salesforce)",
         "S&P GLOBAL",
         "Hyderabad, Telangana, India (+1 other)",
         "About the Role:\n\nGrade Level (for internal use):\n10\n\nTitle: Senior ETL and Backend Developer (Salesforce)\n\nJob Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n\nThe Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n\nThe team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n\nThe Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n\nResponsibilities:\n• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\nMonitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\nQualifications:\n\nBasic Qualifications:\n• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n• A minimum of 8+ years of experience in software engineering & Architecture.\n• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n• A minimum of 3+ years of Salesforce development, administration/Integration.\n• Proficiency in Informatica PowerCenter and other ETL tools.\n• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n• Experience with Salesforce integration and administration.\n• Proficiency in backend development languages (e.g., Java, Python, C#).\n• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n• Excellent problem-solving skills and attention to detail.\n• Ability to work independently and as part of a team.\n• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n\nPreferred Qualifications:\n• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n• Experience with other ETL tools and data integration platforms.\n• Informatica Certified Professional\nSalesforce Certified Administrator or Developer\n• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n• Excellent problem solving, analytical and technical troubleshooting skills.\n• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat’s In It For You?\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nWe're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n\nOur benefits include:\n• Health & Wellness: Health care coverage designed for the mind and body.\n• Flexible Downtime: Generous time off helps keep you energized for your time on.\n• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring and Opportunity at S&P Global:\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\n-----------------------------------------------------------\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n-----------------------------------------------------------\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n\nJob ID: 316835\nPosted On: 2025-06-03\nLocation: Hyderabad, Telangana, India",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjJlUWpKemNOclM4MXR6QzhBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "30 days ago",
         "ETL Developer"
        ],
        [
         "Senior Etl Developer",
         "VIVID RESOURCING",
         "Bilaspur, Chhattisgarh, India",
         "Job Title:\nSenior Data Engineer / ETL Developer\n\nLocation:\nUS, from India\n\nDepartment:\nIT / Data & Analytics\n\nReports To:\nDirector of Data & Analytics\n\nEmployment Type:\nContract, 12 months\n\nPay:\n$30-38 per hour payrolled or self-employed\n\nAbout the Role\n\nWe are seeking a skilled and motivated Senior Data Engineer / ETL Developer to lead the design and implementation of end-to-end data integration solutions powering this new platform. You will work closely with IT, business stakeholders, and BI developers to build scalable data pipelines, robust data models, and a high-performance reporting environment centered around Power BI.\n\nThis role is a key contributor in shaping our data infrastructure and delivering timely, accurate insights across the organization—from the shop floor to the boardroom.\n\nKey Responsibilities\n\nData Engineering & Integration\n• Design, build, and maintain scalable ETL/ELT pipelines to extract, transform, and load data from diverse manufacturing systems (ERP, MES, IoT sensors, SQL databases, flat files, APIs).\n• Develop and manage data workflows using tools such as Azure Data Factory, SQL Server Integration Services (SSIS), or other modern data orchestration platforms.\n\nPlatform & Architecture Support\n• Contribute to the design and architecture of the new internal analytics platform, ensuring flexibility, scalability, and cost-efficiency.\n• Design and optimize data lake and data warehouse solutions using cloud-native tools (preferably Microsoft Azure).\n\nPower BI Enablement\n• Build and optimize semantic data models, dataflows, and datasets for use in Power BI.\n• Collaborate with Power BI developers and business analysts to ensure data models meet reporting requirements, including DAX performance tuning and efficient data structure design.\n\nData Governance & Quality\n• Implement data validation, cleansing, and monitoring processes to ensure data quality, reliability, and accuracy.\n• Contribute to data governance policies, including documentation, lineage tracking, and security controls.\n\nCollaboration & Mentorship\n• Work closely with cross-functional teams (IT, production, quality, finance, supply chain) to gather data requirements and deliver meaningful solutions.\n• Provide technical leadership and mentor junior developers or analysts in best practices for data engineering and analytics.\n\nRequired Qualifications\n• Bachelor’s or Master’s degree in Computer Science, Information Systems, Engineering, or a related field.\n• 5+ years of experience in data engineering, ETL development, or database architecture roles.\n• Proven experience with ETL tools such as Azure Data Factory, SSIS, or Informatica.\n• Advanced SQL skills, including performance tuning, stored procedures, and query optimization.\n• Strong experience with Power BI, including DAX, data modeling, and dataset optimization.\n• Experience with data warehousing (e.G., Azure Synapse, SQL Server, Snowflake) and cloud platforms (preferably Azure).\n• Understanding of manufacturing systems and data (e.G., ERP, MES, shop floor data, SCADA, historians).\n\nPreferred Skills\n• Experience with Python or other scripting languages for automation and data manipulation.\n• Familiarity with time-series data and integration from IoT or edge devices.\n• Knowledge of DevOps practices, CI/CD for data pipelines, and version control using Git.\n• Exposure to data governance frameworks and tools like Purview, Alation, or Collibra.\n• Power BI Service administration experience and integration with Power Platform (e.G., Power Automate, Power Apps)\n\nKey Competencies\n• Strong analytical and problem-solving skills with attention to detail.\n• Excellent communication skills, with the ability to bridge technical and business domains.\n• Self-starter with the ability to lead initiatives and work independently or collaboratively.\n• A passion for continuous improvement and innovation in a manufacturing setting.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRXRsIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IlZpdmlkIFJlc291cmNpbmciLCJhZGRyZXNzX2NpdHkiOiJCaWxhc3B1ciwgQ2hoYXR0aXNnYXJoLCBJbmRpYSIsImh0aWRvY2lkIjoiMnQxYXo2bnUzRU1abUlLLUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "14 hours ago",
         "ETL Developer"
        ],
        [
         "ETL Developer – IBM DataStage",
         "TATA CONSULTANCY SERVICES",
         "Hyderabad, Telangana, India",
         "Job Title: ETL Developer – IBM DataStage\n\nExperience: 5 to 10 years\n\nLocation: Hyderabad, Chennai, Mumbai, Bangalore, Ahmedabad, Indore\n\nEmployment Type: Full-time\n\nJob Summary:\n\nWe are seeking a skilled ETL Developer with hands-on experience in IBM DataStage to join our dynamic data engineering team. The ideal candidate will have a strong background in ETL development, data warehousing concepts, and performance optimization. This role involves designing and implementing robust ETL solutions, collaborating with cross-functional teams, and ensuring high data quality and integrity.\n\nKey Responsibilities:\n• Design, develop, and implement ETL processes using IBM DataStage.\n• Work across the full project lifecycle including low-level design, development, testing, and deployment.\n• Develop parallel jobs using various DataStage stages such as Copy, Join, Merge, Lookup, Funnel, Filter, Sort, Remove Duplicates, Aggregator, Change Capture, and Transformer.\n• Integrate data from heterogeneous sources including Oracle, SQL Server, and flat files.\n• Optimize and troubleshoot existing ETL processes for performance improvements.\n• Collaborate with data analysts and architects to understand business requirements and translate them into technical solutions.\n• Ensure data quality and integrity across multiple data sources.\n• Create and maintain technical documentation for ETL processes.\n• Participate in code reviews and adhere to ETL best practices.\n• Work in Agile environments and use tools like JIRA for tracking tasks and issues.\n• Demonstrate strong problem-solving skills and the ability to troubleshoot technical issues effectively.\n• Understand and support operational requirements as part of business delivery.\n\nRequired Skills:\n• Strong experience with IBM DataStage for ETL development and migration.\n• Solid understanding of database and data warehousing concepts.\n• Proficiency in SQL and UNIX.\n• Experience working with large datasets and complex data transformations.\n• Familiarity with Agile methodologies and tools like JIRA.\n• Excellent communication and collaboration skills.",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIOKAkyBJQk0gRGF0YVN0YWdlIiwiY29tcGFueV9uYW1lIjoiVGF0YSBDb25zdWx0YW5jeSBTZXJ2aWNlcyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoibkdQWmp0dFNpdlVKUWVEYkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "7 days ago",
         "ETL Developer"
        ],
        [
         "Senior Informatica Developer",
         "EVERESTDX INC",
         "Hyderabad, Telangana, India",
         "About the Company:\n\nEverest DX – We are a Digital Platform Services company, headquartered in Stamford. Our Platform/Solution includes Orchestration, Intelligent operations with BOTs’, AI-powered analytics for Enterprise IT. Our vision is to enable Digital Transformation for enterprises to deliver seamless customer experience, business efficiency and actionable insights through an integrated set of futuristic digital technologies.\n\nDigital Transformation Services - Specialized in Design, Build, Develop, Integrate, and Manage cloud solutions and modernize Data centers, build a Cloud-native application and migrate existing applications into secure, multi-cloud environments to support digital transformation. Our Digital\n\nPlatform Services enable organizations to reduce IT resource requirements and improve productivity, in addition to lowering costs and speeding digital transformation.\n\nDigital Platform - Cloud Intelligent Management (CiM) - An Autonomous Hybrid Cloud Management Platform that works across multi-cloud environments. helps enterprise Digital Transformation get most out of the cloud strategy while reducing Cost, Risk and Speed.\n\nTo know more please visit: http://www.everestdx.com\n\nResponsibilities:\n• Candidate should hands-on experience on ETL and SQL.\n• Design, develop, and optimize ETL workflows using Informatica PowerCenter.\n• Implement cloud-based ETL solutions using Informatica IDMC and IICS.\n• Should have expertise on all transformations in Power Center and IDMC/IICS.\n• Should have experience or knowledge on the PC to IICS migration using CDI PC tool or some other tool.\n• Lead data migration projects, transitioning data from on-premise to cloud environments.\n• Write complex SQL queries and perform data validation and transformation.\n• Conduct detailed data analysis to ensure accuracy and integrity of migrated data.\n• Troubleshoot and optimize ETL processes for performance and error handling.\n• Collaborate with cross-functional teams to gather requirements and design solutions.\n• Create and maintain documentation for ETL processes and system configurations.\n• Implement industry best practices for data integration and performance tuning.\n\nRequired Skills:\n• Hands-on experience with Informatica Power Center, IDMC and IICS.\n• Strong expertise in writing complex SQL queries and database management.\n• Experience in data migration projects (on-premise to cloud).\n• Strong data analysis skills for large datasets and ensuring accuracy.\n• Solid understanding of ETL design & development concepts.\n• Familiarity with cloud platforms (AWS, Azure).\n• Experience with version control tools (e.g., Git) and deployment processes.\n\nPreferred Skills:\n• Experience with data lakes, data warehousing, or big data platforms.\n• Familiarity with Agile methodologies.\n• Knowledge of other ETL tools",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgSW5mb3JtYXRpY2EgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiRXZlcmVzdERYIEluYyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiNDdnQ200aEh5aHpSWlhDOEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "2 days ago",
         "ETL Developer"
        ],
        [
         "Spark Engineer",
         "STAFFINGINE LLC",
         "India",
         "- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred",
         "eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "Spark Engineer"
        ],
        [
         "Databricks Engineer - Spark / PySpark",
         "ENKEFALOS TECHNOLOGIES LLP",
         "Anywhere",
         "Databricks Engineer – Spark / PySpark\n\nLocation : Remote / Mysore\n\nJoining : Immediate\n\nExperience : 5+ years\n\nResponsibilities :\n\nWill implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n\nRequirements:\n• PySpark (RDDs, DataFrames, performance tuning)\n• Building gold‐layer data models for financial reporting\n• Experience with complex joins, aggregations, GL hierarchies\n• Version handling (Actuals vs Budget), currency conversions\n\nJob Type: Full-time\n\nPay: ₹500,395.35 - ₹1,840,348.25 per year\n\nBenefits:\n• Flexible schedule\n• Paid sick time\n• Provident Fund\n• Work from home\n\nApplication Question(s):\n• Have you worked on ADF/ADLS ?\n• Do you have hands-on experience of Spark / PySpark\n\nExperience:\n• Databricks Engineering: 4 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "15 days ago",
         "Spark Engineer"
        ],
        [
         "Senior PySpark Data Engineer (Big Data, Cloud Data Solutions, & Python)",
         "SYNECHRON",
         "India",
         "Job Summary\n\nSynechron is seeking a skilled PySpark Data Engineer to design, develop, and optimize data processing solutions leveraging modern big data technologies. In this role, you will lead efforts to build scalable data pipelines, support data integration initiatives, and work closely with cross-functional teams to enable data-driven decision-making. Your expertise will contribute to enhancing business insights and operational efficiency, positioning Synechron as a pioneer in adopting emerging data technologies.\n\nSoftware Requirements\n\nRequired Software Skills:\n• PySpark (Apache Spark with Python) – experience in developing data pipelines\n• Apache Spark ecosystem knowledge\n• Python programming (versions 3.7 or higher)\n• SQL and relational database management systems (e.g., PostgreSQL, MySQL)\n• Cloud platforms (preferably AWS or Azure)\n• Version control: GIT\n• Data workflow orchestration tools like Apache Airflow\n• Data management tools: SQL Developer or equivalent\n\nPreferred Software Skills:\n• Experience with Hadoop ecosystem components\n• Knowledge of containerization (Docker, Kubernetes)\n• Familiarity with data lake and data warehouse solutions (e.g., AWS S3, Redshift, Snowflake)\n• Monitoring and logging tools (e.g., Prometheus, Grafana)\n\nOverall Responsibilities\n• Lead the design and implementation of large-scale data processing solutions using PySpark and related technologies\n• Collaborate with data scientists, analysts, and business teams to understand data requirements and deliver scalable pipelines\n• Mentor junior team members on best practices in data engineering and emerging technologies\n• Evaluate new tools and methodologies to optimize data workflows and improve data quality\n• Ensure data solutions are robust, scalable, and aligned with organizational data governance policies\n• Stay informed on industry trends and technological advancements in big data and analytics\n• Support production environment stability and performance tuning of data pipelines\n• Drive innovative approaches to extract value from large and complex datasets\n\nTechnical Skills (By Category)\n\nProgramming Languages:\n• Required: Python (PySpark experience minimum 2 years)\n• Preferred: Scala (for Spark), SQL, Bash scripting\n\nDatabases/Data Management:\n• Relational databases (PostgreSQL, MySQL)\n• Distributed storage solutions (HDFS, cloud object storage like S3 or Azure Blob Storage)\n• Data warehousing platforms (Snowflake, Redshift – preferred)\n\nCloud Technologies:\n• Required: Experience deploying and managing data solutions on AWS or Azure\n• Preferred: Knowledge of cloud-native services like EMR, Data Factory, or Azure Data Lake\n\nFrameworks and Libraries:\n• Apache Spark (PySpark)\n• Airflow or similar orchestration tools\n• Data processing frameworks (Kafka, Spark Streaming – preferred)\n\nDevelopment Tools and Methodologies:\n• Version control with GIT\n• Agile management tools: Jira, Confluence\n• Continuous integration/deployment pipelines (Jenkins, GitLab CI)\n\nSecurity Protocols:\n• Understanding of data security, access controls, and GDPR compliance in cloud environments\n\nExperience Requirements\n• Minimum of 5+ years in data engineering, with hands-on PySpark experience\n• Proven track record of developing, deploying, and maintaining scalable data pipelines\n• Experience working with data lakes, data warehouses, and cloud data services\n• Demonstrated leadership in projects involving big data technologies\n• Experience mentoring junior team members and collaborating across teams\n• Prior experience in financial, healthcare, or retail sectors is beneficial but not mandatory\n\nDay-to-Day Activities\n• Develop, optimize, and deploy big data pipelines using PySpark and related tools\n• Collaborate with data analysts, data scientists, and business teams to define data requirements\n• Conduct code reviews, troubleshoot pipeline issues, and optimize performance\n• Mentor junior team members on best practices and emerging technologies\n• Design solutions for data ingestion, transformation, and storage\n• Evaluate new tools and frameworks for continuous improvement\n• Maintain documentation, monitor system health, and ensure security compliance\n• Participate in sprint planning, daily stand-ups, and project retrospectives to align priorities\n\nQualifications\n• Bachelor’s or Master’s degree in Computer Science, Information Technology, or related discipline\n• Relevant industry certifications (e.g., AWS Data Analytics, GCP Professional Data Engineer) preferred\n• Proven experience working with PySpark and big data ecosystems\n• Strong understanding of software development lifecycle and data governance standards\n• Commitment to continuous learning and professional development in data engineering technologies\n\nProfessional Competencies\n• Analytical mindset and problem-solving acumen for complex data challenges\n• Effective leadership and team management skills\n• Excellent communication skills tailored to technical and non-technical audiences\n• Adaptability in fast-evolving technological landscapes\n• Strong organizational skills to prioritize tasks and manage multiple projects\n• Innovation-driven with a passion for leveraging emerging data technologies\n\nS YNECHRON’S DIVERSITY & INCLUSION STATEMENT\n\nDiversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.\n\nAll employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.\n\nCandidate Application Notice",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgUHlTcGFyayBEYXRhIEVuZ2luZWVyIChCaWcgRGF0YSwgQ2xvdWQgRGF0YSBTb2x1dGlvbnMsIFx1MDAyNiBQeXRob24pIiwiY29tcGFueV9uYW1lIjoiU3luZWNocm9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IjJYOEpqZUhEQmxiRndHQklBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "1 day ago",
         "Spark Engineer"
        ],
        [
         "Senior Data Engineer (Delta Lake, Spark & Unity Catalog)",
         "306 - GOTO TECHNOLOGIES INDIA PRIVATE LIMITED",
         "India",
         "Job Description Where you’ll work: India (Remote) Engineering at GoTo We’re the trailblazers of remote work technology. We build powerful, flexible work software that empowers everyone to live their best life, at work and beyond. And blaze even more trails along the way. There’s ample room for growth – so you can blaze your own trail here too. When you join a GoTo product team, you’ll take on a key role in this process and see your work be used by millions of users worldwide. Your Day to Day As a Senior Data Engineer, you would be: Design, develop, and maintain robust, scalable, and efficient ETL/ELT data pipelines to process structured and unstructured data from diverse sources. Architect and implement cloud-native data solutions using AWS services including S3, EMR, Lambda, and EKS. Build and optimize large-scale data processing workflows using Apache Spark and Databricks, with hands-on experience implementing and managing Delta Lake tables for high-volume batch and streaming use cases. Manage and govern data using Unity Catalog, maintaining strong data lineage, access controls, and metadata management. Develop and maintain data models (relational and dimensional) to support analytics, reporting, and machine learning use cases. Schedule, monitor, and orchestrate workflows using Apache Airflow or similar orchestration tools. Implement data quality checks, logging, monitoring, and alerting to ensure reliability and visibility of data pipelines. Collaborate with analysts, data scientists, and business stakeholders to deliver high-quality, trusted data for downstream applications and enable self-service BI tooling. Ensure adherence to best practices in data governance, security, and compliance. Mentor junior engineers and contribute to engineering standards, including CI/CD, automated testing, and documentation What We’re Looking For As an Senior Data Engineer, your background will look like: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. 5+ years of relevant experience in data engineering or software development, with a proven track record designing and maintaining production-grade data pipelines. Proficiency in Python programming and SQL for data transformation and analytics. Strong hands-on experience with Apache Spark and Databricks, including direct exposure to Delta Lake for data lake management, ACID transactions, schema enforcement and evolution, and time travel functionalities. Experience managing and organizing data access with Unity Catalog. In-depth experience with AWS services—specifically S3, EMR, Lambda, and EKS—and an excellent understanding of cloud-based data architecture and security best practices. Strong data modeling skills (dimensional, normalized) and understanding of data warehousing and lakehouse paradigms. Hands-on experience orchestrating workflows using Apache Airflow or similar orchestration frameworks. Familiarity with BI and data visualization tools (e.g., Tableau, Power BI). Experience establishing data quality processes, monitoring, and observability for pipelines. Exceptional communication, collaboration, and problem-solving skills, with the ability to thrive in an agile, multicultural team environment. At GoTo, authenticity and inclusive culture are key to our thriving workplace, where diverse perspectives drive innovation and growth. Our team of GoGetters is passionate about learning, exploring, and working together to achieve success while staying committed to delivering exceptional experiences for our customers. We take pride in supporting our employees with comprehensive benefits, wellness programs, and global opportunities for professional and personal development. By maintaining an inclusive environment, we empower our teams to do their best work, make a meaningful impact, and grow their career. Learn more. At GoTo, we’re bold enough to imagine a world of work without limits—where curiosity and AI-driven innovation fuel our constant growth. As the leader in cloud communications and IT, we solve real-world challenges through practical, cutting-edge solutions and an unwavering customer-first mindset. Our culture is rooted in inclusion, ownership, and transparency, fueling an environment where every voice contributes to both personal and collective achievement. Here, collaboration sparks bold ideas, and authenticity is celebrated—empowering you to adapt, evolve, and make a real impact. Join GoTo, and help shape the future of work while accelerating your own growth alongside exceptional people who are redefining what’s possible.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAoRGVsdGEgTGFrZSwgU3BhcmsgXHUwMDI2IFVuaXR5IENhdGFsb2cpIiwiY29tcGFueV9uYW1lIjoiMzA2IC0gR29UbyBUZWNobm9sb2dpZXMgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlJxSFNWX2s0aVZkRWhBNGRBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "10 days ago",
         "Spark Engineer"
        ],
        [
         "Data Engineer-Python,PySpark,SQL ,Spark Architecture,Azure Databricks",
         "SIEMENS HEALTHINEERS",
         "India",
         "jobid\n• 460574\n\njobfamily\n• Research & Development\n\ncompany\n• Siemens Healthcare Private Limited\n\norganization\n• Siemens Healthineers\n\njobType\n• Full-time\n\nexperienceLevel\n• Experienced Professional\n\ncontractType\n• Permanent\n\nAs a Data Engineer , you are required to:\n\nDesign, build, and maintain data pipelines that efficiently process and transport data from various sources to storage systems or processing environments while ensuring data integrity, consistency, and accuracy across the entire data pipeline.\n\nIntegrate data from different systems, often involving data cleaning, transformation (ETL), and validation. Design the structure of databases and data storage systems, including the design of schemas, tables, and relationships between datasets to enable efficient querying. Work closely with data scientists, analysts, and other stakeholders to understand their data needs and ensure that the data is structured in a way that makes it accessible and usable.\n\nStay up-to-date with the latest trends and technologies in the data engineering space, such as new data storage solutions, processing frameworks, and cloud technologies. Evaluate and implement new tools to improve data engineering processes.\n\nQualification : Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Science, Engineering is desirable.\n\nExperience level : At least 3 - 5 years hands-on experience in Data Engineering\n\nDesired Knowledge & Experience :\n• Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\n• Knowing Spark internals: Catalyst/Tungsten/Photon\n• Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\n• IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\n• Test: pytest, Great Expectations\n• CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\n• Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\n• Languages: Python/Functional Programming (FP)\n• SQL : TSQL/Spark SQL/HiveQL\n• Storage : Data Lake and Big Data Storage Design\n\nadditionally it is helpful to know basics of:\n• Data Pipelines : ADF/Synapse Pipelines/Oozie/Airflow\n• Languages: Scala, Java\n• NoSQL : Cosmos, Mongo, Cassandra\n• Cubes : SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\n• SQL Server : TSQL, Stored Procedures\n• Hadoop : HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\n• Data Catalog : Azure Purview, Apache Atlas, Informatica\n\nRequired Soft skills & Other Capabilities :\n\nGreat attention to detail and good analytical abilities.\n\nGood planning and organizational skills\n\nCollaborative approach to sharing ideas and finding solutions\n\nAbility to work independently and also in a global team environment.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVB5dGhvbixQeVNwYXJrLFNRTCAsU3BhcmsgQXJjaGl0ZWN0dXJlLEF6dXJlIERhdGFicmlja3MiLCJjb21wYW55X25hbWUiOiJTaWVtZW5zIEhlYWx0aGluZWVycyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJXVkxrZFF1QU4tSGFXQjIzQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "8 days ago",
         "Spark Engineer"
        ],
        [
         "Cloud Data Engineer (Spark/Databricks)",
         "ANTAL JOB BOARD",
         "Nagpur, Maharashtra, India",
         "Vacancy No\nVN1228\n\nBusiness Unit\nEMEA\n\nJob Location\nIndia\n\nEmployment Type\nFull Time\n\nJob Details and Responsibilities\nWe are seeking an experienced Cloud Data Engineer with a strong background in AWS, Azure, and GCP. The ideal candidate will have extensive experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, and other ETL tools like Informatica, SAP Data Intelligence, etc. You will be responsible for designing, implementing, and maintaining robust data pipelines and building scalable data lakes. Experience with various data platforms like Redshift, Snowflake, Databricks, Synapse, Snowflake and others is essential. Familiarity with data extraction from SAP or ERP systems is a plus.\n\nKey Responsibilities:\n\nDesign and Development:\n• Design, develop, and maintain scalable ETL pipelines using cloud-native tools (AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.).\n• Architect and implement data lakes and data warehouses on cloud platforms (AWS, Azure, GCP).\n• Develop and optimize data ingestion, transformation, and loading processes using Databricks, Snowflake, Redshift, BigQuery and Azure Synapse.\n• Implement ETL processes using tools like Informatica, SAP Data Intelligence, and others.\n• Develop and optimize data processing jobs using Spark Scala.\nData Integration and Management:\n• Integrate various data sources, including relational databases, APIs, unstructured data, and ERP systems into the data lake.\n• Ensure data quality and integrity through rigorous testing and validation.\n• Perform data extraction from SAP or ERP systems when necessary.\nPerformance Optimization:\n• Monitor and optimize the performance of data pipelines and ETL processes.\n• Implement best practices for data management, including data governance, security, and compliance.\nCollaboration and Communication:\n• Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\n• Collaborate with cross-functional teams to design and implement data solutions that meet business needs.\nDocumentation and Maintenance:\n• Document technical solutions, processes, and workflows.\n• Maintain and troubleshoot existing ETL pipelines and data integrations.\n\nQualifications\n\nEducation:\n\nBachelor’s degree in Computer Science, Information Technology, or a related field. Advanced degrees are a plus.\n\nExperience:\n• 7+ years of experience as a Data Engineer or in a similar role.\n• Proven experience with cloud platforms: AWS, Azure, and GCP.\n• Hands-on experience with cloud-native ETL tools such as AWS DMS, AWS Glue, Kafka, Azure Data Factory, GCP Dataflow, etc.\n• Experience with other ETL tools like Informatica, SAP Data Intelligence, etc.\n• Experience in building and managing data lakes and data warehouses.\n• Proficiency with data platforms like Redshift, Snowflake, BigQuery, Databricks, and Azure Synapse.\n• Experience with data extraction from SAP or ERP systems is a plus.\n• Strong experience with Spark and Scala for data processing.\n\nSkills:\n• Strong programming skills in Python, Java, or Scala.\n• Proficient in SQL and query optimization techniques.\n• Familiarity with data modeling, ETL/ELT processes, and data warehousing concepts.\n• Knowledge of data governance, security, and compliance best practices.\n• Excellent problem-solving and analytical skills.\n• Strong communication and collaboration skills.\n\nPreferred Qualifications:\n• Experience with other data tools and technologies such as Apache Spark, or Hadoop.\n• Certifications in cloud platforms (AWS Certified Data Analytics – Specialty, Google Professional Data Engineer, Microsoft Certified: Azure Data Engineer Associate).\n• Experience with CI/CD pipelines and DevOps practices for data engineering\n• Selected applicant will be subject to a background investigation, which will be conducted and the results of which will be used in compliance with applicable law.\n\nWhat we offer in return:\n• Remote Working: Lemongrass always has been and always will offer 100% remote work\n• Flexibility: Work where and when you like most of the time\n• Training: A subscription to A Cloud Guru and generous budget for taking certifications and other resources you’ll find helpful\n• State of the art tech: An opportunity to learn and run the latest industry standard tools\n• Team: Colleagues who will challenge you giving the chance to learn from them and them from you\n\nLemongrass Consulting is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate on the basis of race, religion, color, national origin, religious creed, gender, sexual orientation, gender identity, gender expression, age, genetic information, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics\n\nAbout Lemongrass\nLemongrass (lemongrasscloud.com) is a global leader in SAP consulting, focused on helping organizations transform their business processes through innovative solutions and technologies. With a strong commitment to customer success, Lemongrass partners with companies to drive their digital transformation journeys, enabling them to unlock the full potential of their SAP investments.\n\nWe do this with our continuous innovation, automation, migration and operation, delivered on the world's most comprehensive cloud platforms – AWS, Azure and GCP and SAP Cloud ERP. We have been working with AWS and SAP since 2010 and we are a Premier Amazon Partner Network (APN) Consulting Partner. We are also a Microsoft Gold Partner, a Google Cloud Partner and an SAP Certified Silver Partner.\n\nOur team is what makes Lemongrass exceptional and why we have the excellent reputation in the market that we enjoy today. At Lemongrass, you will work with the smartest and most motivated people in the business. We take pride in our culture of innovation and collaboration that drives us to deliver exceptional benefits to our clients every day.",
         "eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyIChTcGFyay9EYXRhYnJpY2tzKSIsImNvbXBhbnlfbmFtZSI6IkFudGFsIEpvYiBCb2FyZCIsImFkZHJlc3NfY2l0eSI6Ik5hZ3B1ciwgTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJCQXpseTdfU0lsQjByTmF2QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "28 days ago",
         "Spark Engineer"
        ],
        [
         "Data Analyst III",
         "BRISTOL MYERS SQUIBB",
         "Hyderabad, Telangana, India",
         "Working with Us\nChallenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n\nThe US Commercial Analytics, Market Access Data Analyst will play a crucial role in supporting data-driven decision-making processes, generating insights, and providing strategic guidance to optimize our Market Access & Pricing activities in the US. This position requires a deep expertise in secondary analytics and statistical analytic techniques, along with a firm grasp of the US healthcare landscape, especially market access and pricing dynamics. The Business Analyst III will typically be responsible for leading support for multiple therapeutic areas in Market Access and will collaborate with cross-functional teams to enhance understanding of business performance and drive growth.\n\nRoles and Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Perform statistical analyses, data mining and predictive modelling to uncover trends and drivers of performance\n• Collaborate with stakeholders (US based Market Access Business Insights & Analytics team and US Market Access team members) to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects including payer, PBM and patient level subnational & claims data analyses, formulary and policy level analysis, and ad-hoc analytics based on secondary data to support US Market Access strategy\n• Prepare reports, dashboards and presentations to communicate findings, insights and recommendations to stakeholders\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up-to-date with industry trends (US access and health care landscape), best practices, and emerging technologies\n• Partner with US based Market Access BIA/Commercial team members (for a given project scope)\n• Provide overall project oversight and training/mentorship to junior analysts and team members as required\n• Supports culture of fact-based decision making through application of best-in-class, innovative, and appropriate methodologies to address key business questions\n\nSkills & Competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions, around US pricing and reimbursement strategies\n• Proficiency in statistical analysis techniques and applications of those to US commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and strategic thinking, with the ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome Candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• 5-8 years prior Pharmaceutical industry experience with 1-2 years within US Market Access & Pricing Analytics\n• Experience working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims, medical policy and formulary data), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with reimbursement and regulatory requirements and compliance in the US biopharma industry\n• Certification or training in relevant analytics or business intelligence tools is a plus\n\nIf you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n\nUniquely Interesting Work, Life-changing Careers\nWith a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n\nOn-site Protocol\n\nBMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n\nSite-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n\nBMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n\nIf you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUlJIiwiY29tcGFueV9uYW1lIjoiQnJpc3RvbCBNeWVycyBTcXVpYmIiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlZHU3JGRGo4OTcybHdnQWpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "3 days ago",
         "Data Analyst"
        ],
        [
         "Data Science Analyst",
         "IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED",
         "India",
         "About the Role:Within Insights and Commercial Solutions, below are the key service groups:  Marketing Effectiveness (Business Analytics, Forecasting, Competitive Intelligence)  Field Force Operations  Commercial Consulting  Brand Insights  Patient access services (Pricing and market access, etc.)  Primary Market Research Provide analytics support to Novartis internal customers (CPOs & Regional marketing and sales teams) on various projects • Support and facilitate data enabled decision making for Novartis internal customers using data analysis and data science techniques/methods on internal and external (3rd party) data to solve business problems • Support I&CS business in building capabilities by involving in various initiatives like knowledge sharing, on-boarding and training support, support in all business-related tasks/activities, building process documentation and knowledge repositories.Analyst, Data ScienceLocation – Hyderabad #LI Hybrid Key Responsibilities:Delivering projects and managing internal customer expectations across multi-channel marketing analytics, Portfolio Analytics, Targeting and Segmentation, Predictive Analytics, Resource Allocation and Optimization, Other ad-hoc requests.Create and deliver customer requirements as agreed SLAs (timeliness, accuracy, quality, etc.) and drive excellent customer satisfaction. Deliver advanced analytical and statistical solutions for various projects related to promotion evaluation, multi-channel marketing (MCM) campaign design, return on investment (ROI) analysis, resource allocation, segmentation, targeting, and other ad-hoc business questionsSupport exploratory research to identify new areas of application of advanced analytics/data science in providing enhanced decision-making support. Deliver services through a structured project management approach with appropriate documentation and communication throughout the delivery of servicesAutomation of project codes and development of front-end delivery solutions. Support in creation and maintenance of standard operating procedures (SOPs), quality checklists that will enable excellent quality outputs within the function.Support in developing and maintaining knowledge repositories that captures qualitative and quantitative reports of brands, disease areas, macroeconomic trends of Novartis operating markets, etc.Comply with all internal functional operating procedures like time tracking, KPI tracking and reporting, and other internal systems and processes. Comply to all Novartis operating procedures as per legal/IT/HR requirementsEssential Requirements:Technical Skills: Statistical Modeling Experience (Covering but not limited to, Regression, Classification, Supervised and Unsupervised Learning Methods, Optimization techniques, Predictive Models, Deep learning Models, e.g. Neural Networks, Natural Language Processing, Text Mining etc.)Programming languages/tools – R, SAS, Python, SQLVisualization tools – Qlikview, Qliksense, TableauWorking knowledge of MS-Office (MS Excel, VBA, PowerPoint, Access)Pharmaceutical industry domain/datasets knowledge – desirableEducation:Graduation/Post Graduation in Business Administration, Mathematics, Economics, Statistics, Engineering or Quantitative streamsExperience: Experience (Minimum 2years) in analytics in marketing analytics, experience in pharma industry is preferableShould have strong quantitative and systems backgroundStrong analytical thinking with problem solving approachShould have good ability to understand new data sources in short timeframe and embed them into standard analysesA good understanding of pharmaceutical domain and data would be ideal. Should have worked in an international company with exposure to working in a cross-cultural environmentStrong and proactive business results focus, and proven ability to provide insights that increase productivityWhy Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook. https://www.novartis.com/careers/benefits-rewards Commitment to Diversity and Inclusion: Novartis is committed to building an outstanding, inclusive work environment and diverse teams' representative of the patients and communities we serve. Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVuY2UgQW5hbHlzdCIsImNvbXBhbnlfbmFtZSI6IklOMTAgKEZDUlMgPSBJTjAxMCkgTm92YXJ0aXMgSGVhbHRoY2FyZSBQcml2YXRlIExpbWl0ZWQiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiejA4Y3NEMUZUdmdvZExfWkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "7 days ago",
         "Data Analyst"
        ],
        [
         "Data Sr.Modeler/Data Analyst( Immediate Joiner)",
         "THE TALENT QUEST",
         "Hyderabad, Telangana, India",
         "Vacancy with a company focused on digital transformation, specializing in intelligent automation, digitalization, data science & analytics, and mobile enablement. They help businesses improve cost efficiency, productivity, and agility by reducing turnaround time and errors. The company provides services and solutions including operations digital transformation consulting, next-gen shared services setup consulting, cognitive RPA deployment, and AI-enabled CX enhancement. Founded in 2020 ;with HQ in Gurugram, India; the Company is now operating from Noida, Mumbai, Hyderabad, and Bengaluru as well.\n\nJob Role:We are seeking a highly skilled and detail-oriented Data Modeler to join our Data\n\nManagement team. The ideal candidate will be responsible for designing and\n\nimplementing logical and physical data models to support enterprise data\n\ninitiatives. This role requires close collaboration with business stakeholders, data\n\narchitects, and engineers to ensure data is structured and accessible for analytics,\n\nreporting, and operational needs.\n\nThe successful candidate will:\n\nProvides technical expertise in needs identification, data modelling, data\n\nmovement and transformation mapping (source to target), automation and testing\n\nstrategies, translating business needs into technical solutions with adherence to\n\nestablished data guidelines and approaches from a business unit or project\n\nperspective.\n\n7-10 Years industry implementation experience with one or more data\n\nmodelling tools such as Erwin, ERStudio, PowerDesigner etc.\n\n Minimum of 8 years of data architecture, data modelling or similar\n\nexperience\n\n 5-7 years of management experience required\n\n 5-7 years consulting experience preferred\n\n Experience working with dimensionally modelled data\n\n Bachelor’s degree or equivalent experience, Master’s Degree Preferred\n\n Understanding of cloud (Azure, AWS, GCP, Snowflake preferred) and on\n\npremises architectures\n\nJob Types: Full-time, Permanent\n\nPay: Up to ₹3,000,000.00 per year\n\nBenefits:\n• Cell phone reimbursement\n• Internet reimbursement\n• Life insurance\n• Paid sick time\n• Paid time off\n• Work from home\n\nWork Location: In person",
         "eyJqb2JfdGl0bGUiOiJEYXRhIFNyLk1vZGVsZXIvRGF0YSBBbmFseXN0KCBJbW1lZGlhdGUgSm9pbmVyKSIsImNvbXBhbnlfbmFtZSI6IlRoZSBUYWxlbnQgUXVlc3QiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ino0YkdrY3RHOUdZdmFTRVVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "2 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst - INTL - Mexico or India",
         "INSIGHT GLOBAL",
         "Hyderabad, Telangana, India",
         "Project Background:\nMosaic is our financial planning and reporting transformation program, bringing an aligned business process and toolset to all key business units enabling us to get consistent and standardized financial metrics across geographies. Started in 2019 the program is already rolled out to Europe and most of North America and continues across the world.\nThere are three key components to the program:\n1. A standardized planning tool IBM Cognos TM1\n2. A global data warehouse based on Azure Cloud know as SPOT (single point of truth). The SPOT team is made up of functional data analysts & data engineers\n3. An extensive suite of dashboards known as Cockpit using Tableau. The Cockpit team are generally finance analysts with Tableau skills\nRole Background:\nWe are looking for an additional functional/data analyst to join the existing team of functional/data analysts working on the SPOT data warehouse.\nThe role of the functional/data analyst is to be the bridge between the Cockpit team, Data Engineering team, and Cognos TM1 team. The analyst understands the business data requirements and translates these into data definition and data transformation documents that are then build by the data engineers. The analyst can also be involved in testing, data validation and any sustain activity related to the flow and accuracy of the data in SPOT & Cockpit.\nTypically, the functional data analyst receives requirements for new and/or amended data pipelines from the Cockpit team or works with the Cognos TM1 team on new data sets that need to flow to SPOT & Cockpit.\nThe current functional data analysts come from a range of backgrounds from data engineers to business analysts. What unites the team is a passion for data, attention to detail, ability to navigate and understand complex data flows, and a customer-centric mindset. This is a high profile and at times high pressure transformation program and everyone on the team must be responsive to our stakeholders needs.\nKey Accountabilities:\nThis role will primarily work with the North America Cockpit team primarily based in Mexico City, but as part of the team they will be expected to build a good knowledge of all data pipelines and work on any area as needed. The key elements of the role are:\nDevelop and maintain SPOT solution design & data architecture:\no Ensure SPOT solution design & data model is up to date with latest business requirements\no Contribute to the delivery of current technical roadmap that can support the predicted growth in data volumes\nTranslate and communicate business requirements across all IT delivery teams and/or partners:\no Collaborate closely with SPOT Cockpit teams to ensure business requirements received from Global & Regional Planning teams are well-understood and translated into the SPOT solution design (functional & data model)\nAct as an owner in the diagnosis and resolution of any data issues raised with the team, partnering with other teams as needed.\n\nWe are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBJTlRMIC0gTWV4aWNvIG9yIEluZGlhIiwiY29tcGFueV9uYW1lIjoiSW5zaWdodCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Il96ZU5oSUE0Ujg3ak1KZHNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "6 days ago",
         "Data Analyst"
        ],
        [
         "IN Specialist 3- Data Analyst - TRAS -Assurance– Bangalore",
         "PWC",
         "India",
         "Line of ServiceAssuranceIndustry/SectorNot ApplicableSpecialismAssuranceManagement LevelSpecialistJob Description & SummaryAt PwC, our people in audit and assurance focus on providing independent and objective assessments of financial statements, internal controls, and other assurable information enhancing the credibility and reliability of this information with a variety of stakeholders. They evaluate compliance with regulations including assessing governance and risk management processes and related controls.In digital assurance at PwC, you will focus on providing assurance services over clients' digital environment, including processes and controls, cyber security measures, data and AI systems, and their associated governance, to help organisations and their stakeholders build trust in their technology while complying with relevant regulations.*Why PWCAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm’s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations. \"Job Description & SummaryABOUT TECHNOLOGY RISK ASSURANCE SERVICES:The Technology Risk Assurance Services practice in the firm, provides both audit and non-audit services related to controls around the financial reporting process, including financial business process and IT management controls.SPA provides:· Financial and operation applications/business process controls reviews· Database security controls reviews· IT general controls reviews· Infrastructure security reviews· Third party assurance and opinion services· Sarbanes-Oxley readiness, process improvement and sustainability services· Due diligence on systems and controls· Pre- and post-implementation systems reviews· Project assurance services· Data services (e.g., CAATs, data quality reviews)· Computer security reviewsResponsibilities (essential functions and responsibilities of the job):The tasks, roles and responsibilities concerned with the job will include the following:Executing various audit engagements as a support to field in-charge andApplying The Firm’s methodology while executing various job rolesApply current knowledge of IT trends and systems to identify security and risk management issues, and other opportunities for improvementAssist in the planning and execution of business process control reviews, information systems audits, and other Assurance offerings across a variety of industriesA good understanding of clients Business Risks, Audit Risks, IT Risks and the mitigating controls in addressing these risksGain strong comprehension of client operations, processes, and business objectives and utilize that knowledge on engagements.Evaluate and test business processes and controls and identify areas of risks.Perform general computer and application controls reviews including ERPs like SAP, Oracle Application, JD Edwards, MFG Pro, etc.Comply with requirements of ICAI Auditing Standards like AAS 29Data Analysis using CAATs tool like Excel, ACL, SQL etc*Mandatory skill sets - Data Analyst*Preferred Skill Sets – Business Analyst*Years of experience required – 0-1 Years*Education Qualification – Post Graduation in Data ScienceEducation (if blank, degree and/or field of study not specified)Degrees/Field of Study required: Postgraduate (Diploma)Degrees/Field of Study preferred:Certifications (if blank, certifications not specified)Required SkillsData AnalyticsOptional SkillsAccepting Feedback, Accepting Feedback, Active Listening, Artificial Intelligence (AI) Platform, Auditing Methodologies, Cloud Engineering, Communication, Compliance and Governance, Compliance and Standards, Compliance Auditing, Corporate Governance, Cybersecurity Governance, Cybersecurity Risk Management, Data Quality, Data Quality Assessment, Data Quality Assurance Testing, Data Validation, Emotional Regulation, Empathy, Governance Framework, Inclusion, Information Assurance, Information Security Governance, Intellectual Curiosity, Internal Controls {+ 11 more}Desired Languages (If blank, desired languages not specified)Travel RequirementsNot SpecifiedAvailable for Work Visa Sponsorship?NoGovernment Clearance Required?NoJob Posting End Date",
         "eyJqb2JfdGl0bGUiOiJJTiBTcGVjaWFsaXN0IDMtIERhdGEgQW5hbHlzdCAtIFRSQVMgLUFzc3VyYW5jZeKAkyBCYW5nYWxvcmUiLCJjb21wYW55X25hbWUiOiJQd0MiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiZTlNdVBjTnNJWHlmb3ZrSkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "1 day ago",
         "Data Analyst"
        ],
        [
         "Data Analyst 1",
         "BRISTOL MYERS SQUIBB",
         "Hyderabad, Telangana, India",
         "The US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n\nRoles & Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up to date with industry trends, best practices, and emerging technologies\n\nSkills and competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n• commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• Proven experience (1-3 years) in a similar business analyst role\n• Prior Pharmaceutical industry and/or healthcare consulting experience required\n• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with regulatory requirements and compliance in the US biopharma industry\n\nThe US Commercial Analytics Data Analyst I supports data-driven decision-making by generating insights and strategic guidance to enhance commercial performance in the US. This role requires expertise in secondary analytics and a strong understanding of the US healthcare landscape. The analyst typically leads support for specific brands or functions and collaborates with cross-functional teams to improve business performance and drive growth\n\nRoles & Responsibilities:\n• Conduct analysis and interpretation of complex data sets to derive meaningful insights and recommendations that enhance our understanding of business performance and drive growth\n• Collaborate primarily with local senior team members and US based Business Insights & Analytic team members stakeholders to identify business problems and create/conduct analytic plans to address those questions\n• Delivers projects (reports, presentation, & communication) including customer prioritization/segmentation, subnational & claims data analyses, business performance analysis, and ad-hoc analytics based on secondary data to support brand strategy\n• Collaborate with IT teams to develop and enhance data infrastructure, data pipelines and analytical tools for efficient data collection, processing and analysis\n• Stay up to date with industry trends, best practices, and emerging technologies\n\nSkills and competencies:\n• Strong analytical thinking and problem-solving skills with the ability to analyze complex data sets and draw meaningful conclusions\n• commercial business problems\n• Strong project management skills and the ability to work independently or as part of a team\n• Strong communication skills with the ability to present complex information to non-technical stakeholders in a clear manner\n• Strong business acumen and ability to translate analytical findings into actionable insights and recommendations\n\nExperience:\n• We welcome candidates with a bachelor’s or master’s degree in technology or engineering. In addition, a strong record of analytic and quantitative work\n• Proven experience (1-3 years) in a similar business analyst role\n• Prior Pharmaceutical industry and/or healthcare consulting experience required\n• Experience in working with US healthcare datasets (e.g. IQVIA/SHS National, Subnational, Patient Claims), data processing and visualization tools, and statistical software packages (e.g., SQL, R, Python, Tableau)\n• Proven ability to manage multiple projects, meet tight deadlines, and operate with agility and speed\n• Familiarity with regulatory requirements and compliance in the US biopharma industry",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJ1ZUlVZURnaVJaU1ludkdlQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "16 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst Finance (VBA, Power Query, Power BI, Python) 4+ years of experience)",
         "DUPONT",
         "Hyderabad, Telangana, India",
         "At DuPont, our purpose is to empower the world with essential innovations to thrive. We work on things that matter. Whether it’s providing clean water to more than a billion people on the planet, producing materials that are essential in everyday technology devices from smartphones to electric vehicles, or protecting workers around the world. Discover the many reasons the world’s most talented people are choosing to work at DuPont. Why Join Us | DuPont Careers\n\nJob Summary:\n\nThe Data Analyst/Developer will play a crucial role in developing automated data solutions, creating comprehensive dashboards, and utilizing advanced Excel functionalities. This position will require collaboration with various teams to translate data needs into actionable intelligence.\n\nKey Areas of Expertise and Responsibilities:\n\n1. Visual Basic for Applications (VBA)\n• Responsibilities:\n• Develop and maintain complex VBA applications to automate repetitive tasks.\n• Incorporate SAP Scripting within VBA to optimize business processes.\n• Troubleshoot and debug existing VBA code to ensure smooth functionality.\n• Criteria:\n• Advanced proficiency in VBA programming.\n• Demonstrated experience with SAP interfaces and scripting.\n• Ability to write modular, efficient, and maintainable code.\n• Knowledge of Excel object model and its functionalities.\n\n2. Power Query\n• Responsibilities:\n• Utilize Power Query for data extraction, transformation, and loading (ETL) processes.\n• Develop and maintain data models in Excel to streamline data preparation.\n• Create and optimize Power Query scripts for efficient data processing.\n• Criteria:\n• Intermediate experience with Power Query including M language for data transformation.\n• Familiarity with connecting Power Query to various data sources (e.g., databases, APIs).\n• Ability to perform data cleansing and manipulation through Power Query.\n\n3. Power BI\n• Responsibilities:\n• Create interactive, user-friendly dashboards and reports using Power BI.\n• Collaborate with stakeholders to gather reporting requirements and deliver insights.\n• Optimize Power BI reports for performance and usability.\n• Criteria:\n• Intermediate knowledge of Power BI Desktop and Power BI Service.\n• Ability to create DAX measures and calculated columns for enhanced analytics.\n• Familiarity with data visualization best practices and techniques.\n\n4. Python\n• Responsibilities:\n• Develop Python scripts to automate data manipulation and Excel-related tasks.\n• Utilize libraries such as Pandas and NumPy for data analysis and automation.\n• Collaborate with the data team to integrate Python solutions with existing tools.\n• Criteria:\n• Intermediate proficiency in Python, especially in data manipulation and automation.\n• Experience with libraries like Pandas, NumPy, and openpyxl for Excel automation.\n• Understanding of APIs and ability to retrieve data programmatically.\n\nQualifications:\n• Bachelor’s degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n• A minimum of 4 of experience in a data-related role, focusing on the technologies mentioned.\n• Strong analytical and problem-solving skills with attention to detail.\n• Excellent communication skills and the ability to work collaboratively with diverse teams.\n\nPreferred Skills:\n• Experience with SQL and relational databases for data querying and data management.\n• Familiarity with other business intelligence tools (like Tableau or QlikView) is a plus.\n• Knowledge of machine learning principles is an advantage.\n• Understanding of data warehousing concepts and methodologies.\n\nJoin our Talent Community to stay connected with us!\n\nOn May 22, 2024, we announced a plan to separate our Electronics and Water businesses in a tax-free manner to its shareholders. On January 15, 2025, we announced that we are targeting November 1, 2025, for the completion of the intended separation of the Electronics business (the “Intended Electronics Separation”)*. We also announced that we would retain the Water business. We are committed to ensuring a smooth and successful separation process for the Future Electronics business. We look forward to welcoming new talent interested in contributing to the continued success and growth of our evolving organization.\n\n(1)The separation transactions are subject to satisfaction of customary conditions, including final approval by DuPont's Board of Directors, receipt of tax opinion from counsel, the filing and effectiveness of Form 10 registration statements with the U.S. Securities and Exchange Commission, applicable regulatory approvals, and satisfactory completion of financing.  For further discussion of risks, uncertainties and assumptions that could impact the achievement, expected timing and intended benefits of the separation transactions, see DuPont’s announcement.\n\nDuPont is an equal opportunity employer. Qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability or any other protected class. If you need a reasonable accommodation to search or apply for a position, please visit our Accessibility Page for Contact Information.\n\nDuPont offers a comprehensive pay and benefits package. To learn more visit the Compensation and Benefits page.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgRmluYW5jZSAoVkJBLCBQb3dlciBRdWVyeSwgUG93ZXIgQkksIFB5dGhvbikgNCsgeWVhcnMgb2YgZXhwZXJpZW5jZSkiLCJjb21wYW55X25hbWUiOiJEdXBvbnQiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImNGeEdTMkhJQXpJRW5kb2ZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "2 days ago",
         "Data Analyst"
        ],
        [
         "Sustainability Data Analyst",
         "CARRIER",
         "Hyderabad, Telangana, India",
         "Role: Sustainability Data Analyst\n\nLocation: Hyderabad, India\n\nFull/ Part-time: Full time\n\nBuild a career with confidence\n\nCarrier Global Corporation, global leader in intelligent climate and energy solutions is committed to creating solutions that matter for people and our planet for generations to come. From the beginning, we've led in inventing new technologies and entirely new industries. Today, we continue to lead because we have a world-class, diverse workforce that puts the customer at the center of everything we do.\n\nAbout the role\n\nWe are seeking a results-driven Sustainability Data Analyst to join Carrier’s product sustainability analytics team. The ideal candidate will have a strong background in sustainability practices and data analytics, with a focus on analyzing environmental impact and identifying opportunities for improvement. This role involves conducting comprehensive sustainability/energy audits, analyzing data, and developing strategies to enhance sustainability performance.\n\nKey responsibilities:\n• Perform detailed sustainability audits and analyze data to identify trends and areas for improvement.\n• Develop and implement strategies to enhance sustainability and reduce environmental impact.\n• Monitor and evaluate the performance of sustainability initiatives using data analytics.\n• Collaborate with cross-functional teams to optimize sustainability practices.\n• Prepare reports and presentations on sustainability metrics and audit findings.\n• Stay updated on industry trends and best practices in sustainability and data analytics.\n\nMinimum Requirements:\n\nEducation: Bachelor’s or Master’s degree in Mechanical, Energy Engineering, Environmental Science, Sustainability, Data Analytics, or a related field.\n\nExperience: atleast 6 years in sustainability, data analytics, or BI development roles to be able to drive himself towards business goals independently.\n\nKey Skills:\n• Strong analytical skills, attention to detail and ability to think from first principles.\n• Excellent communication and teamwork abilities.\n• Proficiency in data analytics software and tools. Knowledge of python, SQL, Power-BI, would be added advantage.\n• Knowledge of relevant regulations and standards in sustainability.\n• Familiarity with data visualization tools and techniques.\n• Willingness to be flexible, learn new tools, techniques and deliver.\n\nBenefits\n\nWe are committed to offering competitive benefits programs for all of our employees, and enhancing our programs when necessary.\n• Enjoy your best years with our retirement savings plan\n• Have peace of mind and body with our health insurance\n• Make yourself a priority with flexible schedules, parental leave and our holiday purchase scheme\n• Drive forward your career through professional development opportunities\n• Achieve your personal goals with our Employee Assistance Programme.\n\nOur commitment to you\n\nOur greatest assets are the expertise, creativity and passion of our employees. We strive to provide a great place to work that attracts, develops and retains the best talent, promotes employee engagement, fosters teamwork and ultimately drives innovation for the benefit of our customers. We strive to create an environment where you feel that you belong, with diversity and inclusion as the engine to growth and innovation. We develop and deploy best-in-class programs and practices, providing enriching career opportunities, listening to employee feedback and always challenging ourselves to do better. This is The Carrier Way.\n\nJoin us and make a difference.\n\nApply Now!\n\nCarrier is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.\n\nJob Applicant's Privacy Notice:\n\nClick on this link to read the Job Applicant's Privacy Notice",
         "eyJqb2JfdGl0bGUiOiJTdXN0YWluYWJpbGl0eSBEYXRhIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJDYXJyaWVyIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJPMkpEMHBVcG0xc3dxYnc1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "9 days ago",
         "Data Analyst"
        ],
        [
         "Sr. Data Analyst",
         "ICIMS TALENT ACQUISITION",
         "Rai Durg, Telangana, India",
         "Job Overview\n\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.\n\nAbout Us\n\nWhen you join iCIMS, you join the team helping global companies transform business and the world through the power of talent. Our customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile. As the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent. We’re passionate about helping companies build a diverse, winning workforce and about building our home team. We're dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\n\nResponsibilities\n• Perform various data analysis functions to analyze data from a variety of sources including external labor market data and research and internal data sets from our platforms \n• Incorporate information from a variety of systems to produce comprehensive and compelling narratives for thought-leadership initiatives and customer engagements\n• Demonstrate critical thinking - identify the story in context using multiple data sets, and present results. A strong proficiency in data storytelling will be critical to success in this role.\n• Understand principles of quality data visualization and apply them in Tableau to create and maintain custom dashboards for consumption by other employees\n• Find and investigate data quality issues, root causes and recommend remedies to be implemented by the data scientists and engineers  \n• Liaise with teams around our business to understand their problems, determine how our team can help, then use our database to produce the content they need\n• Identify data mapping and enrichment requirements. Familiarity with SQL, especially the logic behind different types of data joins and writing efficient queries, will be necessary\n• Consistently ensure that business is always conducted with integrity and that behavior aligns with iCIMS policies, procedures, and core competencies\n\nAdditional Job Responsibilities: \n• Produce and adapt data visualizations in response to business requests for internal and external use\n• Shows good judgement in prioritizing their own commitments and those of the larger team, while demonstrating initiative and appropriate urgency when needed\n• Mentor junior team members in best practices for analytics, data visualization, and data storytelling. Exemplify these standards and guide teammates in following them.\n• Think creatively to produce unique, actionable insights from complex datasets, which can deliver value to our business and to our customers.\n\nQualifications\n• 5-10 years professional experience working in an analytics capacity\n• Excellent communication skills, especially with regards to data storytelling – finding insights from complex datasets and sharing those findings with key stakeholders\n• Strong data analytics and visualization skills\n• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n• Advanced knowledge of Excel (Pivot tables, VLOOKUPs, IF statements)\n• Familiarity with data guardrails to ensure compliance with applicable data governance regulations and privacy laws (i.e., GDPR)\n\nPreferred\n• Expertise in Tableau Desktop (Tableau Server and Prep are preferable) producing clear and informative graphs and dashboards\n• Proficiency in SQL and either Python or R to extract and prepare data for analysis\n\nEEO Statement\n\niCIMS is a place where everyone belongs. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities. So, regardless of how your diversity expresses itself, you can find a home here at iCIMS.\n\nWe are proud to be an equal opportunity and affirmative action employer. We prohibit discrimination and harassment of any kind based on race, color, religion, national origin, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, veteran status, genetic information, disability, or other applicable legally protected characteristics. If you would like to request an accommodation due to a disability, please contact us at careers@icims.com.\n\nCompensation and Benefits\n\nCompetitive health and wellness benefits include medical insurance (employee and dependent family members), personal accident and group term life insurance, bonding and parental leave, lifestyle spending account reimbursements, wellness services offerings, sick and casual/emergency days, paid holidays, tuition reimbursement, retirals (PF - employer contribution) and gratuity. Benefits and eligibility may vary by location, role, and tenure. Learn more here: https://careers.icims.com/benefits",
         "eyJqb2JfdGl0bGUiOiJTci4gRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiaUNJTVMgVGFsZW50IEFjcXVpc2l0aW9uIiwiYWRkcmVzc19jaXR5IjoiUmFpIER1cmcsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6ImZMUlRBQjlESVFWdFdOeGpBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "21 days ago",
         "Data Analyst"
        ],
        [
         "Master Data Analyst",
         "C32511 ALFA LAVAL INDIA PRIVATE LIMITED",
         "India",
         "Purpose of the Job: To provide direct support to the Sales and Order Handling teams by managing customer master data and administering the Order Management System. This role ensures data integrity, system readiness, and compliance with internal controls during order booking and execution processes. Key Responsibilities: Customer Master Data Management Manage the full lifecycle of customer codes, including creation, amendment, and deactivation. Conduct background checks using the Global World Check system and coordinate with the Export Control Super User for compliance validation. Ensure customer data is configured correctly in the system for seamless order booking and execution. Maintain data accuracy and cleanliness in line with Alfa Laval’s global data standards. System Administration Act as the system administrator for the Customer Order Management system. Maintain system master data and troubleshoot operational issues. Ensure the system is up-to-date and aligned with business requirements. Order Booking Compliance Perform basic compliance checks during order booking. Verify completeness and correctness of mandatory information. Ensure all required approvals are in place as per the authorization matrix. Process Activation Initiate internal processes required for order execution. Collaborate with cross-functional teams to ensure timely and accurate order processing. Key Competencies: Strong attention to detail and data accuracy Knowledge of compliance and export control processes Proficiency in ERP and order management systems Analytical and problem-solving skills Effective communication and collaboration across teams Qualifications & Experience: Bachelor’s degree in Business Administration, Information Systems, or related field 3+ years of experience in data management or order processing roles Familiarity with data governance and compliance frameworks Experience with customer master data and ERP systems (e.g., SAP) \"We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\" Every day, we get opportunities to make a positive impact – on our colleagues, partners, customers and society. Together, we’re pioneering the solutions of the future and unlocking the full potential of precious resources. Trusted to act on initiative, we challenge conventional thinking to develop world-leading technologies that inspire progress in vital areas, including energy, food, water and shipping. As we push forward, the innovative, open spirit that fuels our 140-year-old start-up culture and rapid growth also drives our personal growth. So, as we shape a more resourceful, less wasteful world, we build our careers too. Meet our dedicated people who always go the extra mile Alfa Laval Career Site Or sign up for Job Alerts to stay in touch Sign up for Job Alerts",
         "eyJqb2JfdGl0bGUiOiJNYXN0ZXIgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiQzMyNTExIEFsZmEgTGF2YWwgSW5kaWEgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImhJMnc5MHYxS1E3QXVQTkNBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "6 days ago",
         "Data Analyst"
        ],
        [
         "Engr II-Data Engineering",
         "VERIZON",
         "India",
         "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely — even if they’re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love — driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together — lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat You’ll Be Doing...\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform and analytics teams, you will understand and enable the required data sets from different sources. This includes both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n• Understanding the business requirements.\n• Transforming technical design.\n• Working on data ingestion, preparation and transformation.\n• Developing the scripts for data sourcing and parsing.\n• Developing data streaming applications.\n• Debugging the production failures and identifying the solution.\n• Working on ETL/ELT development.\n\nWhat We’re Looking For...\n\nYou’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems.\n\nYou'll Need To Have\n• Bachelor’s degree or one or more years of relevant experience required, demonstrated through work experience and/or military experience.\n• Experience with Data Warehouse concepts and Data Management life cycle.\n\nEven better if you have one or more of the following:\n• Any related Certification on ETL/ELT developer.\n• Accuracy and attention to detail.\n• Good problem solving, analytical, and research capabilities.\n• Good verbal and written communication.\n• Experience presenting to and influencing partners.\n\nWhy Verizon?\n\nVerizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.\n• We are a ‘pay for performance’ company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.\n• Your benefits are market competitive and delivered by some of the best providers.\n• You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.\n• We offer generous paid time off benefits to help you manage your work life balance and opportunities for flexible working arrangements*.\n• Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.\n• You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.\n\nYour benefits package will vary depending on the country in which you work.\n• subject to business approval\n\nIf Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.\n\nWhere you’ll be working\n\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics.",
         "eyJqb2JfdGl0bGUiOiJFbmdyIElJLURhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJWZXJpem9uIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkpaY3lnNzJpTUJGcHpfUTJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "1 day ago",
         "Data Engineer"
        ],
        [
         "Associate Analyst - Data Engineer",
         "PEPSICO",
         "Hyderabad, Telangana, India",
         "Overview\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT.  The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics, and new product development.  PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\n\nWhat PepsiCo Data Management and Operations does:\n\nMaintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\n\nResponsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\n\nWork cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\n\nIncrease awareness about available data and democratize access to it across the company.\n\n \n\n               As a data engineer, you will be the key technical expert building PepsiCo's data products to drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help developing very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n\nResponsibilities\n• Act as a subject matter expert across different digital projects.\n• Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\n• Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\n• Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\n• Responsible for implementing best practices around systems integration, security, performance, and data management.\n• Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\n• Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\n• Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\n• Develop and optimize procedures to “productionalize” data science models.\n• Define and manage SLA’s for data products and processes running in production.\n• Support large-scale experimentation done by data scientists.\n• Prototype new approaches and build solutions at scale.\n• Research in state-of-the-art methodologies.\n• Create documentation for learnings and knowledge transfer.\n• Create and audit reusable packages or libraries.\n\nQualifications\n• 4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n• 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n• 3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n• 2+ years in cloud data engineering experience in Azure.\n• Fluent with Azure cloud services. Azure Certification is a plus.\n• Experience in Azure Log Analytics\n• Experience with integration of multi cloud services with on-premises technologies.\n• Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\n• Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\n• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n• Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\n• Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\n• Experience with Statistical/ML techniques is a plus.\n• Experience with building solutions in the retail or in the supply chain space is a plus.\n• Experience with version control systems like Github and deployment & CI tools.\n• Working knowledge of agile development, including DevOps and DataOps concepts.\n• B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.\n\n Skills, Abilities, Knowledge:\n• Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\n• Strong change manager. Comfortable with change, especially that which arises through company growth.\n• Ability to understand and translate business requirements into data and technical requirements.\n• High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\n• Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\n• Strong organizational and interpersonal skills; comfortable managing trade-offs.",
         "eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUgQW5hbHlzdCAtIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJQZXBzaUNvIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJkdUI4NkhSU1JwRWNPdlBCQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "1 month ago",
         "Data Engineer"
        ],
        [
         "Senior Big Data Engineer",
         "QUALCOMM",
         "Hyderabad, Telangana, India",
         "Company:\nQualcomm India Private Limited\n\nJob Area:\nEngineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\n• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n• 2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary:\n\nPreferred Qualifications\n• 3+ years of experience as a Data Engineer or in a similar role\n• Experience with data modeling, data warehousing, and building ETL pipelines\n• Solid working experience with Python, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n• Experience with Big Data tools, platforms and architecture with solid working experience with SQL\n• Experience working in a very large data warehousing environment, Distributed System.\n• Solid understanding on various data exchange formats and complexities\n• Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n• Strong data visualization skills\n• Basic understanding of Machine Learning; Prior experience in ML Engineering a plus\n• Ability to manage on-premises data and make it inter-operate with AWS based pipelines\n• Ability to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\nEducation\n• Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n• Preferred Qualifications: Masters in CS/ECE with a Data Science / ML Specialization\n\nMinimum Qualifications:\n• Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n• 3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n• Completes assigned coding tasks to specifications on time without significant errors or bugs.\n• Adapts to changes and setbacks in order to manage pressure and meet deadlines.\n• Collaborates with others inside project team to accomplish project objectives.\n• Communicates with project lead to provide status and information about impending obstacles.\n• Quickly resolves complex software issues and bugs.\n• Gathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n• Seeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n• Participates in technical conversations with tech leads/managers.\n• Anticipates and communicates issues with project team to maintain open communication.\n• Makes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n• Prioritizes project deadlines and deliverables with minimal supervision.\n• Resolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n• Writes readable code for large features or significant bug fixes to support collaboration with other engineers.\n• Determines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n• Unit tests own code to verify the stability and functionality of a feature.\n\nApplicants: Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here. Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries).\n\nQualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n\nTo all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n\nIf you would like more information about this role, please contact Qualcomm Careers.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQmlnIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJRdWFsY29tbSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieVRVTHdrQjB2Ujk5bmJQUUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 days ago",
         "Data Engineer"
        ],
        [
         "Manager Data Engineer - AWS Databricks",
         "BLEND360",
         "Hyderabad, Telangana, India",
         "Company Description\n\nBlend is a premier AI services provider, committed to co-creating meaningful impact for its clients through the power of data science, AI, technology, and people. With a mission to fuel bold visions, Blend tackles significant challenges by seamlessly aligning human expertise with artificial intelligence. The company is dedicated to unlocking value and fostering innovation for its clients by harnessing world-class people and data-driven strategy. We believe that the power of people and AI can have a meaningful impact on your world, creating more fulfilling work and projects for our people and clients. For more information, visit www.blend360.com\n\nJob Description\n\nWe are seeking a seasoned Data Engineering Manager with 8+ years of experience to lead and grow our data engineering capabilities. This role demands strong hands-on expertise in Python, SQL, Spark, and advanced proficiency in AWS and Databricks. As a technical leader, you will be responsible for architecting and optimizing scalable data solutions that enable analytics, data science, and business intelligence across the organization.\n\nKey Responsibilities:\n• Lead the design, development, and optimization of scalable and secure data pipelines using AWS services such as Glue, S3, Lambda, EMR, and Databricks Notebooks, Jobs, and Workflows.\n• Oversee the development and maintenance of data lakes on AWS Databricks, ensuring performance and scalability.\n• Build and manage robust ETL/ELT workflows using Python and SQL, handling both structured and semi-structured data.\n• Implement distributed data processing solutions using Apache Spark/PySpark for large-scale data transformation.\n• Collaborate with cross-functional teams including data scientists, analysts, and product managers to ensure data is accurate, accessible, and well-structured.\n• Enforce best practices for data quality, governance, security, and compliance across the entire data ecosystem.\n• Monitor system performance, troubleshoot issues, and drive continuous improvements in data infrastructure.\n• Conduct code reviews, define coding standards, and promote engineering excellence across the team.\n• Mentor and guide junior data engineers, fostering a culture of technical growth and innovation.\n\nQualifications\n\nRequirements\n• 8+ years of experience in data engineering with proven leadership in managing data projects and teams.\n• Expertise in Python, SQL, Spark (PySpark), and experience with AWS and Databricks in production environments.\n• Strong understanding of modern data architecture, distributed systems, and cloud-native solutions.\n• Excellent problem-solving, communication, and collaboration skills.\n• Prior experience mentoring team members and contributing to strategic technical decisions is highly desirable.",
         "eyJqb2JfdGl0bGUiOiJNYW5hZ2VyIERhdGEgRW5naW5lZXIgLSBBV1MgRGF0YWJyaWNrcyIsImNvbXBhbnlfbmFtZSI6IkJsZW5kMzYwIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJxelhlVGQtWWc2ekE5U3R4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "3 days ago",
         "Data Engineer"
        ],
        [
         "Data Engineer INTL India - EOR 6fb570f8",
         "INSIGHT GLOBAL",
         "Hyderabad, Telangana, India",
         "- In this role, you will be building data pipeline solutions by designing, adopting, and applying big data strategies and architectures. You must have knowledge of scalable system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration, data transformation, data modeling, and data cleansing.\n- The Sr. Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for business stakeholders.\n- Design and develop a logical data model for a large multi-tenant application with intent to facilitate consistency and efficiency.\n- Collaborate with analysts and internal clients to understand the goals of the overall system data architecture, and further develop on business needs/issues.\n- Test/troubleshoot problems and conduct root cause analysis.\n- Own communications with technical contacts at software vendors to escalate software issues, recommend enhancements, and find sustainable workarounds to issues.\n- Work in tandem with analysts and other stakeholders to develop and execute necessary processes and controls around the flow of data to meet data governance standards.\n- Verify accuracy of table changes and data transformation processes\n- Deliver fully tested code prior to prod-deployment when appropriate.\n- Recommend and implement enhancements that address system performance requirements, streamline processes and improve data integrity.\n- Create sound technical documentation and train peer developers on this documentation as development completes.\n- Additional duties as assigned to ensure company success.\nThe compensation for this role varies depending on depth and length of experience, and can range from 27-36 LAKH.\n\nWe are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form. The EEOC \"Know Your Rights\" Poster is available here.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElOVEwgSW5kaWEgLSBFT1IgNmZiNTcwZjgiLCJjb21wYW55X25hbWUiOiJJbnNpZ2h0IEdsb2JhbCIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiZUtoaVo5cWozWC1wLXlBVEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 days ago",
         "Data Engineer"
        ],
        [
         "Data Engineer-Senior II",
         "FEDERAL EXPRESS CORPORATION AMEA",
         "Hyderabad, Telangana, India",
         "Responsible for supporting the development and maintenance of business intelligence and analytics systems, implementing data warehouse architecture, enabling self-service data exploration, designing data models, implementing data security strategies, optimizing production processes, providing user training and support, and overseeing project management activities as a Data Engineer to facilitate data-driven decision-making and generate insights for business stakeholders.\n\n1. Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making.\n2. Implement of business intelligence and analytics systems, ensuring alignment with business requirements.\n3. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets.\n4. Enable self-service data exploration capabilities for users to analyze and visualize data independently.\n5. Develop reporting and analysis applications to generate insights from data for business stakeholders.\n6. Design and implement data models to organize and structure data for analytical purposes.\n7. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information.\n8. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability.\n9. Assist in training and support to users on business intelligence tools and applications.\n10. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions.\n\nEducation: Bachelors’ degree or equivalent in Computer Science, MIS, or similar discipline.\nAccreditation: Specific business accreditation for Business Intelligence.\n\nExperience: Relevant work experience in data engineering based on the following number of years:\nAssociate: Prior experience not required\nStandard I: Two (2) years\nStandard II: Three (3) years\nSenior I: Four (4) years\nSenior II: Five (5) years\n\nKnowledge, Skills and Abilities\n• Fluency in English\n• Analytical Skills\n• Accuracy & Attention to Detail\n• Numerical Skills\n• Planning & Organizing Skills\n• Presentation Skills\n\nPreferred Qualifications:\n\nPay Transparency:\n\nPay:\n\nAdditional Details:\n\nFedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n\nAll qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\nOur Company\n\nFedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World’s Most Admired Companies by \"Fortune\" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\nOur Philosophy\n\nThe People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\nOur Culture\n\nOur culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970’s. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today’s global marketplace.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLVNlbmlvciBJSSIsImNvbXBhbnlfbmFtZSI6IkZlZGVyYWwgRXhwcmVzcyBDb3Jwb3JhdGlvbiBBTUVBIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJhZDU5RmU0Vl94QzdTVGFqQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "1 day ago",
         "Data Engineer"
        ],
        [
         "Lead Data Engineer(Snowflake,PowerBi)",
         "THOMSON REUTERS",
         "Hyderabad, Telangana, India (+1 other)",
         "Want to be part of the Data & Analytics organization, whose strategic goal is to create a world-class Data & Analytics company by building, embedding, and maturing a data-driven culture across Thomson Reuters.\n\nAbout The Role\nWe are looking for a highly motivated individual with strong organizational and technical skills for the position of Lead Data Engineer/ Data Engineering Manager (Snowflake). You will play a critical role working on cutting edge of Data Engineering and analytics, leveraging predictive models, machine learning and generative AI to drive business insights and facilitating informed decision-making and help Thomson Reuters rapidly scale data-driven initiatives.\n\nEffectively communicate across various levels, including Executives, and functions within the global organization.\nDemonstrate strong leadership skills with ability to drive projects/tasks to delivering value\nEngage with stakeholders, business analysts and project team to understand the data requirements.\nDesign analytical frameworks to provide insights into a business problem.\nExplore and visualize multiple data sets to understand data available and prepare data for problem solving.\nDesign database models (if a data mart or operational data store is required to aggregate data for modeling).\n\nAbout You\nYou're a fit for the Lead Data Engineer/ Data Engineering Manager (Snowflake), if your background includes:\nQualifications: B-Tech/M-Tech/MCA or equivalent\nExperience: 7-9 years of corporate experience\nLocation: Bangalore, India\nHands-on experience in developing data models for large scale data warehouse/data Lake – Snowflake, BW\nMap the data journey from operational system sources through any transformations in transit to its delivery into enterprise repositories (Warehouse, Data Lake, Master Data, etc.)\nEnabling on the overall master and reference data strategy, including the procedures to ensure the consistency and quality of Finance reference data.\nExperience across ETL, SQL and other emerging data technologies with experience in integrations of a cloud-based analytics environment\nBuild and refine end-to-end data workflows to offer actionable insights\nFair understanding of Data Strategy, Data Governance Process\nKnowledge in BI analytics and visualization tools: Power BI, Tableau\n\n#LI-NR1\n\nWhat’s in it For You?\n• Hybrid Work Model: We’ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\n• Flexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\n• Career Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow’s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\n• Industry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\n• Culture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\n• Social Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\n• Making a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\n\nAbout Us\n\nThomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\n\nWe are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\n\nAs a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\n\nWe also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here.\n\nLearn more on how to protect yourself from fraudulent job postings here.\n\nMore information about Thomson Reuters can be found on thomsonreuters.com.",
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIoU25vd2ZsYWtlLFBvd2VyQmkpIiwiY29tcGFueV9uYW1lIjoiVGhvbXNvbiBSZXV0ZXJzIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJzSFl5TXBoOXlWbDl4R0pBQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "13 days ago",
         "Data Engineer"
        ],
        [
         "Python Developer – Telegram Bot Integration & Excel Automation",
         "SANGA & ASSOCIATES - EQUIDOTE",
         "Anywhere",
         "Job Title:\n\nPython Developer – Telegram Bot Integration & Excel Automation\n\nJob Description:\n\nWe are seeking a skilled and detail-oriented Python Developer to help automate the process of sending structured trade updates from an Excel file to a Telegram channel using a Telegram bot.\n\nThis is a freelance / part-time project with the potential for ongoing work based on performance.\n\nResponsibilities:\n• Read data from an Excel file that is regularly updated using Python.\n• Format and send messages to a Telegram channel or group using the Telegram Bot API.\n• Implement conditions to filter or trigger messages based on specific columns (e.g., P&L thresholds).\n• Ensure the messages are well-formatted and synchronized.\n• (Optional) Schedule the script to run at regular intervals (e.g., every 5 mins or on update).\n• Ensure error handling, avoid duplicate messages, and maintain clean logs.\n\nRequired Skills:\n• Strong experience with Python scripting\n• Proficiency in using pandas for Excel/CSV handling\n• Working knowledge of the Telegram Bot API\n• Experience with HTTP requests (requests library)\n• Ability to format dynamic messages (Markdown/HTML for Telegram)\n• (Optional but a plus) Familiarity with scheduling tools like schedule, cron jobs, or apscheduler\n\nNice to Have:\n• Understanding of stock market data or options trading (for better context)\n• Experience integrating with trading APIs or using TradingView alerts\n• Basic knowledge of Excel automation or VBA\n\nProject Details:\n• Project Type: One-time setup, with possible ongoing maintenance\n• Location: Remote (India preferred)\n• Start Date: Immediate\n\nHow to Apply:\n\nPlease apply with:\n• A short summary of your experience with Python + Telegram Bots\n• A link to any relevant projects or GitHub repos\n• Your expected rate and estimated time to complete the task\n\nJob Type: Freelance\n\nBenefits:\n• Health insurance\n• Provident Fund\n• Work from home\n\nSchedule:\n• Day shift\n\nSupplemental Pay:\n• Performance bonus\n• Yearly bonus\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIOKAkyBUZWxlZ3JhbSBCb3QgSW50ZWdyYXRpb24gXHUwMDI2IEV4Y2VsIEF1dG9tYXRpb24iLCJjb21wYW55X25hbWUiOiJTQU5HQSBcdTAwMjYgQVNTT0NJQVRFUyAtIEVRVUlET1RFIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNBNXdGSEplRUV3c2lQVWJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "1 day ago",
         "Python Developer"
        ],
        [
         "Python Developer - Remote",
         "XPRESS HEALTH",
         "Anywhere",
         "Job Title: Python Developer\nLocation: Remote\nSalary: Up to ₹12 LPA (based on experience and skillset)\nExperience: 3–6 years (preferred)\nEmployment Type: Full-time\n\nAbout Xpress Health\n\nXpress Health is a healthtech platform transforming clinical workforce management by connecting healthcare professionals to facilities in real time. With operations across Ireland and rapid expansion underway, we’re solving critical staffing challenges in the healthcare sector using advanced technology and automation.\n\nRole Overview\n\nWe are looking for a Python Developer with solid experience in backend development and building scalable, secure systems. You will work closely with cross-functional teams to build backend services, automate operations, and power data-driven features in our real-time healthcare staffing platform. If you're passionate about solving real-world problems through code, this role is for you.\n\nKey Responsibilities\n• Design, develop, and maintain backend services and APIs using Python (preferably with frameworks like Django or Flask).\n• Build scalable systems for real-time scheduling, user management, and analytics.\n• Integrate third-party APIs and internal services securely and efficiently.\n• Work with databases (SQL and NoSQL) to design efficient schemas and queries.\n• Optimize performance and ensure system reliability under scale.\n• Collaborate with frontend, product, and QA teams to deliver complete features.\n• Write clean, maintainable, and well-documented code.\n• Participate in code reviews, system design discussions, and architecture planning.\n\nRequirements\n• 3–6 years of professional experience with Python backend development.\n• Strong knowledge of Django, Flask, or other web frameworks.\n• Proficient in working with relational databases (MySQL/PostgreSQL) and REST APIs.\n• Experience with Git, CI/CD pipelines, and working in an Agile environment.\n• Strong debugging, testing, and problem-solving skills.\n• Good communication and ability to collaborate with remote teams.\n\nPreferred Qualifications\n• Experience in healthcare, staffing, or enterprise SaaS platforms.\n• Familiarity with containerization tools like Docker and orchestration platforms (Kubernetes).\n• Exposure to cloud platforms like AWS, GCP, or Azure.\n• Knowledge of async programming and task queues (e.g., Celery, Redis).\n• Experience working with frontend teams using React/Vue (a plus).\n\nWhat We Offer\n• Competitive salary up to ₹12 LPA, depending on experience.\n• A mission-driven environment working on meaningful, real-world problems.\n• Opportunity to shape a rapidly scaling healthtech product.\n• Flexible work culture with remote options and learning opportunities.\n• Collaborative, cross-functional team with international exposure.\n\nBe part of a product-first team building real-time tech that supports critical healthcare systems. Join Xpress Health as a Python Developer !!!\n\nJob Type: Full-time\n\nPay: Up to ₹1,200,000.00 per year\n\nBenefits:\n• Paid time off\n• Work from home\n\nLocation Type:\n• Remote\n\nSchedule:\n• Evening shift\n• Fixed shift\n• Monday to Friday\n• UK shift\n\nApplication Question(s):\n• What is your current and expected CTC?\n• Are you currently working? If yes, what is your notice period?\n\nExperience:\n• Python : 5 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIC0gUmVtb3RlIiwiY29tcGFueV9uYW1lIjoiWHByZXNzIEhlYWx0aCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJGWjc0LW4wM2NjU2xGWVVOQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "2 days ago",
         "Python Developer"
        ],
        [
         "Full Stack Developer (Python / React JS)",
         "HITACHI CAREERS",
         "India",
         "Our Company\n\nWe're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n\nOur group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n\nImagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to 'fit' every requirement - your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n\nThe team\n\nWe are seeking a talented and experienced Full Stack Developer to join our team, specializing in low-code/no-code process automation. The ideal candidate will be responsible for designing, developing, and implementing solutions using the platform to streamline and automate business processes. This role involves working closely with business stakeholders to understand their requirements and deliver efficient, scalable, and user-friendly applications. Additionally, the candidate should have prior experience in building digital apps on Google Cloud Platform (GCP), Graph DB, and leveraging advanced technologies. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n\nThe role: Full Stack Developer/Specialist\n\nResponsibilities:\n• Design, develop, and maintain applications.\n• Collaborate with business stakeholders to gather requirements and translate them into technical specifications and functional designs. Required skills include Python and React JS. Experience with machine learning model development is a plus.\n• Implement end-to-end solutions, including front-end interfaces and back-end services, ensuring seamless integration and functionality.\n• Develop and maintain custom components and integrations using React JS and other relevant technologies.\n• Optimise applications for performance, scalability, and user experience.\n• Conduct system testing, validation, and troubleshooting to ensure the quality and performance of solutions.\n• Provide training and support to end-users and IT staff on functionalities and best practices.\n• Stay up-to-date with the latest developments in low-code/no-code platforms and industry trends.\n• Participate in project planning, execution, and post-implementation support.\n• Mentor and guide junior developers, fostering a culture of continuous learning and improvement.\n• What you'll bring\nQualifications:\n• Bachelor's degree in Computer Science, Information Technology, or a related field. A Master's degree is a plus.\n• Proven experience as a Full Stack Developer with a portfolio of low-code/no-code applications.\n• Expertise in development and customisation.\n• Proficiency in Python and React JS\n• Strong understanding of front-end technologies such as HTML, CSS, and modern React JS frameworks.\n• Experience with RESTful APIs and web services.\n• Excellent problem-solving skills and attention to detail.\n• Strong communication and collaboration skills.\n• Ability to work independently and as part of a team in a fast-paced environment.\n• Prior experience in building custom digital apps with a strong SQL background leveraging Graph DB, BigQuery, and PostgresSQL is required.\n• Prior experience in building AI applications is a plus.\n• Prior experience with automation tools like UIPath is a plus.\n• Working knowledge of AI/Machine Learning, with a focus on Agentic AI, is a plus.\nPreferred Skills\n• Certification in\n• Experience with other low-code/no-code platforms.\n• Knowledge of DevOps practices and tools, including CI/CD pipelines.\n• Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.\n• Experience with Agile development methodologies.\nAbout us\n\nWe're a global, 1000-strong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n\n#LI-MS3\n\nChampioning diversity, equity, and inclusion\n\nDiversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n\nHow we look after you\n\nWe help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\nWe're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",
         "eyJqb2JfdGl0bGUiOiJGdWxsIFN0YWNrIERldmVsb3BlciAoUHl0aG9uIC8gUmVhY3QgSlMpIiwiY29tcGFueV9uYW1lIjoiSGl0YWNoaSBDYXJlZXJzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImdIVU8wejU5cUlpZkJMaVZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "Python Developer"
        ],
        [
         "Python Developer Role",
         "PITANGENT ANALYTICS AND TECHNOLOGY SOLUTIONS PVT. LTD.",
         "India",
         "Overview\n\nPi tangent Group of Companies is an ISO:9001: 2015 Certified, CMMIL-3, Award winning Software Development Company in Eastern India. It caters to areas like AI/ML to Web development to SAAS engineering. The Group is actively seeking out talented, enthusiastic Python Developers as a value addition to the growing team at Kolkata.\n\nKey Responsibilities\n• Design and develop robust backend applications using Python.\n• Collaborate with front-end developers to integrate user-facing elements with server-side logic.\n• Implement RESTful APIs for seamless communication between server and client.\n• Write reusable, testable, and efficient code following best practices.\n• Manage and optimize multiple databases and data storage solutions.\n• Perform unit and integration testing to ensure software reliability.\n• Participate in code reviews and maintain version control in Git.\n• Gather and analyze user requirements to provide optimal solutions\n• Contribute to project documentation and specifications.\n• Collaborate with QA engineers to troubleshoot and resolve issues.\n• Maintain quality assurance processes to ensure best practices are enforced.\n• Engage in agile development practices, participating in sprints and meetings.\n• Mentor junior developers and provide guidance as needed.\n\nRequired Qualifications\n• Bachelor's degree in computer science or related field.\n• 1-2 yrs of experience in Python development.\n• Strong understanding of Django or Flask web frameworks.\n• Proficient with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB).\n• Experience with version control systems, preferably Git.\n• Solid understanding of RESTful API design principles.\n• Familiarity with front-end technologies (HTML, CSS, JavaScript).\n• Experience with containerization tools such as Docker.\n• Strong communication and teamwork abilities.\n• Familiarity with cloud services (AWS, Azure) is a plus.\n• Understanding of security principles and best practices.\n• Experience with Agile/Scrum methodologies.\n• Proven ability to manage multiple tasks and meet deadlines.\n\nSkills: agile,communication,nosql,scrum,aws lambda,docker,sql,azure,html,javascript,mongodb,css,django,aws,git,postgresql,restful apis,python,flask",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gRGV2ZWxvcGVyIFJvbGUiLCJjb21wYW55X25hbWUiOiJQaXRhbmdlbnQgQW5hbHl0aWNzIGFuZCBUZWNobm9sb2d5IFNvbHV0aW9ucyBQdnQuIEx0ZC4iLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiLUNHQ2RNdUVNQ2V4X3F2MkFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "2 days ago",
         "Python Developer"
        ],
        [
         "Python and Groovy Framework Developer",
         "APTITA",
         "India",
         "Urgent Hiring!!!\n\nRole : Python and Groovy Framework Developer\n\nMandatory Skills: Python, Appium, Groovy, Git\n\nExperience: 3 to 8 Years\n\nLocation: Bengaluru\n\nContract - 1Year\n\nJob Description:\n\nQualifications\n\n Bachelor’s or master’s degree in Computer Science, Software Engineering, or a\n\nrelated field\n\n 6 to 8 years of relevant experience in quality assurance, with a proven track record in\n\nWebKit or browser engine testing, including team leadership responsibilities.\n\n Proficiency in CI/CD and test automation frameworks such as Appium, and scripting\n\nlanguages like Python, or Shell.\n\nJob Overview\n\nWe are seeking a dynamic and experienced Lead Python & Groovy Framework developer\n\nto join our team You will be part of a fast-paced, Agile development team and work on a\n\nvariety of projects, from building new tools and solutions to improving existing ones.\n\nIn this role, you will have the chance to grow your skills and take your career to the next\n\nlevel. We offer a supportive, challenging, and exciting work environment, with\n\nopportunities for professional development, training, and advancement.\n\nIf you are a Python & Groovy Framework Developer Engineer with a passion for\n\ntechnology and a drive to continuously improve processes, we want to hear from you!\n\nIf you are passionate about browser engine technologies, performance optimization, and\n\nleadership, we encourage you to apply!\n\nPrimary Skills:\n\n Strong experience in Python Framework development, with the ability to automate\n\nand optimize processes using Jenkins Pipeline script\n\n Good knowledge in Groovy scripting\n\n Expertise in CI/CD tools such as Jenkins, CircleCI, or GitLab\n\n Good understanding of Appium.\n\nStrong Problem solving and debugging skills.\n\n Excellent communication and collaboration skills, both with technical and non-\n\ntechnical stakeholders\n\n Version Control: Familiarity with version control systems such as Git for reviewing\n\nchanges and ensuring test coverage.\n\n Communication: Strong communication and collaboration skills for working with\n\ncross-functional teams.\n\n Agile Methodologies: Experience with Agile Scrum methodologies\n\nNotice Period: Immediate- 30 Days\n\nEmail to : sharmila.m@aptita.com\n\n·",
         "eyJqb2JfdGl0bGUiOiJQeXRob24gYW5kIEdyb292eSBGcmFtZXdvcmsgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQXB0aXRhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6Imx1ckt5NFVRSi0wb3JkMi1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "2 days ago",
         "Python Developer"
        ],
        [
         "AI Python Developer",
         "ALLIANZ INSURANCE",
         "India",
         "We are seeking a skilled AI Developer proficient in Python to support various AI use cases within our dynamic team. The ideal candidate will possess a strong understanding of Python programming and a basic familiarity with PyTest. This role will also provide opportunities to work with cloud computing technologies, particularly Azure, and develop APIs.\n\nKey Responsibilities:\n• Utilize advanced Python skills to support and enhance AI use cases, ensuring the efficient and effective implementation of AI solutions.\n• Develop and execute comprehensive test scripts using PyTest to ensure the reliability and accuracy of AI models and applications.\n• Collaborate with cross-functional teams, including data scientists, engineers, and product managers, to integrate AI capabilities into existing systems and workflows.\n• Analyze and optimize AI algorithms for performance and scalability, ensuring they meet business requirements.\n• Stay updated with the latest advancements in AI and machine learning technologies and apply them to improve existing solutions.\n\nRequirements:\n\nMust-Have\n• Python: Advanced proficiency with extensive experience in writing efficient, maintainable, and scalable code. Demonstrated ability to solve complex problems using Python.\n• PyTest: Basic experience in writing and executing unit tests, with a fundamental understanding of test-driven development practices.\n\nGood-to-Have\n• Cloud Computing: Familiarity with cloud platforms and services, with hands-on experience in deploying and managing AI applications in a cloud environment.\n• Azure: Experience with Microsoft Azure, particularly in using its AI and machine learning services.\n• APIs: Knowledge of designing, developing, and consuming APIs, particularly RESTful APIs, for integrating AI solutions with other systems and applications\n\nAbout Allianz Technology\n\nAllianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 13,000 employees located in 22 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry. We oversee the full digitalization spectrum – from one of the industry’s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n\nD&I statement\n\nAllianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, race or ethnicity, age, nationality, religion, disability, or philosophy of life. Join us. Let´s care for tomorrow.",
         "eyJqb2JfdGl0bGUiOiJBSSBQeXRob24gRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiQWxsaWFueiBJbnN1cmFuY2UiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQkg5Z0tqUFM2TjBGeExOb0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "24 days ago",
         "Python Developer"
        ],
        [
         "Developer- Angular, Python & Azure",
         "THE VALUE MAXIMIZER",
         "India",
         "About the Role :\n\nAs a Senior Developer, you will play a critical role in designing, implementing, and optimizing IoT applications, Salesforce development, and web solutions. This role is ideal for experienced developers eager to lead projects, mentor junior team members, and shape technical strategies across diverse platforms and technologies. You will be an important part of our international project teams, will work closely with team members from the headquarters and if qualified can lead international software development projects.\n\nKey Responsibilities :\n• Central role and expert for development of IoT frontend applications with Angular, Capacitor, and PrimeNG\n• Design and implement IoT backend systems using C#, .Net 8.0, and the Azure platform\n• Management and optimization of CI/CD pipelines using Azure DevOps and Bicep\n• Drive advanced Salesforce development efforts, including customizations, integrations, and automation\n• Manage and enhance the SharePoint Online intranet platform\n• Architect and implement Power Platform solutions tailored to business needs\n• Develop and maintain complex web applications using Django (Python) and PHP\n• Supervise website development and optimization with Typo3, PHP, and SolR, collaborating closely with the marketing department\n• Monitor and ensure the performance, compliance, and SEO optimization of global websites and portals\n\nKey Requirements :\n• 6-8 years of experience\n• Strong expertise in Angular, Python, and C#\n• Advanced experience with CI/CD pipelines and version control systems (e.g., Azure DevOps or Git)\n• Proficiency in web technologies such as PHP, MySQL, and modern frameworks\n• In-depth knowledge of Salesforce, Azure cloud solutions, and SharePoint Online\n• Proven ability to lead technical projects, mentor team members, and collaborate across departments\n• Excellent problem-solving skills, attention to detail, and a proactive approach to innovation\n• Proficiency in German is an advantage but not a requirement. Candidates must, however, have a minimum of B2-level English proficiency to communicate effectively within the international teams and our headquarters",
         "eyJqb2JfdGl0bGUiOiJEZXZlbG9wZXItIEFuZ3VsYXIsIFB5dGhvbiBcdTAwMjYgQXp1cmUiLCJjb21wYW55X25hbWUiOiJUaGUgVmFsdWUgTWF4aW1pemVyIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlRteXhMclAxTG5oaWFJZEVBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "Python Developer"
        ],
        [
         "Freelance Python Developer",
         "GBIM TECHNOLOGIES PVT.LTD.",
         "Anywhere",
         "We’re Hiring – Freelance Python Developer (Experienced)\nWe are currently looking for a freelance Python developer who brings hands-on experience and can help us build and debug efficient backend systems, automate processes, and solve critical issues with speed and precision.\nKey Expertise Required:\n\nPython (Backend Development)\n\nWeb Scraping & Data Extraction\n\nWeb Automation\n\nFlask | Pandas | ETL\n\nAWS (Basic to Intermediate)\n\nGoogle / Meta / LinkedIn / Third-Party API Integration\n\nProblem-solving mindset – quick in identifying & fixing bugs/errors\n\nIf you are a solution-driven individual with a solid track record in handling Python-based backend projects and automation tasks, we’d love to connect with you!\nPlease DM or share your portfolio/work samples at [hr@gbim.com / sarabjeet.mann@gbim.in ].\n#PythonDeveloper #FreelanceOpportunity #WebScraping #Automation #BackendDevelopment #Flask #AWS #APIIntegration #DataExtraction #HiringNow #LinkedInJobs\n\nJob Type: Full-time\n\nPay: ₹500.00 - ₹10,000.00 per hour\n\nLocation Type:\n• Remote\n\nSchedule:\n• Day shift\n• Monday to Friday\n\nWork Location: Remote\n\nSpeak with the employer\n+91-XXXXXXXXXX",
         "eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgUHl0aG9uIERldmVsb3BlciIsImNvbXBhbnlfbmFtZSI6IkdCSU0gVGVjaG5vbG9naWVzIFB2dC5MdGQuIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImNtMXFFOGh4Y2xUN3Z1ck9BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "3 days ago",
         "Python Developer"
        ],
        [
         "DET-Senior GIG Python Developer-GDSNF02",
         "EY",
         "India",
         "At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. Senior Python Developer – Client – Merck, H and M, TIAA, MINT Job Description Bachelor’s or master’s degree with 3+ years of strong Python development experience Design, develop, and maintain high-performance web applications using Python and related frameworks. Strong understanding of Python OOPs, Data type, Data Structure and algorithm , Exception handling, Decarotor , Generator, Iterator, Automation. Strong understanding of Python Libraries (Pandas, TensorFlow, Numpy, SciPy) Experience in Cloud Azure / AWS Develop, optimize, and manage complex APIs (RESTful or GraphQL). Collaborate with cross-functional teams to define, design, and ship new features. Troubleshoot and resolve advanced technical issues in development and production environments. Conduct technical evaluations of new tools and frameworks, recommending their adoption when appropriate. Stay ahead of emerging trends in Python development, ensuring the team remains at the forefront of innovation. Advanced proficiency in Python and frameworks like Django,Flask, or FastAPI. Good understanding of Database Postgres / MySQL & ORM Library i.e. SQL Alchemy/ any ORM libraries Understanding of Code Repository tools i.e. GIT , SVN Strong understanding of DevOps principles(Docker, Kubernetes and microservices) EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",
         "eyJqb2JfdGl0bGUiOiJERVQtU2VuaW9yIEdJRyBQeXRob24gRGV2ZWxvcGVyLUdEU05GMDIiLCJjb21wYW55X25hbWUiOiJFWSIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJ4Mk5pUFZHVVJ3cjczaVE1QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "18 hours ago",
         "Python Developer"
        ],
        [
         "Informatica ETL Developer: Agile Dev Team Member IV",
         "CAPGEMINI",
         "Hyderabad, Telangana, India",
         "The ideal candidate will be responsible for designing, developing, and deploying scalable ETL processes using Informatica PowerCenter to support our data warehousing and analytics initiatives. You will collaborate with business and technical stakeholders to ensure high data quality, availability, and performance. Key Responsibilities:Design, develop, and maintain ETL workflows and mappings using Informatica PowerCenter or Informatica Intelligent Cloud Services (IICS).Extract, transform, and load data from various source systems (e.g., SQL Server, Oracle, flat files, cloud APIs) into data warehouses or operational data stores.Optimize ETL performance, conduct tuning, and ensure error handling and logging.Collaborate with data architects and analysts to understand data requirements and deliver high-quality data solutions.Work with QA teams to support data validation and testing efforts.Support data integration, migration, and transformation initiatives.Document ETL processes, data flows, and job schedules.Monitor daily ETL jobs and resolve production issues in a timely manner.RequirementsBachelor’s degree in Computer Science, Information Systems, or a related field (or equivalent work experience).3+ years of experience with Informatica PowerCenter or Informatica IICS.Strong SQL skills and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).Solid understanding of data warehousing concepts and dimensional modeling.Experience in performance tuning and troubleshooting ETL processes.Hands-on experience with job scheduling tools (e.g., Autosys, Control-M, Tidal).Familiarity with version control systems and DevOps practices.Preferred Qualifications:Experience with cloud data platforms (e.g., Snowflake, AWS Redshift, Azure Synapse).Exposure to data governance and data quality tools.Knowledge of scripting languages (e.g., Shell, Python).Experience working in Agile/Scrum environments.Familiarity with BI tools (e.g., Tableau, Power BI) is a plus. BenefitsThis position comes with competitive compensation and benefits package: Competitive salary and performance-based bonuses Comprehensive benefits package Home Office model Career development and training opportunities Flexible work arrangements (remote and/or office-based) Dynamic and inclusive work culture within a globally known group Private Health Insurance Pension Plan Paid Time Off Training & Development *Note: Benefits differ based on employee level",
         "eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyOiBBZ2lsZSBEZXYgVGVhbSBNZW1iZXIgSVYiLCJjb21wYW55X25hbWUiOiJDYXBnZW1pbmkiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InFZVlFqNTFEM2VzZ0lZSExBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 days ago",
         "ETL Developer"
        ],
        [
         "Senior ETL and Backend Developer (Salesforce)",
         "S&P GLOBAL",
         "Hyderabad, Telangana, India (+1 other)",
         "About the Role:\n\nGrade Level (for internal use):\n10\n\nTitle: Senior ETL and Backend Developer (Salesforce)\n\nJob Location: Hyderabad, Ahmedabad, Gurgaon, Virtual-India\n\nThe Team: We are seeking a skilled Senior ETL and Backend Developer with extensive experience in Informatica and Salesforce. The ideal candidate will be responsible for designing, developing, and maintaining ETL processes and backend systems to ensure seamless data integration and management.\n\nThe team works in a challenging environment that gives ample opportunities to use innovative ideas to solve complex problems. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\n\nThe Impact: You will be making significant contribution in building solutions for the Web applications using new front-end technologies & Micro services. The work you do will deliver products to build solutions for S&P Global Commodity Insights customers.\n\nResponsibilities:\n• ETL Development: Design, develop, and maintain ETL processes using Informatica PowerCenter and other ETL tools.\n• Data Integration: Integrate data from various sources, including databases, APIs, flat files, and cloud storage, into data warehouses or data lakes.\n• Backend Development: Develop and maintain backend systems using relevant programming languages and frameworks.\n• Salesforce Integration: Implement and manage data integration between Salesforce and other systems.\n• Performance Tuning: Optimize ETL processes and backend systems for speed and efficiency.\n• Data Quality: Ensure data quality and integrity through rigorous testing and validation.\nMonitoring and Maintenance: Continuously monitor ETL processes and backend systems for errors or performance issues and make necessary adjustments.\n• Collaboration: Work closely with data architects, data analysts, and business stakeholders to understand data requirements and deliver solutions.\nQualifications:\n\nBasic Qualifications:\n• Bachelor's /Master’s Degree in Computer Science, Information Systems or equivalent.\n• A minimum of 8+ years of experience in software engineering & Architecture.\n• A minimum 5+ years of experience in ETL development, backend development, and data integration.\n• A minimum of 3+ years of Salesforce development, administration/Integration.\n• Proficiency in Informatica PowerCenter and other ETL tools.\n• Strong knowledge of SQL and database management systems (e.g., Oracle, SQL Server).\n• Experience with Salesforce integration and administration.\n• Proficiency in backend development languages (e.g., Java, Python, C#).\n• Familiarity with cloud platforms (e.g., AWS, Azure) is a plus.\n• Excellent problem-solving skills and attention to detail.\n• Ability to work independently and as part of a team.\n• Nice to have – GenAI, Java, Spring boot, Knockout JS, requireJS, Node.js, Lodash, Typescript, VSTest/ MSTest/ nUnit.\n\nPreferred Qualifications:\n• Proficient with software development lifecycle (SDLC) methodologies like SAFe, Agile, Test- driven development.\n• Experience with other ETL tools and data integration platforms.\n• Informatica Certified Professional\nSalesforce Certified Administrator or Developer\n• Knowledge of back-end technologies such as C#/.NET, Java or Python.\n• Excellent problem solving, analytical and technical troubleshooting skills.\n• Able to work well individually and with a team. Good work ethic, self-starter, and results oriented. \n• Excellent communication skills are essential, with strong verbal and writing proficiencies.\n\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat’s In It For You?\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nWe're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n\nOur benefits include:\n• Health & Wellness: Health care coverage designed for the mind and body.\n• Flexible Downtime: Generous time off helps keep you energized for your time on.\n• Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n• Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n• Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n• Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring and Opportunity at S&P Global:\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\n-----------------------------------------------------------\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. \n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n-----------------------------------------------------------\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)\n\nJob ID: 316835\nPosted On: 2025-06-03\nLocation: Hyderabad, Telangana, India",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRVRMIGFuZCBCYWNrZW5kIERldmVsb3BlciAoU2FsZXNmb3JjZSkiLCJjb21wYW55X25hbWUiOiJTXHUwMDI2UCBHbG9iYWwiLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IjVGNUtpak5fRjROQXhDcERBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "19 days ago",
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "ZENSAR TECHNOLOGIES",
         "Madhavaram, Telangana, India",
         "Job Description\n\nPrimary Skill Set\n• ETL Informatica\n• SQL\n• Unix\n• Realtime Data Integration (CDC) using Power Exchange (Can be optional if candidate good in other skills)\n\nGood to Have\n\nExperience on working with Mainframe Databases/files\n\nETL Batch Scheduling tools like TWS/Tidal\n\nRoles & Responsibilities\n\nInformatica PowerCenter, Unix scripting, SQL/PLSQL\n\nKnowledge of Informatica Power Exchange is preferred\n\nExperience With Mainframe Sources/targets Is Preferred\n• Bachelor’s degree in Computer Science or similar field or equivalent work experience.\n• 5-8 years of development experience with Informatica Power Center on Data Integration projects.\n• Strong analytic, problem-solving and organizational skills.\n• Excellent SQL knowledge and ability to write the complex queries keeping performance aspect in mind.\n• Experience with analysis of business requirements, designing and writing technical specifications to design.\n• Hands-on experience to process mainframe files using Informatica Power Exchange.\n• Hands-on experience with UNIX shell scripting.\n• Participate in testing and issue resolution to validate functionality and performance.\n• Hands-on experience on any job scheduling tool, TWS is preferred.\n• Good written and verbal communication skills.\n\nLocation\n\n1 st Preference: Noida\n\n2 nd Preference: Hyderabad\n\n3 rd Preference: Gurgaon",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiWmVuc2FyIFRlY2hub2xvZ2llcyIsImFkZHJlc3NfY2l0eSI6Ik1hZGhhdmFyYW0sIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InJTQzlGVjVoRHhXZ1h3QURBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "11 days ago",
         "ETL Developer"
        ],
        [
         "Data ETL Developer / BI Engineer",
         "AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL",
         "India",
         "ETL Developer\n\nAmex GBT is a place where colleagues find inspiration in travel as a force for good and – through their work – can make an impact on our industry. We’re here to help our colleagues achieve success and offer an inclusive and collaborative culture where your voice is valued.\n\nWe are looking for an experienced Data ETL Developer / BI Engineer who loves solving complex problems across a full spectrum of data & technologies. You will lead the building effort of GBT's new BI platform and manage the legacy platform to seamlessly support our business function around data and analytics. You will create dashboards, databases, and other platforms that allow for the efficient collection and evaluation of BI data.\n\nWhat You’ll Do on a Typical Day:\n• Design, implement, and maintain systems that collect and analyze business intelligence data.\n• Design and architect an analytical data store or cluster for the enterprise and implement data pipelines that extract, transform, and load data into an information product that helps the organization reach strategic goals.\n• Create physical and logical data models to store and share data that can be easily consumed for different BI needs.\n• Develop Tableau dashboards and features.\n• Create scalable and high-performance data load and management process to make data available near real-time to support on-demand analytics and insights.\n• Translate complex technical and functional requirements into detailed designs.\n• Investigate and analyze alternative solutions to data storing, processing, etc., to ensure the most streamlined approaches are implemented.\n• Serve as a mentor to junior staff by conducting technical training sessions and reviewing project outputs\n• Design & develop, and maintain a data model implementing ETL processes.\n• Manage and maintain the database, warehouse, & cluster with other dependent infrastructure.\n• Work closely with data, products, and another team to implement data analytic solutions.\n• Support production application and Incident management.\n• Help define data governance policies and support data versioning processes\n• Maintain security and data privacy by working closely with the Data Protection Officer internally.\n• Analyze a vast number of data stores and uncover insights\n\nWhat We’re Looking For:\n• Degree in computer sciences or engineering\n• Overall, 3-5 years of experience in data & data warehouse, ETL, and data modeling.\n• 2+ years of experience working and managing large data stores, complex data pipelines, and BI solutions.\n• Strong experience in SQL and writing complex queries.\n• Hands-on experience with Tableau development.\n• Hands-on working experience on Redshift, data modeling, data warehouse, ETL tool, Python, and Shell scripting.\n• Understanding of data warehousing and data modeling techniques\n• Strong data engineering skills on the AWS Cloud Platform are essential.\n• Knowledge of Linux, SQL, and any scripting language\n• Good interpersonal skills and a positive attitude\n• Experience in travel data would be a plus.\n\nLocation\nGurgaon, India\n\nThe #TeamGBT Experience\n\nWork and life: Find your happy medium at Amex GBT.\n• Flexible benefits are tailored to each country and start the day you do. These include health and welfare insurance plans, retirement programs, parental leave, adoption assistance, and wellbeing resources to support you and your immediate family.\n• Travel perks: get a choice of deals each week from major travel providers on everything from flights to hotels to cruises and car rentals.\n• Develop the skills you want when the time is right for you, with access to over 20,000 courses on our learning platform, leadership courses, and new job openings available to internal candidates first.\n• We strive to champion Inclusion in every aspect of our business at Amex GBT. You can connect with colleagues through our global INclusion Groups, centered around common identities or initiatives, to discuss challenges, obstacles, achievements, and drive company awareness and action.\n• And much more!\n\nAll applicants will receive equal consideration for employment without regard to age, sex, gender (and characteristics related to sex and gender), pregnancy (and related medical conditions), race, color, citizenship, religion, disability, or any other class or characteristic protected by law.\n\nClick Here for Additional Disclosures in Accordance with the LA County Fair Chance Ordinance.\n\nFurthermore, we are committed to providing reasonable accommodation to qualified individuals with disabilities. Please let your recruiter know if you need an accommodation at any point during the hiring process. For details regarding how we protect your data, please consult the Amex GBT Recruitment Privacy Statement.\n\nWhat if I don’t meet every requirement? If you’re passionate about our mission and believe you’d be a phenomenal addition to our team, don’t worry about “checking every box;\" please apply anyway. You may be exactly the person we’re looking for!\nExperience Level\nMid Level\n\nMore about this Data ETL Developer / BI Engineer job\n\nAmerican Express Global Business Travel is aggressively hiring for the job profile of Data ETL Developer / BI Engineer at undefined in null locality. Kindly go through the FAQs below to get all answers related to the given job.\n\n1. How much salary can I expect as a Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n\nAns. You can expect a minimum salary of 0 INR. The salary offered will depend on your skills, experience and performance in the interview.\n\n2. What is the eligibility criteria to apply for Data ETL Developer / BI Engineer in American Express Global Business Travel in undefined?\n\nAns. The candidate should have completed undefined degree and people who have 3 to 5 years are eligible to apply for this job. You can apply for more jobs in undefined to get hired quickly.\n\n3. Is there any specific skill required for this job?\n\nAns. The candidate should have undefined skills and sound communication skills for this job.\n\n4. Who can apply for this job?\n\nAns. Both Male and Female candidates can apply for this job.\n\n5. Is it a work from home job?\n\nAns. No, it’s not a work from home job and can’t be done online. You can explore and apply for other work from home jobs in undefined at apna.\n\n6. Are there any charges or deposits required while applying for the role or while joining?\n\nAns. No work-related deposit needs to be made during your employment with the company.\n\n7. How can I apply for this job?\n\nAns. Go to the apna app and apply for this job. Click on the apply button and call HR directly to schedule your interview.\n\n8. What is the last date to apply?\n\nAns. The last date to apply for this job is .\n\nFor more details, download apna app and find Full Time jobs in undefined. Through apna, you can find jobs in 74 cities across India. Join NOW!",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVUTCBEZXZlbG9wZXIgLyBCSSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkFtZXJpY2FuIEV4cHJlc3MgR2xvYmFsIEJ1c2luZXNzIFRyYXZlbCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJnY3VjckFjcm5KVmRmdjRxQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "6 days ago",
         "ETL Developer"
        ],
        [
         "Informatica ETL Developer - SQL/Power Center",
         "RENOVISION AUTOMATION SERVICES PVT.LTD.",
         "Telangana, India",
         "Experience - 6+ yearsWork Mode - HybridJob Summary : We are seeking a skilled Informatica ETL Developer with 5+ years of experience in ETL and Business Intelligence projects. The ideal candidate will have a strong background in Informatica PowerCenter, a solid understanding of data warehousing concepts, and hands-on experience in SQL, performance tuning, and production support. This role involves designing and maintaining robust ETL pipelines to support digital transformation initiatives for clients in manufacturing, automotive, transportation, and engineering domains.Key Responsibilities : - Design, develop, and maintain ETL workflows using Informatica PowerCenter.- Troubleshoot and optimize ETL jobs for performance and reliability.- Analyze complex data sets and write advanced SQL queries for data validation and transformation.- Collaborate with data architects and business analysts to implement data warehousing solutions.- Apply SDLC methodologies throughout the ETL development lifecycle.- Support production environments by identifying and resolving data and performance issues.- Work with Unix shell scripting for job automation and scheduling.Required Skills : - 35 years of hands-on experience with Informatica PowerCenter.- Proficiency in SQL and familiarity with NoSQL platforms.- Experience in ETL performance tuning and troubleshooting.- Solid understanding of Unix/Linux environments and scripting.- Excellent verbal and written communication skills.Preferred Qualifications : - AWS Certification or experience with cloud-based data integration is a plus.- Exposure to data modeling and data governance practices. (ref: hirist.tech)",
         "eyJqb2JfdGl0bGUiOiJJbmZvcm1hdGljYSBFVEwgRGV2ZWxvcGVyIC0gU1FML1Bvd2VyIENlbnRlciIsImNvbXBhbnlfbmFtZSI6IlJlbm92aXNpb24gQXV0b21hdGlvbiBTZXJ2aWNlcyBQdnQuTHRkLiIsImFkZHJlc3NfY2l0eSI6IlRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6IlVqTnczajVrcnJpS21VQjdBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "ETL Developer"
        ],
        [
         "ETL Developer",
         "LUXOFT",
         "Maharashtra, India",
         "Project Description:\n\nOur client is an EU subsidiary of a Global Financial Bank working in multiple markets and asset classes.\n\nDWH/ETL developer will work closely with the Development Lead to design, build interfaces and integrate data from a variety from internal and external data sources into the new Enterprise Data Warehouse environment. The ETL Developer will be responsible for developing ETL primarily utilizing Microsoft & Azure technologies within industry recognized ETL standards, architecture, and best practices.\n\nResponsibilities:\n• Act as a technical expert in the designing, coding, unit testing, supporting, and debugging of data warehouse software components in all aspects of SDLC\n• Apply cloud and ETL engineering skills to solve problems and design approaches\n• Troubleshoot and debug ETL pipelines and creating unit tests for ETL pipelines.\n• Assess query performance and actively contribute to optimizing the code\n• Write technical documentation and specifications\n• Support internal audit by submitting required evidence\n• Create reports and dashboards in the BI portal\n• Work with Development Lead, DWH Architect and QA Engineers to plan, implement and deliver best ETL strategies\n• Work with business analysts to understand requirements to create technical design specifications, gaining a sound understanding of business processes for related applications so that integration processes fulfill the end-user requirements\n• Communicate effectively in a collaborative, complex and high performing team environment as per Agile principles\n\nMandatory Skills Description:\n• Proven work experience as an ETL Developer\n• Advanced knowledge of relational databases and dimensional Data Warehouse modelling concepts\n• Good understanding of physical and logical data modeling\n• Very good understanding of modern SaaS/PaaS data solutions in a cost conscious approach\n• Expert level of knowledge of Microsoft Data stack\n• Experience in developing and deploying data oriented solutions in Cloud (Azure/Synapse Analytics/Fabric)\n• Experience in designing and implementing data transformation and ETL layers using Data Factory, Notebooks\n• Experience with PowerBI for report & dashboard creation. PowerQuery and/or DAX is an advantage.\n• Experience in/understanding of Azure Data Lake Storage\n• Knowledge/use of CI/CD tools and principles, preferably Azure DevOps or Bamboo\n• Strong SQL knowledge, able to create complex SQL queries and good understanding of stored procedures, views, indexes, functions, etc.\n• Good working knowledge of at least one Scripting language. Python is an advantage.\n• Experience with GIT repositories and working with branches. GitHub, Azure DevOps or Bitbucket experience are preferable.\n• Ability to troubleshoot and solve complex technical problems\n• Good understanding of software development best practices\n• Working experience in Agile projects; preferably using JIRA\n• Experience in working in high priority projects preferably greenfield project experience\n• Able to communicate complex information clearly and concisely.\n• Able to work independently and also to collaborate across the organization\n• Highly developed problem-solving skills with minimal supervision\n• Understanding of data governance and enterprise concepts preferably in banking environment\n• Verbal and written communication skills in English are essential.\n\nNice-to-Have Skills Description:\n• Microsoft Fabric\n• Snowflake\n• Background in SSIS/SSAS/SSRS\n• Azure DevTest Labs, ARM templates\n• Azure PurView\n• Banking/finance experience",
         "eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyIiwiY29tcGFueV9uYW1lIjoiTHV4b2Z0IiwiYWRkcmVzc19jaXR5IjoiTWFoYXJhc2h0cmEsIEluZGlhIiwiaHRpZG9jaWQiOiJlZF9YUGZ2UVNVcTZXdXltQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "ETL Developer"
        ],
        [
         "Data Engineer (Hadoop, Spark, Scala, Hive)",
         "VISA",
         "India",
         "Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.\n\nMake an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n\nJob Description\n\nTranslate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\n\nGood to have GenAI Exposure and Agentic AI Knowledge.\n\nWork with business partners directly to seek clarity on requirements.\n\nDefine solutions in terms of components, modules, and algorithms.\n\nDesign, develop, document, and implement new programs and subprograms, as well as enhancements, modifications and corrections to existing software.\n\nCreate technical documentation and procedures for installation and maintenance.\n\nWrite Unit Tests covering known use cases using appropriate tools.\n\nIntegrate test frameworks in the development process.\n\nWork with operations to get the solutions deployed.\n\nTake ownership of production deployment of code.\n\nCome up with Coding and Design best practices.\n\nThrive in a self-motivated, internal-innovation driven environment.\n\nAdapt quickly to new application knowledge and changes.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\nQualifications\n\nBasic Qualifications\n\n-Minimum of 6 months of work experience or a Bachelor's Degree\n\nPreferred Qualifications\n\n-Bachelor degree in Computer Science.\n\n-Minimum of 1 plus years of software development experience in Hadoop using\n\nSpark, Scala, Hive.\n\n-Expertise in Object Oriented Programming Language Java, Python.\n\n-Experience using CI CD Process, version control and bug tracking tools.\n\n-Result-oriented with strong analytical and problem-solving skills.\n\n-Experience with automation of job execution, validation and comparison of data\n\nfiles on Hadoop Environment at the field level.\n\n-Experience in leading a small team and being a team player.\n\n-Strong communication skills with proven ability to present complex ideas and\n\ndocument them in a clear and concise way.\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChIYWRvb3AsIFNwYXJrLCBTY2FsYSwgSGl2ZSkiLCJjb21wYW55X25hbWUiOiJWaXNhIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImVULTZKTlc2NHVzTUxhdS1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 days ago",
         "Spark Engineer"
        ],
        [
         "Spark Engineer",
         "STAFFINGINE LLC",
         "India",
         "- 8+ years Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferredMandatory skills- Solid in Scala programming- Good at Memory management- Familiar with data transfer- AWS experience is preferred",
         "eyJqb2JfdGl0bGUiOiJTcGFyayBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN0YWZmaW5naW5lIExMQyIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJoYTc4S0kwNnFxOTRVZjNvQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "1 day ago",
         "Spark Engineer"
        ],
        [
         "Databricks Engineer - Spark / PySpark",
         "ENKEFALOS TECHNOLOGIES LLP",
         "Anywhere",
         "Databricks Engineer – Spark / PySpark\n\nLocation : Remote / Mysore\n\nJoining : Immediate\n\nExperience : 5+ years\n\nResponsibilities :\n\nWill implement all cleansing, transformation, and semantic modeling logic on Databricks using PySpark, targeting financial facts and dimensions from SAP manual dumps.\n\nRequirements:\n• PySpark (RDDs, DataFrames, performance tuning)\n• Building gold‐layer data models for financial reporting\n• Experience with complex joins, aggregations, GL hierarchies\n• Version handling (Actuals vs Budget), currency conversions\n\nJob Type: Full-time\n\nPay: ₹500,395.35 - ₹1,840,348.25 per year\n\nBenefits:\n• Flexible schedule\n• Paid sick time\n• Provident Fund\n• Work from home\n\nApplication Question(s):\n• Have you worked on ADF/ADLS ?\n• Do you have hands-on experience of Spark / PySpark\n\nExperience:\n• Databricks Engineering: 4 years (Required)\n\nWork Location: Remote",
         "eyJqb2JfdGl0bGUiOiJEYXRhYnJpY2tzIEVuZ2luZWVyIC0gU3BhcmsgLyBQeVNwYXJrIiwiY29tcGFueV9uYW1lIjoiRW5rZWZhbG9zIFRlY2hub2xvZ2llcyBMTFAiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiazRSYjkyTDZZSUJlY2hVTEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 days ago",
         "Spark Engineer"
        ],
        [
         "Pi Square Technologies - Spark & Scala Engineer",
         "SANDEEP RAJA",
         "India",
         "Job Summary :\n\nWe are seeking a skilled and motivated Spark & Scala Engineer with 46 years of hands-on experience in building scalable data processing applications. The ideal candidate will have a strong background in big data ecosystems and a solid understanding of distributed computing using Apache Spark with Scala.\n\nRoles and Responsibilities :\n\n- Design, build, and maintain efficient, reusable, and reliable Apache Spark applications.\n\n- Optimize Spark applications for maximum speed and scalability.\n\n- Implement data ingestion and ETL processes.\n\n- Collaborate with data scientists and architects to implement complex big data solutions.\n\n- Debug and resolve issues in Spark applications.\n\n- Stay up to date with the latest trends in big data technologies and Apache Spark.\n\n- Write clean, readable, and maintainable code.\n\n- Participate in code reviews and contribute to team knowledge sharing.\n\nRequired Skills :\n\n- 46 years of experience working with Apache Spark (core, SQL, streaming).\n\n- Strong proficiency in Scala programming.\n\n- Experience in building and optimizing data pipelines and ETL workflows.\n\n- Proficient in working with big data tools and frameworks (e.g., Hive, HDFS, Kafka).",
         "eyJqb2JfdGl0bGUiOiJQaSBTcXVhcmUgVGVjaG5vbG9naWVzIC0gU3BhcmsgXHUwMDI2IFNjYWxhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoic2FuZGVlcCByYWphIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IlFMNDBNZDljZDlqQkxBX3BBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "27 days ago",
         "Spark Engineer"
        ],
        [
         "Spark Developer",
         "INFOSYS",
         "India",
         "• Primary skills:Technology->Big Data - Data Processing->Spark\n\nA day in the life of an Infoscion\n• As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\n• You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\n• You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\n• You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\n• You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n• Knowledge of more than one technology\n• Basics of Architecture and Design fundamentals\n• Knowledge of Testing tools\n• Knowledge of agile methodologies\n• Understanding of Project life cycle activities on development and maintenance projects\n• Understanding of one or more Estimation methodologies, Knowledge of Quality processes\n• Basics of business domain to understand the business requirements\n• Analytical abilities, Strong Technical Skills, Good communication skills\n• Good understanding of the technology and domain\n• Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\n• Awareness of latest technologies and trends\n• Excellent problem solving, analytical and debugging skills",
         "eyJqb2JfdGl0bGUiOiJTcGFyayBEZXZlbG9wZXIiLCJjb21wYW55X25hbWUiOiJJbmZvc3lzIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6IkZ2VmRvVC1NQnFvWHRRUmJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "16 days ago",
         "Spark Engineer"
        ],
        [
         "SW Engineer (Java and Bigdata/Hadoop/Spark) 1yr",
         "VISA",
         "India",
         "Job Description\n\nThis position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. They will be an integral part of the Payment Products Development team, focusing on the development of software solutions that leverage data to address business challenges. They will be extensively involved in hands-on activities including proof of concepts, development, and testing. They should demonstrate adaptability and readiness to change tasks according to the team's requirements.\n\nKey Responsibilities\n• Deliver high-quality products and features, taking a holistic view from the initial idea to final delivery. This involves design, development, testing, and accountability for release deployments as well as providing support post-deployment.\n• Understand the product in its entirety and ensure the timely delivery of your assigned module within an agile team setting.\n• Accurately scope efforts, identify risks, and clearly communicate trade-offs with team members and other stakeholders.\n• Coordinate and participate in Continuous Integration activities, testing automation frameworks, and other related tasks, in addition to contributing to core product code.\n• Effectively communicate status updates, issues, and potential risks accurately and promptly.\n• Perform other tasks related to data governance and system infrastructure as required.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\nQualifications\n\nBasic Qualifications\n\n-Bachelor's degree in Computer Science or equivalent field\n\n-Relevant working experience of up to 2 years in the industry\n\n-Proven experience in software development, particularly in data-centric\n\nprojects, demonstrating adherence to standard development best practices\n\n-Strong understanding and practical experience with data structures and\n\nalgorithms, with a passion for tackling complex problems\n\n-Proficiency in Java programming\n\n-Hands-on experience with Big Data technologies such as Hadoop, Spark, and\n\nHive\n\n-Strong knowledge of Unix/Linux operating systems and Shell Scripting\n\n-Proficiency in working with RDBMS and SQL\n\n-Basic knowledge of manual and automated testing\n\n-Familiarity with version control systems, specifically Git\n\n-Awareness of and experience with software design patterns\n\n-Experience working within an Agile framework\n\nPreferred Qualifications\n\n-Proficiency in Scala & Kafka programming is a good to have\n\n-Experience with Airflow for workflow management\n\n-Familiarity with AI concepts and tools, including GitHub Copilot for code\n\ndevelopment\n\n-Exposure to AI/ML development is an added advantage\n\n-Proficiency in working with In-memory Databases like Redis\n\n-Good knowledge of API development is highly advantageous\n\n-Strong verbal and written communication skills, with a proactive and self-\n\nmotivated approach to improving existing processes to enable faster\n\niterations.\n\n-Demonstrated intellectual and analytical rigor, with a keen attention to detail\n\n-Team-oriented, energetic, and collaborative approach to work, coupled with a\n\ndiplomatic and adaptable style\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
         "eyJqb2JfdGl0bGUiOiJTVyBFbmdpbmVlciAoSmF2YSBhbmQgQmlnZGF0YS9IYWRvb3AvU3BhcmspIDF5ciIsImNvbXBhbnlfbmFtZSI6IlZJU0EiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoidHV1TkJsMDF3amRIc0xnSEFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "5 days ago",
         "Spark Engineer"
        ],
        [
         "Big Data Lead/ Lead Data Engineer/Spark Tech Lead",
         "TANISHA SYSTEMS  INC",
         "India",
         "Job Description:Mandatory:10+ years of experience in solution, design and development of applications using Java 8+/J2EE, Spring, Spring-Boot, Micro Services, RESTful Services and with experience in Big Data and with experience working in heavy data background needed.Develop, program, and maintain applications using the Apache Spark open source frameworkWork with different aspects of the Spark ecosystem, including Spark SQL, Data Frames, Datasets, and streaming.Spark Developer must have strong programming skills in Java, Scala, or PythonFamiliar with big data processing tools and techniquesProven experience as a Spark Developer or a related roleStrong programming skills in Java, Scala, or PythonFamiliarity with big data processing tools and techniquesExperience with the Hadoop ecosystemGood understanding of distributed systemsExperience with streaming data platformsMust have strong experience in Big Data and with experience working in heavy data background needed.Must be strong in Cloud AWS event-based architecture, Kubernetes, ELK (Elasticsearch, Logstash & Kibana)Must have excellent experience in designing and Implementing cloud-based solutions in various AWS Services (: s3, Lambda, Step Function, AMQ, SNS, SQS, CloudWatch Events, etc.)Must be well experienced in design and development of Microservice using Spring-Boot and REST API and with GraphQLMust have solid knowledge and experience in NoSQL (MongoDB)Good knowledge and experience in any Queue based implementations.Strong knowledge/experience in ORM Framework - JPA / HibernateGood knowledge in technical concepts – Security, Transaction, Monitoring, PerformanceShould we well have versed with TDD/ATDDShould have experience on Java, Python and Spark2+ years of experience in designing and Implementing cloud-based solutions in various AWS ServicesStrong experience in DevOps tool chain (Jenkins, Artifactory, Ansible/Chef/Puppet/Spinnaker, Maven/Gradle, Atlassian Tool suite)Very Good knowledge and experience in Non-Functional (Technical) Requirements like Security, Transaction, Performance, etc.Excellent analytical and problem-solving skills Nice to have:· Experience in Experience with OAuth implementation using Ping Identity· Experience in API Management (Apigee) or Service Mesh (Istio)· Good knowledge and experience in Queue/Topic (Active-MQ) based implementations.· Good knowledge and experience in Scheduler and Batch Jobs· Experience with scripting languages using Unix· Preferably certified in AWS",
         "eyJqb2JfdGl0bGUiOiJCaWcgRGF0YSBMZWFkLyBMZWFkIERhdGEgRW5naW5lZXIvU3BhcmsgVGVjaCBMZWFkIiwiY29tcGFueV9uYW1lIjoiVGFuaXNoYSBTeXN0ZW1zICBJbmMiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiQmRWaDV5cnl3aTREUFNOT0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 hours ago",
         "Spark Engineer"
        ],
        [
         "Data Insights Analyst",
         "IN10 (FCRS = IN010) NOVARTIS HEALTHCARE PRIVATE LIMITED",
         "India",
         "Responsible for leveraging Google Analytics 4 (GA4) to extract, analyze, and interpret data to support business objectives and decision-making processes. Requires proficient understanding and translating business requirements into actionable insights and recommendations derived from GA4 data.Key Responsibilities:1.Data Extraction & Analysis: Extract and analyze GA4 data to provide meaningful insights into user behavior, website performance, and digital marketing effectiveness.2.Reporting & Dashboarding: Develop custom reports, dashboards, and data visualizations using GA4 data to monitor key metrics and performance indicators.3.Insights and Recommendations: Providing actionable insights and recommendations to optimize website performance, user experience, and marketing campaigns.4.Customization & Configurations: Configure GA4 data streams, events, and conversions to align with business goals and KPIs (key performance indicators). Utilize custom dimensions, metrics, and attribution models to capture and analyze data relevant to unique business needs.5.Pharma knowledge: Understanding of the Pharma industry landscape, including regulatory compliance, patient journey mapping, and healthcare data privacy considerationsMinimum Requirement:·2-4 Years of experience in digital analytics, hands on experience on Google Analytics 4.·Proficiency in Google Analytics and Google Tag Manager.·Strong analytical skills and ability to interpret data.·Understanding of digital marketing concepts and website optimization techniques.·Excellent communication skills to present findings and recommendations effectively·GA4 Certification is strongly preferred",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEluc2lnaHRzIEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJJTjEwIChGQ1JTID0gSU4wMTApIE5vdmFydGlzIEhlYWx0aGNhcmUgUHJpdmF0ZSBMaW1pdGVkIiwiYWRkcmVzc19jaXR5IjoiSW5kaWEiLCJodGlkb2NpZCI6ImFiMU5uYTFGN1dHbFd4eC1BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "5 days ago",
         "Data Analyst"
        ],
        [
         "Senior Data Management Analyst",
         "WELLS FARGO",
         "Hyderabad, Telangana, India",
         "About this role:\n\nWells Fargo is seeking a Senior Data Management Analyst\n\nIn this role, you will:\n• Lead or participate in moderately complex programs and initiatives for data quality, governance, and metadata activities\n• Design and conduct moderately complex analysis to identify and remediate data quality, data integrity, process, and control gaps\n• Analyze, assess, and test data controls and data systems to ensure quality and risk compliance standards are met and adhere to data governance standards and procedures\n• Identify data quality metrics and execute data quality audits to benchmark the state of data quality\n• Develop recommendations for optimal approaches to resolve data quality issues and implement plans for assessing the quality of new data sources leveraging domain expertise and data, business, or process analysis to inform and support solution design\n• Lead project teams and mentor less experienced staff members\n• Drive planning and coordination on moderately complex remediation efforts acting as central point of contact\n• Consult with clients to assess the current state of data and metadata quality within area of assigned responsibility\n• Participate in cross-functional groups to develop companywide data governance strategies\n• Provide input into communication routines with stakeholders, business partners, and experienced leaders\n\nRequired Qualifications:\n• 4+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\n• Experience in large enterprise data initiatives\n• Contact center business or technology experience\n• Experience implementing, managing, or utilizing big data environments (Hadoop, Big Query, etc.)\n• Experience using standard BI tools (Tableau, Power BI, MicroStrategy, etc.) preferably from big data environments\n\nPosting End Date:\n24 Jun 2025\n• Job posting may come down early due to volume of applicants.\n\nWe Value Equal Opportunity\n\nWells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n\nEmployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n\nCandidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n\nApplicants with Disabilities\n\nTo request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n\nDrug and Alcohol Policy\n\nWells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n\nWells Fargo Recruitment and Hiring Requirements:\n\na. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n\nb. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBNYW5hZ2VtZW50IEFuYWx5c3QiLCJjb21wYW55X25hbWUiOiJXZWxscyBGYXJnbyIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoieEFWbXpFcV9ySTNSUG1faUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Associate/Analyst - Data Analytics",
         "D. E. SHAW INDIA",
         "Hyderabad, Telangana, India",
         "The Financial Research (FinRes) group provides research and analysis support to the entire investment life cycle of the firm’s global proprietary trading strategies. The group caters to almost all the quantitative and qualitative strategies of the firm and its research support covers multiple asset classes, sectors, and geographies.",
         "eyJqb2JfdGl0bGUiOiJBc3NvY2lhdGUvQW5hbHlzdCAtIERhdGEgQW5hbHl0aWNzIiwiY29tcGFueV9uYW1lIjoiRC4gRS4gU2hhdyBJbmRpYSIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiak9McEJja1dWaXhiWjJ4c0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "14 hours ago",
         "Data Analyst"
        ],
        [
         "Senior Analyst- Data Risk Office",
         "BRISTOL MYERS SQUIBB",
         "Hyderabad, Telangana, India",
         "Working with Us\nChallenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.\n\nRoles & Responsibilities\n\nFunctional and Technical\n• Execution and monitoring of data privacy office key activties.\n• Assist in the development and implementation of data privacy policies and procedures to ensure compliance with international, federal, and state regulations, including GDPR, CCPA, and HIPAA.\n• Work closely with legal, IT, HR, and other departments to address data privacy concerns and implement best practices.\n• Assist in the development and delivery of data privacy training and awareness programs across the organization. Keeps up to date with evolution of regulations impacting privacy, ethics, and data.\n• Experienced in configuring and working with various industry leading data risk and privacy tools like OneTrust, TrustArc, Microsoft Purview etc.\n• Demonstrate critical thinking around interpreting business and industry challenges and recommending best practices-based solutions to improve products, processes, systems and reduce risk.\n• Implement organizational IT controls in accordance with applicable regulations and the ability to evaluate and understand the impact of new regulations and requirements.\n\nPeople Management:\n• Responsible for training and mentoring junior staff to meet BMS standards.\n• Preferred experience with working in a multi-cultural, multi-location and diverse environments.\n\nQualifications & Experience\n• B.E./B.Tech. or equivalent in computer science, engineering, life science field\n• Recognized privacy/DLP certifications and experience preferred.\n• At least years of privacy program management, compliance, or strong operations management experience (regulated or healthcare or tech preferred).\n• Knowledge of HIPAA, GDPR, CPRA, PIPL etc., and other privacy regulations is a must.\n• Ability to make decisions that impact own work and other groups/teams and works under minimal supervision.\n• Demonstrates openness to learning and developing. Takes a responsibility for their own and team’s development and growth.\n• Demonstrates an understanding of factors driving team performance and how they contribute to the team's overall success.\n• Excellent English Oral and written communication skills including the ability to deliver clear and articulate presentations.\n• Ability to use PowerPoint, Excel, Word, or other technologies to communicate complex topics to stakeholders, manage personal workload, and track projects and issues.\n\nIf you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n\nUniquely Interesting Work, Life-changing Careers\nWith a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n\nOn-site Protocol\n\nBMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\n\nSite-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n\nBMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n\nIf you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers.bms.com/california-residents/\n\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHlzdC0gRGF0YSBSaXNrIE9mZmljZSIsImNvbXBhbnlfbmFtZSI6IkJyaXN0b2wgTXllcnMgU3F1aWJiIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiJsWm9Rdlc1dnAyWjZyYVlMQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst II – Product Information Capabilities | Digital & Technology",
         "GENERAL MILLS INDIA",
         "India",
         "India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.\n\nPosition Title\n\nSoftware Engineer II – Product Information Capability\n\nFunction/Group\n\nDigital & Technology\n\nLocation\n\nMumbai\n\nShift Timing\n\nRegular\n\nRole Reports to\n\nD&T Manager – Product Information Capability\n\nRemote/Hybrid/in-Office\n\nHybrid\n\nAbout General Mills\n\nWe make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and Häagen-Dazs, we’ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\n\nHow we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\n\nus into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\n\nGeneral Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\n\nWith our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital & Technology (D&T) Innovation, Technology & Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy & Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\n\nWe advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\n\nJob Overview\n\nFunction Overview\n\nThe Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The team's expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\n\nThe team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the \"Work with Heart\" philosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the following Link\n\nPurpose of the role\n\nThis is an exciting time to work in General Mills' Supply Chain & ITQ Organization! We are accelerating Digital Transformation of our Product Information Capabilities (PIC) capabilities to provide a competitive advantage to our business. To this end, we are seeking a skilled and motivated STIBO Developer to join our Product Information Capabilities team. As a STIBO Developer, you will play a crucial role in designing, developing, and implementing solutions within our STIBO STEP platform to support our product information management (PIM) and master data management (MDM) processes. You will collaborate with cross-functional teams to understand business requirements, translate them into technical specifications, and deliver high-quality solutions that meet our business needs.\n\nKey Accountabilities\n• Design, develop, and maintain STIBO STEP solutions to support product information management processes.\n• Write and maintain code for business rules to ensure data quality and consistency.\n• Configure outbound and inbound integrations to exchange data with other systems.\n• Configure gateway endpoints for seamless data flow.\n• Develop and maintain data models within STIBO STEP to accurately represent product information.\n• Build web UI screens for data entry, validation, and reporting.\n• Develop solutions based on documented requirements and specifications.\n• Participate in agile project development, including sprint planning, daily stand-ups, and retrospectives.\n• Collaborate with business analysts, data architects, and other developers to ensure solutions align with business needs and technical standards.\n• Troubleshoot and resolve issues related to STIBO STEP implementations.\n• Stay up-to-date with the latest STIBO STEP features and best practices.\n• Create and maintain technical documentation for STIBO STEP solutions.\n\nMinimum Qualifications\n• Education – Full time graduation from an accredited university (Mandatory- Note: This is the minimum education criteria which cannot be altered)\n• Experience with tools and concepts related to MDM and/or Digital Product Publishing, such as data governance, data quality, data integration and data exporting\n• Exposure to Product Information Management Systems (PIM/MDM)\n• Technical expertise into Stibo platform\n• Experience with Data Syndication partners such as 1WorldSync, Syndigo and Salsify.\n• Exposure to GDSN Standards\n• Strong team player and collaborator with core focus on solution making; Effective analytical and technical skills; Ability to work in a cross functional team environment\n\nPreferred Qualifications\n• Product Information Management / Master Data Management\n• STIBO STEP certification\n• Business Analysis skills\n• SQL, Cloud GCP\n• Agile / SCRUM Delivery\n• Familiarity with Service Bus Integration\n• Preferably experience in Consumer Goods industry.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgSUkg4oCTIFByb2R1Y3QgSW5mb3JtYXRpb24gQ2FwYWJpbGl0aWVzIHwgRGlnaXRhbCBcdTAwMjYgVGVjaG5vbG9neSIsImNvbXBhbnlfbmFtZSI6IkdlbmVyYWwgTWlsbHMgSW5kaWEiLCJhZGRyZXNzX2NpdHkiOiJJbmRpYSIsImh0aWRvY2lkIjoiTWplT1ByZWI4UVZPLVpDVUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "2 days ago",
         "Data Analyst"
        ],
        [
         "Lead Data Management Analyst",
         "WELLS FARGO",
         "Hyderabad, Telangana, India",
         "About this role:\n\nWells Fargo is seeking a Lead Data Management Analyst\n\nIn this role, you will:\n• Organize and lead complex companywide initiatives to ensure that data quality is maintained so that data can effectively support business processes\n• Oversee analysis and reporting in support of regulatory requirements\n• Identify and recommend analysis of data quality or integrity issues\n• Evaluate data quality metrics and data quality audits to benchmark the state of data quality\n• Make decisions in complex and multi-faceted situations requiring solid understanding of data governance standards and procedures\n• Identify new data sources and develop recommendations for assessing the quality of new data\n• Lead project teams and mentor less experienced staff members\n• Recommend remediation of process or control gaps that align to management strategy\n• Serve as relationship manager for a line of business\n• Consult with and provide recommendations to senior management regarding assessments of the current state of data and metadata quality within area of assigned responsibility\n• Represent client in cross-functional groups to develop companywide data governance strategies\n• Strategically collaborate and consult with peers, colleagues, and mid-level to senior managers to coordinate and drive collaboration on solution design and remediation execution\n\nRequired Qualifications:\n• 5+ years of Data Management, Business Analysis, Analytics, or Project Management experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\n• Experience in Data Management, Business Analysis, Analytics, Project Management.\n\nPosting End Date:\n24 Jun 2025\n• Job posting may come down early due to volume of applicants.\n\nWe Value Equal Opportunity\n\nWells Fargo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.\n\nEmployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit’s risk appetite and all risk and compliance program requirements.\n\nCandidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\n\nApplicants with Disabilities\n\nTo request a medical accommodation during the application or interview process, visit Disability Inclusion at Wells Fargo.\n\nDrug and Alcohol Policy\n\nWells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\n\nWells Fargo Recruitment and Hiring Requirements:\n\na. Third-Party recordings are prohibited unless authorized by Wells Fargo.\n\nb. Wells Fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
         "eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgTWFuYWdlbWVudCBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiV2VsbHMgRmFyZ28iLCJhZGRyZXNzX2NpdHkiOiJIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6Ik5YbnE1ZlFtc3M5WXNVQ0JBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Senior Data Analyst, Marketing Science",
         "CRUNCHYROLL",
         "Hyderabad, Telangana, India",
         "About the role\n\nWe are looking for a hands-on Senior Marketing Science Analyst with a passion for understanding data, tracking business trends, and storytelling to join the Center for Data and Insights team. This position will report to the Vice President, Business Intelligence & Analytics, and will collaborate with data engineers, campaign managers, and marketing leaders to investigate campaign trends, build business insights, and recommend proactive measures to enable better business decisions every day. The right person for this position is a service-minded, empathetic problem-solver who will be motivated by the opportunity to build a centralized insights service team from the ground up!\n• Partner with Branch/ Adjust MMPs, direct response, and media buying agencies to define and implement campaign measurement of integrated marketing campaigns, including TV, OOH, Digital, and Social Media across a wide range of business lines (theatrical, streaming, e-commerce, etc.)\n• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n• Evaluating media measurement leveraging MMM (Marketing Mix Modeling), MTA (Multi-Touch Attribution), and Incrementality A/B Testing\n• Lead / Assist in development of testing roadmaps, measurement plans, KPI & KLI reporting based on set marketing objectives linked to key campaigns and always-on digital marketing tactics.\n• Deliver ad hoc analysis for client stakeholders throughout marketing, working with the channel leads, planning, and client service departments.\n• Partner with paid marketing teams in building weekly insight decks for the Marketing leadership team by collaborating effectively across teams, documenting the impact of strategic initiatives on benchmarks, and tracking the progress towards company goals.\n• Partner with cross-functional teams to design analytics and reporting tools that will be instrumental to distribute certified dashboards and presentations.\n• Connect ideas into cohesive, well-grounded recommendations, using creative, structured, and analytical thinking with the help of effective data visualization\n• Work with offshore and onsite teams and lead the sprint planning/management\n• Maintain a culture of high-quality output and outstanding customer service by effectively communicating at all levels, ensuring that work gets done, and responding effectively to\n\nAbout You\n• 5+ years of experience with data analysis, paid campaign analytics, statistics, experimentation, and optimization.\n• 3+ years of experience in writing complex SQL queries, experience in marketing data automation.\n• 3+ years of experience with data visualization tools like Tableau, Superset, etc.\n• Understanding of install and user-level paid campaign tracking to support multi-touch attribution via UTMs and MMP data for all major marketing channels\n• Proficiency in data analysis, including defining critical metrics, statistical and predictive modeling concepts, descriptive statistics, and experimental design\n• Experience in Marketing analytics tools like Google Analytics, Adjust, Braze, Branch, etc.\n• Experience working with large data sets (Terabytes of data/ billions of records).\n• Deep expertise in measuring marketing performance against lifetime value metrics.\n• Outstanding teamwork skills: You have excellent interpersonal skills. You’re a good listener. You place the success of the company ahead of any particular idea. Proven ability to work across a globally, matrixed organization.\n• BS in Statistics, Computer Science, Information Systems, or a related field\n\nAbout the Team\n\nThe Center for Data and Insights (CDI) is the centralized team of data engineering, BI, analytics, and data science experts, passionate about servicing the organization with timely and certified reports and insights! The mission of the group is to inspire, support, and guide our stakeholders to be data-aware as well as build out the systems of intelligence to discover insights and act on them.\n\nWhy you will love working at Crunchyroll\n\nIn addition to getting to work with fun, passionate and inspired colleagues, you will also enjoy the following benefits and perks:\n• Best-in class medical, dental, and vision private insurance healthcare coverage\n• Access to counseling & mental health sessions 24/7 through our Employee Assistance Program (EAP)\n• Free premium access to Crunchyroll\n• Professional Development\n• Company's Paid Parental Leave\n• up to 26 weeks for birthing parents\n• up to 12 weeks for non-birthing parents\n• Hybrid Work Schedule\n• Paid Time Off\n• Flex Time Off\n• 5 Yasumi Days\n• Half-Day Fridays during the summer\n• Winter Break\n\n#LifeAtCrunchyroll #LI-Hybrid",
         "eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBBbmFseXN0LCBNYXJrZXRpbmcgU2NpZW5jZSIsImNvbXBhbnlfbmFtZSI6IkNydW5jaHlyb2xsIiwiYWRkcmVzc19jaXR5IjoiSHlkZXJhYmFkLCBUZWxhbmdhbmEsIEluZGlhIiwiaHRpZG9jaWQiOiI3QU16TGktYmRTM1gybEV4QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "4 days ago",
         "Data Analyst"
        ],
        [
         "Principal Data Analyst",
         "STORABLE",
         "Serilingampalle (M), Hyderabad, Telangana, India",
         "About the Role:\nWe’re seeking a strategic, data-driven professional to lead our Business Analysis initiatives for our Marketplace product within the Self-Storage vertical. This is a ‘hands on keyboard’ high-impact role that bridges business strategy, analytical rigor, and data visualization —ideal for someone who thrives on translating data into actionable insights that drive performance. As the Principal Data Analyst, you will own our marketplace performance metrics, lead the development of world-class dashboards and reporting frameworks, and partner closely with Product, Marketing, Operations, and Finance teams to inform strategic decisions. You’ll drive both hands-on analysis and organizational data literacy while mentoring a team of analysts and collaborating with engineering on data infrastructure.\nKey Responsibilities:\n\nOwn and continuously optimize key Marketplace KPIs (conversion, revenue yield, inventory, pricing, etc.).\nServe as the steward of metric definitions and business logic—ensuring consistency, accuracy, and alignment across stakeholders.\nHave a deep understanding of how to run a BI environment. Proactive, insightful, curious.\nBuild and scale world class self-service dashboards and Tableau reports for internal and executive stakeholders.\nLead cross-functional analysis projects to uncover trends, diagnose problems, and identify growth and efficiency opportunities.\nTranslate complex data into clear, actionable insights and concise narratives for business and executive audiences.\nDrive rigorous SQL-based analysis from raw datasets, ensuring high-quality, reproducible outputs.\nGuide and collaborate with junior analysts in other groups and collaborate with engineering to shape data pipelines and definitions.\nCollaborate with cross functional partners and teams (Product,Marketing, Finance, Business Insights) to test hypotheses, evaluate feature/campaign performance, and measure outcomes.\nIdentify data gaps or integrity risks and work with stakeholders to improve reliability and accuracy.\nChampion a culture of curiosity, experimentation, and evidence-based decision-making.\nProactively keep other teams in the loop on new data capabilities unlocked and changes being made.\n\nRequirements:\n\n5-7 years of experience in business analysis, data analytics, or related roles, ideally with at least 2+ years in a leadership capacity.\nAdvanced proficiency in SQL with experience working with complex data models and the ability to query complex relational datasets.\nOther data engineering experience is a significant plus to facilitate sourcing/formating of data.\nDeep understanding of performance metrics and business levers in a supply/demand or marketplace environment.\nExperience leading analytics efforts in partnership with cross-functional teams including Product, Marketing, Finance, and Engineering.\nProven track record of developing intuitive, scalable dashboards using Tableau (or similar BI platforms).\nStrong business acumen with the ability to connect the dots between business objectives and analytical frameworks.\nExceptional communication skills with the ability to distill technical findings for non-technical audiences.\nCapable of influencing and informing executive stakeholders with clear, concise insights.\nDemonstrated experience owning KPIs, building data products, and drive insights into action in a fast paced environment.\nAbility to navigate ambiguity, manage and prioritize competing needs, and work cross-functionally.\n\nPreferred Qualifications:\n\nExperience in the storage, real estate, or marketplace industries strongly preferred\nFamiliarity with modern data stack tools such as Snowflake, dbt, or similar Experience analyzing marketplace dynamics or supply/demand business models\nExposure to experimentation frameworks, A/B testing, or uplift modeling\nPrior exposure to high-growth SaaS or Marketplace operations\nData engineering capabilities—especially in sourcing, transforming, or centralizing disparate datasets.\n\nAbout Us:\nAt Storable, we believe storage operators should have one partner they can trust to help you get the results they need for their business. That’s why we’ve built the industry’s first fully integrated platform that offers facility management software, facility websites, marketing programs and services, payments, and deeply integrated tenant insurance capabilities all in one solution.\nWe leverage our platform in combination with our over 25 years of storage industry expertise to help our thousands of storage customers achieve their tenant experience and operational efficiency objectives every single day.\nImportant Notice: Protect Yourself from Fraudulent Activities Targeting Job Seekers\nWe’ve been made aware of fraudulent activities where an individual or group is pretending to represent Storable, attempting to deceive job seekers by using our company’s good name and stellar reputation.\nTo protect yourself, please consider the following guidelines:\n– Official Communication: All genuine communication from Storable will come from official email addresses, ending in “@storable.com.” Be cautious of any communication that doesn’t match this criteria.– No Unsolicited Offers: We do not extend job offers without a formal interview process. If you receive an unsolicited job offer claiming to be from Storable or any of its representatives, it’s a red flag.– Verification: If you’re uncertain about the legitimacy of any job offer or communication claiming to be from Storable, please directly contact our HR department directly at POps@storable.com for verification.\nYour security and trust are paramount to us. If you suspect you’ve been contacted by someone falsely claiming to be from Storable or using our company’s name for any dubious purpose, please immediately report the incident to POps@storable.com\nWe’re committed to ensuring a transparent and secure hiring process.\nThank you for your vigilance and interest in joining our team.",
         "eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBBbmFseXN0IiwiY29tcGFueV9uYW1lIjoiU3RvcmFibGUiLCJhZGRyZXNzX2NpdHkiOiJTZXJpbGluZ2FtcGFsbGUgKE0pLCBIeWRlcmFiYWQsIFRlbGFuZ2FuYSwgSW5kaWEiLCJodGlkb2NpZCI6InljaTdhWW0wWnR2MEMzbkZBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lGU1c1a2FXRSJ9",
         "18 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst 1",
         "UNITEDHEALTH GROUP",
         "India",
         "At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.\n\nPrimary Responsibilities:\n• Validate data with administrative source systems (source of truth)\n• Analyze complex datasets\n• Generate actionable insights and recommendations based on data analysis\n• Database Management:\n• Develop and maintain data models, data dictionaries, and other documentation\n• Troubleshoot and resolve database-related issues\n• Data Extraction and Transformation:\n• Support capital project that will, in part, import and transform data from various sources for broker bonus calculations\n• Ensure data integrity and quality through rigorous validation and testing\n• Data Visualization and Reporting:\n• Create visually appealing and informative dashboards and reports\n• Present findings and insights to both technical and non-technical stakeholders in a clear and concise manner\n• Continuous Learning and Improvement:\n• Stay up to date with the latest data analysis techniques and tools\n• Identify opportunities to improve data analysis processes and methodologies\n• Actively participate in knowledge sharing and mentoring within the team\n• Comply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\n• Undergraduate degree or equivalent experience\n• 4+ years Experience as SAS Data Analyst\n• Experience with relational database management systems (e.g., MySQL, Oracle, SQL Server)\n• Experience with statistical analysis\n• Familiarity with data visualization tools (e.g., Tableau, Power BI)\n• Proven excellent problem-solving and critical thinking skills\n• Proven solid communication and presentation skills to effectively convey complex data analysis findings to both technical and non-technical stakeholders\n• Proven ability to work independently and collaboratively in a fast-paced, deadline-driven environment\n• Proven detail-oriented with a focus on accuracy and data integrity\n\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone - of every race, gender, sexuality, age, location and income - deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.\n\n#NTRQ",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgMSIsImNvbXBhbnlfbmFtZSI6IlVuaXRlZEhlYWx0aCBHcm91cCIsImFkZHJlc3NfY2l0eSI6IkluZGlhIiwiaHRpZG9jaWQiOiJhdHFPMEswckdadGwwUy1xQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJRlNXNWthV0UifQ==",
         "3 days ago",
         "Data Analyst"
        ],
        [
         "Data Analyst – Competitive Benchmarking & Reporting",
         "REPUTATION",
         "Hyderabad, Telangana, India",
         "Why Work at Reputation?\n• Reputation reached over $100m in Annual Recurring Revenue (ARR) in 2022 and continues to grow worldwide.\n• We've raised over $200 million in funding from A-list venture capital firms such as Bessemer Venture Partner and Kleiner Perkins, including $150 million in equity financing from Marlin Equity Partners in January 2022.\n• Reputation is trusted by more than 250 partners, including Google, Facebook, Salesforce, J.D. Power, Amazon and Web.com.\n• Our industry leading platform has been recognized by Forrester and Gartner as a vendor of choice in Voice of the Customer, Customer Feedback Management, and Social Suites research reports.\n• The platform is used by 10+ major automotive OEMs and 16,000 auto dealerships, more than 250 healthcare systems, and over 100 leading property management firms.\n• Our executive management team is committed to building a performance-based culture where excellence is rewarded and careers are developed.\n• Who thrives at Reputation? Managers who embody a player-coach mentality. Employees who value teamwork and cross-functional collaboration. People who emphasize perseverance and hustle over quick wins and luck.\n• Our Mission: We exist to forge relationships between companies and communities.\n\nWe are seeking a Data Analyst with expertise in data stewardship and analysis to manage our competitive benchmark data tenants and produce customer-facing reports for marketing and sales. The ideal candidate is proficient in BigQuery (SQL), detail-oriented, and capable of working independently on complex data projects.\n\nResponsibilities:\n• Data Stewardship & Governance: Ensure data accuracy, integrity, and accessibility by managing competitive benchmark data tenants, implementing verification processes, and maintaining governance best practices.\n• Data Pipeline Management: Develop and maintain data ingestion pipelines, including API integrations and web scraping, ensuring timely updates and scalability.\n• Data Validation & Quality Control: Oversee large-scale data input and validation, balancing hands-on data manipulation with automation strategies. Collaborate with data owners to resolve quality issues.\n• Advanced Analytics & Reporting: Conduct in-depth data analysis to uncover trends, patterns, and insights that drive business decisions. Scope, design, and execute analysis projects with minimal supervision.\n• Industry & Market Insights: Develop data-driven industry reports for marketing and sales, leveraging analytics to provide insights on market trends and competitive landscapes.\n• Cross-Functional Collaboration: Work closely with product managers, engineers, marketing, and leadership to resolve data issues, provide training, and ensure alignment on data best practices.\n• Tool & Process Optimization: Design scalable internal tools and reporting solutions to enhance data accessibility, automation, and usability across teams.\n• Stakeholder Communication: Translate complex data insights into actionable recommendations for key stakeholders, including non-technical audiences.\n\nQualifications:\n• 5+ years of proven experience in data analysis, data stewardship, and managing large-scale datasets, with a strong focus on ensuring data accuracy, integrity, and accessibility.\n• Must be proficient with SQL and NoSQL (BigQuery), as well as have the ability to thrive in an environment that often requires patience and creativity to query, extract and validate data in which there are high levels of uncertainty.\n• Strong prior professional experience managing databases and using applicable tools is required.\n• Experience with and knowledge of ETL processes and data migration.\n• Understanding of and prior experience with General Data Protection Regulation.\n• Demonstrated experience and proficiency in using Python in real-world, business applications are substantial advantages to candidates.\n• Highly articulate with outstanding verbal and written communication skills; you are able to explain data-driven insights and analyses intuitively to clients, executives, and technical and non-technical coworkers in both large and small groups.\n• You should be comfortable taking feedback from everyone – Product Managers, Engineers, customers, the CEO, etc. – to help understand business objectives and develop the most powerful analyses and tools possible.\n• Proven ability to operate in a fast-paced, data-driven environment.\n\nWhen you join Reputation, you can expect:\n• Flexible working arrangements.\n• Career growth with paid training tuition opportunities.\n• Active Employee Resource Groups (ERGs) to engage with.\n• An equitable work environment.\n\nOur employees say it best:\n\nAccording to Glassdoor, 94% of our employees approve of our CEO, Joe Burton.\n\nOur employees highlight our:\n• Ample Opportunities- “There are many opportunities to learn and grow. Many open roles are replaced with internal promotions.”\n• Positive Culture- \"Great opportunity and exceptional culture.\" “You will never have a better culture anywhere else. Period.”\n• Training and Tools- “All managers truly want you to succeed, and you are given great tools and training to be successful in your role.”\n• Balance- “Great work life balance and awesome team environment!”\n\nDiversity Programs & Initiatives:\n\nOur Reputation Nation spans around the world. This global perspective allows us to intentionally unlock the magic that comes from diversity of experience to contribute to our success.\n\nAt Reputation, we believe in:\n• Diversity: Embracing a culture that values uniqueness.\n• Inclusion: Inviting diverse groups to take part in company life.\n• Belonging: Helping each individual feel accepted for who they are.\n\n\"At Reputation, we see diversity and inclusion as the foundation for an equitable workplace. Our goal is to empower all of our employees, regardless of their background, to make an impact in their work each and every day.\" - Joe Burton, CEO, Reputation\n\nAdditionally, we offer a variety of benefits and perks, such as:\n• Health Insurance & Wellness Benefits: Group Health Insurance: Medical Insurance with floater policy of up to 10,00,000 for employee + spouse + 2 dependent children + 2 parents / parent-in-laws\n• Maternity Benefits: Medical insurance up to 75,000 INR, 26 weeks of leave for birth, adoption or surrogacy\n• Life Insurance: Insurance at 3x annual cost to the company (Term Insurance, GPA)\n• Accident/Disability Insurance: Insured at 3x base salary for permanent total disability, permanent partial disability and temporary total disability (GPA)\n• OPD: of 7500 per annum per employee\n\nLeaves\n• 10 Company observed holidays a year (Refer to the Holiday Calendar for the Year)\n• 12 Casual/Sick leaves (Pro-rata calculated)\n• 02 Earned Leaves per Month (Pro-rata calculated)\n• 04 Employee Recharge days (aka company holiday/office closed)\n• Maternity & Paternity (6 months)\n• Bereavement Leave (10 Days)\n\nCar Lease:\nReputation offers Car Lease Program that allows employees to lease a car with no upfront cost or down payment. They benefit from a fixed monthly lease rental and 20-30% tax savings.\n\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\nTo learn more about how we handle the personal data of applicants, visit our Candidate Privacy Notice.\n\nApplicants only - No 3rd party agency candidates.",
         "eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3Qg4oCTIENvbXBldGl0aXZlIEJlbmNobWFya2luZyBcdTAwMjYgUmVwb3J0aW5nIiwiY29tcGFueV9uYW1lIjoiUmVwdXRhdGlvbiIsImFkZHJlc3NfY2l0eSI6Ikh5ZGVyYWJhZCwgVGVsYW5nYW5hLCBJbmRpYSIsImh0aWRvY2lkIjoiT0NPR3UtU1dxWjh4X0gzS0FBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUZTVzVrYVdFIn0=",
         "6 days ago",
         "Data Analyst"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "job_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "posted_at",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "search_role",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "silver_df=silver_df.dropna(subset=[\"posted_at\"])\n",
    "\n",
    "#display(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86e8df9a-cc0d-4e9a-a7d3-77d34eaa2c6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving the Data in Silver Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bace9c19-b7ce-4bc5-b66f-0d40ffaf45af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"lakehouse.silver.jobs\")\n",
    "print(\"✅ Data Written to Silver Layer!\")\n",
    "#display(silver_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}