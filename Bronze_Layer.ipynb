{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2798b163-a992-4611-bea3-fc44c5bb1a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initializing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927d4115-c3c9-4b71-abce-5942e657f669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Bronze_JobETL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f16b1ba-061b-42c2-b274-780496e32ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Defining required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d357742-e8f5-4307-8093-e8b49e9f073b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json, requests\n",
    "\n",
    "API_KEY = \"7f3373f399923bd80f8619eb7f51866eed4af252a228066f0dcfe50d5e5c220a\"\n",
    "\n",
    "roles = [\"Data Engineer\", \"Python Developer\", \"ETL Developer\", \"Spark Engineer\", \"Data Analyst\"]\n",
    "location = \"India\"\n",
    "all_jobs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f65c8c3-e969-4ee5-aa41-4eacad38d3ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Running a Loop over all the Job Roles and fetching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "663dd93f-eed2-4688-a691-c53c282cfb2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for role in roles:\n",
    "    params = {\n",
    "        \"engine\": \"google_jobs\",\n",
    "        \"q\": role,\n",
    "        \"location\": location,\n",
    "        \"api_key\": API_KEY\n",
    "    }\n",
    "    res = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
    "    print(\"✅Reading LIVE Data from Google Jobs API\")\n",
    "    jobs = res.json().get(\"jobs_results\", [])\n",
    "                        \n",
    "    for job in jobs:\n",
    "        job[\"search_role\"] = role\n",
    "    all_jobs.extend(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "205614ba-e501-4164-838d-ca9979aa3fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating a DataFrame from list of dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee7a24d-fa96-4407-8d63-6cd3393a83bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# bronze_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(job) for job in all_jobs]))\n",
    "# Directly accessing the underlying Spark driver JVM using the attribute 'sparkContext' is not supported on serverless compute.\n",
    "bronze_df = spark.createDataFrame(all_jobs)\n",
    "\n",
    "#display(bronze_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1689a217-c4e4-4422-b023-4a05afb27b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving the Data in Bronze Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df89a70-65ba-44a2-b094-99e6029548fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"lakehouse.bronze.jobs_raw\")\n",
    "print(\"✅Data written to Bronze Layer\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}